{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c848b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== vocab begin ==========\n",
      "[!] Begin to generate the vocab\n",
      "[!] ./processed/dailydialog: already exists\n",
      "100%|████████████████████████████████████| 76052/76052 [01:37<00:00, 783.84it/s]\n",
      "[!] whole vocab size: 17292\n",
      "[!] Save the vocab into ./processed/dailydialog/iptvocab.pkl, vocab_size: 17296\n",
      "100%|███████████████████████████████████| 76052/76052 [00:21<00:00, 3564.69it/s]\n",
      "[!] whole vocab size: 17317\n",
      "[!] Save the vocab into ./processed/dailydialog/optvocab.pkl, vocab_size: 17321\n",
      "100%|███████████████████████████████████| 76052/76052 [00:19<00:00, 3828.61it/s]\n",
      "100%|████████████████████████████████████| 76052/76052 [01:30<00:00, 842.57it/s]\n",
      "[!] whole vocab size: 18087\n",
      "[!] Save the vocab into ./processed/dailydialog/vocab.pkl, vocab_size: 18091\n",
      "========== vocab done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh vocab dailydialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30aa670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75f13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 21 20:26:40 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           On   | 00000000:D8:00.0 Off |                  N/A |\r\n",
      "| 58%   77C    P2   203W / 280W |  13821MiB / 24190MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      4549      C   python                                      6731MiB |\r\n",
      "|    0      5505      C   python                                      7079MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2f3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== stat begin ==========\n",
      "[!] analyze the graph coverage information\n",
      "[!] train information:\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-train\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.4574/262/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 77.7447/860/2\n",
      "[!] turn length(max/min/avg): 34/1/5.0943\n",
      "[!] the metadata of dailydialog-tgt-train\n",
      "[!] length of the responses(avg, max, min): 14.8881/279/2\n",
      "[!] total words: 23868\n",
      "==========================================\n",
      "[!] test information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-test\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.5436/119/3\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.1785/446/3\n",
      "[!] turn length(max/min/avg): 25/1/4.9653\n",
      "[!] the metadata of dailydialog-tgt-test\n",
      "[!] length of the responses(avg, max, min): 15.0675/205/3\n",
      "[!] total words: 7305\n",
      "==========================================\n",
      "[!] dev information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-dev\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.2159/109/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.6121/674/3\n",
      "[!] turn length(max/min/avg): 30/1/5.1007\n",
      "[!] the metadata of dailydialog-tgt-dev\n",
      "[!] length of the responses(avg, max, min): 14.6976/167/2\n",
      "[!] total words: 7137\n",
      "==========================================\n",
      "========== stat done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh stat dailydialog 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce5147c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== train begin ==========\n",
      "[!] ./processed/dailydialog/HRED: already exists\n",
      "[!] ./processed/dailydialog/HRED/trainlog.txt doesn't exist\n",
      "[!] back up finished\n",
      "[!] Begin to train the model\n",
      "/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] Parameters:\n",
      "Namespace(batch_size=128, bleu='nltk', context_hidden=512, context_threshold=2, contextrnn=True, d_model=512, dataset='dailydialog', debug=False, decoder_hidden=512, dev_graph='./processed/dailydialog/dev-graph.pkl', dim_feedforward=2048, dropout=0.3, dynamic_tfr=15, dynamic_tfr_counter=10, dynamic_tfr_threshold=1.0, dynamic_tfr_weight=0.0, embed_size=256, epochs=50, gat_heads=8, grad_clip=3.0, graph=0, hierarchical=1, kl_annealing_iter=20000, lr=0.0001, lr_gamma=0.5, lr_mini=1e-06, max_threshold=100, maxlen=50, min_threshold=0, model='HRED', nhead=4, num_decoder_layers=8, num_encoder_layers=8, patience=5, position_embed_size=30, pred='./processed/dailydialog/HRED/pure-pred.txt', resume_training_epoch=0, seed=30, src_dev='./data/dailydialog/src-dev.txt', src_test='./data/dailydialog/src-test.txt', src_train='./data/dailydialog/src-train.txt', src_vocab='./processed/dailydialog/iptvocab.pkl', teach_force=1.0, test_graph='./processed/dailydialog/test-graph.pkl', tgt_dev='./data/dailydialog/tgt-dev.txt', tgt_maxlen=30, tgt_test='./data/dailydialog/tgt-test.txt', tgt_train='./data/dailydialog/tgt-train.txt', tgt_vocab='./processed/dailydialog/optvocab.pkl', train_graph='./processed/dailydialog/train-graph.pkl', transformer_decode=0, utter_hidden=512, utter_n_layer=2, warmup_step=4000, z_hidden=100)\n",
      "[!] load vocab over, src/tgt vocab size: 17296, 17321\n",
      "[!] Net:\n",
      "HRED(\n",
      "  (utter_encoder): Utterance_encoder(\n",
      "    (embed): Embedding(17296, 256)\n",
      "    (gru): GRU(256, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (context_encoder): Context_encoder(\n",
      "    (gru): GRU(512, 512, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(17321, 256)\n",
      "    (gru): GRU(768, 512, num_layers=2, dropout=0.3)\n",
      "    (out): Linear(in_features=512, out_features=17321, bias=True)\n",
      "    (attn): Attention(\n",
      "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[!] Parameters size: 32060073\n",
      "[!] Optimizer Adam\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 9.7822: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 9.7822: : 1it [00:01,  1.88s/it]\u001b[A\n",
      "batch 2, training loss: 9.7511: : 1it [00:02,  1.88s/it]\u001b[A\n",
      "batch 2, training loss: 9.7511: : 2it [00:02,  1.20s/it]\u001b[A\n",
      "batch 3, training loss: 9.7301: : 2it [00:03,  1.20s/it]\u001b[A\n",
      "batch 3, training loss: 9.7301: : 3it [00:03,  1.12it/s]\u001b[A\n",
      "batch 4, training loss: 9.6995: : 3it [00:03,  1.12it/s]\u001b[A\n",
      "batch 4, training loss: 9.6995: : 4it [00:03,  1.21it/s]\u001b[A\n",
      "batch 5, training loss: 9.6595: : 4it [00:04,  1.21it/s]\u001b[A\n",
      "batch 5, training loss: 9.6595: : 5it [00:04,  1.38it/s]\u001b[A\n",
      "batch 6, training loss: 9.6394: : 5it [00:05,  1.38it/s]\u001b[A\n",
      "batch 6, training loss: 9.6394: : 6it [00:05,  1.39it/s]\u001b[A\n",
      "batch 7, training loss: 9.6015: : 6it [00:05,  1.39it/s]\u001b[A\n",
      "batch 7, training loss: 9.6015: : 7it [00:05,  1.52it/s]\u001b[A\n",
      "batch 8, training loss: 9.5737: : 7it [00:06,  1.52it/s]\u001b[A\n",
      "batch 8, training loss: 9.5737: : 8it [00:06,  1.48it/s]\u001b[A\n",
      "batch 9, training loss: 9.5009: : 8it [00:06,  1.48it/s]\u001b[A\n",
      "batch 9, training loss: 9.5009: : 9it [00:06,  1.58it/s]\u001b[A\n",
      "batch 10, training loss: 9.428: : 9it [00:07,  1.58it/s]\u001b[A\n",
      "batch 10, training loss: 9.428: : 10it [00:07,  1.51it/s]\u001b[A\n",
      "batch 11, training loss: 9.3668: : 10it [00:08,  1.51it/s]\u001b[A\n",
      "batch 11, training loss: 9.3668: : 11it [00:08,  1.60it/s]\u001b[A\n",
      "batch 12, training loss: 9.3047: : 11it [00:08,  1.60it/s]\u001b[A\n",
      "batch 12, training loss: 9.3047: : 12it [00:08,  1.53it/s]\u001b[A\n",
      "batch 13, training loss: 9.2078: : 12it [00:09,  1.53it/s]\u001b[A\n",
      "batch 13, training loss: 9.2078: : 13it [00:09,  1.62it/s]\u001b[A\n",
      "batch 14, training loss: 9.1013: : 13it [00:10,  1.62it/s]\u001b[A\n",
      "batch 14, training loss: 9.1013: : 14it [00:10,  1.55it/s]\u001b[A\n",
      "batch 15, training loss: 8.9812: : 14it [00:10,  1.55it/s]\u001b[A\n",
      "batch 15, training loss: 8.9812: : 15it [00:10,  1.63it/s]\u001b[A\n",
      "batch 16, training loss: 8.8651: : 15it [00:11,  1.63it/s]\u001b[A\n",
      "batch 16, training loss: 8.8651: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 8.7801: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 8.7801: : 17it [00:11,  1.63it/s]\u001b[A\n",
      "batch 18, training loss: 8.6076: : 17it [00:12,  1.63it/s]\u001b[A\n",
      "batch 18, training loss: 8.6076: : 18it [00:12,  1.56it/s]\u001b[A\n",
      "batch 19, training loss: 8.4322: : 18it [00:13,  1.56it/s]\u001b[A\n",
      "batch 19, training loss: 8.4322: : 19it [00:13,  1.63it/s]\u001b[A\n",
      "batch 20, training loss: 8.3041: : 19it [00:13,  1.63it/s]\u001b[A\n",
      "batch 20, training loss: 8.3041: : 20it [00:13,  1.55it/s]\u001b[A\n",
      "batch 21, training loss: 8.1891: : 20it [00:14,  1.55it/s]\u001b[A\n",
      "batch 21, training loss: 8.1891: : 21it [00:14,  1.63it/s]\u001b[A\n",
      "batch 22, training loss: 7.9605: : 21it [00:15,  1.63it/s]\u001b[A\n",
      "batch 22, training loss: 7.9605: : 22it [00:15,  1.55it/s]\u001b[A\n",
      "batch 23, training loss: 7.9074: : 22it [00:15,  1.55it/s]\u001b[A\n",
      "batch 23, training loss: 7.9074: : 23it [00:15,  1.63it/s]\u001b[A\n",
      "batch 24, training loss: 7.7508: : 23it [00:16,  1.63it/s]\u001b[A\n",
      "batch 24, training loss: 7.7508: : 24it [00:16,  1.55it/s]\u001b[A\n",
      "batch 25, training loss: 7.7102: : 24it [00:16,  1.55it/s]\u001b[A\n",
      "batch 25, training loss: 7.7102: : 25it [00:16,  1.62it/s]\u001b[A\n",
      "batch 26, training loss: 7.5063: : 25it [00:17,  1.62it/s]\u001b[A\n",
      "batch 26, training loss: 7.5063: : 26it [00:17,  1.56it/s]\u001b[A\n",
      "batch 27, training loss: 7.4585: : 26it [00:18,  1.56it/s]\u001b[A\n",
      "batch 27, training loss: 7.4585: : 27it [00:18,  1.64it/s]\u001b[A\n",
      "batch 28, training loss: 7.3007: : 27it [00:18,  1.64it/s]\u001b[A\n",
      "batch 28, training loss: 7.3007: : 28it [00:18,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 7.2598: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 7.2598: : 29it [00:19,  1.64it/s]\u001b[A\n",
      "batch 30, training loss: 7.1829: : 29it [00:20,  1.64it/s]\u001b[A\n",
      "batch 30, training loss: 7.1829: : 30it [00:20,  1.57it/s]\u001b[A\n",
      "batch 31, training loss: 7.005: : 30it [00:20,  1.57it/s] \u001b[A\n",
      "batch 31, training loss: 7.005: : 31it [00:20,  1.64it/s]\u001b[A\n",
      "batch 32, training loss: 6.9778: : 31it [00:21,  1.64it/s]\u001b[A\n",
      "batch 32, training loss: 6.9778: : 32it [00:21,  1.56it/s]\u001b[A\n",
      "batch 33, training loss: 6.8112: : 32it [00:21,  1.56it/s]\u001b[A\n",
      "batch 33, training loss: 6.8112: : 33it [00:21,  1.64it/s]\u001b[A\n",
      "batch 34, training loss: 6.7643: : 33it [00:22,  1.64it/s]\u001b[A\n",
      "batch 34, training loss: 6.7643: : 34it [00:22,  1.56it/s]\u001b[A\n",
      "batch 35, training loss: 6.7343: : 34it [00:23,  1.56it/s]\u001b[A\n",
      "batch 35, training loss: 6.7343: : 35it [00:23,  1.63it/s]\u001b[A\n",
      "batch 36, training loss: 6.646: : 35it [00:23,  1.63it/s] \u001b[A\n",
      "batch 36, training loss: 6.646: : 36it [00:23,  1.56it/s]\u001b[A\n",
      "batch 37, training loss: 6.4898: : 36it [00:24,  1.56it/s]\u001b[A\n",
      "batch 37, training loss: 6.4898: : 37it [00:24,  1.63it/s]\u001b[A\n",
      "batch 38, training loss: 6.4003: : 37it [00:25,  1.63it/s]\u001b[A\n",
      "batch 38, training loss: 6.4003: : 38it [00:25,  1.56it/s]\u001b[A\n",
      "batch 39, training loss: 6.3993: : 38it [00:25,  1.56it/s]\u001b[A\n",
      "batch 39, training loss: 6.3993: : 39it [00:25,  1.64it/s]\u001b[A\n",
      "batch 40, training loss: 6.3975: : 39it [00:26,  1.64it/s]\u001b[A\n",
      "batch 40, training loss: 6.3975: : 40it [00:26,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 6.431: : 40it [00:26,  1.57it/s] \u001b[A\n",
      "batch 41, training loss: 6.431: : 41it [00:26,  1.64it/s]\u001b[A\n",
      "batch 42, training loss: 6.2955: : 41it [00:27,  1.64it/s]\u001b[A\n",
      "batch 42, training loss: 6.2955: : 42it [00:27,  1.57it/s]\u001b[A\n",
      "batch 43, training loss: 6.0956: : 42it [00:28,  1.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 6.0956: : 43it [00:28,  1.64it/s]\u001b[A\n",
      "batch 44, training loss: 6.0621: : 43it [00:28,  1.64it/s]\u001b[A\n",
      "batch 44, training loss: 6.0621: : 44it [00:28,  1.57it/s]\u001b[A\n",
      "batch 45, training loss: 6.0054: : 44it [00:29,  1.57it/s]\u001b[A\n",
      "batch 45, training loss: 6.0054: : 45it [00:29,  1.64it/s]\u001b[A\n",
      "batch 46, training loss: 6.0505: : 45it [00:30,  1.64it/s]\u001b[A\n",
      "batch 46, training loss: 6.0505: : 46it [00:30,  1.56it/s]\u001b[A\n",
      "batch 47, training loss: 5.9844: : 46it [00:30,  1.56it/s]\u001b[A\n",
      "batch 47, training loss: 5.9844: : 47it [00:30,  1.64it/s]\u001b[A\n",
      "batch 48, training loss: 5.9014: : 47it [00:31,  1.64it/s]\u001b[A\n",
      "batch 48, training loss: 5.9014: : 48it [00:31,  1.56it/s]\u001b[A\n",
      "batch 49, training loss: 5.9631: : 48it [00:31,  1.56it/s]\u001b[A\n",
      "batch 49, training loss: 5.9631: : 49it [00:31,  1.64it/s]\u001b[A\n",
      "batch 50, training loss: 5.8359: : 49it [00:32,  1.64it/s]\u001b[A\n",
      "batch 50, training loss: 5.8359: : 50it [00:32,  1.56it/s]\u001b[A\n",
      "batch 51, training loss: 5.8275: : 50it [00:33,  1.56it/s]\u001b[A\n",
      "batch 51, training loss: 5.8275: : 51it [00:33,  1.64it/s]\u001b[A\n",
      "batch 52, training loss: 5.7204: : 51it [00:33,  1.64it/s]\u001b[A\n",
      "batch 52, training loss: 5.7204: : 52it [00:33,  1.57it/s]\u001b[A\n",
      "batch 53, training loss: 5.7111: : 52it [00:34,  1.57it/s]\u001b[A\n",
      "batch 53, training loss: 5.7111: : 53it [00:34,  1.66it/s]\u001b[A\n",
      "batch 54, training loss: 5.5756: : 53it [00:35,  1.66it/s]\u001b[A\n",
      "batch 54, training loss: 5.5756: : 54it [00:35,  1.58it/s]\u001b[A\n",
      "batch 55, training loss: 5.6891: : 54it [00:35,  1.58it/s]\u001b[A\n",
      "batch 55, training loss: 5.6891: : 55it [00:35,  1.64it/s]\u001b[A\n",
      "batch 56, training loss: 5.6436: : 55it [00:36,  1.64it/s]\u001b[A\n",
      "batch 56, training loss: 5.6436: : 56it [00:36,  1.57it/s]\u001b[A\n",
      "batch 57, training loss: 5.6034: : 56it [00:36,  1.57it/s]\u001b[A\n",
      "batch 57, training loss: 5.6034: : 57it [00:36,  1.64it/s]\u001b[A\n",
      "batch 58, training loss: 5.6175: : 57it [00:37,  1.64it/s]\u001b[A\n",
      "batch 58, training loss: 5.6175: : 58it [00:37,  1.56it/s]\u001b[A\n",
      "batch 59, training loss: 5.5025: : 58it [00:38,  1.56it/s]\u001b[A\n",
      "batch 59, training loss: 5.5025: : 59it [00:38,  1.64it/s]\u001b[A\n",
      "batch 60, training loss: 5.5222: : 59it [00:38,  1.64it/s]\u001b[A\n",
      "batch 60, training loss: 5.5222: : 60it [00:38,  1.56it/s]\u001b[A\n",
      "batch 61, training loss: 5.5825: : 60it [00:39,  1.56it/s]\u001b[A\n",
      "batch 61, training loss: 5.5825: : 61it [00:39,  1.64it/s]\u001b[A\n",
      "batch 62, training loss: 5.4575: : 61it [00:40,  1.64it/s]\u001b[A\n",
      "batch 62, training loss: 5.4575: : 62it [00:40,  1.56it/s]\u001b[A\n",
      "batch 63, training loss: 5.527: : 62it [00:40,  1.56it/s] \u001b[A\n",
      "batch 63, training loss: 5.527: : 63it [00:40,  1.63it/s]\u001b[A\n",
      "batch 64, training loss: 5.5057: : 63it [00:41,  1.63it/s]\u001b[A\n",
      "batch 64, training loss: 5.5057: : 64it [00:41,  1.57it/s]\u001b[A\n",
      "batch 65, training loss: 5.4755: : 64it [00:41,  1.57it/s]\u001b[A\n",
      "batch 65, training loss: 5.4755: : 65it [00:41,  1.65it/s]\u001b[A\n",
      "batch 66, training loss: 5.5113: : 65it [00:42,  1.65it/s]\u001b[A\n",
      "batch 66, training loss: 5.5113: : 66it [00:42,  1.57it/s]\u001b[A\n",
      "batch 67, training loss: 5.4211: : 66it [00:43,  1.57it/s]\u001b[A\n",
      "batch 67, training loss: 5.4211: : 67it [00:43,  1.63it/s]\u001b[A\n",
      "batch 68, training loss: 5.4538: : 67it [00:43,  1.63it/s]\u001b[A\n",
      "batch 68, training loss: 5.4538: : 68it [00:43,  1.55it/s]\u001b[A\n",
      "batch 69, training loss: 5.3924: : 68it [00:44,  1.55it/s]\u001b[A\n",
      "batch 69, training loss: 5.3924: : 69it [00:44,  1.64it/s]\u001b[A\n",
      "batch 70, training loss: 5.472: : 69it [00:45,  1.64it/s] \u001b[A\n",
      "batch 70, training loss: 5.472: : 70it [00:45,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 5.3825: : 70it [00:45,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 5.3825: : 71it [00:45,  1.65it/s]\u001b[A\n",
      "batch 72, training loss: 5.3699: : 71it [00:46,  1.65it/s]\u001b[A\n",
      "batch 72, training loss: 5.3699: : 72it [00:46,  1.59it/s]\u001b[A\n",
      "batch 73, training loss: 5.4315: : 72it [00:46,  1.59it/s]\u001b[A\n",
      "batch 73, training loss: 5.4315: : 73it [00:46,  1.64it/s]\u001b[A\n",
      "batch 74, training loss: 5.3295: : 73it [00:47,  1.64it/s]\u001b[A\n",
      "batch 74, training loss: 5.3295: : 74it [00:47,  1.59it/s]\u001b[A\n",
      "batch 75, training loss: 5.4265: : 74it [00:48,  1.59it/s]\u001b[A\n",
      "batch 75, training loss: 5.4265: : 75it [00:48,  1.63it/s]\u001b[A\n",
      "batch 76, training loss: 5.2835: : 75it [00:48,  1.63it/s]\u001b[A\n",
      "batch 76, training loss: 5.2835: : 76it [00:48,  1.57it/s]\u001b[A\n",
      "batch 77, training loss: 5.4069: : 76it [00:49,  1.57it/s]\u001b[A\n",
      "batch 77, training loss: 5.4069: : 77it [00:49,  1.64it/s]\u001b[A\n",
      "batch 78, training loss: 5.3972: : 77it [00:50,  1.64it/s]\u001b[A\n",
      "batch 78, training loss: 5.3972: : 78it [00:50,  1.55it/s]\u001b[A\n",
      "batch 79, training loss: 5.3793: : 78it [00:50,  1.55it/s]\u001b[A\n",
      "batch 79, training loss: 5.3793: : 79it [00:50,  1.63it/s]\u001b[A\n",
      "batch 80, training loss: 5.341: : 79it [00:51,  1.63it/s] \u001b[A\n",
      "batch 80, training loss: 5.341: : 80it [00:51,  1.56it/s]\u001b[A\n",
      "batch 81, training loss: 5.3394: : 80it [00:51,  1.56it/s]\u001b[A\n",
      "batch 81, training loss: 5.3394: : 81it [00:51,  1.62it/s]\u001b[A\n",
      "batch 82, training loss: 5.4016: : 81it [00:52,  1.62it/s]\u001b[A\n",
      "batch 82, training loss: 5.4016: : 82it [00:52,  1.56it/s]\u001b[A\n",
      "batch 83, training loss: 5.3138: : 82it [00:53,  1.56it/s]\u001b[A\n",
      "batch 83, training loss: 5.3138: : 83it [00:53,  1.63it/s]\u001b[A\n",
      "batch 84, training loss: 5.4021: : 83it [00:53,  1.63it/s]\u001b[A\n",
      "batch 84, training loss: 5.4021: : 84it [00:53,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 5.4492: : 84it [00:54,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 5.4492: : 85it [00:54,  1.63it/s]\u001b[A\n",
      "batch 86, training loss: 5.3362: : 85it [00:55,  1.63it/s]\u001b[A\n",
      "batch 86, training loss: 5.3362: : 86it [00:55,  1.56it/s]\u001b[A\n",
      "batch 87, training loss: 5.4105: : 86it [00:55,  1.56it/s]\u001b[A\n",
      "batch 87, training loss: 5.4105: : 87it [00:55,  1.66it/s]\u001b[A\n",
      "batch 88, training loss: 6.292: : 87it [00:56,  1.66it/s] \u001b[A\n",
      "batch 88, training loss: 6.292: : 88it [00:56,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 6.295: : 88it [00:57,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 6.295: : 89it [00:57,  1.57it/s]\u001b[A\n",
      "batch 90, training loss: 6.2084: : 89it [00:57,  1.57it/s]\u001b[A\n",
      "batch 90, training loss: 6.2084: : 90it [00:57,  1.50it/s]\u001b[A\n",
      "batch 91, training loss: 6.2882: : 90it [00:58,  1.50it/s]\u001b[A\n",
      "batch 91, training loss: 6.2882: : 91it [00:58,  1.56it/s]\u001b[A\n",
      "batch 92, training loss: 6.2713: : 91it [00:59,  1.56it/s]\u001b[A\n",
      "batch 92, training loss: 6.2713: : 92it [00:59,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 6.2126: : 92it [00:59,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 6.2126: : 93it [00:59,  1.56it/s]\u001b[A\n",
      "batch 94, training loss: 6.259: : 93it [01:00,  1.56it/s] \u001b[A\n",
      "batch 94, training loss: 6.259: : 94it [01:00,  1.49it/s]\u001b[A\n",
      "batch 95, training loss: 6.2654: : 94it [01:00,  1.49it/s]\u001b[A\n",
      "batch 95, training loss: 6.2654: : 95it [01:00,  1.56it/s]\u001b[A\n",
      "batch 96, training loss: 6.1975: : 95it [01:01,  1.56it/s]\u001b[A\n",
      "batch 96, training loss: 6.1975: : 96it [01:01,  1.48it/s]\u001b[A\n",
      "batch 97, training loss: 6.1957: : 96it [01:02,  1.48it/s]\u001b[A\n",
      "batch 97, training loss: 6.1957: : 97it [01:02,  1.53it/s]\u001b[A\n",
      "batch 98, training loss: 6.1342: : 97it [01:03,  1.53it/s]\u001b[A\n",
      "batch 98, training loss: 6.1342: : 98it [01:03,  1.48it/s]\u001b[A\n",
      "batch 99, training loss: 6.023: : 98it [01:03,  1.48it/s] \u001b[A\n",
      "batch 99, training loss: 6.023: : 99it [01:03,  1.54it/s]\u001b[A\n",
      "batch 100, training loss: 5.8752: : 99it [01:04,  1.54it/s]\u001b[A\n",
      "batch 100, training loss: 5.8752: : 100it [01:04,  1.48it/s]\u001b[A\n",
      "batch 101, training loss: 5.991: : 100it [01:04,  1.48it/s] \u001b[A\n",
      "batch 101, training loss: 5.991: : 101it [01:04,  1.55it/s]\u001b[A^C\n",
      "batch 101, training loss: 5.991: : 101it [01:05,  1.55it/s]\n",
      "  0%|                                                    | 0/50 [01:05<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 797, in <module>\n",
      "    main(**args_dict)\n",
      "  File \"train.py\", line 633, in main\n",
      "    kl_annealing_iter=kwargs['kl_annealing_iter'])\n",
      "  File \"train.py\", line 76, in train\n",
      "    output = net(sbatch, tbatch, turn_lengths)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/c/chengyu/VisDial/MultiTurnDialogZoo/model/HRED.py\", line 221, in forward\n",
      "    output, hidden = self.decoder(output, hidden, context_output)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/c/chengyu/VisDial/MultiTurnDialogZoo/model/HRED.py\", line 164, in forward\n",
      "    output, hidden = self.gru(rnn_input, last_hidden)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/rnn.py\", line 716, in forward\n",
      "    self.dropout, self.training, self.bidirectional, self.batch_first)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!./run.sh train dailydialog HRED 0 50 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup ./run.sh train dailydialog HRED 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6ceb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== eval begin ==========\n",
      "/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5039.74it/s]\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Model HRED Result\n",
      "BLEU-1: 0.1864\n",
      "BLEU-2: 0.0781\n",
      "BLEU-3: 0.04\n",
      "BLEU-4: 0.0223\n",
      "ROUGE: 0.0393\n",
      "Distinct-1: 0.023; Distinct-2: 0.1101\n",
      "Ref distinct-1: 0.0588; Ref distinct-2: 0.3619\n",
      "BERTScore: 0.1266\n",
      "========== eval done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh eval dailydialog HRED 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ec3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
