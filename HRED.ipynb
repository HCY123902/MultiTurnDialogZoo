{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c848b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== vocab begin ==========\n",
      "[!] Begin to generate the vocab\n",
      "[!] ./processed/dailydialog: already exists\n",
      "100%|████████████████████████████████████| 76052/76052 [01:37<00:00, 783.84it/s]\n",
      "[!] whole vocab size: 17292\n",
      "[!] Save the vocab into ./processed/dailydialog/iptvocab.pkl, vocab_size: 17296\n",
      "100%|███████████████████████████████████| 76052/76052 [00:21<00:00, 3564.69it/s]\n",
      "[!] whole vocab size: 17317\n",
      "[!] Save the vocab into ./processed/dailydialog/optvocab.pkl, vocab_size: 17321\n",
      "100%|███████████████████████████████████| 76052/76052 [00:19<00:00, 3828.61it/s]\n",
      "100%|████████████████████████████████████| 76052/76052 [01:30<00:00, 842.57it/s]\n",
      "[!] whole vocab size: 18087\n",
      "[!] Save the vocab into ./processed/dailydialog/vocab.pkl, vocab_size: 18091\n",
      "========== vocab done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh vocab dailydialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30aa670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75f13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 21 20:26:40 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           On   | 00000000:D8:00.0 Off |                  N/A |\r\n",
      "| 58%   77C    P2   203W / 280W |  13821MiB / 24190MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      4549      C   python                                      6731MiB |\r\n",
      "|    0      5505      C   python                                      7079MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2f3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== stat begin ==========\n",
      "[!] analyze the graph coverage information\n",
      "[!] train information:\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-train\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.4574/262/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 77.7447/860/2\n",
      "[!] turn length(max/min/avg): 34/1/5.0943\n",
      "[!] the metadata of dailydialog-tgt-train\n",
      "[!] length of the responses(avg, max, min): 14.8881/279/2\n",
      "[!] total words: 23868\n",
      "==========================================\n",
      "[!] test information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-test\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.5436/119/3\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.1785/446/3\n",
      "[!] turn length(max/min/avg): 25/1/4.9653\n",
      "[!] the metadata of dailydialog-tgt-test\n",
      "[!] length of the responses(avg, max, min): 15.0675/205/3\n",
      "[!] total words: 7305\n",
      "==========================================\n",
      "[!] dev information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-dev\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.2159/109/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.6121/674/3\n",
      "[!] turn length(max/min/avg): 30/1/5.1007\n",
      "[!] the metadata of dailydialog-tgt-dev\n",
      "[!] length of the responses(avg, max, min): 14.6976/167/2\n",
      "[!] total words: 7137\n",
      "==========================================\n",
      "========== stat done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh stat dailydialog 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5147c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== train begin ==========\n",
      "[!] ./processed/dailydialog/HRED: already exists\n",
      "[!] back up finished\n",
      "[!] Begin to train the model\n",
      "/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] Parameters:\n",
      "Namespace(batch_size=128, bleu='nltk', context_hidden=512, context_threshold=2, contextrnn=True, d_model=512, dataset='dailydialog', debug=False, decoder_hidden=512, dev_graph='./processed/dailydialog/dev-graph.pkl', dim_feedforward=2048, dropout=0.3, dynamic_tfr=15, dynamic_tfr_counter=10, dynamic_tfr_threshold=1.0, dynamic_tfr_weight=0.0, embed_size=256, epochs=40, gat_heads=8, grad_clip=3.0, graph=0, hierarchical=1, kl_annealing_iter=20000, lr=0.0001, lr_gamma=0.5, lr_mini=1e-06, max_threshold=100, maxlen=50, min_threshold=0, model='HRED', nhead=4, num_decoder_layers=8, num_encoder_layers=8, patience=5, position_embed_size=30, pred='./processed/dailydialog/HRED/pure-pred.txt', seed=30, src_dev='./data/dailydialog/src-dev.txt', src_test='./data/dailydialog/src-test.txt', src_train='./data/dailydialog/src-train.txt', src_vocab='./processed/dailydialog/iptvocab.pkl', teach_force=1.0, test_graph='./processed/dailydialog/test-graph.pkl', tgt_dev='./data/dailydialog/tgt-dev.txt', tgt_maxlen=30, tgt_test='./data/dailydialog/tgt-test.txt', tgt_train='./data/dailydialog/tgt-train.txt', tgt_vocab='./processed/dailydialog/optvocab.pkl', train_graph='./processed/dailydialog/train-graph.pkl', transformer_decode=0, utter_hidden=512, utter_n_layer=2, warmup_step=4000, z_hidden=100)\n",
      "[!] load vocab over, src/tgt vocab size: 17296, 17321\n",
      "[!] Net:\n",
      "HRED(\n",
      "  (utter_encoder): Utterance_encoder(\n",
      "    (embed): Embedding(17296, 256)\n",
      "    (gru): GRU(256, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (context_encoder): Context_encoder(\n",
      "    (gru): GRU(512, 512, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(17321, 256)\n",
      "    (gru): GRU(768, 512, num_layers=2, dropout=0.3)\n",
      "    (out): Linear(in_features=512, out_features=17321, bias=True)\n",
      "    (attn): Attention(\n",
      "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[!] Parameters size: 32060073\n",
      "[!] Optimizer Adam\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 9.7799: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 9.7799: : 1it [00:01,  1.99s/it]\u001b[A\n",
      "batch 2, training loss: 9.7495: : 1it [00:02,  1.99s/it]\u001b[A\n",
      "batch 2, training loss: 9.7495: : 2it [00:02,  1.23s/it]\u001b[A\n",
      "batch 3, training loss: 9.7242: : 2it [00:03,  1.23s/it]\u001b[A\n",
      "batch 3, training loss: 9.7242: : 3it [00:03,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 9.6979: : 3it [00:04,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 9.6979: : 4it [00:04,  1.11it/s]\u001b[A\n",
      "batch 5, training loss: 9.6641: : 4it [00:04,  1.11it/s]\u001b[A\n",
      "batch 5, training loss: 9.6641: : 5it [00:04,  1.21it/s]\u001b[A\n",
      "batch 6, training loss: 9.641: : 5it [00:05,  1.21it/s] \u001b[A\n",
      "batch 6, training loss: 9.641: : 6it [00:05,  1.28it/s]\u001b[A\n",
      "batch 7, training loss: 9.5992: : 6it [00:06,  1.28it/s]\u001b[A\n",
      "batch 7, training loss: 9.5992: : 7it [00:06,  1.33it/s]\u001b[A\n",
      "batch 8, training loss: 9.5716: : 7it [00:06,  1.33it/s]\u001b[A\n",
      "batch 8, training loss: 9.5716: : 8it [00:06,  1.48it/s]\u001b[A\n",
      "batch 9, training loss: 9.5035: : 8it [00:07,  1.48it/s]\u001b[A\n",
      "batch 9, training loss: 9.5035: : 9it [00:07,  1.56it/s]\u001b[A\n",
      "batch 10, training loss: 9.4243: : 9it [00:07,  1.56it/s]\u001b[A\n",
      "batch 10, training loss: 9.4243: : 10it [00:07,  1.55it/s]\u001b[A\n",
      "batch 11, training loss: 9.3628: : 10it [00:08,  1.55it/s]\u001b[A\n",
      "batch 11, training loss: 9.3628: : 11it [00:08,  1.53it/s]\u001b[A\n",
      "batch 12, training loss: 9.3005: : 11it [00:09,  1.53it/s]\u001b[A\n",
      "batch 12, training loss: 9.3005: : 12it [00:09,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 9.2027: : 12it [00:09,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 9.2027: : 13it [00:09,  1.53it/s]\u001b[A\n",
      "batch 14, training loss: 9.0996: : 13it [00:10,  1.53it/s]\u001b[A\n",
      "batch 14, training loss: 9.0996: : 14it [00:10,  1.55it/s]\u001b[A\n",
      "batch 15, training loss: 8.9763: : 14it [00:11,  1.55it/s]\u001b[A\n",
      "batch 15, training loss: 8.9763: : 15it [00:11,  1.59it/s]\u001b[A\n",
      "batch 16, training loss: 8.8626: : 15it [00:11,  1.59it/s]\u001b[A\n",
      "batch 16, training loss: 8.8626: : 16it [00:11,  1.57it/s]\u001b[A\n",
      "batch 17, training loss: 8.78: : 16it [00:12,  1.57it/s]  \u001b[A\n",
      "batch 17, training loss: 8.78: : 17it [00:12,  1.52it/s]\u001b[A\n",
      "batch 18, training loss: 8.6069: : 17it [00:13,  1.52it/s]\u001b[A\n",
      "batch 18, training loss: 8.6069: : 18it [00:13,  1.50it/s]\u001b[A\n",
      "batch 19, training loss: 8.4282: : 18it [00:13,  1.50it/s]\u001b[A\n",
      "batch 19, training loss: 8.4282: : 19it [00:13,  1.49it/s]\u001b[A\n",
      "batch 20, training loss: 8.3043: : 19it [00:14,  1.49it/s]\u001b[A\n",
      "batch 20, training loss: 8.3043: : 20it [00:14,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 8.1842: : 20it [00:14,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 8.1842: : 21it [00:14,  1.65it/s]\u001b[A\n",
      "batch 22, training loss: 7.961: : 21it [00:15,  1.65it/s] \u001b[A\n",
      "batch 22, training loss: 7.961: : 22it [00:15,  1.61it/s]\u001b[A\n",
      "batch 23, training loss: 7.9088: : 22it [00:16,  1.61it/s]\u001b[A\n",
      "batch 23, training loss: 7.9088: : 23it [00:16,  1.58it/s]\u001b[A\n",
      "batch 24, training loss: 7.7532: : 23it [00:16,  1.58it/s]\u001b[A\n",
      "batch 24, training loss: 7.7532: : 24it [00:16,  1.57it/s]\u001b[A\n",
      "batch 25, training loss: 7.7101: : 24it [00:17,  1.57it/s]\u001b[A\n",
      "batch 25, training loss: 7.7101: : 25it [00:17,  1.58it/s]\u001b[A\n",
      "batch 26, training loss: 7.5065: : 25it [00:18,  1.58it/s]\u001b[A\n",
      "batch 26, training loss: 7.5065: : 26it [00:18,  1.59it/s]\u001b[A\n",
      "batch 27, training loss: 7.4592: : 26it [00:18,  1.59it/s]\u001b[A\n",
      "batch 27, training loss: 7.4592: : 27it [00:18,  1.58it/s]\u001b[A\n",
      "batch 28, training loss: 7.3006: : 27it [00:19,  1.58it/s]\u001b[A\n",
      "batch 28, training loss: 7.3006: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 7.2636: : 28it [00:20,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 7.2636: : 29it [00:20,  1.53it/s]\u001b[A\n",
      "batch 30, training loss: 7.1866: : 29it [00:20,  1.53it/s]\u001b[A\n",
      "batch 30, training loss: 7.1866: : 30it [00:20,  1.50it/s]\u001b[A\n",
      "batch 31, training loss: 7.0071: : 30it [00:21,  1.50it/s]\u001b[A\n",
      "batch 31, training loss: 7.0071: : 31it [00:21,  1.49it/s]\u001b[A\n",
      "batch 32, training loss: 6.9812: : 31it [00:22,  1.49it/s]\u001b[A\n",
      "batch 32, training loss: 6.9812: : 32it [00:22,  1.61it/s]\u001b[A\n",
      "batch 33, training loss: 6.8115: : 32it [00:22,  1.61it/s]\u001b[A\n",
      "batch 33, training loss: 6.8115: : 33it [00:22,  1.65it/s]\u001b[A\n",
      "batch 34, training loss: 6.7637: : 33it [00:23,  1.65it/s]\u001b[A\n",
      "batch 34, training loss: 6.7637: : 34it [00:23,  1.61it/s]\u001b[A\n",
      "batch 35, training loss: 6.7327: : 34it [00:23,  1.61it/s]\u001b[A\n",
      "batch 35, training loss: 6.7327: : 35it [00:23,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 6.6415: : 35it [00:24,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 6.6415: : 36it [00:24,  1.55it/s]\u001b[A\n",
      "batch 37, training loss: 6.4897: : 36it [00:25,  1.55it/s]\u001b[A\n",
      "batch 37, training loss: 6.4897: : 37it [00:25,  1.56it/s]\u001b[A\n",
      "batch 38, training loss: 6.3993: : 37it [00:25,  1.56it/s]\u001b[A\n",
      "batch 38, training loss: 6.3993: : 38it [00:25,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 6.3985: : 38it [00:26,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 6.3985: : 39it [00:26,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 6.3964: : 39it [00:27,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 6.3964: : 40it [00:27,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 6.4268: : 40it [00:27,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 6.4268: : 41it [00:27,  1.54it/s]\u001b[A\n",
      "batch 42, training loss: 6.2978: : 41it [00:28,  1.54it/s]\u001b[A\n",
      "batch 42, training loss: 6.2978: : 42it [00:28,  1.50it/s]\u001b[A\n",
      "batch 43, training loss: 6.0929: : 42it [00:29,  1.50it/s]\u001b[A\n",
      "batch 43, training loss: 6.0929: : 43it [00:29,  1.49it/s]\u001b[A\n",
      "batch 44, training loss: 6.0623: : 43it [00:29,  1.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 44, training loss: 6.0623: : 44it [00:29,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 6.0083: : 44it [00:30,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 6.0083: : 45it [00:30,  1.64it/s]\u001b[A\n",
      "batch 46, training loss: 6.0505: : 45it [00:30,  1.64it/s]\u001b[A\n",
      "batch 46, training loss: 6.0505: : 46it [00:30,  1.62it/s]\u001b[A\n",
      "batch 47, training loss: 5.9843: : 46it [00:31,  1.62it/s]\u001b[A\n",
      "batch 47, training loss: 5.9843: : 47it [00:31,  1.59it/s]\u001b[A\n",
      "batch 48, training loss: 5.901: : 47it [00:32,  1.59it/s] \u001b[A\n",
      "batch 48, training loss: 5.901: : 48it [00:32,  1.56it/s]\u001b[A\n",
      "batch 49, training loss: 5.9637: : 48it [00:32,  1.56it/s]\u001b[A\n",
      "batch 49, training loss: 5.9637: : 49it [00:32,  1.60it/s]\u001b[A\n",
      "batch 50, training loss: 5.8361: : 49it [00:33,  1.60it/s]\u001b[A\n",
      "batch 50, training loss: 5.8361: : 50it [00:33,  1.64it/s]\u001b[A\n",
      "batch 51, training loss: 5.829: : 50it [00:34,  1.64it/s] \u001b[A\n",
      "batch 51, training loss: 5.829: : 51it [00:34,  1.60it/s]\u001b[A\n",
      "batch 52, training loss: 5.72: : 51it [00:34,  1.60it/s] \u001b[A\n",
      "batch 52, training loss: 5.72: : 52it [00:34,  1.56it/s]\u001b[A\n",
      "batch 53, training loss: 5.7115: : 52it [00:35,  1.56it/s]\u001b[A\n",
      "batch 53, training loss: 5.7115: : 53it [00:35,  1.54it/s]\u001b[A\n",
      "batch 54, training loss: 5.575: : 53it [00:36,  1.54it/s] \u001b[A\n",
      "batch 54, training loss: 5.575: : 54it [00:36,  1.51it/s]\u001b[A\n",
      "batch 55, training loss: 5.6894: : 54it [00:36,  1.51it/s]\u001b[A\n",
      "batch 55, training loss: 5.6894: : 55it [00:36,  1.47it/s]\u001b[A\n",
      "batch 56, training loss: 5.6411: : 55it [00:37,  1.47it/s]\u001b[A\n",
      "batch 56, training loss: 5.6411: : 56it [00:37,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 5.6016: : 56it [00:37,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 5.6016: : 57it [00:37,  1.64it/s]\u001b[A\n",
      "batch 58, training loss: 5.619: : 57it [00:38,  1.64it/s] \u001b[A\n",
      "batch 58, training loss: 5.619: : 58it [00:38,  1.61it/s]\u001b[A\n",
      "batch 59, training loss: 5.5021: : 58it [00:39,  1.61it/s]\u001b[A\n",
      "batch 59, training loss: 5.5021: : 59it [00:39,  1.57it/s]\u001b[A\n",
      "batch 60, training loss: 5.5205: : 59it [00:39,  1.57it/s]\u001b[A\n",
      "batch 60, training loss: 5.5205: : 60it [00:39,  1.70it/s]\u001b[A\n",
      "batch 61, training loss: 5.5786: : 60it [00:40,  1.70it/s]\u001b[A\n",
      "batch 61, training loss: 5.5786: : 61it [00:40,  1.69it/s]\u001b[A\n",
      "batch 62, training loss: 5.4612: : 61it [00:40,  1.69it/s]\u001b[A\n",
      "batch 62, training loss: 5.4612: : 62it [00:40,  1.66it/s]\u001b[A\n",
      "batch 63, training loss: 5.5293: : 62it [00:41,  1.66it/s]\u001b[A\n",
      "batch 63, training loss: 5.5293: : 63it [00:41,  1.61it/s]\u001b[A\n",
      "batch 64, training loss: 5.5062: : 63it [00:42,  1.61it/s]\u001b[A\n",
      "batch 64, training loss: 5.5062: : 64it [00:42,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 5.4744: : 64it [00:42,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 5.4744: : 65it [00:42,  1.63it/s]\u001b[A\n",
      "batch 66, training loss: 5.5118: : 65it [00:43,  1.63it/s]\u001b[A\n",
      "batch 66, training loss: 5.5118: : 66it [00:43,  1.71it/s]\u001b[A\n",
      "batch 67, training loss: 5.4218: : 66it [00:43,  1.71it/s]\u001b[A\n",
      "batch 67, training loss: 5.4218: : 67it [00:43,  1.64it/s]\u001b[A\n",
      "batch 68, training loss: 5.4525: : 67it [00:44,  1.64it/s]\u001b[A\n",
      "batch 68, training loss: 5.4525: : 68it [00:44,  1.57it/s]\u001b[A\n",
      "batch 69, training loss: 5.3946: : 68it [00:45,  1.57it/s]\u001b[A\n",
      "batch 69, training loss: 5.3946: : 69it [00:45,  1.52it/s]\u001b[A\n",
      "batch 70, training loss: 5.473: : 69it [00:46,  1.52it/s] \u001b[A\n",
      "batch 70, training loss: 5.473: : 70it [00:46,  1.52it/s]\u001b[A\n",
      "batch 71, training loss: 5.3839: : 70it [00:46,  1.52it/s]\u001b[A\n",
      "batch 71, training loss: 5.3839: : 71it [00:46,  1.57it/s]\u001b[A\n",
      "batch 72, training loss: 5.3709: : 71it [00:47,  1.57it/s]\u001b[A\n",
      "batch 72, training loss: 5.3709: : 72it [00:47,  1.65it/s]\u001b[A\n",
      "batch 73, training loss: 5.4345: : 72it [00:47,  1.65it/s]\u001b[A\n",
      "batch 73, training loss: 5.4345: : 73it [00:47,  1.61it/s]\u001b[A\n",
      "batch 74, training loss: 5.3315: : 73it [00:48,  1.61it/s]\u001b[A\n",
      "batch 74, training loss: 5.3315: : 74it [00:48,  1.55it/s]\u001b[A\n",
      "batch 75, training loss: 5.4279: : 74it [00:49,  1.55it/s]\u001b[A\n",
      "batch 75, training loss: 5.4279: : 75it [00:49,  1.52it/s]\u001b[A\n",
      "batch 76, training loss: 5.2841: : 75it [00:49,  1.52it/s]\u001b[A\n",
      "batch 76, training loss: 5.2841: : 76it [00:49,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 5.4113: : 76it [00:50,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 5.4113: : 77it [00:50,  1.63it/s]\u001b[A\n",
      "batch 78, training loss: 5.4004: : 77it [00:50,  1.63it/s]\u001b[A\n",
      "batch 78, training loss: 5.4004: : 78it [00:50,  1.66it/s]\u001b[A\n",
      "batch 79, training loss: 5.3828: : 78it [00:51,  1.66it/s]\u001b[A\n",
      "batch 79, training loss: 5.3828: : 79it [00:51,  1.63it/s]\u001b[A\n",
      "batch 80, training loss: 5.3445: : 79it [00:52,  1.63it/s]\u001b[A\n",
      "batch 80, training loss: 5.3445: : 80it [00:52,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 5.3437: : 80it [00:52,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 5.3437: : 81it [00:52,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 5.4054: : 81it [00:53,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 5.4054: : 82it [00:53,  1.57it/s]\u001b[A\n",
      "batch 83, training loss: 5.3163: : 82it [00:54,  1.57it/s]\u001b[A\n",
      "batch 83, training loss: 5.3163: : 83it [00:54,  1.57it/s]\u001b[A\n",
      "batch 84, training loss: 5.4065: : 83it [00:54,  1.57it/s]\u001b[A\n",
      "batch 84, training loss: 5.4065: : 84it [00:54,  1.59it/s]\u001b[A\n",
      "batch 85, training loss: 5.453: : 84it [00:55,  1.59it/s] \u001b[A\n",
      "batch 85, training loss: 5.453: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 5.3421: : 85it [00:56,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 5.3421: : 86it [00:56,  1.52it/s]\u001b[A\n",
      "batch 87, training loss: 5.4146: : 86it [00:56,  1.52it/s]\u001b[A\n",
      "batch 87, training loss: 5.4146: : 87it [00:56,  1.52it/s]\u001b[A\n",
      "batch 88, training loss: 6.2958: : 87it [00:57,  1.52it/s]\u001b[A\n",
      "batch 88, training loss: 6.2958: : 88it [00:57,  1.45it/s]\u001b[A\n",
      "batch 89, training loss: 6.2966: : 88it [00:58,  1.45it/s]\u001b[A\n",
      "batch 89, training loss: 6.2966: : 89it [00:58,  1.40it/s]\u001b[A\n",
      "batch 90, training loss: 6.2121: : 89it [00:59,  1.40it/s]\u001b[A\n",
      "batch 90, training loss: 6.2121: : 90it [00:59,  1.38it/s]\u001b[A\n",
      "batch 91, training loss: 6.2898: : 90it [00:59,  1.38it/s]\u001b[A\n",
      "batch 91, training loss: 6.2898: : 91it [00:59,  1.53it/s]\u001b[A\n",
      "batch 92, training loss: 6.2772: : 91it [01:00,  1.53it/s]\u001b[A\n",
      "batch 92, training loss: 6.2772: : 92it [01:00,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 6.2177: : 92it [01:01,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 6.2177: : 93it [01:01,  1.46it/s]\u001b[A\n",
      "batch 94, training loss: 6.266: : 93it [01:01,  1.46it/s] \u001b[A\n",
      "batch 94, training loss: 6.266: : 94it [01:01,  1.40it/s]\u001b[A\n",
      "batch 95, training loss: 6.2695: : 94it [01:02,  1.40it/s]\u001b[A\n",
      "batch 95, training loss: 6.2695: : 95it [01:02,  1.39it/s]\u001b[A\n",
      "batch 96, training loss: 6.2042: : 95it [01:03,  1.39it/s]\u001b[A\n",
      "batch 96, training loss: 6.2042: : 96it [01:03,  1.35it/s]\u001b[A\n",
      "batch 97, training loss: 6.2019: : 96it [01:04,  1.35it/s]\u001b[A\n",
      "batch 97, training loss: 6.2019: : 97it [01:04,  1.32it/s]\u001b[A\n",
      "batch 98, training loss: 6.1396: : 97it [01:04,  1.32it/s]\u001b[A\n",
      "batch 98, training loss: 6.1396: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 6.0281: : 98it [01:05,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 6.0281: : 99it [01:05,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 5.8808: : 99it [01:06,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 5.8808: : 100it [01:06,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 5.9978: : 100it [01:07,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 5.9978: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 5.9031: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 5.9031: : 102it [01:08,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 5.8869: : 102it [01:08,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 5.8869: : 103it [01:08,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 5.8327: : 103it [01:09,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 5.8327: : 104it [01:09,  1.30it/s]\u001b[A\n",
      "batch 105, training loss: 5.8754: : 104it [01:10,  1.30it/s]\u001b[A\n",
      "batch 105, training loss: 5.8754: : 105it [01:10,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 5.933: : 105it [01:11,  1.31it/s] \u001b[A\n",
      "batch 106, training loss: 5.933: : 106it [01:11,  1.32it/s]\u001b[A\n",
      "batch 107, training loss: 5.7713: : 106it [01:11,  1.32it/s]\u001b[A\n",
      "batch 107, training loss: 5.7713: : 107it [01:11,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 5.8474: : 107it [01:12,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 5.8474: : 108it [01:12,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 5.9119: : 108it [01:13,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 5.9119: : 109it [01:13,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 6.025: : 109it [01:14,  1.30it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 110, training loss: 6.025: : 110it [01:14,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 5.7383: : 110it [01:14,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 5.7383: : 111it [01:14,  1.30it/s]\u001b[A\n",
      "batch 112, training loss: 5.8028: : 111it [01:15,  1.30it/s]\u001b[A\n",
      "batch 112, training loss: 5.8028: : 112it [01:15,  1.31it/s]\u001b[A\n",
      "batch 113, training loss: 5.7922: : 112it [01:16,  1.31it/s]\u001b[A\n",
      "batch 113, training loss: 5.7922: : 113it [01:16,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 5.8275: : 113it [01:17,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 5.8275: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 5.7315: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 5.7315: : 115it [01:17,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 5.7269: : 115it [01:18,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 5.7269: : 116it [01:18,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 5.7148: : 116it [01:19,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 5.7148: : 117it [01:19,  1.32it/s]\u001b[A\n",
      "batch 118, training loss: 5.8031: : 117it [01:20,  1.32it/s]\u001b[A\n",
      "batch 118, training loss: 5.8031: : 118it [01:20,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 5.6866: : 118it [01:21,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 5.6866: : 119it [01:21,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 5.6077: : 119it [01:21,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 5.6077: : 120it [01:21,  1.32it/s]\u001b[A\n",
      "batch 121, training loss: 5.7328: : 120it [01:22,  1.32it/s]\u001b[A\n",
      "batch 121, training loss: 5.7328: : 121it [01:22,  1.31it/s]\u001b[A\n",
      "batch 122, training loss: 5.583: : 121it [01:23,  1.31it/s] \u001b[A\n",
      "batch 122, training loss: 5.583: : 122it [01:23,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 5.6669: : 122it [01:24,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 5.6669: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 5.6252: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 5.6252: : 124it [01:24,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 5.6676: : 124it [01:25,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 5.6676: : 125it [01:25,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 5.696: : 125it [01:26,  1.29it/s] \u001b[A\n",
      "batch 126, training loss: 5.696: : 126it [01:26,  1.30it/s]\u001b[A\n",
      "batch 127, training loss: 5.4913: : 126it [01:27,  1.30it/s]\u001b[A\n",
      "batch 127, training loss: 5.4913: : 127it [01:27,  1.32it/s]\u001b[A\n",
      "batch 128, training loss: 5.5915: : 127it [01:27,  1.32it/s]\u001b[A\n",
      "batch 128, training loss: 5.5915: : 128it [01:27,  1.28it/s]\u001b[A\n",
      "batch 129, training loss: 5.5761: : 128it [01:28,  1.28it/s]\u001b[A\n",
      "batch 129, training loss: 5.5761: : 129it [01:28,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 5.599: : 129it [01:29,  1.30it/s] \u001b[A\n",
      "batch 130, training loss: 5.599: : 130it [01:29,  1.31it/s]\u001b[A\n",
      "batch 131, training loss: 5.6606: : 130it [01:30,  1.31it/s]\u001b[A\n",
      "batch 131, training loss: 5.6606: : 131it [01:30,  1.34it/s]\u001b[A\n",
      "batch 132, training loss: 5.5434: : 131it [01:30,  1.34it/s]\u001b[A\n",
      "batch 132, training loss: 5.5434: : 132it [01:30,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 5.4082: : 132it [01:31,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 5.4082: : 133it [01:31,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 5.5167: : 133it [01:32,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 5.5167: : 134it [01:32,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 5.5071: : 134it [01:33,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 5.5071: : 135it [01:33,  1.33it/s]\u001b[A\n",
      "batch 136, training loss: 5.4519: : 135it [01:33,  1.33it/s]\u001b[A\n",
      "batch 136, training loss: 5.4519: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 5.5612: : 136it [01:34,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 5.5612: : 137it [01:34,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 5.5179: : 137it [01:35,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 5.5179: : 138it [01:35,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 5.3614: : 138it [01:36,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 5.3614: : 139it [01:36,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 5.432: : 139it [01:37,  1.30it/s] \u001b[A\n",
      "batch 140, training loss: 5.432: : 140it [01:37,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 5.4603: : 140it [01:37,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 5.4603: : 141it [01:37,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 5.4696: : 141it [01:38,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 5.4696: : 142it [01:38,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 5.3403: : 142it [01:39,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 5.3403: : 143it [01:39,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 5.3328: : 143it [01:40,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 5.3328: : 144it [01:40,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 5.4087: : 144it [01:40,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 5.4087: : 145it [01:40,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 5.4221: : 145it [01:41,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 5.4221: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 5.2816: : 146it [01:42,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 5.2816: : 147it [01:42,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 5.3903: : 147it [01:43,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 5.3903: : 148it [01:43,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 5.4017: : 148it [01:43,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 5.4017: : 149it [01:43,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 5.4462: : 149it [01:44,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 5.4462: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 5.4098: : 150it [01:45,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 5.4098: : 151it [01:45,  1.32it/s]\u001b[A\n",
      "batch 152, training loss: 5.3533: : 151it [01:46,  1.32it/s]\u001b[A\n",
      "batch 152, training loss: 5.3533: : 152it [01:46,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 5.3368: : 152it [01:46,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 5.3368: : 153it [01:46,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 5.4144: : 153it [01:47,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 5.4144: : 154it [01:47,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 5.4252: : 154it [01:48,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 5.4252: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 5.2698: : 155it [01:49,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 5.2698: : 156it [01:49,  1.30it/s]\u001b[A\n",
      "batch 157, training loss: 5.3897: : 156it [01:50,  1.30it/s]\u001b[A\n",
      "batch 157, training loss: 5.3897: : 157it [01:50,  1.30it/s]\u001b[A\n",
      "batch 158, training loss: 5.3608: : 157it [01:50,  1.30it/s]\u001b[A\n",
      "batch 158, training loss: 5.3608: : 158it [01:50,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 5.2897: : 158it [01:51,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 5.2897: : 159it [01:51,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 5.3602: : 159it [01:52,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 5.3602: : 160it [01:52,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 5.2551: : 160it [01:53,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 5.2551: : 161it [01:53,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 5.277: : 161it [01:53,  1.31it/s] \u001b[A\n",
      "batch 162, training loss: 5.277: : 162it [01:53,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 5.4147: : 162it [01:54,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 5.4147: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 5.2924: : 163it [01:55,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 5.2924: : 164it [01:55,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 5.3257: : 164it [01:56,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 5.3257: : 165it [01:56,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 5.2845: : 165it [01:56,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 5.2845: : 166it [01:56,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 5.3616: : 166it [01:57,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 5.3616: : 167it [01:57,  1.31it/s]\u001b[A\n",
      "batch 168, training loss: 5.3442: : 167it [01:58,  1.31it/s]\u001b[A\n",
      "batch 168, training loss: 5.3442: : 168it [01:58,  1.30it/s]\u001b[A\n",
      "batch 169, training loss: 5.4102: : 168it [01:59,  1.30it/s]\u001b[A\n",
      "batch 169, training loss: 5.4102: : 169it [01:59,  1.30it/s]\u001b[A\n",
      "batch 170, training loss: 5.2952: : 169it [02:00,  1.30it/s]\u001b[A\n",
      "batch 170, training loss: 5.2952: : 170it [02:00,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 4.9886: : 170it [02:00,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 4.9886: : 171it [02:00,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 5.8308: : 171it [02:01,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 5.8308: : 172it [02:01,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 5.6743: : 172it [02:01,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 5.6743: : 173it [02:01,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 5.8148: : 173it [02:02,  1.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 174, training loss: 5.8148: : 174it [02:02,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 5.7315: : 174it [02:03,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 5.7315: : 175it [02:03,  1.35it/s]\u001b[A\n",
      "batch 176, training loss: 5.6562: : 175it [02:04,  1.35it/s]\u001b[A\n",
      "batch 176, training loss: 5.6562: : 176it [02:04,  1.31it/s]\u001b[A\n",
      "batch 177, training loss: 5.6599: : 176it [02:05,  1.31it/s]\u001b[A\n",
      "batch 177, training loss: 5.6599: : 177it [02:05,  1.33it/s]\u001b[A\n",
      "batch 178, training loss: 5.7211: : 177it [02:05,  1.33it/s]\u001b[A\n",
      "batch 178, training loss: 5.7211: : 178it [02:05,  1.30it/s]\u001b[A\n",
      "batch 179, training loss: 5.6774: : 178it [02:06,  1.30it/s]\u001b[A\n",
      "batch 179, training loss: 5.6774: : 179it [02:06,  1.30it/s]\u001b[A\n",
      "batch 180, training loss: 5.7705: : 179it [02:07,  1.30it/s]\u001b[A\n",
      "batch 180, training loss: 5.7705: : 180it [02:07,  1.27it/s]\u001b[A\n",
      "batch 181, training loss: 5.714: : 180it [02:08,  1.27it/s] \u001b[A\n",
      "batch 181, training loss: 5.714: : 181it [02:08,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 5.6583: : 181it [02:09,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 5.6583: : 182it [02:09,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 5.6932: : 182it [02:09,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 5.6932: : 183it [02:09,  1.25it/s]\u001b[A\n",
      "batch 184, training loss: 5.5383: : 183it [02:10,  1.25it/s]\u001b[A\n",
      "batch 184, training loss: 5.5383: : 184it [02:10,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 5.5506: : 184it [02:11,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 5.5506: : 185it [02:11,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 5.6319: : 185it [02:12,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 5.6319: : 186it [02:12,  1.24it/s]\u001b[A\n",
      "batch 187, training loss: 5.6316: : 186it [02:13,  1.24it/s]\u001b[A\n",
      "batch 187, training loss: 5.6316: : 187it [02:13,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 5.7015: : 187it [02:13,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 5.7015: : 188it [02:13,  1.26it/s]\u001b[A\n",
      "batch 189, training loss: 5.6612: : 188it [02:14,  1.26it/s]\u001b[A\n",
      "batch 189, training loss: 5.6612: : 189it [02:14,  1.25it/s]\u001b[A\n",
      "batch 190, training loss: 5.3977: : 189it [02:15,  1.25it/s]\u001b[A\n",
      "batch 190, training loss: 5.3977: : 190it [02:15,  1.27it/s]\u001b[A\n",
      "batch 191, training loss: 5.4446: : 190it [02:16,  1.27it/s]\u001b[A\n",
      "batch 191, training loss: 5.4446: : 191it [02:16,  1.26it/s]\u001b[A\n",
      "batch 192, training loss: 5.5234: : 191it [02:17,  1.26it/s]\u001b[A\n",
      "batch 192, training loss: 5.5234: : 192it [02:17,  1.25it/s]\u001b[A\n",
      "batch 193, training loss: 5.3893: : 192it [02:17,  1.25it/s]\u001b[A\n",
      "batch 193, training loss: 5.3893: : 193it [02:17,  1.26it/s]\u001b[A\n",
      "batch 194, training loss: 5.4654: : 193it [02:18,  1.26it/s]\u001b[A\n",
      "batch 194, training loss: 5.4654: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 5.3956: : 194it [02:19,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 5.3956: : 195it [02:19,  1.26it/s]\u001b[A\n",
      "batch 196, training loss: 5.3733: : 195it [02:20,  1.26it/s]\u001b[A\n",
      "batch 196, training loss: 5.3733: : 196it [02:20,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 5.47: : 196it [02:21,  1.24it/s]  \u001b[A\n",
      "batch 197, training loss: 5.47: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 5.3759: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 5.3759: : 198it [02:21,  1.27it/s]\u001b[A\n",
      "batch 199, training loss: 5.3322: : 198it [02:22,  1.27it/s]\u001b[A\n",
      "batch 199, training loss: 5.3322: : 199it [02:22,  1.25it/s]\u001b[A\n",
      "batch 200, training loss: 5.4918: : 199it [02:23,  1.25it/s]\u001b[A\n",
      "batch 200, training loss: 5.4918: : 200it [02:23,  1.26it/s]\u001b[A\n",
      "batch 201, training loss: 5.3097: : 200it [02:24,  1.26it/s]\u001b[A\n",
      "batch 201, training loss: 5.3097: : 201it [02:24,  1.26it/s]\u001b[A\n",
      "batch 202, training loss: 5.2111: : 201it [02:25,  1.26it/s]\u001b[A\n",
      "batch 202, training loss: 5.2111: : 202it [02:25,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 5.3299: : 202it [02:25,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 5.3299: : 203it [02:25,  1.26it/s]\u001b[A\n",
      "batch 204, training loss: 5.4104: : 203it [02:26,  1.26it/s]\u001b[A\n",
      "batch 204, training loss: 5.4104: : 204it [02:26,  1.25it/s]\u001b[A\n",
      "batch 205, training loss: 5.3765: : 204it [02:27,  1.25it/s]\u001b[A\n",
      "batch 205, training loss: 5.3765: : 205it [02:27,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 5.4262: : 205it [02:28,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 5.4262: : 206it [02:28,  1.25it/s]\u001b[A\n",
      "batch 207, training loss: 5.328: : 206it [02:29,  1.25it/s] \u001b[A\n",
      "batch 207, training loss: 5.328: : 207it [02:29,  1.27it/s]\u001b[A\n",
      "batch 208, training loss: 5.3148: : 207it [02:29,  1.27it/s]\u001b[A\n",
      "batch 208, training loss: 5.3148: : 208it [02:29,  1.42it/s]\u001b[A\n",
      "batch 209, training loss: 5.3039: : 208it [02:30,  1.42it/s]\u001b[A\n",
      "batch 209, training loss: 5.3039: : 209it [02:30,  1.39it/s]\u001b[A\n",
      "batch 210, training loss: 5.2959: : 209it [02:31,  1.39it/s]\u001b[A\n",
      "batch 210, training loss: 5.2959: : 210it [02:31,  1.33it/s]\u001b[A\n",
      "batch 211, training loss: 5.2739: : 210it [02:31,  1.33it/s]\u001b[A\n",
      "batch 211, training loss: 5.2739: : 211it [02:31,  1.30it/s]\u001b[A\n",
      "batch 212, training loss: 5.3206: : 211it [02:32,  1.30it/s]\u001b[A\n",
      "batch 212, training loss: 5.3206: : 212it [02:32,  1.31it/s]\u001b[A\n",
      "batch 213, training loss: 5.4628: : 212it [02:33,  1.31it/s]\u001b[A\n",
      "batch 213, training loss: 5.4628: : 213it [02:33,  1.29it/s]\u001b[A\n",
      "batch 214, training loss: 5.3247: : 213it [02:34,  1.29it/s]\u001b[A\n",
      "batch 214, training loss: 5.3247: : 214it [02:34,  1.28it/s]\u001b[A\n",
      "batch 215, training loss: 5.1728: : 214it [02:35,  1.28it/s]\u001b[A\n",
      "batch 215, training loss: 5.1728: : 215it [02:35,  1.27it/s]\u001b[A\n",
      "batch 216, training loss: 5.2725: : 215it [02:35,  1.27it/s]\u001b[A\n",
      "batch 216, training loss: 5.2725: : 216it [02:35,  1.28it/s]\u001b[A\n",
      "batch 217, training loss: 5.3821: : 216it [02:36,  1.28it/s]\u001b[A\n",
      "batch 217, training loss: 5.3821: : 217it [02:36,  1.25it/s]\u001b[A\n",
      "batch 218, training loss: 5.3309: : 217it [02:37,  1.25it/s]\u001b[A\n",
      "batch 218, training loss: 5.3309: : 218it [02:37,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 5.4577: : 218it [02:38,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 5.4577: : 219it [02:38,  1.27it/s]\u001b[A\n",
      "batch 220, training loss: 5.4242: : 219it [02:39,  1.27it/s]\u001b[A\n",
      "batch 220, training loss: 5.4242: : 220it [02:39,  1.26it/s]\u001b[A\n",
      "batch 221, training loss: 5.3489: : 220it [02:39,  1.26it/s]\u001b[A\n",
      "batch 221, training loss: 5.3489: : 221it [02:39,  1.26it/s]\u001b[A\n",
      "batch 222, training loss: 5.2922: : 221it [02:40,  1.26it/s]\u001b[A\n",
      "batch 222, training loss: 5.2922: : 222it [02:40,  1.26it/s]\u001b[A\n",
      "batch 223, training loss: 5.3019: : 222it [02:41,  1.26it/s]\u001b[A\n",
      "batch 223, training loss: 5.3019: : 223it [02:41,  1.25it/s]\u001b[A\n",
      "batch 224, training loss: 5.2572: : 223it [02:42,  1.25it/s]\u001b[A\n",
      "batch 224, training loss: 5.2572: : 224it [02:42,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 5.4016: : 224it [02:43,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 5.4016: : 225it [02:43,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 5.3836: : 225it [02:43,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 5.3836: : 226it [02:43,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 5.2479: : 226it [02:44,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 5.2479: : 227it [02:44,  1.25it/s]\u001b[A\n",
      "batch 228, training loss: 5.2434: : 227it [02:45,  1.25it/s]\u001b[A\n",
      "batch 228, training loss: 5.2434: : 228it [02:45,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 5.2758: : 228it [02:46,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 5.2758: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 5.4014: : 229it [02:47,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 5.4014: : 230it [02:47,  1.24it/s]\u001b[A\n",
      "batch 231, training loss: 5.4301: : 230it [02:47,  1.24it/s]\u001b[A\n",
      "batch 231, training loss: 5.4301: : 231it [02:47,  1.24it/s]\u001b[A\n",
      "batch 232, training loss: 5.2257: : 231it [02:48,  1.24it/s]\u001b[A\n",
      "batch 232, training loss: 5.2257: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 5.3465: : 232it [02:49,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 5.3465: : 233it [02:49,  1.24it/s]\u001b[A\n",
      "batch 234, training loss: 5.4123: : 233it [02:50,  1.24it/s]\u001b[A\n",
      "batch 234, training loss: 5.4123: : 234it [02:50,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 5.3044: : 234it [02:51,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 5.3044: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 5.3068: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 5.3068: : 236it [02:51,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 5.1766: : 236it [02:52,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 5.1766: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 5.2938: : 237it [02:53,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 238, training loss: 5.2938: : 238it [02:53,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 5.2383: : 238it [02:54,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 5.2383: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 5.3: : 239it [02:55,  1.24it/s]   \u001b[A\n",
      "batch 240, training loss: 5.3: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 5.3043: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 5.3043: : 241it [02:55,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 5.1713: : 241it [02:56,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 5.1713: : 242it [02:56,  1.25it/s]\u001b[A\n",
      "batch 243, training loss: 5.286: : 242it [02:57,  1.25it/s] \u001b[A\n",
      "batch 243, training loss: 5.286: : 243it [02:57,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 5.2446: : 243it [02:58,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 5.2446: : 244it [02:58,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 5.3157: : 244it [02:59,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 5.3157: : 245it [02:59,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 5.2854: : 245it [02:59,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 5.2854: : 246it [02:59,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 5.2651: : 246it [03:00,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 5.2651: : 247it [03:00,  1.27it/s]\u001b[A\n",
      "batch 248, training loss: 5.176: : 247it [03:01,  1.27it/s] \u001b[A\n",
      "batch 248, training loss: 5.176: : 248it [03:01,  1.26it/s]\u001b[A\n",
      "batch 249, training loss: 5.1578: : 248it [03:02,  1.26it/s]\u001b[A\n",
      "batch 249, training loss: 5.1578: : 249it [03:02,  1.27it/s]\u001b[A\n",
      "batch 250, training loss: 5.2912: : 249it [03:03,  1.27it/s]\u001b[A\n",
      "batch 250, training loss: 5.2912: : 250it [03:03,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 5.3256: : 250it [03:03,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 5.3256: : 251it [03:03,  1.26it/s]\u001b[A\n",
      "batch 252, training loss: 5.1584: : 251it [03:04,  1.26it/s]\u001b[A\n",
      "batch 252, training loss: 5.1584: : 252it [03:04,  1.49it/s]\u001b[A\n",
      "batch 253, training loss: 5.5001: : 252it [03:05,  1.49it/s]\u001b[A\n",
      "batch 253, training loss: 5.5001: : 253it [03:05,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 5.6777: : 253it [03:06,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 5.6777: : 254it [03:06,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 5.532: : 254it [03:07,  1.25it/s] \u001b[A\n",
      "batch 255, training loss: 5.532: : 255it [03:07,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 5.4528: : 255it [03:07,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 5.4528: : 256it [03:07,  1.18it/s]\u001b[A\n",
      "batch 257, training loss: 5.5733: : 256it [03:08,  1.18it/s]\u001b[A\n",
      "batch 257, training loss: 5.5733: : 257it [03:08,  1.15it/s]\u001b[A\n",
      "batch 258, training loss: 5.5973: : 257it [03:09,  1.15it/s]\u001b[A\n",
      "batch 258, training loss: 5.5973: : 258it [03:09,  1.13it/s]\u001b[A\n",
      "batch 259, training loss: 5.49: : 258it [03:10,  1.13it/s]  \u001b[A\n",
      "batch 259, training loss: 5.49: : 259it [03:10,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 5.495: : 259it [03:11,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 5.495: : 260it [03:11,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 5.5587: : 260it [03:12,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 5.5587: : 261it [03:12,  1.10it/s]\u001b[A\n",
      "batch 262, training loss: 5.4039: : 261it [03:13,  1.10it/s]\u001b[A\n",
      "batch 262, training loss: 5.4039: : 262it [03:13,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 5.4214: : 262it [03:14,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 5.4214: : 263it [03:14,  1.09it/s]\u001b[A\n",
      "batch 264, training loss: 5.451: : 263it [03:15,  1.09it/s] \u001b[A\n",
      "batch 264, training loss: 5.451: : 264it [03:15,  1.11it/s]\u001b[A\n",
      "batch 265, training loss: 5.3408: : 264it [03:16,  1.11it/s]\u001b[A\n",
      "batch 265, training loss: 5.3408: : 265it [03:16,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 5.4829: : 265it [03:17,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 5.4829: : 266it [03:17,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 5.3762: : 266it [03:17,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 5.3762: : 267it [03:17,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 5.3314: : 267it [03:18,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 5.3314: : 268it [03:18,  1.12it/s]\u001b[A\n",
      "batch 269, training loss: 5.3028: : 268it [03:19,  1.12it/s]\u001b[A\n",
      "batch 269, training loss: 5.3028: : 269it [03:19,  1.12it/s]\u001b[A\n",
      "batch 270, training loss: 5.3041: : 269it [03:20,  1.12it/s]\u001b[A\n",
      "batch 270, training loss: 5.3041: : 270it [03:20,  1.16it/s]\u001b[A\n",
      "batch 271, training loss: 5.3072: : 270it [03:21,  1.16it/s]\u001b[A\n",
      "batch 271, training loss: 5.3072: : 271it [03:21,  1.18it/s]\u001b[A\n",
      "batch 272, training loss: 5.324: : 271it [03:22,  1.18it/s] \u001b[A\n",
      "batch 272, training loss: 5.324: : 272it [03:22,  1.15it/s]\u001b[A\n",
      "batch 273, training loss: 5.3541: : 272it [03:23,  1.15it/s]\u001b[A\n",
      "batch 273, training loss: 5.3541: : 273it [03:23,  1.18it/s]\u001b[A\n",
      "batch 274, training loss: 5.3668: : 273it [03:23,  1.18it/s]\u001b[A\n",
      "batch 274, training loss: 5.3668: : 274it [03:23,  1.20it/s]\u001b[A\n",
      "batch 275, training loss: 5.2323: : 274it [03:24,  1.20it/s]\u001b[A\n",
      "batch 275, training loss: 5.2323: : 275it [03:24,  1.16it/s]\u001b[A\n",
      "batch 276, training loss: 5.1528: : 275it [03:25,  1.16it/s]\u001b[A\n",
      "batch 276, training loss: 5.1528: : 276it [03:25,  1.20it/s]\u001b[A\n",
      "batch 277, training loss: 5.304: : 276it [03:26,  1.20it/s] \u001b[A\n",
      "batch 277, training loss: 5.304: : 277it [03:26,  1.21it/s]\u001b[A\n",
      "batch 278, training loss: 5.2206: : 277it [03:27,  1.21it/s]\u001b[A\n",
      "batch 278, training loss: 5.2206: : 278it [03:27,  1.17it/s]\u001b[A\n",
      "batch 279, training loss: 5.2788: : 278it [03:28,  1.17it/s]\u001b[A\n",
      "batch 279, training loss: 5.2788: : 279it [03:28,  1.15it/s]\u001b[A\n",
      "batch 280, training loss: 5.1914: : 279it [03:29,  1.15it/s]\u001b[A\n",
      "batch 280, training loss: 5.1914: : 280it [03:29,  1.14it/s]\u001b[A\n",
      "batch 281, training loss: 5.1981: : 280it [03:30,  1.14it/s]\u001b[A\n",
      "batch 281, training loss: 5.1981: : 281it [03:30,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 5.1853: : 281it [03:30,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 5.1853: : 282it [03:30,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 5.1727: : 282it [03:31,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 5.1727: : 283it [03:31,  1.12it/s]\u001b[A\n",
      "batch 284, training loss: 5.1736: : 283it [03:32,  1.12it/s]\u001b[A\n",
      "batch 284, training loss: 5.1736: : 284it [03:32,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 5.2125: : 284it [03:33,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 5.2125: : 285it [03:33,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 5.3473: : 285it [03:34,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 5.3473: : 286it [03:34,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 5.1239: : 286it [03:35,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 5.1239: : 287it [03:35,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 5.1057: : 287it [03:36,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 5.1057: : 288it [03:36,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 5.2364: : 288it [03:37,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 5.2364: : 289it [03:37,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 5.0497: : 289it [03:38,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 5.0497: : 290it [03:38,  1.08it/s]\u001b[A\n",
      "batch 291, training loss: 5.3134: : 290it [03:39,  1.08it/s]\u001b[A\n",
      "batch 291, training loss: 5.3134: : 291it [03:39,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 5.1095: : 291it [03:40,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 5.1095: : 292it [03:40,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 5.1528: : 292it [03:40,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 5.1528: : 293it [03:40,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 5.2297: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 5.2297: : 294it [03:41,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 5.1859: : 294it [03:42,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 5.1859: : 295it [03:42,  1.12it/s]\u001b[A\n",
      "batch 296, training loss: 5.0731: : 295it [03:43,  1.12it/s]\u001b[A\n",
      "batch 296, training loss: 5.0731: : 296it [03:43,  1.12it/s]\u001b[A\n",
      "batch 297, training loss: 5.1911: : 296it [03:44,  1.12it/s]\u001b[A\n",
      "batch 297, training loss: 5.1911: : 297it [03:44,  1.12it/s]\u001b[A\n",
      "batch 298, training loss: 5.119: : 297it [03:45,  1.12it/s] \u001b[A\n",
      "batch 298, training loss: 5.119: : 298it [03:45,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 5.2148: : 298it [03:46,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 5.2148: : 299it [03:46,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 5.287: : 299it [03:47,  1.11it/s] \u001b[A\n",
      "batch 300, training loss: 5.287: : 300it [03:47,  1.14it/s]\u001b[A\n",
      "batch 301, training loss: 5.1897: : 300it [03:47,  1.14it/s]\u001b[A\n",
      "batch 301, training loss: 5.1897: : 301it [03:47,  1.23it/s]\u001b[A\n",
      "batch 302, training loss: 5.1986: : 301it [03:48,  1.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 302, training loss: 5.1986: : 302it [03:48,  1.21it/s]\u001b[A\n",
      "batch 303, training loss: 5.1734: : 302it [03:49,  1.21it/s]\u001b[A\n",
      "batch 303, training loss: 5.1734: : 303it [03:49,  1.20it/s]\u001b[A\n",
      "batch 304, training loss: 5.1254: : 303it [03:50,  1.20it/s]\u001b[A\n",
      "batch 304, training loss: 5.1254: : 304it [03:50,  1.19it/s]\u001b[A\n",
      "batch 305, training loss: 5.1933: : 304it [03:51,  1.19it/s]\u001b[A\n",
      "batch 305, training loss: 5.1933: : 305it [03:51,  1.16it/s]\u001b[A\n",
      "batch 306, training loss: 5.2187: : 305it [03:52,  1.16it/s]\u001b[A\n",
      "batch 306, training loss: 5.2187: : 306it [03:52,  1.14it/s]\u001b[A\n",
      "batch 307, training loss: 5.3464: : 306it [03:53,  1.14it/s]\u001b[A\n",
      "batch 307, training loss: 5.3464: : 307it [03:53,  1.12it/s]\u001b[A\n",
      "batch 308, training loss: 4.9976: : 307it [03:54,  1.12it/s]\u001b[A\n",
      "batch 308, training loss: 4.9976: : 308it [03:54,  1.12it/s]\u001b[A\n",
      "batch 309, training loss: 5.301: : 308it [03:54,  1.12it/s] \u001b[A\n",
      "batch 309, training loss: 5.301: : 309it [03:54,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 5.1645: : 309it [03:55,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 5.1645: : 310it [03:55,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 5.1843: : 310it [03:56,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 5.1843: : 311it [03:56,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 5.0349: : 311it [03:57,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 5.0349: : 312it [03:57,  1.12it/s]\u001b[A\n",
      "batch 313, training loss: 5.1363: : 312it [03:58,  1.12it/s]\u001b[A\n",
      "batch 313, training loss: 5.1363: : 313it [03:58,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 5.1713: : 313it [03:59,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 5.1713: : 314it [03:59,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 5.2424: : 314it [04:00,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 5.2424: : 315it [04:00,  1.18it/s]\u001b[A\n",
      "batch 316, training loss: 5.3198: : 315it [04:00,  1.18it/s]\u001b[A\n",
      "batch 316, training loss: 5.3198: : 316it [04:00,  1.21it/s]\u001b[A\n",
      "batch 317, training loss: 5.0713: : 316it [04:01,  1.21it/s]\u001b[A\n",
      "batch 317, training loss: 5.0713: : 317it [04:01,  1.26it/s]\u001b[A\n",
      "batch 318, training loss: 5.4043: : 317it [04:02,  1.26it/s]\u001b[A\n",
      "batch 318, training loss: 5.4043: : 318it [04:02,  1.15it/s]\u001b[A\n",
      "batch 319, training loss: 5.4365: : 318it [04:03,  1.15it/s]\u001b[A\n",
      "batch 319, training loss: 5.4365: : 319it [04:03,  1.12it/s]\u001b[A\n",
      "batch 320, training loss: 5.4246: : 319it [04:04,  1.12it/s]\u001b[A\n",
      "batch 320, training loss: 5.4246: : 320it [04:04,  1.09it/s]\u001b[A\n",
      "batch 321, training loss: 5.5209: : 320it [04:05,  1.09it/s]\u001b[A\n",
      "batch 321, training loss: 5.5209: : 321it [04:05,  1.04it/s]\u001b[A\n",
      "batch 322, training loss: 5.5382: : 321it [04:06,  1.04it/s]\u001b[A\n",
      "batch 322, training loss: 5.5382: : 322it [04:06,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 5.3257: : 322it [04:07,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 5.3257: : 323it [04:07,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 5.4243: : 323it [04:08,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 5.4243: : 324it [04:08,  1.00s/it]\u001b[A\n",
      "batch 325, training loss: 5.4455: : 324it [04:09,  1.00s/it]\u001b[A\n",
      "batch 325, training loss: 5.4455: : 325it [04:09,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 5.3172: : 325it [04:10,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 5.3172: : 326it [04:10,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 5.4285: : 326it [04:11,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 5.4285: : 327it [04:11,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 5.2734: : 327it [04:12,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 5.2734: : 328it [04:12,  1.00s/it]\u001b[A\n",
      "batch 329, training loss: 5.2387: : 328it [04:13,  1.00s/it]\u001b[A\n",
      "batch 329, training loss: 5.2387: : 329it [04:13,  1.00s/it]\u001b[A\n",
      "batch 330, training loss: 5.3135: : 329it [04:14,  1.00s/it]\u001b[A\n",
      "batch 330, training loss: 5.3135: : 330it [04:14,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 5.3573: : 330it [04:15,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 5.3573: : 331it [04:15,  1.00s/it]\u001b[A\n",
      "batch 332, training loss: 5.258: : 331it [04:16,  1.00s/it] \u001b[A\n",
      "batch 332, training loss: 5.258: : 332it [04:16,  1.02s/it]\u001b[A\n",
      "batch 333, training loss: 5.2237: : 332it [04:17,  1.02s/it]\u001b[A\n",
      "batch 333, training loss: 5.2237: : 333it [04:17,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 5.2879: : 333it [04:18,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 5.2879: : 334it [04:18,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 5.432: : 334it [04:19,  1.01it/s] \u001b[A\n",
      "batch 335, training loss: 5.432: : 335it [04:19,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 5.3535: : 335it [04:20,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 5.3535: : 336it [04:20,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 5.3372: : 336it [04:21,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 5.3372: : 337it [04:21,  1.00it/s]\u001b[A\n",
      "batch 338, training loss: 5.2405: : 337it [04:22,  1.00it/s]\u001b[A\n",
      "batch 338, training loss: 5.2405: : 338it [04:22,  1.04it/s]\u001b[A\n",
      "batch 339, training loss: 5.299: : 338it [04:23,  1.04it/s] \u001b[A\n",
      "batch 339, training loss: 5.299: : 339it [04:23,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 5.4574: : 339it [04:24,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 5.4574: : 340it [04:24,  1.05it/s]\u001b[A\n",
      "batch 341, training loss: 5.1549: : 340it [04:25,  1.05it/s]\u001b[A\n",
      "batch 341, training loss: 5.1549: : 341it [04:25,  1.03it/s]\u001b[A\n",
      "batch 342, training loss: 5.2661: : 341it [04:26,  1.03it/s]\u001b[A\n",
      "batch 342, training loss: 5.2661: : 342it [04:26,  1.01it/s]\u001b[A\n",
      "batch 343, training loss: 5.2359: : 342it [04:27,  1.01it/s]\u001b[A\n",
      "batch 343, training loss: 5.2359: : 343it [04:27,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 5.3004: : 343it [04:28,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 5.3004: : 344it [04:28,  1.00s/it]\u001b[A\n",
      "batch 345, training loss: 5.2294: : 344it [04:29,  1.00s/it]\u001b[A\n",
      "batch 345, training loss: 5.2294: : 345it [04:29,  1.03it/s]\u001b[A\n",
      "batch 346, training loss: 5.2346: : 345it [04:30,  1.03it/s]\u001b[A\n",
      "batch 346, training loss: 5.2346: : 346it [04:30,  1.01it/s]\u001b[A\n",
      "batch 347, training loss: 5.1665: : 346it [04:31,  1.01it/s]\u001b[A\n",
      "batch 347, training loss: 5.1665: : 347it [04:31,  1.01it/s]\u001b[A\n",
      "batch 348, training loss: 5.1981: : 347it [04:32,  1.01it/s]\u001b[A\n",
      "batch 348, training loss: 5.1981: : 348it [04:32,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 5.2908: : 348it [04:33,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 5.2908: : 349it [04:33,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 5.2479: : 349it [04:34,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 5.2479: : 350it [04:34,  1.03it/s]\u001b[A\n",
      "batch 351, training loss: 5.1575: : 350it [04:35,  1.03it/s]\u001b[A\n",
      "batch 351, training loss: 5.1575: : 351it [04:35,  1.01it/s]\u001b[A\n",
      "batch 352, training loss: 5.127: : 351it [04:36,  1.01it/s] \u001b[A\n",
      "batch 352, training loss: 5.127: : 352it [04:36,  1.00it/s]\u001b[A\n",
      "batch 353, training loss: 5.2268: : 352it [04:37,  1.00it/s]\u001b[A\n",
      "batch 353, training loss: 5.2268: : 353it [04:37,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 5.2464: : 353it [04:38,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 5.2464: : 354it [04:38,  1.00s/it]\u001b[A\n",
      "batch 355, training loss: 5.25: : 354it [04:39,  1.00s/it]  \u001b[A\n",
      "batch 355, training loss: 5.25: : 355it [04:39,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 5.0719: : 355it [04:40,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 5.0719: : 356it [04:40,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 5.0784: : 356it [04:41,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 5.0784: : 357it [04:41,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 5.2547: : 357it [04:42,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 5.2547: : 358it [04:42,  1.00it/s]\u001b[A\n",
      "batch 359, training loss: 5.0839: : 358it [04:43,  1.00it/s]\u001b[A\n",
      "batch 359, training loss: 5.0839: : 359it [04:43,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 5.2219: : 359it [04:44,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 5.2219: : 360it [04:44,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 5.1513: : 360it [04:45,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 5.1513: : 361it [04:45,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 5.2881: : 361it [04:46,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 5.2881: : 362it [04:46,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 5.1531: : 362it [04:47,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 5.1531: : 363it [04:47,  1.02it/s]\u001b[A\n",
      "batch 364, training loss: 5.0883: : 363it [04:48,  1.02it/s]\u001b[A\n",
      "batch 364, training loss: 5.0883: : 364it [04:48,  1.01it/s]\u001b[A\n",
      "batch 365, training loss: 5.0653: : 364it [04:49,  1.01it/s]\u001b[A\n",
      "batch 365, training loss: 5.0653: : 365it [04:49,  1.01it/s]\u001b[A\n",
      "batch 366, training loss: 5.2182: : 365it [04:50,  1.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 366, training loss: 5.2182: : 366it [04:50,  1.01it/s]\u001b[A\n",
      "batch 367, training loss: 5.112: : 366it [04:51,  1.01it/s] \u001b[A\n",
      "batch 367, training loss: 5.112: : 367it [04:51,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 5.204: : 367it [04:52,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 5.204: : 368it [04:52,  1.02it/s]\u001b[A\n",
      "batch 369, training loss: 5.2439: : 368it [04:53,  1.02it/s]\u001b[A\n",
      "batch 369, training loss: 5.2439: : 369it [04:53,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 5.1099: : 369it [04:54,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 5.1099: : 370it [04:54,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 5.1569: : 370it [04:55,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 5.1569: : 371it [04:55,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 5.1449: : 371it [04:56,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 5.1449: : 372it [04:56,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 5.1094: : 372it [04:57,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 5.1094: : 373it [04:57,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 5.1771: : 373it [04:58,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 5.1771: : 374it [04:58,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 5.0511: : 374it [04:59,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 5.0511: : 375it [04:59,  1.15it/s]\u001b[A\n",
      "batch 376, training loss: 5.3154: : 375it [05:00,  1.15it/s]\u001b[A\n",
      "batch 376, training loss: 5.3154: : 376it [05:00,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 5.3997: : 376it [05:01,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 5.3997: : 377it [05:01,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 5.2182: : 377it [05:02,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 5.2182: : 378it [05:02,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 5.256: : 378it [05:03,  1.03s/it] \u001b[A\n",
      "batch 379, training loss: 5.256: : 379it [05:03,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 5.276: : 379it [05:04,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 5.276: : 380it [05:04,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 5.3008: : 380it [05:05,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 5.3008: : 381it [05:05,  1.09s/it]\u001b[A\n",
      "batch 382, training loss: 5.1922: : 381it [05:06,  1.09s/it]\u001b[A\n",
      "batch 382, training loss: 5.1922: : 382it [05:06,  1.10s/it]\u001b[A\n",
      "batch 383, training loss: 5.2112: : 382it [05:07,  1.10s/it]\u001b[A\n",
      "batch 383, training loss: 5.2112: : 383it [05:07,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 5.3643: : 383it [05:09,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 5.3643: : 384it [05:09,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 5.2125: : 384it [05:10,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 5.2125: : 385it [05:10,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 5.2019: : 385it [05:11,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 5.2019: : 386it [05:11,  1.10s/it]\u001b[A\n",
      "batch 387, training loss: 5.3002: : 386it [05:12,  1.10s/it]\u001b[A\n",
      "batch 387, training loss: 5.3002: : 387it [05:12,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 5.0716: : 387it [05:13,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 5.0716: : 388it [05:13,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 5.2102: : 388it [05:14,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 5.2102: : 389it [05:14,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 5.2068: : 389it [05:15,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 5.2068: : 390it [05:15,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 5.2765: : 390it [05:16,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 5.2765: : 391it [05:16,  1.13s/it]\u001b[A\n",
      "batch 392, training loss: 5.2406: : 391it [05:17,  1.13s/it]\u001b[A\n",
      "batch 392, training loss: 5.2406: : 392it [05:17,  1.12s/it]\u001b[A\n",
      "batch 393, training loss: 5.0954: : 392it [05:19,  1.12s/it]\u001b[A\n",
      "batch 393, training loss: 5.0954: : 393it [05:19,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 5.0502: : 393it [05:20,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 5.0502: : 394it [05:20,  1.10s/it]\u001b[A\n",
      "batch 395, training loss: 5.095: : 394it [05:21,  1.10s/it] \u001b[A\n",
      "batch 395, training loss: 5.095: : 395it [05:21,  1.09s/it]\u001b[A\n",
      "batch 396, training loss: 5.2718: : 395it [05:22,  1.09s/it]\u001b[A\n",
      "batch 396, training loss: 5.2718: : 396it [05:22,  1.10s/it]\u001b[A\n",
      "batch 397, training loss: 5.1039: : 396it [05:23,  1.10s/it]\u001b[A\n",
      "batch 397, training loss: 5.1039: : 397it [05:23,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 5.1318: : 397it [05:24,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 5.1318: : 398it [05:24,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 5.2283: : 398it [05:25,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 5.2283: : 399it [05:25,  1.09s/it]\u001b[A\n",
      "batch 400, training loss: 5.0651: : 399it [05:26,  1.09s/it]\u001b[A\n",
      "batch 400, training loss: 5.0651: : 400it [05:26,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 5.0429: : 400it [05:27,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 5.0429: : 401it [05:27,  1.08s/it]\u001b[A\n",
      "batch 402, training loss: 5.0937: : 401it [05:28,  1.08s/it]\u001b[A\n",
      "batch 402, training loss: 5.0937: : 402it [05:28,  1.07s/it]\u001b[A\n",
      "batch 403, training loss: 5.2374: : 402it [05:29,  1.07s/it]\u001b[A\n",
      "batch 403, training loss: 5.2374: : 403it [05:29,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.9533: : 403it [05:31,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.9533: : 404it [05:31,  1.10s/it]\u001b[A\n",
      "batch 405, training loss: 5.135: : 404it [05:32,  1.10s/it] \u001b[A\n",
      "batch 405, training loss: 5.135: : 405it [05:32,  1.12s/it]\u001b[A\n",
      "batch 406, training loss: 5.0667: : 405it [05:33,  1.12s/it]\u001b[A\n",
      "batch 406, training loss: 5.0667: : 406it [05:33,  1.02s/it]\u001b[A\n",
      "batch 407, training loss: 5.1578: : 406it [05:34,  1.02s/it]\u001b[A\n",
      "batch 407, training loss: 5.1578: : 407it [05:34,  1.03s/it]\u001b[A\n",
      "batch 408, training loss: 4.933: : 407it [05:35,  1.03s/it] \u001b[A\n",
      "batch 408, training loss: 4.933: : 408it [05:35,  1.05s/it]\u001b[A\n",
      "batch 409, training loss: 5.1367: : 408it [05:36,  1.05s/it]\u001b[A\n",
      "batch 409, training loss: 5.1367: : 409it [05:36,  1.03s/it]\u001b[A\n",
      "batch 410, training loss: 5.0197: : 409it [05:37,  1.03s/it]\u001b[A\n",
      "batch 410, training loss: 5.0197: : 410it [05:37,  1.06s/it]\u001b[A\n",
      "batch 411, training loss: 5.1199: : 410it [05:38,  1.06s/it]\u001b[A\n",
      "batch 411, training loss: 5.1199: : 411it [05:38,  1.07s/it]\u001b[A\n",
      "batch 412, training loss: 5.0891: : 411it [05:39,  1.07s/it]\u001b[A\n",
      "batch 412, training loss: 5.0891: : 412it [05:39,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 5.0733: : 412it [05:40,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 5.0733: : 413it [05:40,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 4.9567: : 413it [05:41,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 4.9567: : 414it [05:41,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 4.8509: : 414it [05:42,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 4.8509: : 415it [05:42,  1.07s/it]\u001b[A\n",
      "batch 416, training loss: 4.8982: : 415it [05:43,  1.07s/it]\u001b[A\n",
      "batch 416, training loss: 4.8982: : 416it [05:43,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 5.1556: : 416it [05:45,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 5.1556: : 417it [05:45,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 5.069: : 417it [05:46,  1.09s/it] \u001b[A\n",
      "batch 418, training loss: 5.069: : 418it [05:46,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 4.8505: : 418it [05:47,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 4.8505: : 419it [05:47,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.9779: : 419it [05:48,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.9779: : 420it [05:48,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 5.0616: : 420it [05:49,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 5.0616: : 421it [05:49,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 5.0476: : 421it [05:50,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 5.0476: : 422it [05:50,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 4.9401: : 422it [05:51,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 4.9401: : 423it [05:51,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 5.1907: : 423it [05:52,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 5.1907: : 424it [05:52,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 5.2566: : 424it [05:53,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 5.2566: : 425it [05:53,  1.14s/it]\u001b[A\n",
      "batch 426, training loss: 5.1561: : 425it [05:55,  1.14s/it]\u001b[A\n",
      "batch 426, training loss: 5.1561: : 426it [05:55,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 5.2066: : 426it [05:56,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 5.2066: : 427it [05:56,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 5.2611: : 427it [05:57,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 5.2611: : 428it [05:57,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 5.217: : 428it [05:58,  1.18s/it] \u001b[A\n",
      "batch 429, training loss: 5.217: : 429it [05:58,  1.20s/it]\u001b[A\n",
      "batch 430, training loss: 5.1608: : 429it [05:59,  1.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 430, training loss: 5.1608: : 430it [05:59,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 5.2783: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 5.2783: : 431it [06:01,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 5.1656: : 431it [06:02,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 5.1656: : 432it [06:02,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 5.1214: : 432it [06:03,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 5.1214: : 433it [06:03,  1.22s/it]\u001b[A\n",
      "batch 434, training loss: 5.1251: : 433it [06:04,  1.22s/it]\u001b[A\n",
      "batch 434, training loss: 5.1251: : 434it [06:04,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 5.0925: : 434it [06:06,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 5.0925: : 435it [06:06,  1.23s/it]\u001b[A\n",
      "batch 436, training loss: 5.1815: : 435it [06:07,  1.23s/it]\u001b[A\n",
      "batch 436, training loss: 5.1815: : 436it [06:07,  1.23s/it]\u001b[A\n",
      "batch 437, training loss: 5.0601: : 436it [06:08,  1.23s/it]\u001b[A\n",
      "batch 437, training loss: 5.0601: : 437it [06:08,  1.23s/it]\u001b[A\n",
      "batch 438, training loss: 5.1668: : 437it [06:09,  1.23s/it]\u001b[A\n",
      "batch 438, training loss: 5.1668: : 438it [06:09,  1.23s/it]\u001b[A\n",
      "batch 439, training loss: 5.2102: : 438it [06:11,  1.23s/it]\u001b[A\n",
      "batch 439, training loss: 5.2102: : 439it [06:11,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 5.0155: : 439it [06:12,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 5.0155: : 440it [06:12,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 5.2041: : 440it [06:13,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 5.2041: : 441it [06:13,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 4.9904: : 441it [06:14,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 4.9904: : 442it [06:14,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 5.1019: : 442it [06:15,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 5.1019: : 443it [06:15,  1.23s/it]\u001b[A\n",
      "batch 444, training loss: 5.067: : 443it [06:17,  1.23s/it] \u001b[A\n",
      "batch 444, training loss: 5.067: : 444it [06:17,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 5.0414: : 444it [06:18,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 5.0414: : 445it [06:18,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 5.1443: : 445it [06:19,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 5.1443: : 446it [06:19,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 5.0451: : 446it [06:20,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 5.0451: : 447it [06:20,  1.22s/it]\u001b[A\n",
      "batch 448, training loss: 4.9907: : 447it [06:22,  1.22s/it]\u001b[A\n",
      "batch 448, training loss: 4.9907: : 448it [06:22,  1.21s/it]\u001b[A\n",
      "batch 449, training loss: 5.0622: : 448it [06:23,  1.21s/it]\u001b[A\n",
      "batch 449, training loss: 5.0622: : 449it [06:23,  1.21s/it]\u001b[A\n",
      "batch 450, training loss: 5.085: : 449it [06:24,  1.21s/it] \u001b[A\n",
      "batch 450, training loss: 5.085: : 450it [06:24,  1.19s/it]\u001b[A\n",
      "batch 451, training loss: 5.0862: : 450it [06:25,  1.19s/it]\u001b[A\n",
      "batch 451, training loss: 5.0862: : 451it [06:25,  1.16s/it]\u001b[A\n",
      "batch 452, training loss: 5.1299: : 451it [06:26,  1.16s/it]\u001b[A\n",
      "batch 452, training loss: 5.1299: : 452it [06:26,  1.19s/it]\u001b[A\n",
      "batch 453, training loss: 5.095: : 452it [06:27,  1.19s/it] \u001b[A\n",
      "batch 453, training loss: 5.095: : 453it [06:27,  1.20s/it]\u001b[A\n",
      "batch 454, training loss: 5.0009: : 453it [06:29,  1.20s/it]\u001b[A\n",
      "batch 454, training loss: 5.0009: : 454it [06:29,  1.20s/it]\u001b[A\n",
      "batch 455, training loss: 4.9153: : 454it [06:30,  1.20s/it]\u001b[A\n",
      "batch 455, training loss: 4.9153: : 455it [06:30,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.9485: : 455it [06:31,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.9485: : 456it [06:31,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 5.1201: : 456it [06:32,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 5.1201: : 457it [06:32,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 4.9253: : 457it [06:34,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 4.9253: : 458it [06:34,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.9959: : 458it [06:35,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.9959: : 459it [06:35,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 5.0418: : 459it [06:36,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 5.0418: : 460it [06:36,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 5.1406: : 460it [06:37,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 5.1406: : 461it [06:37,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 5.0011: : 461it [06:39,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 5.0011: : 462it [06:39,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 4.9453: : 462it [06:40,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 4.9453: : 463it [06:40,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 4.9787: : 463it [06:41,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 4.9787: : 464it [06:41,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 4.9732: : 464it [06:42,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 4.9732: : 465it [06:42,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 4.9882: : 465it [06:43,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 4.9882: : 466it [06:43,  1.19s/it]\u001b[A\n",
      "batch 467, training loss: 4.933: : 466it [06:44,  1.19s/it] \u001b[A\n",
      "batch 467, training loss: 4.933: : 467it [06:44,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 5.0137: : 467it [06:46,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 5.0137: : 468it [06:46,  1.23s/it]\u001b[A\n",
      "batch 469, training loss: 4.9675: : 468it [06:47,  1.23s/it]\u001b[A\n",
      "batch 469, training loss: 4.9675: : 469it [06:47,  1.22s/it]\u001b[A\n",
      "batch 470, training loss: 5.0604: : 469it [06:48,  1.22s/it]\u001b[A\n",
      "batch 470, training loss: 5.0604: : 470it [06:48,  1.25s/it]\u001b[A\n",
      "batch 471, training loss: 5.1069: : 470it [06:49,  1.25s/it]\u001b[A\n",
      "batch 471, training loss: 5.1069: : 471it [06:49,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 5.0344: : 471it [06:51,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 5.0344: : 472it [06:51,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 5.0101: : 472it [06:52,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 5.0101: : 473it [06:52,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 5.1153: : 473it [06:53,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 5.1153: : 474it [06:53,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 5.0231: : 474it [06:55,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 5.0231: : 475it [06:55,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 5.1016: : 475it [06:56,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 5.1016: : 476it [06:56,  1.27s/it]\u001b[A\n",
      "batch 477, training loss: 4.8934: : 476it [06:57,  1.27s/it]\u001b[A\n",
      "batch 477, training loss: 4.8934: : 477it [06:57,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.9748: : 477it [06:58,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.9748: : 478it [06:58,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.9551: : 478it [07:00,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.9551: : 479it [07:00,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 4.9446: : 479it [07:01,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 4.9446: : 480it [07:01,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 5.0105: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 5.0105: : 481it [07:02,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 4.9952: : 481it [07:03,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 4.9952: : 482it [07:03,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 4.8997: : 482it [07:05,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 4.8997: : 483it [07:05,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.9472: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.9472: : 484it [07:06,  1.16s/it]\u001b[A\n",
      "batch 485, training loss: 4.929: : 484it [07:07,  1.16s/it] \u001b[A\n",
      "batch 485, training loss: 4.929: : 485it [07:07,  1.17s/it]\u001b[A\n",
      "batch 486, training loss: 4.9801: : 485it [07:08,  1.17s/it]\u001b[A\n",
      "batch 486, training loss: 4.9801: : 486it [07:08,  1.21s/it]\u001b[A\n",
      "batch 487, training loss: 5.0712: : 486it [07:09,  1.21s/it]\u001b[A\n",
      "batch 487, training loss: 5.0712: : 487it [07:09,  1.22s/it]\u001b[A\n",
      "batch 488, training loss: 4.9319: : 487it [07:11,  1.22s/it]\u001b[A\n",
      "batch 488, training loss: 4.9319: : 488it [07:11,  1.24s/it]\u001b[A\n",
      "batch 489, training loss: 4.8082: : 488it [07:12,  1.24s/it]\u001b[A\n",
      "batch 489, training loss: 4.8082: : 489it [07:12,  1.24s/it]\u001b[A\n",
      "batch 490, training loss: 4.901: : 489it [07:13,  1.24s/it] \u001b[A\n",
      "batch 490, training loss: 4.901: : 490it [07:13,  1.24s/it]\u001b[A\n",
      "batch 491, training loss: 4.8814: : 490it [07:14,  1.24s/it]\u001b[A\n",
      "batch 491, training loss: 4.8814: : 491it [07:14,  1.25s/it]\u001b[A\n",
      "batch 492, training loss: 4.856: : 491it [07:16,  1.25s/it] \u001b[A\n",
      "batch 492, training loss: 4.856: : 492it [07:16,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 4.9693: : 492it [07:17,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 4.9693: : 493it [07:17,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 5.0059: : 493it [07:18,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 494, training loss: 5.0059: : 494it [07:18,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 4.7842: : 494it [07:19,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 4.7842: : 495it [07:19,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 4.9042: : 495it [07:21,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 4.9042: : 496it [07:21,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 4.7877: : 496it [07:22,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 4.7877: : 497it [07:22,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 4.8738: : 497it [07:23,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 4.8738: : 498it [07:23,  1.25s/it]\u001b[A\n",
      "batch 499, training loss: 4.9336: : 498it [07:24,  1.25s/it]\u001b[A\n",
      "batch 499, training loss: 4.9336: : 499it [07:24,  1.11s/it]\u001b[A\n",
      "batch 500, training loss: 5.0825: : 499it [07:25,  1.11s/it]\u001b[A\n",
      "batch 500, training loss: 5.0825: : 500it [07:25,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 4.9643: : 500it [07:27,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 4.9643: : 501it [07:27,  1.20s/it]\u001b[A\n",
      "batch 502, training loss: 4.8705: : 501it [07:28,  1.20s/it]\u001b[A\n",
      "batch 502, training loss: 4.8705: : 502it [07:28,  1.24s/it]\u001b[A\n",
      "batch 503, training loss: 4.9941: : 502it [07:29,  1.24s/it]\u001b[A\n",
      "batch 503, training loss: 4.9941: : 503it [07:29,  1.25s/it]\u001b[A\n",
      "batch 504, training loss: 4.9874: : 503it [07:30,  1.25s/it]\u001b[A\n",
      "batch 504, training loss: 4.9874: : 504it [07:30,  1.27s/it]\u001b[A\n",
      "batch 505, training loss: 5.0458: : 504it [07:32,  1.27s/it]\u001b[A\n",
      "batch 505, training loss: 5.0458: : 505it [07:32,  1.27s/it]\u001b[A\n",
      "batch 506, training loss: 5.0009: : 505it [07:33,  1.27s/it]\u001b[A\n",
      "batch 506, training loss: 5.0009: : 506it [07:33,  1.28s/it]\u001b[A\n",
      "batch 507, training loss: 4.865: : 506it [07:34,  1.28s/it] \u001b[A\n",
      "batch 507, training loss: 4.865: : 507it [07:34,  1.27s/it]\u001b[A\n",
      "batch 508, training loss: 4.9592: : 507it [07:36,  1.27s/it]\u001b[A\n",
      "batch 508, training loss: 4.9592: : 508it [07:36,  1.29s/it]\u001b[A\n",
      "batch 509, training loss: 4.8927: : 508it [07:37,  1.29s/it]\u001b[A\n",
      "batch 509, training loss: 4.8927: : 509it [07:37,  1.28s/it]\u001b[A\n",
      "batch 510, training loss: 4.9547: : 509it [07:38,  1.28s/it]\u001b[A\n",
      "batch 510, training loss: 4.9547: : 510it [07:38,  1.29s/it]\u001b[A\n",
      "batch 511, training loss: 4.9595: : 510it [07:39,  1.29s/it]\u001b[A\n",
      "batch 511, training loss: 4.9595: : 511it [07:39,  1.28s/it]\u001b[A\n",
      "batch 512, training loss: 4.9417: : 511it [07:41,  1.28s/it]\u001b[A\n",
      "batch 512, training loss: 4.9417: : 512it [07:41,  1.29s/it]\u001b[A\n",
      "batch 513, training loss: 4.9803: : 512it [07:42,  1.29s/it]\u001b[A\n",
      "batch 513, training loss: 4.9803: : 513it [07:42,  1.28s/it]\u001b[A\n",
      "batch 514, training loss: 4.817: : 513it [07:43,  1.28s/it] \u001b[A\n",
      "batch 514, training loss: 4.817: : 514it [07:43,  1.29s/it]\u001b[A\n",
      "batch 515, training loss: 5.0179: : 514it [07:45,  1.29s/it]\u001b[A\n",
      "batch 515, training loss: 5.0179: : 515it [07:45,  1.28s/it]\u001b[A\n",
      "batch 516, training loss: 4.8973: : 515it [07:46,  1.28s/it]\u001b[A\n",
      "batch 516, training loss: 4.8973: : 516it [07:46,  1.29s/it]\u001b[A\n",
      "batch 517, training loss: 4.8499: : 516it [07:47,  1.29s/it]\u001b[A\n",
      "batch 517, training loss: 4.8499: : 517it [07:47,  1.28s/it]\u001b[A\n",
      "batch 518, training loss: 4.8834: : 517it [07:48,  1.28s/it]\u001b[A\n",
      "batch 518, training loss: 4.8834: : 518it [07:48,  1.29s/it]\u001b[A\n",
      "batch 519, training loss: 4.7592: : 518it [07:50,  1.29s/it]\u001b[A\n",
      "batch 519, training loss: 4.7592: : 519it [07:50,  1.32s/it]\u001b[A\n",
      "batch 520, training loss: 4.9217: : 519it [07:51,  1.32s/it]\u001b[A\n",
      "batch 520, training loss: 4.9217: : 520it [07:51,  1.33s/it]\u001b[A\n",
      "batch 521, training loss: 4.9189: : 520it [07:53,  1.33s/it]\u001b[A\n",
      "batch 521, training loss: 4.9189: : 521it [07:53,  1.35s/it]\u001b[A\n",
      "batch 522, training loss: 4.991: : 521it [07:54,  1.35s/it] \u001b[A\n",
      "batch 522, training loss: 4.991: : 522it [07:54,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 4.8407: : 522it [07:55,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 4.8407: : 523it [07:55,  1.34s/it]\u001b[A\n",
      "batch 524, training loss: 5.0133: : 523it [07:57,  1.34s/it]\u001b[A\n",
      "batch 524, training loss: 5.0133: : 524it [07:57,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 4.9844: : 524it [07:58,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 4.9844: : 525it [07:58,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.8384: : 525it [07:59,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.8384: : 526it [07:59,  1.35s/it]\u001b[A\n",
      "batch 527, training loss: 4.7327: : 526it [08:01,  1.35s/it]\u001b[A\n",
      "batch 527, training loss: 4.7327: : 527it [08:01,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.9208: : 527it [08:02,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.9208: : 528it [08:02,  1.36s/it]\u001b[A\n",
      "batch 529, training loss: 4.8957: : 528it [08:04,  1.36s/it]\u001b[A\n",
      "batch 529, training loss: 4.8957: : 529it [08:04,  1.37s/it]\u001b[A\n",
      "batch 530, training loss: 4.9544: : 529it [08:05,  1.37s/it]\u001b[A\n",
      "batch 530, training loss: 4.9544: : 530it [08:05,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 4.8315: : 530it [08:07,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 4.8315: : 531it [08:07,  1.46s/it]\u001b[A\n",
      "batch 532, training loss: 4.7146: : 531it [08:08,  1.46s/it]\u001b[A\n",
      "batch 532, training loss: 4.7146: : 532it [08:08,  1.46s/it]\u001b[A\n",
      "batch 533, training loss: 4.8913: : 532it [08:10,  1.46s/it]\u001b[A\n",
      "batch 533, training loss: 4.8913: : 533it [08:10,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.9426: : 533it [08:11,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.9426: : 534it [08:11,  1.47s/it]\u001b[A\n",
      "batch 535, training loss: 4.784: : 534it [08:12,  1.47s/it] \u001b[A\n",
      "batch 535, training loss: 4.784: : 535it [08:12,  1.47s/it]\u001b[A\n",
      "batch 536, training loss: 4.708: : 535it [08:14,  1.47s/it]\u001b[A\n",
      "batch 536, training loss: 4.708: : 536it [08:14,  1.49s/it]\u001b[A\n",
      "batch 537, training loss: 4.72: : 536it [08:15,  1.49s/it] \u001b[A\n",
      "batch 537, training loss: 4.72: : 537it [08:15,  1.47s/it]\u001b[A\n",
      "batch 538, training loss: 4.898: : 537it [08:17,  1.47s/it]\u001b[A\n",
      "batch 538, training loss: 4.898: : 538it [08:17,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 4.8121: : 538it [08:18,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 4.8121: : 539it [08:18,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 4.7752: : 539it [08:20,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 4.7752: : 540it [08:20,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.8553: : 540it [08:21,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.8553: : 541it [08:21,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.8375: : 541it [08:23,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.8375: : 542it [08:23,  1.45s/it]\u001b[A\n",
      "batch 543, training loss: 4.8007: : 542it [08:24,  1.45s/it]\u001b[A\n",
      "batch 543, training loss: 4.8007: : 543it [08:24,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 4.8369: : 543it [08:26,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 4.8369: : 544it [08:26,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 4.8502: : 544it [08:27,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 4.8502: : 545it [08:27,  1.51s/it]\u001b[A\n",
      "batch 546, training loss: 4.7289: : 545it [08:29,  1.51s/it]\u001b[A\n",
      "batch 546, training loss: 4.7289: : 546it [08:29,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 4.6601: : 546it [08:30,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 4.6601: : 547it [08:30,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 4.8073: : 547it [08:31,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 4.8073: : 548it [08:31,  1.27s/it]\u001b[A\n",
      "batch 549, training loss: 4.8194: : 548it [08:33,  1.27s/it]\u001b[A\n",
      "batch 549, training loss: 4.8194: : 549it [08:33,  1.37s/it]\u001b[A\n",
      "batch 550, training loss: 4.6852: : 549it [08:34,  1.37s/it]\u001b[A\n",
      "batch 550, training loss: 4.6852: : 550it [08:34,  1.44s/it]\u001b[A\n",
      "batch 551, training loss: 4.776: : 550it [08:36,  1.44s/it] \u001b[A\n",
      "batch 551, training loss: 4.776: : 551it [08:36,  1.50s/it]\u001b[A\n",
      "batch 552, training loss: 4.7746: : 551it [08:38,  1.50s/it]\u001b[A\n",
      "batch 552, training loss: 4.7746: : 552it [08:38,  1.52s/it]\u001b[A\n",
      "batch 553, training loss: 4.8185: : 552it [08:39,  1.52s/it]\u001b[A\n",
      "batch 553, training loss: 4.8185: : 553it [08:39,  1.55s/it]\u001b[A\n",
      "batch 554, training loss: 4.5233: : 553it [08:40,  1.55s/it]\u001b[A\n",
      "batch 554, training loss: 4.5233: : 554it [08:40,  1.43s/it]\u001b[A\n",
      "batch 555, training loss: 4.7033: : 554it [08:42,  1.43s/it]\u001b[A\n",
      "batch 555, training loss: 4.7033: : 555it [08:42,  1.46s/it]\u001b[A\n",
      "batch 556, training loss: 4.8583: : 555it [08:43,  1.46s/it]\u001b[A\n",
      "batch 556, training loss: 4.8583: : 556it [08:43,  1.49s/it]\u001b[A\n",
      "batch 557, training loss: 4.6267: : 556it [08:45,  1.49s/it]\u001b[A\n",
      "batch 557, training loss: 4.6267: : 557it [08:45,  1.51s/it]\u001b[A\n",
      "batch 558, training loss: 4.5908: : 557it [08:47,  1.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 558, training loss: 4.5908: : 558it [08:47,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 4.6704: : 558it [08:48,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 4.6704: : 559it [08:48,  1.57s/it]\u001b[A\n",
      "batch 560, training loss: 4.6766: : 559it [08:50,  1.57s/it]\u001b[A\n",
      "batch 560, training loss: 4.6766: : 560it [08:50,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 4.6754: : 560it [08:51,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 4.6754: : 561it [08:51,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 4.5309: : 561it [08:53,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 4.5309: : 562it [08:53,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 4.6521: : 562it [08:55,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 4.6521: : 563it [08:55,  1.61s/it]\u001b[A\n",
      "batch 564, training loss: 4.606: : 563it [08:56,  1.61s/it] \u001b[A\n",
      "batch 564, training loss: 4.606: : 564it [08:56,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 4.6414: : 564it [08:58,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 4.6414: : 565it [08:58,  1.57s/it]\u001b[A\n",
      "batch 566, training loss: 4.8033: : 565it [08:59,  1.57s/it]\u001b[A\n",
      "batch 566, training loss: 4.8033: : 566it [08:59,  1.60s/it]\u001b[A\n",
      "batch 567, training loss: 4.725: : 566it [09:01,  1.60s/it] \u001b[A\n",
      "batch 567, training loss: 4.725: : 567it [09:01,  1.60s/it]\u001b[A\n",
      "batch 568, training loss: 4.8306: : 567it [09:03,  1.60s/it]\u001b[A\n",
      "batch 568, training loss: 4.8306: : 568it [09:03,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 4.7753: : 568it [09:04,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 4.7753: : 569it [09:04,  1.63s/it]\u001b[A\n",
      "batch 570, training loss: 4.959: : 569it [09:06,  1.63s/it] \u001b[A\n",
      "batch 570, training loss: 4.959: : 570it [09:06,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 4.7967: : 570it [09:08,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 4.7967: : 571it [09:08,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 4.8049: : 571it [09:09,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 4.8049: : 572it [09:09,  1.63s/it]\u001b[A\n",
      "batch 573, training loss: 4.7866: : 572it [09:11,  1.63s/it]\u001b[A\n",
      "batch 573, training loss: 4.7866: : 573it [09:11,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 4.8007: : 573it [09:13,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 4.8007: : 574it [09:13,  1.64s/it]\u001b[A\n",
      "batch 575, training loss: 4.7659: : 574it [09:13,  1.64s/it]\u001b[A\n",
      "batch 575, training loss: 4.7659: : 575it [09:13,  1.41s/it]\u001b[A\n",
      "batch 576, training loss: 4.7611: : 575it [09:15,  1.41s/it]\u001b[A\n",
      "batch 576, training loss: 4.7611: : 576it [09:15,  1.50s/it]\u001b[A\n",
      "batch 577, training loss: 4.6862: : 576it [09:17,  1.50s/it]\u001b[A\n",
      "batch 577, training loss: 4.6862: : 577it [09:17,  1.55s/it]\u001b[A\n",
      "batch 578, training loss: 4.6955: : 577it [09:19,  1.55s/it]\u001b[A\n",
      "batch 578, training loss: 4.6955: : 578it [09:19,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 4.6686: : 578it [09:20,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 4.6686: : 579it [09:20,  1.64s/it]\u001b[A\n",
      "batch 580, training loss: 4.644: : 579it [09:22,  1.64s/it] \u001b[A\n",
      "batch 580, training loss: 4.644: : 580it [09:22,  1.66s/it]\u001b[A\n",
      "batch 581, training loss: 4.6226: : 580it [09:24,  1.66s/it]\u001b[A\n",
      "batch 581, training loss: 4.6226: : 581it [09:24,  1.71s/it]\u001b[A\n",
      "batch 582, training loss: 4.6493: : 581it [09:26,  1.71s/it]\u001b[A\n",
      "batch 582, training loss: 4.6493: : 582it [09:26,  1.73s/it]\u001b[A\n",
      "batch 583, training loss: 4.666: : 582it [09:26,  1.73s/it] \u001b[A\n",
      "batch 583, training loss: 4.666: : 583it [09:26,  1.40s/it]\u001b[A\n",
      "batch 584, training loss: 4.8296: : 583it [09:28,  1.40s/it]\u001b[A\n",
      "batch 584, training loss: 4.8296: : 584it [09:28,  1.55s/it]\u001b[A\n",
      "batch 585, training loss: 4.7985: : 584it [09:30,  1.55s/it]\u001b[A\n",
      "batch 585, training loss: 4.7985: : 585it [09:30,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 4.8753: : 585it [09:32,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 4.8753: : 586it [09:32,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 4.7771: : 586it [09:34,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 4.7771: : 587it [09:34,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 4.6661: : 587it [09:36,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 4.6661: : 588it [09:36,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 4.7007: : 588it [09:38,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 4.7007: : 589it [09:38,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 4.7853: : 589it [09:40,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 4.7853: : 590it [09:40,  1.89s/it]\u001b[A\n",
      "batch 591, training loss: 4.5416: : 590it [09:41,  1.89s/it]\u001b[A\n",
      "batch 591, training loss: 4.5416: : 591it [09:41,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 4.6401: : 591it [09:43,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 4.6401: : 592it [09:43,  1.88s/it]\u001b[A\n",
      "batch 593, training loss: 4.6183: : 592it [09:45,  1.88s/it]\u001b[A\n",
      "batch 593, training loss: 4.6183: : 593it [09:45,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 4.7554: : 593it [09:47,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 4.7554: : 594it [09:47,  1.97s/it]\u001b[A\n",
      "batch 595, training loss: 4.8342: : 594it [09:49,  1.97s/it]\u001b[A\n",
      "batch 595, training loss: 4.8342: : 595it [09:49,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 4.6668: : 595it [09:51,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 4.6668: : 596it [09:51,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 4.5273: : 596it [09:53,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 4.5273: : 597it [09:53,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 4.8032: : 597it [09:55,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 4.8032: : 598it [09:55,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 4.7326: : 598it [09:57,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 4.7326: : 599it [09:57,  1.83s/it]\u001b[A\n",
      "batch 600, training loss: 4.4619: : 599it [09:59,  1.83s/it]\u001b[A\n",
      "batch 600, training loss: 4.4619: : 600it [09:59,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 4.2469: : 600it [10:00,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 4.2469: : 601it [10:00,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 4.6065: : 601it [10:02,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 4.6065: : 602it [10:02,  1.77s/it]\u001b[A\n",
      "batch 603, training loss: 4.5951: : 602it [10:04,  1.77s/it]\u001b[A\n",
      "batch 603, training loss: 4.5951: : 603it [10:04,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 4.7734: : 603it [10:06,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 4.7734: : 604it [10:06,  1.83s/it]\u001b[A\n",
      "batch 605, training loss: 4.8086: : 604it [10:08,  1.83s/it]\u001b[A\n",
      "batch 605, training loss: 4.8086: : 605it [10:08,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 4.724: : 605it [10:09,  1.78s/it] \u001b[A\n",
      "batch 606, training loss: 4.724: : 606it [10:09,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 4.5441: : 606it [10:11,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 4.5441: : 607it [10:11,  1.67s/it]\u001b[A\n",
      "batch 608, training loss: 4.94: : 607it [10:12,  1.67s/it]  \u001b[A\n",
      "batch 608, training loss: 4.94: : 608it [10:12,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.8437: : 608it [10:13,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.8437: : 609it [10:13,  1.49s/it]\u001b[A\n",
      "batch 610, training loss: 4.4623: : 609it [10:15,  1.49s/it]\u001b[A\n",
      "batch 610, training loss: 4.4623: : 610it [10:15,  1.42s/it]\u001b[A\n",
      "batch 611, training loss: 4.5192: : 610it [10:15,  1.42s/it]\u001b[A\n",
      "batch 611, training loss: 4.5192: : 611it [10:15,  1.26s/it]\u001b[A\n",
      "batch 612, training loss: 3.6711: : 611it [10:16,  1.26s/it]\u001b[A\n",
      "batch 612, training loss: 3.6711: : 612it [10:16,  1.12s/it]\u001b[A\n",
      "batch 613, training loss: 4.5574: : 612it [10:17,  1.12s/it]\u001b[A\n",
      "batch 613, training loss: 4.5574: : 613it [10:17,  1.03it/s]\u001b[A\n",
      "batch 613, training loss: 4.5574: : 616it [10:17,  1.00s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 4.8293: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 4.8293: : 1it [00:00,  4.13it/s]\u001b[A\n",
      "batch 1, dev loss: 5.032: : 1it [00:00,  4.13it/s] \u001b[A\n",
      "batch 1, dev loss: 5.032: : 2it [00:00,  4.96it/s]\u001b[A\n",
      "batch 2, dev loss: 4.6468: : 2it [00:00,  4.96it/s]\u001b[A\n",
      "batch 2, dev loss: 4.6468: : 3it [00:00,  5.32it/s]\u001b[A\n",
      "batch 3, dev loss: 4.7706: : 3it [00:00,  5.32it/s]\u001b[A\n",
      "batch 3, dev loss: 4.7706: : 4it [00:00,  5.37it/s]\u001b[A\n",
      "batch 4, dev loss: 4.7663: : 4it [00:00,  5.37it/s]\u001b[A\n",
      "batch 4, dev loss: 4.7663: : 5it [00:00,  5.46it/s]\u001b[A\n",
      "batch 5, dev loss: 4.7719: : 5it [00:01,  5.46it/s]\u001b[A\n",
      "batch 5, dev loss: 4.7719: : 6it [00:01,  5.52it/s]\u001b[A\n",
      "batch 6, dev loss: 4.89: : 6it [00:01,  5.52it/s]  \u001b[A\n",
      "batch 6, dev loss: 4.89: : 7it [00:01,  5.14it/s]\u001b[A\n",
      "batch 7, dev loss: 4.7037: : 7it [00:01,  5.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7, dev loss: 4.7037: : 8it [00:01,  5.16it/s]\u001b[A\n",
      "batch 8, dev loss: 4.8893: : 8it [00:01,  5.16it/s]\u001b[A\n",
      "batch 8, dev loss: 4.8893: : 9it [00:01,  5.42it/s]\u001b[A\n",
      "batch 9, dev loss: 4.7449: : 9it [00:01,  5.42it/s]\u001b[A\n",
      "batch 9, dev loss: 4.7449: : 10it [00:01,  5.48it/s]\u001b[A\n",
      "batch 10, dev loss: 4.8406: : 10it [00:02,  5.48it/s]\u001b[A\n",
      "batch 10, dev loss: 4.8406: : 11it [00:02,  5.44it/s]\u001b[A\n",
      "batch 11, dev loss: 4.9143: : 11it [00:02,  5.44it/s]\u001b[A\n",
      "batch 11, dev loss: 4.9143: : 12it [00:02,  5.36it/s]\u001b[A\n",
      "batch 12, dev loss: 4.781: : 12it [00:02,  5.36it/s] \u001b[A\n",
      "batch 12, dev loss: 4.781: : 13it [00:02,  5.33it/s]\u001b[A\n",
      "batch 13, dev loss: 4.8994: : 13it [00:02,  5.33it/s]\u001b[A\n",
      "batch 13, dev loss: 4.8994: : 14it [00:02,  4.81it/s]\u001b[A\n",
      "batch 14, dev loss: 4.9403: : 14it [00:02,  4.81it/s]\u001b[A\n",
      "batch 14, dev loss: 4.9403: : 15it [00:02,  4.98it/s]\u001b[A\n",
      "batch 15, dev loss: 4.9278: : 15it [00:02,  4.98it/s]\u001b[A\n",
      "batch 15, dev loss: 4.9278: : 16it [00:02,  5.82it/s]\u001b[A\n",
      "batch 16, dev loss: 5.0476: : 16it [00:03,  5.82it/s]\u001b[A\n",
      "batch 16, dev loss: 5.0476: : 17it [00:03,  5.33it/s]\u001b[A\n",
      "batch 17, dev loss: 4.8491: : 17it [00:03,  5.33it/s]\u001b[A\n",
      "batch 17, dev loss: 4.8491: : 18it [00:03,  5.18it/s]\u001b[A\n",
      "batch 18, dev loss: 4.7745: : 18it [00:03,  5.18it/s]\u001b[A\n",
      "batch 18, dev loss: 4.7745: : 19it [00:03,  5.01it/s]\u001b[A\n",
      "batch 19, dev loss: 4.9424: : 19it [00:03,  5.01it/s]\u001b[A\n",
      "batch 19, dev loss: 4.9424: : 20it [00:03,  5.09it/s]\u001b[A\n",
      "batch 20, dev loss: 4.761: : 20it [00:04,  5.09it/s] \u001b[A\n",
      "batch 20, dev loss: 4.761: : 21it [00:04,  5.06it/s]\u001b[A\n",
      "batch 21, dev loss: 4.6895: : 21it [00:04,  5.06it/s]\u001b[A\n",
      "batch 21, dev loss: 4.6895: : 22it [00:04,  4.94it/s]\u001b[A\n",
      "batch 22, dev loss: 4.833: : 22it [00:04,  4.94it/s] \u001b[A\n",
      "batch 22, dev loss: 4.833: : 23it [00:04,  4.91it/s]\u001b[A\n",
      "batch 23, dev loss: 4.8232: : 23it [00:04,  4.91it/s]\u001b[A\n",
      "batch 23, dev loss: 4.8232: : 24it [00:04,  5.38it/s]\u001b[A\n",
      "batch 24, dev loss: 4.8216: : 24it [00:04,  5.38it/s]\u001b[A\n",
      "batch 24, dev loss: 4.8216: : 25it [00:04,  4.69it/s]\u001b[A\n",
      "batch 25, dev loss: 4.8373: : 25it [00:05,  4.69it/s]\u001b[A\n",
      "batch 25, dev loss: 4.8373: : 26it [00:05,  4.53it/s]\u001b[A\n",
      "batch 26, dev loss: 4.8001: : 26it [00:05,  4.53it/s]\u001b[A\n",
      "batch 26, dev loss: 4.8001: : 27it [00:05,  4.58it/s]\u001b[A\n",
      "batch 27, dev loss: 4.7419: : 27it [00:05,  4.58it/s]\u001b[A\n",
      "batch 27, dev loss: 4.7419: : 28it [00:05,  4.37it/s]\u001b[A\n",
      "batch 28, dev loss: 4.8898: : 28it [00:05,  4.37it/s]\u001b[A\n",
      "batch 28, dev loss: 4.8898: : 29it [00:05,  4.19it/s]\u001b[A\n",
      "batch 29, dev loss: 4.833: : 29it [00:06,  4.19it/s] \u001b[A\n",
      "batch 29, dev loss: 4.833: : 30it [00:06,  3.83it/s]\u001b[A\n",
      "batch 30, dev loss: 4.968: : 30it [00:06,  3.83it/s]\u001b[A\n",
      "batch 30, dev loss: 4.968: : 31it [00:06,  4.53it/s]\u001b[A\n",
      "batch 31, dev loss: 4.8981: : 31it [00:06,  4.53it/s]\u001b[A\n",
      "batch 31, dev loss: 4.8981: : 32it [00:06,  4.29it/s]\u001b[A\n",
      "batch 32, dev loss: 4.8819: : 32it [00:06,  4.29it/s]\u001b[A\n",
      "batch 32, dev loss: 4.8819: : 33it [00:06,  4.14it/s]\u001b[A\n",
      "batch 33, dev loss: 4.6663: : 33it [00:07,  4.14it/s]\u001b[A\n",
      "batch 33, dev loss: 4.6663: : 34it [00:07,  4.17it/s]\u001b[A\n",
      "batch 34, dev loss: 5.0768: : 34it [00:07,  4.17it/s]\u001b[A\n",
      "batch 34, dev loss: 5.0768: : 35it [00:07,  4.00it/s]\u001b[A\n",
      "batch 35, dev loss: 4.9034: : 35it [00:07,  4.00it/s]\u001b[A\n",
      "batch 35, dev loss: 4.9034: : 36it [00:07,  3.94it/s]\u001b[A\n",
      "batch 36, dev loss: 4.825: : 36it [00:07,  3.94it/s] \u001b[A\n",
      "batch 36, dev loss: 4.825: : 37it [00:07,  4.26it/s]\u001b[A\n",
      "batch 37, dev loss: 4.6953: : 37it [00:08,  4.26it/s]\u001b[A\n",
      "batch 37, dev loss: 4.6953: : 38it [00:08,  4.06it/s]\u001b[A\n",
      "batch 38, dev loss: 4.9157: : 38it [00:08,  4.06it/s]\u001b[A\n",
      "batch 38, dev loss: 4.9157: : 39it [00:08,  3.90it/s]\u001b[A\n",
      "batch 39, dev loss: 4.9041: : 39it [00:08,  3.90it/s]\u001b[A\n",
      "batch 39, dev loss: 4.9041: : 40it [00:08,  3.73it/s]\u001b[A\n",
      "batch 40, dev loss: 4.9552: : 40it [00:08,  3.73it/s]\u001b[A\n",
      "batch 40, dev loss: 4.9552: : 41it [00:08,  3.64it/s]\u001b[A\n",
      "batch 41, dev loss: 4.7066: : 41it [00:09,  3.64it/s]\u001b[A\n",
      "batch 41, dev loss: 4.7066: : 42it [00:09,  3.83it/s]\u001b[A\n",
      "batch 42, dev loss: 4.846: : 42it [00:09,  3.83it/s] \u001b[A\n",
      "batch 42, dev loss: 4.846: : 43it [00:09,  3.62it/s]\u001b[A\n",
      "batch 43, dev loss: 4.875: : 43it [00:09,  3.62it/s]\u001b[A\n",
      "batch 43, dev loss: 4.875: : 44it [00:09,  3.14it/s]\u001b[A\n",
      "batch 44, dev loss: 4.7844: : 44it [00:10,  3.14it/s]\u001b[A\n",
      "batch 44, dev loss: 4.7844: : 45it [00:10,  3.22it/s]\u001b[A\n",
      "batch 45, dev loss: 5.0379: : 45it [00:10,  3.22it/s]\u001b[A\n",
      "batch 45, dev loss: 5.0379: : 46it [00:10,  3.23it/s]\u001b[A\n",
      "batch 46, dev loss: 4.6311: : 46it [00:10,  3.23it/s]\u001b[A\n",
      "batch 46, dev loss: 4.6311: : 47it [00:10,  3.15it/s]\u001b[A\n",
      "batch 47, dev loss: 4.8213: : 47it [00:11,  3.15it/s]\u001b[A\n",
      "batch 47, dev loss: 4.8213: : 48it [00:11,  3.19it/s]\u001b[A\n",
      "batch 48, dev loss: 4.6179: : 48it [00:11,  3.19it/s]\u001b[A\n",
      "batch 48, dev loss: 4.6179: : 49it [00:11,  2.88it/s]\u001b[A\n",
      "batch 49, dev loss: 4.7407: : 49it [00:11,  2.88it/s]\u001b[A\n",
      "batch 49, dev loss: 4.7407: : 50it [00:11,  3.33it/s]\u001b[A\n",
      "batch 50, dev loss: 4.7858: : 50it [00:12,  3.33it/s]\u001b[A\n",
      "batch 50, dev loss: 4.7858: : 51it [00:12,  3.33it/s]\u001b[A\n",
      "batch 51, dev loss: 4.736: : 51it [00:12,  3.33it/s] \u001b[A\n",
      "batch 51, dev loss: 4.736: : 52it [00:12,  3.15it/s]\u001b[A\n",
      "batch 52, dev loss: 4.607: : 52it [00:12,  3.15it/s]\u001b[A\n",
      "batch 52, dev loss: 4.607: : 53it [00:12,  3.30it/s]\u001b[A\n",
      "batch 53, dev loss: 4.7958: : 53it [00:13,  3.30it/s]\u001b[A\n",
      "batch 53, dev loss: 4.7958: : 54it [00:13,  2.85it/s]\u001b[A\n",
      "batch 54, dev loss: 4.6175: : 54it [00:13,  2.85it/s]\u001b[A\n",
      "batch 54, dev loss: 4.6175: : 55it [00:13,  2.83it/s]\u001b[A\n",
      "batch 55, dev loss: 4.7205: : 55it [00:13,  2.83it/s]\u001b[A\n",
      "batch 55, dev loss: 4.7205: : 56it [00:13,  2.83it/s]\u001b[A\n",
      "batch 56, dev loss: 4.5681: : 56it [00:14,  2.83it/s]\u001b[A\n",
      "batch 56, dev loss: 4.5681: : 57it [00:14,  3.04it/s]\u001b[A\n",
      "batch 57, dev loss: 4.5297: : 57it [00:14,  3.04it/s]\u001b[A\n",
      "batch 57, dev loss: 4.5297: : 58it [00:14,  2.86it/s]\u001b[A\n",
      "batch 58, dev loss: 4.7821: : 58it [00:14,  2.86it/s]\u001b[A\n",
      "batch 58, dev loss: 4.7821: : 59it [00:14,  2.89it/s]\u001b[A\n",
      "batch 59, dev loss: 4.8168: : 59it [00:15,  2.89it/s]\u001b[A\n",
      "batch 59, dev loss: 4.8168: : 60it [00:15,  2.86it/s]\u001b[A\n",
      "batch 60, dev loss: 4.4467: : 60it [00:15,  2.86it/s]\u001b[A\n",
      "batch 60, dev loss: 4.4467: : 61it [00:15,  3.06it/s]\u001b[A\n",
      "batch 61, dev loss: 4.5143: : 61it [00:15,  3.06it/s]\u001b[A\n",
      "batch 61, dev loss: 4.5143: : 62it [00:15,  3.30it/s]\u001b[A\n",
      "batch 62, dev loss: 4.4357: : 62it [00:15,  3.30it/s]\u001b[A\n",
      "batch 62, dev loss: 4.4357: : 63it [00:15,  3.55it/s]\u001b[A\n",
      "batch 63, dev loss: 4.871: : 63it [00:16,  3.55it/s] \u001b[A\n",
      "batch 63, dev loss: 4.871: : 64it [00:16,  3.77it/s]\u001b[A\n",
      "batch 64, dev loss: 4.4556: : 64it [00:16,  3.77it/s]\u001b[A\n",
      "batch 64, dev loss: 4.4556: : 65it [00:16,  3.87it/s]\u001b[A\n",
      "batch 65, dev loss: 4.6146: : 65it [00:16,  3.87it/s]\u001b[A\n",
      "batch 65, dev loss: 4.6146: : 66it [00:16,  3.98it/s]\u001b[A\n",
      "batch 66, dev loss: 4.6825: : 66it [00:16,  3.98it/s]\u001b[A\n",
      "batch 66, dev loss: 4.6825: : 67it [00:16,  4.08it/s]\u001b[A\n",
      "batch 67, dev loss: 3.8484: : 67it [00:17,  4.08it/s]\u001b[A\n",
      "batch 67, dev loss: 3.8484: : 68it [00:17,  4.11it/s]\u001b[A\n",
      "batch 68, dev loss: 4.2108: : 68it [00:17,  4.11it/s]\u001b[A\n",
      "batch 68, dev loss: 4.2108: : 69it [00:17,  3.87it/s]\u001b[A\n",
      "batch 69, dev loss: 3.7687: : 69it [00:17,  3.87it/s]\u001b[A\n",
      "batch 69, dev loss: 3.7687: : 70it [00:17,  4.22it/s]\u001b[A\n",
      "batch 70, dev loss: 4.4846: : 70it [00:17,  4.22it/s]\u001b[A\n",
      "batch 70, dev loss: 4.4846: : 71it [00:17,  4.50it/s]\u001b[A\n",
      "batch 71, dev loss: 4.6541: : 71it [00:17,  4.50it/s]\u001b[A\n",
      "batch 71, dev loss: 4.6541: : 72it [00:17,  4.53it/s]\u001b[A\n",
      "batch 72, dev loss: 5.0915: : 72it [00:18,  4.53it/s]\u001b[A\n",
      "batch 72, dev loss: 5.0915: : 76it [00:18,  4.15it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:00,  1.17it/s]\u001b[A\n",
      "2it [00:01,  1.19it/s]\u001b[A\n",
      "3it [00:02,  1.16it/s]\u001b[A\n",
      "4it [00:03,  1.27it/s]\u001b[A\n",
      "5it [00:04,  1.21it/s]\u001b[A\n",
      "6it [00:04,  1.32it/s]\u001b[A\n",
      "7it [00:05,  1.28it/s]\u001b[A\n",
      "8it [00:06,  1.43it/s]\u001b[A\n",
      "9it [00:07,  1.31it/s]\u001b[A\n",
      "10it [00:07,  1.22it/s]\u001b[A\n",
      "11it [00:08,  1.17it/s]\u001b[A\n",
      "12it [00:09,  1.25it/s]\u001b[A\n",
      "13it [00:10,  1.21it/s]\u001b[A\n",
      "14it [00:11,  1.16it/s]\u001b[A\n",
      "15it [00:12,  1.14it/s]\u001b[A\n",
      "16it [00:13,  1.20it/s]\u001b[A\n",
      "17it [00:14,  1.12it/s]\u001b[A\n",
      "18it [00:15,  1.07it/s]\u001b[A\n",
      "19it [00:16,  1.04it/s]\u001b[A\n",
      "20it [00:17,  1.01it/s]\u001b[A\n",
      "21it [00:18,  1.02s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22it [00:19,  1.04it/s]\u001b[A\n",
      "23it [00:20,  1.01it/s]\u001b[A\n",
      "24it [00:20,  1.25it/s]\u001b[A\n",
      "25it [00:21,  1.17it/s]\u001b[A\n",
      "26it [00:22,  1.04it/s]\u001b[A\n",
      "27it [00:23,  1.05s/it]\u001b[A\n",
      "28it [00:25,  1.07s/it]\u001b[A\n",
      "29it [00:26,  1.17s/it]\u001b[A\n",
      "30it [00:27,  1.13s/it]\u001b[A\n",
      "31it [00:28,  1.16s/it]\u001b[A\n",
      "32it [00:30,  1.27s/it]\u001b[A\n",
      "33it [00:31,  1.37s/it]\u001b[A\n",
      "34it [00:33,  1.34s/it]\u001b[A\n",
      "35it [00:34,  1.30s/it]\u001b[A\n",
      "36it [00:34,  1.02it/s]\u001b[A\n",
      "37it [00:35,  1.10s/it]\u001b[A\n",
      "38it [00:37,  1.20s/it]\u001b[A\n",
      "39it [00:39,  1.32s/it]\u001b[A\n",
      "40it [00:40,  1.30s/it]\u001b[A\n",
      "41it [00:40,  1.02s/it]\u001b[A\n",
      "42it [00:42,  1.28s/it]\u001b[A\n",
      "43it [00:44,  1.46s/it]\u001b[A\n",
      "44it [00:46,  1.58s/it]\u001b[A\n",
      "45it [00:47,  1.54s/it]\u001b[A\n",
      "46it [00:49,  1.65s/it]\u001b[A\n",
      "47it [00:50,  1.54s/it]\u001b[A\n",
      "48it [00:52,  1.49s/it]\u001b[A\n",
      "49it [00:52,  1.08s/it]\u001b[A\n",
      "50it [00:53,  1.18s/it]\u001b[A\n",
      "51it [00:55,  1.24s/it]\u001b[A\n",
      "52it [00:55,  1.01it/s]\u001b[A\n",
      "53it [00:56,  1.17it/s]\u001b[A\n",
      "54it [00:57,  1.10it/s]\u001b[A\n",
      "55it [00:58,  1.12s/it]\u001b[A\n",
      "56it [00:59,  1.06s/it]\u001b[A\n",
      "57it [01:01,  1.19s/it]\u001b[A\n",
      "58it [01:02,  1.20s/it]\u001b[A\n",
      "59it [01:03,  1.14s/it]\u001b[A\n",
      "60it [01:04,  1.08s/it]\u001b[A\n",
      "61it [01:04,  1.08it/s]\u001b[A\n",
      "62it [01:05,  1.16it/s]\u001b[A\n",
      "63it [01:06,  1.37it/s]\u001b[A\n",
      "64it [01:06,  1.59it/s]\u001b[A\n",
      "65it [01:06,  1.81it/s]\u001b[A\n",
      "66it [01:07,  2.21it/s]\u001b[A\n",
      "67it [01:07,  2.66it/s]\u001b[A\n",
      "68it [01:07,  2.74it/s]\u001b[A\n",
      "69it [01:07,  2.76it/s]\u001b[A\n",
      "70it [01:08,  1.02it/s]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                       | 30/6740 [00:00<00:29, 225.46it/s]\u001b[A\n",
      " 10%|███▋                                  | 655/6740 [00:00<00:01, 3344.00it/s]\u001b[A\n",
      " 18%|██████▊                              | 1243/6740 [00:00<00:01, 4418.37it/s]\u001b[A\n",
      " 26%|█████████▌                           | 1751/6740 [00:00<00:01, 4667.28it/s]\u001b[A\n",
      " 34%|████████████▍                        | 2262/6740 [00:00<00:00, 4819.70it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 2758/6740 [00:00<00:00, 4851.45it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 3252/6740 [00:00<00:00, 4872.67it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 3746/6740 [00:00<00:00, 4872.44it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 4238/6740 [00:00<00:00, 4854.33it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 4728/6740 [00:01<00:00, 4864.51it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 5217/6740 [00:01<00:00, 4826.42it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 5711/6740 [00:01<00:00, 4854.30it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 6205/6740 [00:01<00:00, 4875.98it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 4646.93it/s]\u001b[A\n",
      "Epoch: 1, tfr: 1.0, loss(train/dev): 5.4208/4.7504, ppl(dev/test): 115.6305/127.\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 4.7323: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 4.7323: : 1it [00:01,  1.79s/it]\u001b[A\n",
      "batch 2, training loss: 4.6247: : 1it [00:02,  1.79s/it]\u001b[A\n",
      "batch 2, training loss: 4.6247: : 2it [00:02,  1.05s/it]\u001b[A\n",
      "batch 3, training loss: 4.737: : 2it [00:02,  1.05s/it] \u001b[A\n",
      "batch 3, training loss: 4.737: : 3it [00:02,  1.15it/s]\u001b[A\n",
      "batch 4, training loss: 4.7503: : 3it [00:03,  1.15it/s]\u001b[A\n",
      "batch 4, training loss: 4.7503: : 4it [00:03,  1.25it/s]\u001b[A\n",
      "batch 5, training loss: 4.5338: : 4it [00:04,  1.25it/s]\u001b[A\n",
      "batch 5, training loss: 4.5338: : 5it [00:04,  1.31it/s]\u001b[A\n",
      "batch 6, training loss: 4.7127: : 5it [00:05,  1.31it/s]\u001b[A\n",
      "batch 6, training loss: 4.7127: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 4.7214: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 4.7214: : 7it [00:05,  1.48it/s]\u001b[A\n",
      "batch 8, training loss: 4.7603: : 7it [00:06,  1.48it/s]\u001b[A\n",
      "batch 8, training loss: 4.7603: : 8it [00:06,  1.61it/s]\u001b[A\n",
      "batch 9, training loss: 4.572: : 8it [00:06,  1.61it/s] \u001b[A\n",
      "batch 9, training loss: 4.572: : 9it [00:06,  1.59it/s]\u001b[A\n",
      "batch 10, training loss: 4.6122: : 9it [00:07,  1.59it/s]\u001b[A\n",
      "batch 10, training loss: 4.6122: : 10it [00:07,  1.55it/s]\u001b[A\n",
      "batch 11, training loss: 4.6661: : 10it [00:08,  1.55it/s]\u001b[A\n",
      "batch 11, training loss: 4.6661: : 11it [00:08,  1.52it/s]\u001b[A\n",
      "batch 12, training loss: 4.6988: : 11it [00:08,  1.52it/s]\u001b[A\n",
      "batch 12, training loss: 4.6988: : 12it [00:08,  1.50it/s]\u001b[A\n",
      "batch 13, training loss: 4.5859: : 12it [00:09,  1.50it/s]\u001b[A\n",
      "batch 13, training loss: 4.5859: : 13it [00:09,  1.56it/s]\u001b[A\n",
      "batch 14, training loss: 4.8417: : 13it [00:09,  1.56it/s]\u001b[A\n",
      "batch 14, training loss: 4.8417: : 14it [00:09,  1.61it/s]\u001b[A\n",
      "batch 15, training loss: 4.7172: : 14it [00:10,  1.61it/s]\u001b[A\n",
      "batch 15, training loss: 4.7172: : 15it [00:10,  1.59it/s]\u001b[A\n",
      "batch 16, training loss: 4.7013: : 15it [00:11,  1.59it/s]\u001b[A\n",
      "batch 16, training loss: 4.7013: : 16it [00:11,  1.53it/s]\u001b[A\n",
      "batch 17, training loss: 4.8307: : 16it [00:12,  1.53it/s]\u001b[A\n",
      "batch 17, training loss: 4.8307: : 17it [00:12,  1.50it/s]\u001b[A\n",
      "batch 18, training loss: 4.7021: : 17it [00:12,  1.50it/s]\u001b[A\n",
      "batch 18, training loss: 4.7021: : 18it [00:12,  1.51it/s]\u001b[A\n",
      "batch 19, training loss: 4.5005: : 18it [00:13,  1.51it/s]\u001b[A\n",
      "batch 19, training loss: 4.5005: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 4.5972: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 4.5972: : 20it [00:13,  1.66it/s]\u001b[A\n",
      "batch 21, training loss: 4.6764: : 20it [00:14,  1.66it/s]\u001b[A\n",
      "batch 21, training loss: 4.6764: : 21it [00:14,  1.62it/s]\u001b[A\n",
      "batch 22, training loss: 4.4965: : 21it [00:15,  1.62it/s]\u001b[A\n",
      "batch 22, training loss: 4.4965: : 22it [00:15,  1.57it/s]\u001b[A\n",
      "batch 23, training loss: 4.6059: : 22it [00:15,  1.57it/s]\u001b[A\n",
      "batch 23, training loss: 4.6059: : 23it [00:15,  1.53it/s]\u001b[A\n",
      "batch 24, training loss: 4.5306: : 23it [00:16,  1.53it/s]\u001b[A\n",
      "batch 24, training loss: 4.5306: : 24it [00:16,  1.52it/s]\u001b[A\n",
      "batch 25, training loss: 4.6338: : 24it [00:17,  1.52it/s]\u001b[A\n",
      "batch 25, training loss: 4.6338: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 4.439: : 25it [00:17,  1.57it/s] \u001b[A\n",
      "batch 26, training loss: 4.439: : 26it [00:17,  1.62it/s]\u001b[A\n",
      "batch 27, training loss: 4.57: : 26it [00:18,  1.62it/s] \u001b[A\n",
      "batch 27, training loss: 4.57: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 4.5045: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 4.5045: : 28it [00:18,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 4.5909: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 4.5909: : 29it [00:19,  1.52it/s]\u001b[A\n",
      "batch 30, training loss: 4.6791: : 29it [00:20,  1.52it/s]\u001b[A\n",
      "batch 30, training loss: 4.6791: : 30it [00:20,  1.51it/s]\u001b[A\n",
      "batch 31, training loss: 4.4776: : 30it [00:20,  1.51it/s]\u001b[A\n",
      "batch 31, training loss: 4.4776: : 31it [00:20,  1.61it/s]\u001b[A\n",
      "batch 32, training loss: 4.6225: : 31it [00:21,  1.61it/s]\u001b[A\n",
      "batch 32, training loss: 4.6225: : 32it [00:21,  1.64it/s]\u001b[A\n",
      "batch 33, training loss: 4.5644: : 32it [00:22,  1.64it/s]\u001b[A\n",
      "batch 33, training loss: 4.5644: : 33it [00:22,  1.62it/s]\u001b[A\n",
      "batch 34, training loss: 4.5347: : 33it [00:22,  1.62it/s]\u001b[A\n",
      "batch 34, training loss: 4.5347: : 34it [00:22,  1.58it/s]\u001b[A\n",
      "batch 35, training loss: 4.6514: : 34it [00:23,  1.58it/s]\u001b[A\n",
      "batch 35, training loss: 4.6514: : 35it [00:23,  1.54it/s]\u001b[A\n",
      "batch 36, training loss: 4.6506: : 35it [00:24,  1.54it/s]\u001b[A\n",
      "batch 36, training loss: 4.6506: : 36it [00:24,  1.52it/s]\u001b[A\n",
      "batch 37, training loss: 4.4978: : 36it [00:24,  1.52it/s]\u001b[A\n",
      "batch 37, training loss: 4.4978: : 37it [00:24,  1.56it/s]\u001b[A\n",
      "batch 38, training loss: 4.3892: : 37it [00:25,  1.56it/s]\u001b[A\n",
      "batch 38, training loss: 4.3892: : 38it [00:25,  1.60it/s]\u001b[A\n",
      "batch 39, training loss: 4.4855: : 38it [00:25,  1.60it/s]\u001b[A\n",
      "batch 39, training loss: 4.4855: : 39it [00:25,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 4.6277: : 39it [00:26,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 4.6277: : 40it [00:26,  1.56it/s]\u001b[A\n",
      "batch 41, training loss: 4.7967: : 40it [00:27,  1.56it/s]\u001b[A\n",
      "batch 41, training loss: 4.7967: : 41it [00:27,  1.52it/s]\u001b[A\n",
      "batch 42, training loss: 4.6485: : 41it [00:27,  1.52it/s]\u001b[A\n",
      "batch 42, training loss: 4.6485: : 42it [00:27,  1.49it/s]\u001b[A\n",
      "batch 43, training loss: 4.4189: : 42it [00:28,  1.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 4.4189: : 43it [00:28,  1.49it/s]\u001b[A\n",
      "batch 44, training loss: 4.4102: : 43it [00:29,  1.49it/s]\u001b[A\n",
      "batch 44, training loss: 4.4102: : 44it [00:29,  1.49it/s]\u001b[A\n",
      "batch 45, training loss: 4.4206: : 44it [00:29,  1.49it/s]\u001b[A\n",
      "batch 45, training loss: 4.4206: : 45it [00:29,  1.55it/s]\u001b[A\n",
      "batch 46, training loss: 4.5884: : 45it [00:30,  1.55it/s]\u001b[A\n",
      "batch 46, training loss: 4.5884: : 46it [00:30,  1.60it/s]\u001b[A\n",
      "batch 47, training loss: 4.5493: : 46it [00:31,  1.60it/s]\u001b[A\n",
      "batch 47, training loss: 4.5493: : 47it [00:31,  1.59it/s]\u001b[A\n",
      "batch 48, training loss: 4.5258: : 47it [00:31,  1.59it/s]\u001b[A\n",
      "batch 48, training loss: 4.5258: : 48it [00:31,  1.53it/s]\u001b[A\n",
      "batch 49, training loss: 4.7103: : 48it [00:32,  1.53it/s]\u001b[A\n",
      "batch 49, training loss: 4.7103: : 49it [00:32,  1.52it/s]\u001b[A\n",
      "batch 50, training loss: 4.4844: : 49it [00:33,  1.52it/s]\u001b[A\n",
      "batch 50, training loss: 4.4844: : 50it [00:33,  1.48it/s]\u001b[A\n",
      "batch 51, training loss: 4.5642: : 50it [00:33,  1.48it/s]\u001b[A\n",
      "batch 51, training loss: 4.5642: : 51it [00:33,  1.47it/s]\u001b[A\n",
      "batch 52, training loss: 4.4806: : 51it [00:34,  1.47it/s]\u001b[A\n",
      "batch 52, training loss: 4.4806: : 52it [00:34,  1.46it/s]\u001b[A\n",
      "batch 53, training loss: 4.4865: : 52it [00:35,  1.46it/s]\u001b[A\n",
      "batch 53, training loss: 4.4865: : 53it [00:35,  1.45it/s]\u001b[A\n",
      "batch 54, training loss: 4.367: : 53it [00:35,  1.45it/s] \u001b[A\n",
      "batch 54, training loss: 4.367: : 54it [00:35,  1.51it/s]\u001b[A\n",
      "batch 55, training loss: 4.5381: : 54it [00:36,  1.51it/s]\u001b[A\n",
      "batch 55, training loss: 4.5381: : 55it [00:36,  1.55it/s]\u001b[A\n",
      "batch 56, training loss: 4.5561: : 55it [00:37,  1.55it/s]\u001b[A\n",
      "batch 56, training loss: 4.5561: : 56it [00:37,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 4.5134: : 56it [00:37,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 4.5134: : 57it [00:37,  1.56it/s]\u001b[A\n",
      "batch 58, training loss: 4.6058: : 57it [00:38,  1.56it/s]\u001b[A\n",
      "batch 58, training loss: 4.6058: : 58it [00:38,  1.51it/s]\u001b[A\n",
      "batch 59, training loss: 4.4159: : 58it [00:39,  1.51it/s]\u001b[A\n",
      "batch 59, training loss: 4.4159: : 59it [00:39,  1.49it/s]\u001b[A\n",
      "batch 60, training loss: 4.4206: : 59it [00:39,  1.49it/s]\u001b[A\n",
      "batch 60, training loss: 4.4206: : 60it [00:39,  1.61it/s]\u001b[A\n",
      "batch 61, training loss: 4.606: : 60it [00:40,  1.61it/s] \u001b[A\n",
      "batch 61, training loss: 4.606: : 61it [00:40,  1.63it/s]\u001b[A\n",
      "batch 62, training loss: 4.466: : 61it [00:40,  1.63it/s]\u001b[A\n",
      "batch 62, training loss: 4.466: : 62it [00:40,  1.62it/s]\u001b[A\n",
      "batch 63, training loss: 4.5761: : 62it [00:41,  1.62it/s]\u001b[A\n",
      "batch 63, training loss: 4.5761: : 63it [00:41,  1.59it/s]\u001b[A\n",
      "batch 64, training loss: 4.5288: : 63it [00:42,  1.59it/s]\u001b[A\n",
      "batch 64, training loss: 4.5288: : 64it [00:42,  1.55it/s]\u001b[A\n",
      "batch 65, training loss: 4.5747: : 64it [00:42,  1.55it/s]\u001b[A\n",
      "batch 65, training loss: 4.5747: : 65it [00:42,  1.54it/s]\u001b[A\n",
      "batch 66, training loss: 4.5432: : 65it [00:43,  1.54it/s]\u001b[A\n",
      "batch 66, training loss: 4.5432: : 66it [00:43,  1.56it/s]\u001b[A\n",
      "batch 67, training loss: 4.4732: : 66it [00:44,  1.56it/s]\u001b[A\n",
      "batch 67, training loss: 4.4732: : 67it [00:44,  1.58it/s]\u001b[A\n",
      "batch 68, training loss: 4.5263: : 67it [00:44,  1.58it/s]\u001b[A\n",
      "batch 68, training loss: 4.5263: : 68it [00:44,  1.59it/s]\u001b[A\n",
      "batch 69, training loss: 4.4381: : 68it [00:45,  1.59it/s]\u001b[A\n",
      "batch 69, training loss: 4.4381: : 69it [00:45,  1.56it/s]\u001b[A\n",
      "batch 70, training loss: 4.6: : 69it [00:46,  1.56it/s]   \u001b[A\n",
      "batch 70, training loss: 4.6: : 70it [00:46,  1.51it/s]\u001b[A\n",
      "batch 71, training loss: 4.4683: : 70it [00:46,  1.51it/s]\u001b[A\n",
      "batch 71, training loss: 4.4683: : 71it [00:46,  1.50it/s]\u001b[A\n",
      "batch 72, training loss: 4.4876: : 71it [00:47,  1.50it/s]\u001b[A\n",
      "batch 72, training loss: 4.4876: : 72it [00:47,  1.61it/s]\u001b[A\n",
      "batch 73, training loss: 4.5411: : 72it [00:47,  1.61it/s]\u001b[A\n",
      "batch 73, training loss: 4.5411: : 73it [00:47,  1.64it/s]\u001b[A\n",
      "batch 74, training loss: 4.4202: : 73it [00:48,  1.64it/s]\u001b[A\n",
      "batch 74, training loss: 4.4202: : 74it [00:48,  1.61it/s]\u001b[A\n",
      "batch 75, training loss: 4.5935: : 74it [00:49,  1.61it/s]\u001b[A\n",
      "batch 75, training loss: 4.5935: : 75it [00:49,  1.58it/s]\u001b[A\n",
      "batch 76, training loss: 4.3776: : 75it [00:49,  1.58it/s]\u001b[A\n",
      "batch 76, training loss: 4.3776: : 76it [00:49,  1.56it/s]\u001b[A\n",
      "batch 77, training loss: 4.5335: : 76it [00:50,  1.56it/s]\u001b[A\n",
      "batch 77, training loss: 4.5335: : 77it [00:50,  1.57it/s]\u001b[A\n",
      "batch 78, training loss: 4.5124: : 77it [00:51,  1.57it/s]\u001b[A\n",
      "batch 78, training loss: 4.5124: : 78it [00:51,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 4.5365: : 78it [00:51,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 4.5365: : 79it [00:51,  1.59it/s]\u001b[A\n",
      "batch 80, training loss: 4.4517: : 79it [00:52,  1.59it/s]\u001b[A\n",
      "batch 80, training loss: 4.4517: : 80it [00:52,  1.57it/s]\u001b[A\n",
      "batch 81, training loss: 4.4898: : 80it [00:53,  1.57it/s]\u001b[A\n",
      "batch 81, training loss: 4.4898: : 81it [00:53,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 4.5541: : 81it [00:53,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 4.5541: : 82it [00:53,  1.50it/s]\u001b[A\n",
      "batch 83, training loss: 4.4556: : 82it [00:54,  1.50it/s]\u001b[A\n",
      "batch 83, training loss: 4.4556: : 83it [00:54,  1.49it/s]\u001b[A\n",
      "batch 84, training loss: 4.5565: : 83it [00:54,  1.49it/s]\u001b[A\n",
      "batch 84, training loss: 4.5565: : 84it [00:54,  1.60it/s]\u001b[A\n",
      "batch 85, training loss: 4.6554: : 84it [00:55,  1.60it/s]\u001b[A\n",
      "batch 85, training loss: 4.6554: : 85it [00:55,  1.62it/s]\u001b[A\n",
      "batch 86, training loss: 4.4968: : 85it [00:56,  1.62it/s]\u001b[A\n",
      "batch 86, training loss: 4.4968: : 86it [00:56,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 4.6087: : 86it [00:56,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 4.6087: : 87it [00:56,  1.61it/s]\u001b[A\n",
      "batch 88, training loss: 4.8042: : 87it [00:57,  1.61it/s]\u001b[A\n",
      "batch 88, training loss: 4.8042: : 88it [00:57,  1.52it/s]\u001b[A\n",
      "batch 89, training loss: 4.8665: : 88it [00:58,  1.52it/s]\u001b[A\n",
      "batch 89, training loss: 4.8665: : 89it [00:58,  1.44it/s]\u001b[A\n",
      "batch 90, training loss: 4.7714: : 89it [00:59,  1.44it/s]\u001b[A\n",
      "batch 90, training loss: 4.7714: : 90it [00:59,  1.39it/s]\u001b[A\n",
      "batch 91, training loss: 4.9201: : 90it [00:59,  1.39it/s]\u001b[A\n",
      "batch 91, training loss: 4.9201: : 91it [00:59,  1.37it/s]\u001b[A\n",
      "batch 92, training loss: 4.8115: : 91it [01:00,  1.37it/s]\u001b[A\n",
      "batch 92, training loss: 4.8115: : 92it [01:00,  1.34it/s]\u001b[A\n",
      "batch 93, training loss: 4.7542: : 92it [01:01,  1.34it/s]\u001b[A\n",
      "batch 93, training loss: 4.7542: : 93it [01:01,  1.31it/s]\u001b[A\n",
      "batch 94, training loss: 4.7954: : 93it [01:02,  1.31it/s]\u001b[A\n",
      "batch 94, training loss: 4.7954: : 94it [01:02,  1.30it/s]\u001b[A\n",
      "batch 95, training loss: 4.8521: : 94it [01:03,  1.30it/s]\u001b[A\n",
      "batch 95, training loss: 4.8521: : 95it [01:03,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 4.7921: : 95it [01:03,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 4.7921: : 96it [01:03,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.8535: : 96it [01:04,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.8535: : 97it [01:04,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 4.8302: : 97it [01:05,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 4.8302: : 98it [01:05,  1.30it/s]\u001b[A\n",
      "batch 99, training loss: 4.6388: : 98it [01:06,  1.30it/s]\u001b[A\n",
      "batch 99, training loss: 4.6388: : 99it [01:06,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 4.5101: : 99it [01:06,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 4.5101: : 100it [01:06,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 4.6731: : 100it [01:07,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 4.6731: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 4.6075: : 101it [01:08,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 4.6075: : 102it [01:08,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 4.5969: : 102it [01:09,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 4.5969: : 103it [01:09,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 4.4595: : 103it [01:09,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 4.4595: : 104it [01:09,  1.33it/s]\u001b[A\n",
      "batch 105, training loss: 4.6312: : 104it [01:10,  1.33it/s]\u001b[A\n",
      "batch 105, training loss: 4.6312: : 105it [01:10,  1.33it/s]\u001b[A\n",
      "batch 106, training loss: 4.7097: : 105it [01:11,  1.33it/s]\u001b[A\n",
      "batch 106, training loss: 4.7097: : 106it [01:11,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 4.3969: : 106it [01:12,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 4.3969: : 107it [01:12,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 4.6398: : 107it [01:12,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 4.6398: : 108it [01:12,  1.33it/s]\u001b[A\n",
      "batch 109, training loss: 4.6786: : 108it [01:13,  1.33it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 109, training loss: 4.6786: : 109it [01:13,  1.32it/s]\u001b[A\n",
      "batch 110, training loss: 4.8372: : 109it [01:14,  1.32it/s]\u001b[A\n",
      "batch 110, training loss: 4.8372: : 110it [01:14,  1.31it/s]\u001b[A\n",
      "batch 111, training loss: 4.5534: : 110it [01:15,  1.31it/s]\u001b[A\n",
      "batch 111, training loss: 4.5534: : 111it [01:15,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 4.6115: : 111it [01:15,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 4.6115: : 112it [01:15,  1.33it/s]\u001b[A\n",
      "batch 113, training loss: 4.6249: : 112it [01:16,  1.33it/s]\u001b[A\n",
      "batch 113, training loss: 4.6249: : 113it [01:16,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 4.6878: : 113it [01:17,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 4.6878: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 4.5677: : 114it [01:18,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 4.5677: : 115it [01:18,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 4.5492: : 115it [01:18,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 4.5492: : 116it [01:18,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 4.5944: : 116it [01:19,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 4.5944: : 117it [01:19,  1.30it/s]\u001b[A\n",
      "batch 118, training loss: 4.6783: : 117it [01:20,  1.30it/s]\u001b[A\n",
      "batch 118, training loss: 4.6783: : 118it [01:20,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 4.5715: : 118it [01:21,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 4.5715: : 119it [01:21,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 4.5325: : 119it [01:22,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 4.5325: : 120it [01:22,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 4.7392: : 120it [01:22,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 4.7392: : 121it [01:22,  1.31it/s]\u001b[A\n",
      "batch 122, training loss: 4.4486: : 121it [01:23,  1.31it/s]\u001b[A\n",
      "batch 122, training loss: 4.4486: : 122it [01:23,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 4.6247: : 122it [01:24,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 4.6247: : 123it [01:24,  1.31it/s]\u001b[A\n",
      "batch 124, training loss: 4.5983: : 123it [01:25,  1.31it/s]\u001b[A\n",
      "batch 124, training loss: 4.5983: : 124it [01:25,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 4.6071: : 124it [01:25,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 4.6071: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 4.668: : 125it [01:26,  1.30it/s] \u001b[A\n",
      "batch 126, training loss: 4.668: : 126it [01:26,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 4.4174: : 126it [01:27,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 4.4174: : 127it [01:27,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 4.6042: : 127it [01:28,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 4.6042: : 128it [01:28,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 4.5807: : 128it [01:28,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 4.5807: : 129it [01:28,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 4.5845: : 129it [01:29,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 4.5845: : 130it [01:29,  1.30it/s]\u001b[A\n",
      "batch 131, training loss: 4.737: : 130it [01:30,  1.30it/s] \u001b[A\n",
      "batch 131, training loss: 4.737: : 131it [01:30,  1.40it/s]\u001b[A\n",
      "batch 132, training loss: 4.56: : 131it [01:30,  1.40it/s] \u001b[A\n",
      "batch 132, training loss: 4.56: : 132it [01:30,  1.45it/s]\u001b[A\n",
      "batch 133, training loss: 4.4465: : 132it [01:31,  1.45it/s]\u001b[A\n",
      "batch 133, training loss: 4.4465: : 133it [01:31,  1.42it/s]\u001b[A\n",
      "batch 134, training loss: 4.5193: : 133it [01:32,  1.42it/s]\u001b[A\n",
      "batch 134, training loss: 4.5193: : 134it [01:32,  1.40it/s]\u001b[A\n",
      "batch 135, training loss: 4.5387: : 134it [01:33,  1.40it/s]\u001b[A\n",
      "batch 135, training loss: 4.5387: : 135it [01:33,  1.38it/s]\u001b[A\n",
      "batch 136, training loss: 4.4739: : 135it [01:33,  1.38it/s]\u001b[A\n",
      "batch 136, training loss: 4.4739: : 136it [01:33,  1.35it/s]\u001b[A\n",
      "batch 137, training loss: 4.6266: : 136it [01:34,  1.35it/s]\u001b[A\n",
      "batch 137, training loss: 4.6266: : 137it [01:34,  1.34it/s]\u001b[A\n",
      "batch 138, training loss: 4.6281: : 137it [01:35,  1.34it/s]\u001b[A\n",
      "batch 138, training loss: 4.6281: : 138it [01:35,  1.35it/s]\u001b[A\n",
      "batch 139, training loss: 4.4153: : 138it [01:36,  1.35it/s]\u001b[A\n",
      "batch 139, training loss: 4.4153: : 139it [01:36,  1.33it/s]\u001b[A\n",
      "batch 140, training loss: 4.5072: : 139it [01:36,  1.33it/s]\u001b[A\n",
      "batch 140, training loss: 4.5072: : 140it [01:36,  1.32it/s]\u001b[A\n",
      "batch 141, training loss: 4.5234: : 140it [01:37,  1.32it/s]\u001b[A\n",
      "batch 141, training loss: 4.5234: : 141it [01:37,  1.32it/s]\u001b[A\n",
      "batch 142, training loss: 4.5364: : 141it [01:38,  1.32it/s]\u001b[A\n",
      "batch 142, training loss: 4.5364: : 142it [01:38,  1.34it/s]\u001b[A\n",
      "batch 143, training loss: 4.4278: : 142it [01:39,  1.34it/s]\u001b[A\n",
      "batch 143, training loss: 4.4278: : 143it [01:39,  1.32it/s]\u001b[A\n",
      "batch 144, training loss: 4.4232: : 143it [01:40,  1.32it/s]\u001b[A\n",
      "batch 144, training loss: 4.4232: : 144it [01:40,  1.32it/s]\u001b[A\n",
      "batch 145, training loss: 4.5168: : 144it [01:40,  1.32it/s]\u001b[A\n",
      "batch 145, training loss: 4.5168: : 145it [01:40,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 4.5344: : 145it [01:41,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 4.5344: : 146it [01:41,  1.32it/s]\u001b[A\n",
      "batch 147, training loss: 4.3877: : 146it [01:42,  1.32it/s]\u001b[A\n",
      "batch 147, training loss: 4.3877: : 147it [01:42,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 4.5272: : 147it [01:43,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 4.5272: : 148it [01:43,  1.31it/s]\u001b[A\n",
      "batch 149, training loss: 4.566: : 148it [01:43,  1.31it/s] \u001b[A\n",
      "batch 149, training loss: 4.566: : 149it [01:43,  1.32it/s]\u001b[A\n",
      "batch 150, training loss: 4.5921: : 149it [01:44,  1.32it/s]\u001b[A\n",
      "batch 150, training loss: 4.5921: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 4.5212: : 150it [01:45,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 4.5212: : 151it [01:45,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 4.4886: : 151it [01:46,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 4.4886: : 152it [01:46,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 4.5134: : 152it [01:46,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 4.5134: : 153it [01:46,  1.33it/s]\u001b[A\n",
      "batch 154, training loss: 4.5729: : 153it [01:47,  1.33it/s]\u001b[A\n",
      "batch 154, training loss: 4.5729: : 154it [01:47,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 4.6354: : 154it [01:48,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 4.6354: : 155it [01:48,  1.31it/s]\u001b[A\n",
      "batch 156, training loss: 4.3783: : 155it [01:49,  1.31it/s]\u001b[A\n",
      "batch 156, training loss: 4.3783: : 156it [01:49,  1.31it/s]\u001b[A\n",
      "batch 157, training loss: 4.5253: : 156it [01:49,  1.31it/s]\u001b[A\n",
      "batch 157, training loss: 4.5253: : 157it [01:49,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 4.5166: : 157it [01:50,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 4.5166: : 158it [01:50,  1.30it/s]\u001b[A\n",
      "batch 159, training loss: 4.4338: : 158it [01:51,  1.30it/s]\u001b[A\n",
      "batch 159, training loss: 4.4338: : 159it [01:51,  1.31it/s]\u001b[A\n",
      "batch 160, training loss: 4.574: : 159it [01:52,  1.31it/s] \u001b[A\n",
      "batch 160, training loss: 4.574: : 160it [01:52,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 4.4383: : 160it [01:52,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 4.4383: : 161it [01:52,  1.32it/s]\u001b[A\n",
      "batch 162, training loss: 4.438: : 161it [01:53,  1.32it/s] \u001b[A\n",
      "batch 162, training loss: 4.438: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 4.6354: : 162it [01:54,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 4.6354: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 4.4595: : 163it [01:55,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 4.4595: : 164it [01:55,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 4.4953: : 164it [01:56,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 4.4953: : 165it [01:56,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 4.4513: : 165it [01:56,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 4.4513: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 4.5882: : 166it [01:57,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 4.5882: : 167it [01:57,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 4.5003: : 167it [01:58,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 4.5003: : 168it [01:58,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 4.5972: : 168it [01:59,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 4.5972: : 169it [01:59,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 4.4616: : 169it [01:59,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 4.4616: : 170it [01:59,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 4.1272: : 170it [02:00,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 4.1272: : 171it [02:00,  1.56it/s]\u001b[A\n",
      "batch 172, training loss: 4.8901: : 171it [02:01,  1.56it/s]\u001b[A\n",
      "batch 172, training loss: 4.8901: : 172it [02:01,  1.45it/s]\u001b[A\n",
      "batch 173, training loss: 4.6971: : 172it [02:01,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 173, training loss: 4.6971: : 173it [02:01,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 4.8829: : 173it [02:02,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 4.8829: : 174it [02:02,  1.35it/s]\u001b[A\n",
      "batch 175, training loss: 4.7914: : 174it [02:03,  1.35it/s]\u001b[A\n",
      "batch 175, training loss: 4.7914: : 175it [02:03,  1.32it/s]\u001b[A\n",
      "batch 176, training loss: 4.7085: : 175it [02:04,  1.32it/s]\u001b[A\n",
      "batch 176, training loss: 4.7085: : 176it [02:04,  1.29it/s]\u001b[A\n",
      "batch 177, training loss: 4.728: : 176it [02:05,  1.29it/s] \u001b[A\n",
      "batch 177, training loss: 4.728: : 177it [02:05,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 4.7635: : 177it [02:05,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 4.7635: : 178it [02:05,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 4.7775: : 178it [02:06,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 4.7775: : 179it [02:06,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 4.8594: : 179it [02:07,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 4.8594: : 180it [02:07,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.8439: : 180it [02:08,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.8439: : 181it [02:08,  1.25it/s]\u001b[A\n",
      "batch 182, training loss: 4.7515: : 181it [02:09,  1.25it/s]\u001b[A\n",
      "batch 182, training loss: 4.7515: : 182it [02:09,  1.24it/s]\u001b[A\n",
      "batch 183, training loss: 4.8386: : 182it [02:09,  1.24it/s]\u001b[A\n",
      "batch 183, training loss: 4.8386: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 4.6256: : 183it [02:10,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 4.6256: : 184it [02:10,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 4.6361: : 184it [02:11,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 4.6361: : 185it [02:11,  1.23it/s]\u001b[A\n",
      "batch 186, training loss: 4.7844: : 185it [02:12,  1.23it/s]\u001b[A\n",
      "batch 186, training loss: 4.7844: : 186it [02:12,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 4.7604: : 186it [02:13,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 4.7604: : 187it [02:13,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 4.8661: : 187it [02:13,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 4.8661: : 188it [02:13,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 4.8463: : 188it [02:14,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 4.8463: : 189it [02:14,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 4.5135: : 189it [02:15,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 4.5135: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 4.5354: : 190it [02:16,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 4.5354: : 191it [02:16,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.6729: : 191it [02:17,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.6729: : 192it [02:17,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 4.5209: : 192it [02:17,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 4.5209: : 193it [02:17,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 4.619: : 193it [02:18,  1.24it/s] \u001b[A\n",
      "batch 194, training loss: 4.619: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.538: : 194it [02:19,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.538: : 195it [02:19,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 4.547: : 195it [02:20,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 4.547: : 196it [02:20,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.6382: : 196it [02:21,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.6382: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.5592: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.5592: : 198it [02:21,  1.25it/s]\u001b[A\n",
      "batch 199, training loss: 4.551: : 198it [02:22,  1.25it/s] \u001b[A\n",
      "batch 199, training loss: 4.551: : 199it [02:22,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.6936: : 199it [02:23,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.6936: : 200it [02:23,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 4.4738: : 200it [02:24,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 4.4738: : 201it [02:24,  1.22it/s]\u001b[A\n",
      "batch 202, training loss: 4.3742: : 201it [02:25,  1.22it/s]\u001b[A\n",
      "batch 202, training loss: 4.3742: : 202it [02:25,  1.22it/s]\u001b[A\n",
      "batch 203, training loss: 4.5695: : 202it [02:26,  1.22it/s]\u001b[A\n",
      "batch 203, training loss: 4.5695: : 203it [02:26,  1.23it/s]\u001b[A\n",
      "batch 204, training loss: 4.6083: : 203it [02:26,  1.23it/s]\u001b[A\n",
      "batch 204, training loss: 4.6083: : 204it [02:26,  1.22it/s]\u001b[A\n",
      "batch 205, training loss: 4.586: : 204it [02:27,  1.22it/s] \u001b[A\n",
      "batch 205, training loss: 4.586: : 205it [02:27,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 4.6863: : 205it [02:28,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 4.6863: : 206it [02:28,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 4.5073: : 206it [02:29,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 4.5073: : 207it [02:29,  1.22it/s]\u001b[A\n",
      "batch 208, training loss: 4.545: : 207it [02:30,  1.22it/s] \u001b[A\n",
      "batch 208, training loss: 4.545: : 208it [02:30,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 4.5143: : 208it [02:31,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 4.5143: : 209it [02:31,  1.22it/s]\u001b[A\n",
      "batch 210, training loss: 4.512: : 209it [02:31,  1.22it/s] \u001b[A\n",
      "batch 210, training loss: 4.512: : 210it [02:31,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 4.4861: : 210it [02:32,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 4.4861: : 211it [02:32,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 4.5385: : 211it [02:33,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 4.5385: : 212it [02:33,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 4.7161: : 212it [02:34,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 4.7161: : 213it [02:34,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 4.5928: : 213it [02:35,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 4.5928: : 214it [02:35,  1.25it/s]\u001b[A\n",
      "batch 215, training loss: 4.3876: : 214it [02:35,  1.25it/s]\u001b[A\n",
      "batch 215, training loss: 4.3876: : 215it [02:35,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 4.5303: : 215it [02:36,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 4.5303: : 216it [02:36,  1.24it/s]\u001b[A\n",
      "batch 217, training loss: 4.6602: : 216it [02:37,  1.24it/s]\u001b[A\n",
      "batch 217, training loss: 4.6602: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 4.5616: : 217it [02:38,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 4.5616: : 218it [02:38,  1.22it/s]\u001b[A\n",
      "batch 219, training loss: 4.7644: : 218it [02:39,  1.22it/s]\u001b[A\n",
      "batch 219, training loss: 4.7644: : 219it [02:39,  1.21it/s]\u001b[A\n",
      "batch 220, training loss: 4.668: : 219it [02:39,  1.21it/s] \u001b[A\n",
      "batch 220, training loss: 4.668: : 220it [02:39,  1.22it/s]\u001b[A\n",
      "batch 221, training loss: 4.5908: : 220it [02:40,  1.22it/s]\u001b[A\n",
      "batch 221, training loss: 4.5908: : 221it [02:40,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 4.5732: : 221it [02:41,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 4.5732: : 222it [02:41,  1.22it/s]\u001b[A\n",
      "batch 223, training loss: 4.5672: : 222it [02:42,  1.22it/s]\u001b[A\n",
      "batch 223, training loss: 4.5672: : 223it [02:42,  1.22it/s]\u001b[A\n",
      "batch 224, training loss: 4.5141: : 223it [02:43,  1.22it/s]\u001b[A\n",
      "batch 224, training loss: 4.5141: : 224it [02:43,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 4.6409: : 224it [02:44,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 4.6409: : 225it [02:44,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 4.6279: : 225it [02:44,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 4.6279: : 226it [02:44,  1.23it/s]\u001b[A\n",
      "batch 227, training loss: 4.4758: : 226it [02:45,  1.23it/s]\u001b[A\n",
      "batch 227, training loss: 4.4758: : 227it [02:45,  1.21it/s]\u001b[A\n",
      "batch 228, training loss: 4.5241: : 227it [02:46,  1.21it/s]\u001b[A\n",
      "batch 228, training loss: 4.5241: : 228it [02:46,  1.23it/s]\u001b[A\n",
      "batch 229, training loss: 4.5264: : 228it [02:47,  1.23it/s]\u001b[A\n",
      "batch 229, training loss: 4.5264: : 229it [02:47,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 4.6368: : 229it [02:48,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 4.6368: : 230it [02:48,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 4.7348: : 230it [02:48,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 4.7348: : 231it [02:48,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 4.4889: : 231it [02:49,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 4.4889: : 232it [02:49,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 4.6024: : 232it [02:50,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 4.6024: : 233it [02:50,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 4.6936: : 233it [02:51,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 4.6936: : 234it [02:51,  1.22it/s]\u001b[A\n",
      "batch 235, training loss: 4.5957: : 234it [02:52,  1.22it/s]\u001b[A\n",
      "batch 235, training loss: 4.5957: : 235it [02:52,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 4.5781: : 235it [02:53,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 4.5781: : 236it [02:53,  1.23it/s]\u001b[A\n",
      "batch 237, training loss: 4.4555: : 236it [02:53,  1.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 237, training loss: 4.4555: : 237it [02:53,  1.22it/s]\u001b[A\n",
      "batch 238, training loss: 4.5832: : 237it [02:54,  1.22it/s]\u001b[A\n",
      "batch 238, training loss: 4.5832: : 238it [02:54,  1.22it/s]\u001b[A\n",
      "batch 239, training loss: 4.5127: : 238it [02:55,  1.22it/s]\u001b[A\n",
      "batch 239, training loss: 4.5127: : 239it [02:55,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 4.5161: : 239it [02:56,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 4.5161: : 240it [02:56,  1.23it/s]\u001b[A\n",
      "batch 241, training loss: 4.5912: : 240it [02:57,  1.23it/s]\u001b[A\n",
      "batch 241, training loss: 4.5912: : 241it [02:57,  1.23it/s]\u001b[A\n",
      "batch 242, training loss: 4.39: : 241it [02:57,  1.23it/s]  \u001b[A\n",
      "batch 242, training loss: 4.39: : 242it [02:57,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 4.5642: : 242it [02:58,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 4.5642: : 243it [02:58,  1.23it/s]\u001b[A\n",
      "batch 244, training loss: 4.5012: : 243it [02:59,  1.23it/s]\u001b[A\n",
      "batch 244, training loss: 4.5012: : 244it [02:59,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 4.6036: : 244it [03:00,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 4.6036: : 245it [03:00,  1.40it/s]\u001b[A\n",
      "batch 246, training loss: 4.5152: : 245it [03:00,  1.40it/s]\u001b[A\n",
      "batch 246, training loss: 4.5152: : 246it [03:00,  1.33it/s]\u001b[A\n",
      "batch 247, training loss: 4.5023: : 246it [03:01,  1.33it/s]\u001b[A\n",
      "batch 247, training loss: 4.5023: : 247it [03:01,  1.30it/s]\u001b[A\n",
      "batch 248, training loss: 4.4334: : 247it [03:02,  1.30it/s]\u001b[A\n",
      "batch 248, training loss: 4.4334: : 248it [03:02,  1.29it/s]\u001b[A\n",
      "batch 249, training loss: 4.3991: : 248it [03:03,  1.29it/s]\u001b[A\n",
      "batch 249, training loss: 4.3991: : 249it [03:03,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 4.6236: : 249it [03:04,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 4.6236: : 250it [03:04,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 4.6266: : 250it [03:04,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 4.6266: : 251it [03:04,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 4.4455: : 251it [03:05,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 4.4455: : 252it [03:05,  1.45it/s]\u001b[A\n",
      "batch 253, training loss: 4.6609: : 252it [03:06,  1.45it/s]\u001b[A\n",
      "batch 253, training loss: 4.6609: : 253it [03:06,  1.32it/s]\u001b[A\n",
      "batch 254, training loss: 4.8226: : 253it [03:07,  1.32it/s]\u001b[A\n",
      "batch 254, training loss: 4.8226: : 254it [03:07,  1.23it/s]\u001b[A\n",
      "batch 255, training loss: 4.6856: : 254it [03:08,  1.23it/s]\u001b[A\n",
      "batch 255, training loss: 4.6856: : 255it [03:08,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 4.5968: : 255it [03:08,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 4.5968: : 256it [03:08,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 4.7389: : 256it [03:09,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 4.7389: : 257it [03:09,  1.13it/s]\u001b[A\n",
      "batch 258, training loss: 4.7931: : 257it [03:10,  1.13it/s]\u001b[A\n",
      "batch 258, training loss: 4.7931: : 258it [03:10,  1.11it/s]\u001b[A\n",
      "batch 259, training loss: 4.6784: : 258it [03:11,  1.11it/s]\u001b[A\n",
      "batch 259, training loss: 4.6784: : 259it [03:11,  1.10it/s]\u001b[A\n",
      "batch 260, training loss: 4.6889: : 259it [03:12,  1.10it/s]\u001b[A\n",
      "batch 260, training loss: 4.6889: : 260it [03:12,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 4.7871: : 260it [03:13,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 4.7871: : 261it [03:13,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 4.6256: : 261it [03:14,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 4.6256: : 262it [03:14,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 4.6353: : 262it [03:15,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 4.6353: : 263it [03:15,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 4.6795: : 263it [03:16,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 4.6795: : 264it [03:16,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 4.537: : 264it [03:17,  1.10it/s] \u001b[A\n",
      "batch 265, training loss: 4.537: : 265it [03:17,  1.09it/s]\u001b[A\n",
      "batch 266, training loss: 4.7301: : 265it [03:18,  1.09it/s]\u001b[A\n",
      "batch 266, training loss: 4.7301: : 266it [03:18,  1.08it/s]\u001b[A\n",
      "batch 267, training loss: 4.6191: : 266it [03:19,  1.08it/s]\u001b[A\n",
      "batch 267, training loss: 4.6191: : 267it [03:19,  1.08it/s]\u001b[A\n",
      "batch 268, training loss: 4.5751: : 267it [03:19,  1.08it/s]\u001b[A\n",
      "batch 268, training loss: 4.5751: : 268it [03:19,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 4.5531: : 268it [03:20,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 4.5531: : 269it [03:20,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 4.5793: : 269it [03:21,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 4.5793: : 270it [03:21,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 4.5981: : 270it [03:22,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 4.5981: : 271it [03:22,  1.10it/s]\u001b[A\n",
      "batch 272, training loss: 4.5568: : 271it [03:23,  1.10it/s]\u001b[A\n",
      "batch 272, training loss: 4.5568: : 272it [03:23,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 4.6545: : 272it [03:24,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 4.6545: : 273it [03:24,  1.09it/s]\u001b[A\n",
      "batch 274, training loss: 4.6107: : 273it [03:25,  1.09it/s]\u001b[A\n",
      "batch 274, training loss: 4.6107: : 274it [03:25,  1.09it/s]\u001b[A\n",
      "batch 275, training loss: 4.4974: : 274it [03:26,  1.09it/s]\u001b[A\n",
      "batch 275, training loss: 4.4974: : 275it [03:26,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 4.4323: : 275it [03:27,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 4.4323: : 276it [03:27,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 4.5678: : 276it [03:28,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 4.5678: : 277it [03:28,  1.11it/s]\u001b[A\n",
      "batch 278, training loss: 4.4845: : 277it [03:29,  1.11it/s]\u001b[A\n",
      "batch 278, training loss: 4.4845: : 278it [03:29,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 4.5477: : 278it [03:30,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 4.5477: : 279it [03:30,  1.09it/s]\u001b[A\n",
      "batch 280, training loss: 4.3926: : 279it [03:30,  1.09it/s]\u001b[A\n",
      "batch 280, training loss: 4.3926: : 280it [03:30,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 4.4611: : 280it [03:31,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 4.4611: : 281it [03:31,  1.09it/s]\u001b[A\n",
      "batch 282, training loss: 4.4476: : 281it [03:32,  1.09it/s]\u001b[A\n",
      "batch 282, training loss: 4.4476: : 282it [03:32,  1.09it/s]\u001b[A\n",
      "batch 283, training loss: 4.4701: : 282it [03:33,  1.09it/s]\u001b[A\n",
      "batch 283, training loss: 4.4701: : 283it [03:33,  1.08it/s]\u001b[A\n",
      "batch 284, training loss: 4.4471: : 283it [03:34,  1.08it/s]\u001b[A\n",
      "batch 284, training loss: 4.4471: : 284it [03:34,  1.10it/s]\u001b[A\n",
      "batch 285, training loss: 4.4859: : 284it [03:35,  1.10it/s]\u001b[A\n",
      "batch 285, training loss: 4.4859: : 285it [03:35,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 4.6258: : 285it [03:36,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 4.6258: : 286it [03:36,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 4.3747: : 286it [03:37,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 4.3747: : 287it [03:37,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 4.4026: : 287it [03:38,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 4.4026: : 288it [03:38,  1.12it/s]\u001b[A\n",
      "batch 289, training loss: 4.5462: : 288it [03:39,  1.12it/s]\u001b[A\n",
      "batch 289, training loss: 4.5462: : 289it [03:39,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 4.3336: : 289it [03:40,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 4.3336: : 290it [03:40,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.6105: : 290it [03:40,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.6105: : 291it [03:40,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 4.3696: : 291it [03:41,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 4.3696: : 292it [03:41,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 4.4831: : 292it [03:42,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 4.4831: : 293it [03:42,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 4.5318: : 293it [03:43,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 4.5318: : 294it [03:43,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 4.4531: : 294it [03:44,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 4.4531: : 295it [03:44,  1.08it/s]\u001b[A\n",
      "batch 296, training loss: 4.3535: : 295it [03:45,  1.08it/s]\u001b[A\n",
      "batch 296, training loss: 4.3535: : 296it [03:45,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 4.5072: : 296it [03:46,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 4.5072: : 297it [03:46,  1.11it/s]\u001b[A\n",
      "batch 298, training loss: 4.3946: : 297it [03:47,  1.11it/s]\u001b[A\n",
      "batch 298, training loss: 4.3946: : 298it [03:47,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 4.4961: : 298it [03:48,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 4.4961: : 299it [03:48,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 4.5804: : 299it [03:49,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 4.5804: : 300it [03:49,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 4.5258: : 300it [03:50,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 301, training loss: 4.5258: : 301it [03:50,  1.08it/s]\u001b[A\n",
      "batch 302, training loss: 4.4815: : 301it [03:50,  1.08it/s]\u001b[A\n",
      "batch 302, training loss: 4.4815: : 302it [03:50,  1.11it/s]\u001b[A\n",
      "batch 303, training loss: 4.4804: : 302it [03:51,  1.11it/s]\u001b[A\n",
      "batch 303, training loss: 4.4804: : 303it [03:51,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 4.4825: : 303it [03:52,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 4.4825: : 304it [03:52,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 4.4771: : 304it [03:53,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 4.4771: : 305it [03:53,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.5088: : 305it [03:54,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.5088: : 306it [03:54,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 4.6066: : 306it [03:55,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 4.6066: : 307it [03:55,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 4.2684: : 307it [03:56,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 4.2684: : 308it [03:56,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 4.6163: : 308it [03:57,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 4.6163: : 309it [03:57,  1.08it/s]\u001b[A\n",
      "batch 310, training loss: 4.4161: : 309it [03:58,  1.08it/s]\u001b[A\n",
      "batch 310, training loss: 4.4161: : 310it [03:58,  1.11it/s]\u001b[A\n",
      "batch 311, training loss: 4.4904: : 310it [03:59,  1.11it/s]\u001b[A\n",
      "batch 311, training loss: 4.4904: : 311it [03:59,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 4.3153: : 311it [04:00,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 4.3153: : 312it [04:00,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 4.4168: : 312it [04:01,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 4.4168: : 313it [04:01,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 4.4902: : 313it [04:01,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 4.4902: : 314it [04:01,  1.09it/s]\u001b[A\n",
      "batch 315, training loss: 4.5674: : 314it [04:02,  1.09it/s]\u001b[A\n",
      "batch 315, training loss: 4.5674: : 315it [04:02,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 4.6914: : 315it [04:03,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 4.6914: : 316it [04:03,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 4.3492: : 316it [04:04,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 4.3492: : 317it [04:04,  1.15it/s]\u001b[A\n",
      "batch 318, training loss: 4.6276: : 317it [04:05,  1.15it/s]\u001b[A\n",
      "batch 318, training loss: 4.6276: : 318it [04:05,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 4.7252: : 318it [04:06,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 4.7252: : 319it [04:06,  1.07it/s]\u001b[A\n",
      "batch 320, training loss: 4.7114: : 319it [04:07,  1.07it/s]\u001b[A\n",
      "batch 320, training loss: 4.7114: : 320it [04:07,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 4.7781: : 320it [04:08,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 4.7781: : 321it [04:08,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.8442: : 321it [04:09,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.8442: : 322it [04:09,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 4.573: : 322it [04:10,  1.01it/s] \u001b[A\n",
      "batch 323, training loss: 4.573: : 323it [04:10,  1.04it/s]\u001b[A\n",
      "batch 324, training loss: 4.6607: : 323it [04:11,  1.04it/s]\u001b[A\n",
      "batch 324, training loss: 4.6607: : 324it [04:11,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.7213: : 324it [04:12,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.7213: : 325it [04:12,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 4.6166: : 325it [04:13,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 4.6166: : 326it [04:13,  1.00it/s]\u001b[A\n",
      "batch 327, training loss: 4.7477: : 326it [04:14,  1.00it/s]\u001b[A\n",
      "batch 327, training loss: 4.7477: : 327it [04:14,  1.00s/it]\u001b[A\n",
      "batch 328, training loss: 4.5734: : 327it [04:15,  1.00s/it]\u001b[A\n",
      "batch 328, training loss: 4.5734: : 328it [04:15,  1.00it/s]\u001b[A\n",
      "batch 329, training loss: 4.5228: : 328it [04:16,  1.00it/s]\u001b[A\n",
      "batch 329, training loss: 4.5228: : 329it [04:16,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.5827: : 329it [04:17,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.5827: : 330it [04:17,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 4.6659: : 330it [04:18,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 4.6659: : 331it [04:18,  1.00it/s]\u001b[A\n",
      "batch 332, training loss: 4.5395: : 331it [04:19,  1.00it/s]\u001b[A\n",
      "batch 332, training loss: 4.5395: : 332it [04:19,  1.00s/it]\u001b[A\n",
      "batch 333, training loss: 4.4997: : 332it [04:20,  1.00s/it]\u001b[A\n",
      "batch 333, training loss: 4.4997: : 333it [04:20,  1.03s/it]\u001b[A\n",
      "batch 334, training loss: 4.6168: : 333it [04:21,  1.03s/it]\u001b[A\n",
      "batch 334, training loss: 4.6168: : 334it [04:21,  1.02s/it]\u001b[A\n",
      "batch 335, training loss: 4.7148: : 334it [04:22,  1.02s/it]\u001b[A\n",
      "batch 335, training loss: 4.7148: : 335it [04:22,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.6455: : 335it [04:23,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.6455: : 336it [04:23,  1.02s/it]\u001b[A\n",
      "batch 337, training loss: 4.6511: : 336it [04:24,  1.02s/it]\u001b[A\n",
      "batch 337, training loss: 4.6511: : 337it [04:24,  1.00s/it]\u001b[A\n",
      "batch 338, training loss: 4.5414: : 337it [04:25,  1.00s/it]\u001b[A\n",
      "batch 338, training loss: 4.5414: : 338it [04:25,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 4.5969: : 338it [04:26,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 4.5969: : 339it [04:26,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 4.8123: : 339it [04:27,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 4.8123: : 340it [04:27,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 4.4616: : 340it [04:28,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 4.4616: : 341it [04:28,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 4.5732: : 341it [04:29,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 4.5732: : 342it [04:29,  1.01it/s]\u001b[A\n",
      "batch 343, training loss: 4.6089: : 342it [04:30,  1.01it/s]\u001b[A\n",
      "batch 343, training loss: 4.6089: : 343it [04:30,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 4.643: : 343it [04:31,  1.01it/s] \u001b[A\n",
      "batch 344, training loss: 4.643: : 344it [04:31,  1.08it/s]\u001b[A\n",
      "batch 345, training loss: 4.5703: : 344it [04:32,  1.08it/s]\u001b[A\n",
      "batch 345, training loss: 4.5703: : 345it [04:32,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 4.5895: : 345it [04:33,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 4.5895: : 346it [04:33,  1.04it/s]\u001b[A\n",
      "batch 347, training loss: 4.4983: : 346it [04:34,  1.04it/s]\u001b[A\n",
      "batch 347, training loss: 4.4983: : 347it [04:34,  1.02it/s]\u001b[A\n",
      "batch 348, training loss: 4.563: : 347it [04:35,  1.02it/s] \u001b[A\n",
      "batch 348, training loss: 4.563: : 348it [04:35,  1.01it/s]\u001b[A\n",
      "batch 349, training loss: 4.6767: : 348it [04:36,  1.01it/s]\u001b[A\n",
      "batch 349, training loss: 4.6767: : 349it [04:36,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.5719: : 349it [04:37,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.5719: : 350it [04:37,  1.00it/s]\u001b[A\n",
      "batch 351, training loss: 4.464: : 350it [04:38,  1.00it/s] \u001b[A\n",
      "batch 351, training loss: 4.464: : 351it [04:38,  1.00s/it]\u001b[A\n",
      "batch 352, training loss: 4.5298: : 351it [04:39,  1.00s/it]\u001b[A\n",
      "batch 352, training loss: 4.5298: : 352it [04:39,  1.01s/it]\u001b[A\n",
      "batch 353, training loss: 4.5526: : 352it [04:40,  1.01s/it]\u001b[A\n",
      "batch 353, training loss: 4.5526: : 353it [04:40,  1.01s/it]\u001b[A\n",
      "batch 354, training loss: 4.6107: : 353it [04:41,  1.01s/it]\u001b[A\n",
      "batch 354, training loss: 4.6107: : 354it [04:41,  1.01s/it]\u001b[A\n",
      "batch 355, training loss: 4.6035: : 354it [04:42,  1.01s/it]\u001b[A\n",
      "batch 355, training loss: 4.6035: : 355it [04:42,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 4.4055: : 355it [04:43,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 4.4055: : 356it [04:43,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 4.3939: : 356it [04:44,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 4.3939: : 357it [04:44,  1.00s/it]\u001b[A\n",
      "batch 358, training loss: 4.5855: : 357it [04:45,  1.00s/it]\u001b[A\n",
      "batch 358, training loss: 4.5855: : 358it [04:45,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 4.4259: : 358it [04:46,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 4.4259: : 359it [04:46,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 4.579: : 359it [04:47,  1.01s/it] \u001b[A\n",
      "batch 360, training loss: 4.579: : 360it [04:47,  1.02it/s]\u001b[A\n",
      "batch 361, training loss: 4.5226: : 360it [04:48,  1.02it/s]\u001b[A\n",
      "batch 361, training loss: 4.5226: : 361it [04:48,  1.01it/s]\u001b[A\n",
      "batch 362, training loss: 4.6496: : 361it [04:49,  1.01it/s]\u001b[A\n",
      "batch 362, training loss: 4.6496: : 362it [04:49,  1.00s/it]\u001b[A\n",
      "batch 363, training loss: 4.5251: : 362it [04:50,  1.00s/it]\u001b[A\n",
      "batch 363, training loss: 4.5251: : 363it [04:50,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 4.4184: : 363it [04:51,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 4.4184: : 364it [04:51,  1.00s/it]\u001b[A\n",
      "batch 365, training loss: 4.3997: : 364it [04:52,  1.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365, training loss: 4.3997: : 365it [04:52,  1.03it/s]\u001b[A\n",
      "batch 366, training loss: 4.5702: : 365it [04:53,  1.03it/s]\u001b[A\n",
      "batch 366, training loss: 4.5702: : 366it [04:53,  1.02it/s]\u001b[A\n",
      "batch 367, training loss: 4.461: : 366it [04:54,  1.02it/s] \u001b[A\n",
      "batch 367, training loss: 4.461: : 367it [04:54,  1.00it/s]\u001b[A\n",
      "batch 368, training loss: 4.5676: : 367it [04:55,  1.00it/s]\u001b[A\n",
      "batch 368, training loss: 4.5676: : 368it [04:55,  1.00it/s]\u001b[A\n",
      "batch 369, training loss: 4.5846: : 368it [04:56,  1.00it/s]\u001b[A\n",
      "batch 369, training loss: 4.5846: : 369it [04:56,  1.00s/it]\u001b[A\n",
      "batch 370, training loss: 4.4566: : 369it [04:57,  1.00s/it]\u001b[A\n",
      "batch 370, training loss: 4.4566: : 370it [04:57,  1.00it/s]\u001b[A\n",
      "batch 371, training loss: 4.525: : 370it [04:58,  1.00it/s] \u001b[A\n",
      "batch 371, training loss: 4.525: : 371it [04:58,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 4.4782: : 371it [04:59,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 4.4782: : 372it [04:59,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 4.4721: : 372it [05:00,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 4.4721: : 373it [05:00,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 4.4997: : 373it [05:01,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 4.4997: : 374it [05:01,  1.00it/s]\u001b[A\n",
      "batch 375, training loss: 4.4102: : 374it [05:01,  1.00it/s]\u001b[A\n",
      "batch 375, training loss: 4.4102: : 375it [05:01,  1.15it/s]\u001b[A\n",
      "batch 376, training loss: 4.635: : 375it [05:03,  1.15it/s] \u001b[A\n",
      "batch 376, training loss: 4.635: : 376it [05:03,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 4.7361: : 376it [05:04,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 4.7361: : 377it [05:04,  1.01it/s]\u001b[A\n",
      "batch 378, training loss: 4.5825: : 377it [05:05,  1.01it/s]\u001b[A\n",
      "batch 378, training loss: 4.5825: : 378it [05:05,  1.02s/it]\u001b[A\n",
      "batch 379, training loss: 4.5628: : 378it [05:06,  1.02s/it]\u001b[A\n",
      "batch 379, training loss: 4.5628: : 379it [05:06,  1.04s/it]\u001b[A\n",
      "batch 380, training loss: 4.6399: : 379it [05:07,  1.04s/it]\u001b[A\n",
      "batch 380, training loss: 4.6399: : 380it [05:07,  1.06s/it]\u001b[A\n",
      "batch 381, training loss: 4.663: : 380it [05:08,  1.06s/it] \u001b[A\n",
      "batch 381, training loss: 4.663: : 381it [05:08,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 4.5188: : 381it [05:09,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 4.5188: : 382it [05:09,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 4.5587: : 382it [05:10,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 4.5587: : 383it [05:10,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.7195: : 383it [05:11,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.7195: : 384it [05:11,  1.07s/it]\u001b[A\n",
      "batch 385, training loss: 4.5869: : 384it [05:12,  1.07s/it]\u001b[A\n",
      "batch 385, training loss: 4.5869: : 385it [05:12,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 4.5225: : 385it [05:13,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 4.5225: : 386it [05:13,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 4.6524: : 386it [05:15,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 4.6524: : 387it [05:15,  1.10s/it]\u001b[A\n",
      "batch 388, training loss: 4.3708: : 387it [05:16,  1.10s/it]\u001b[A\n",
      "batch 388, training loss: 4.3708: : 388it [05:16,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 4.5495: : 388it [05:17,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 4.5495: : 389it [05:17,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 4.6161: : 389it [05:18,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 4.6161: : 390it [05:18,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.6274: : 390it [05:19,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.6274: : 391it [05:19,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.5977: : 391it [05:20,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.5977: : 392it [05:20,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 4.4551: : 392it [05:21,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 4.4551: : 393it [05:21,  1.08s/it]\u001b[A\n",
      "batch 394, training loss: 4.3782: : 393it [05:22,  1.08s/it]\u001b[A\n",
      "batch 394, training loss: 4.3782: : 394it [05:22,  1.06s/it]\u001b[A\n",
      "batch 395, training loss: 4.4506: : 394it [05:23,  1.06s/it]\u001b[A\n",
      "batch 395, training loss: 4.4506: : 395it [05:23,  1.10s/it]\u001b[A\n",
      "batch 396, training loss: 4.6378: : 395it [05:24,  1.10s/it]\u001b[A\n",
      "batch 396, training loss: 4.6378: : 396it [05:24,  1.10s/it]\u001b[A\n",
      "batch 397, training loss: 4.434: : 396it [05:26,  1.10s/it] \u001b[A\n",
      "batch 397, training loss: 4.434: : 397it [05:26,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 4.5021: : 397it [05:27,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 4.5021: : 398it [05:27,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.6225: : 398it [05:28,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.6225: : 399it [05:28,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 4.3822: : 399it [05:29,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 4.3822: : 400it [05:29,  1.11s/it]\u001b[A\n",
      "batch 401, training loss: 4.3768: : 400it [05:30,  1.11s/it]\u001b[A\n",
      "batch 401, training loss: 4.3768: : 401it [05:30,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 4.4339: : 401it [05:31,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 4.4339: : 402it [05:31,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.6233: : 402it [05:32,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.6233: : 403it [05:32,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.2948: : 403it [05:33,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.2948: : 404it [05:33,  1.12s/it]\u001b[A\n",
      "batch 405, training loss: 4.5092: : 404it [05:34,  1.12s/it]\u001b[A\n",
      "batch 405, training loss: 4.5092: : 405it [05:34,  1.12s/it]\u001b[A\n",
      "batch 406, training loss: 4.3993: : 405it [05:35,  1.12s/it]\u001b[A\n",
      "batch 406, training loss: 4.3993: : 406it [05:35,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 4.4803: : 406it [05:37,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 4.4803: : 407it [05:37,  1.11s/it]\u001b[A\n",
      "batch 408, training loss: 4.3125: : 407it [05:38,  1.11s/it]\u001b[A\n",
      "batch 408, training loss: 4.3125: : 408it [05:38,  1.12s/it]\u001b[A\n",
      "batch 409, training loss: 4.5536: : 408it [05:39,  1.12s/it]\u001b[A\n",
      "batch 409, training loss: 4.5536: : 409it [05:39,  1.13s/it]\u001b[A\n",
      "batch 410, training loss: 4.3839: : 409it [05:40,  1.13s/it]\u001b[A\n",
      "batch 410, training loss: 4.3839: : 410it [05:40,  1.12s/it]\u001b[A\n",
      "batch 411, training loss: 4.4808: : 410it [05:41,  1.12s/it]\u001b[A\n",
      "batch 411, training loss: 4.4808: : 411it [05:41,  1.11s/it]\u001b[A\n",
      "batch 412, training loss: 4.4586: : 411it [05:42,  1.11s/it]\u001b[A\n",
      "batch 412, training loss: 4.4586: : 412it [05:42,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 4.4313: : 412it [05:43,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 4.4313: : 413it [05:43,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 4.3027: : 413it [05:44,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 4.3027: : 414it [05:44,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 4.2079: : 414it [05:46,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 4.2079: : 415it [05:46,  1.12s/it]\u001b[A\n",
      "batch 416, training loss: 4.2716: : 415it [05:47,  1.12s/it]\u001b[A\n",
      "batch 416, training loss: 4.2716: : 416it [05:47,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 4.5416: : 416it [05:48,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 4.5416: : 417it [05:48,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 4.4327: : 417it [05:49,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 4.4327: : 418it [05:49,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 4.2501: : 418it [05:50,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 4.2501: : 419it [05:50,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.3678: : 419it [05:51,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.3678: : 420it [05:51,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 4.4457: : 420it [05:52,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 4.4457: : 421it [05:52,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 4.4595: : 421it [05:53,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 4.4595: : 422it [05:53,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 4.3181: : 422it [05:54,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 4.3181: : 423it [05:54,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 4.5415: : 423it [05:55,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 4.5415: : 424it [05:55,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.6497: : 424it [05:57,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.6497: : 425it [05:57,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 4.5665: : 425it [05:58,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 4.5665: : 426it [05:58,  1.17s/it]\u001b[A\n",
      "batch 427, training loss: 4.5805: : 426it [05:59,  1.17s/it]\u001b[A\n",
      "batch 427, training loss: 4.5805: : 427it [05:59,  1.19s/it]\u001b[A\n",
      "batch 428, training loss: 4.7009: : 427it [06:00,  1.19s/it]\u001b[A\n",
      "batch 428, training loss: 4.7009: : 428it [06:00,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 4.6201: : 428it [06:02,  1.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 429, training loss: 4.6201: : 429it [06:02,  1.20s/it]\u001b[A\n",
      "batch 430, training loss: 4.5645: : 429it [06:03,  1.20s/it]\u001b[A\n",
      "batch 430, training loss: 4.5645: : 430it [06:03,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.6357: : 430it [06:04,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.6357: : 431it [06:04,  1.09s/it]\u001b[A\n",
      "batch 432, training loss: 4.5078: : 431it [06:05,  1.09s/it]\u001b[A\n",
      "batch 432, training loss: 4.5078: : 432it [06:05,  1.09s/it]\u001b[A\n",
      "batch 433, training loss: 4.531: : 432it [06:06,  1.09s/it] \u001b[A\n",
      "batch 433, training loss: 4.531: : 433it [06:06,  1.14s/it]\u001b[A\n",
      "batch 434, training loss: 4.5066: : 433it [06:07,  1.14s/it]\u001b[A\n",
      "batch 434, training loss: 4.5066: : 434it [06:07,  1.16s/it]\u001b[A\n",
      "batch 435, training loss: 4.4754: : 434it [06:08,  1.16s/it]\u001b[A\n",
      "batch 435, training loss: 4.4754: : 435it [06:08,  1.18s/it]\u001b[A\n",
      "batch 436, training loss: 4.5409: : 435it [06:10,  1.18s/it]\u001b[A\n",
      "batch 436, training loss: 4.5409: : 436it [06:10,  1.19s/it]\u001b[A\n",
      "batch 437, training loss: 4.4823: : 436it [06:11,  1.19s/it]\u001b[A\n",
      "batch 437, training loss: 4.4823: : 437it [06:11,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 4.5406: : 437it [06:12,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 4.5406: : 438it [06:12,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 4.6013: : 438it [06:13,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 4.6013: : 439it [06:13,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 4.4311: : 439it [06:14,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 4.4311: : 440it [06:14,  1.22s/it]\u001b[A\n",
      "batch 441, training loss: 4.6427: : 440it [06:16,  1.22s/it]\u001b[A\n",
      "batch 441, training loss: 4.6427: : 441it [06:16,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 4.3745: : 441it [06:17,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 4.3745: : 442it [06:17,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 4.4515: : 442it [06:18,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 4.4515: : 443it [06:18,  1.23s/it]\u001b[A\n",
      "batch 444, training loss: 4.4587: : 443it [06:19,  1.23s/it]\u001b[A\n",
      "batch 444, training loss: 4.4587: : 444it [06:19,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 4.4522: : 444it [06:21,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 4.4522: : 445it [06:21,  1.23s/it]\u001b[A\n",
      "batch 446, training loss: 4.5772: : 445it [06:22,  1.23s/it]\u001b[A\n",
      "batch 446, training loss: 4.5772: : 446it [06:22,  1.23s/it]\u001b[A\n",
      "batch 447, training loss: 4.4669: : 446it [06:23,  1.23s/it]\u001b[A\n",
      "batch 447, training loss: 4.4669: : 447it [06:23,  1.24s/it]\u001b[A\n",
      "batch 448, training loss: 4.3588: : 447it [06:24,  1.24s/it]\u001b[A\n",
      "batch 448, training loss: 4.3588: : 448it [06:24,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 4.5067: : 448it [06:26,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 4.5067: : 449it [06:26,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 4.4944: : 449it [06:27,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 4.4944: : 450it [06:27,  1.23s/it]\u001b[A\n",
      "batch 451, training loss: 4.5077: : 450it [06:28,  1.23s/it]\u001b[A\n",
      "batch 451, training loss: 4.5077: : 451it [06:28,  1.24s/it]\u001b[A\n",
      "batch 452, training loss: 4.522: : 451it [06:29,  1.24s/it] \u001b[A\n",
      "batch 452, training loss: 4.522: : 452it [06:29,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.5491: : 452it [06:31,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.5491: : 453it [06:31,  1.24s/it]\u001b[A\n",
      "batch 454, training loss: 4.4414: : 453it [06:32,  1.24s/it]\u001b[A\n",
      "batch 454, training loss: 4.4414: : 454it [06:32,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 4.304: : 454it [06:33,  1.23s/it] \u001b[A\n",
      "batch 455, training loss: 4.304: : 455it [06:33,  1.24s/it]\u001b[A\n",
      "batch 456, training loss: 4.3594: : 455it [06:34,  1.24s/it]\u001b[A\n",
      "batch 456, training loss: 4.3594: : 456it [06:34,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 4.5296: : 456it [06:35,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 4.5296: : 457it [06:35,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 4.3486: : 457it [06:37,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 4.3486: : 458it [06:37,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.3856: : 458it [06:38,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.3856: : 459it [06:38,  1.24s/it]\u001b[A\n",
      "batch 460, training loss: 4.4288: : 459it [06:39,  1.24s/it]\u001b[A\n",
      "batch 460, training loss: 4.4288: : 460it [06:39,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 4.5203: : 460it [06:40,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 4.5203: : 461it [06:40,  1.24s/it]\u001b[A\n",
      "batch 462, training loss: 4.4247: : 461it [06:42,  1.24s/it]\u001b[A\n",
      "batch 462, training loss: 4.4247: : 462it [06:42,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 4.3161: : 462it [06:43,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 4.3161: : 463it [06:43,  1.24s/it]\u001b[A\n",
      "batch 464, training loss: 4.4155: : 463it [06:44,  1.24s/it]\u001b[A\n",
      "batch 464, training loss: 4.4155: : 464it [06:44,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 4.4415: : 464it [06:45,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 4.4415: : 465it [06:45,  1.19s/it]\u001b[A\n",
      "batch 466, training loss: 4.3933: : 465it [06:46,  1.19s/it]\u001b[A\n",
      "batch 466, training loss: 4.3933: : 466it [06:46,  1.23s/it]\u001b[A\n",
      "batch 467, training loss: 4.3409: : 466it [06:48,  1.23s/it]\u001b[A\n",
      "batch 467, training loss: 4.3409: : 467it [06:48,  1.26s/it]\u001b[A\n",
      "batch 468, training loss: 4.4489: : 467it [06:49,  1.26s/it]\u001b[A\n",
      "batch 468, training loss: 4.4489: : 468it [06:49,  1.26s/it]\u001b[A\n",
      "batch 469, training loss: 4.3916: : 468it [06:50,  1.26s/it]\u001b[A\n",
      "batch 469, training loss: 4.3916: : 469it [06:50,  1.26s/it]\u001b[A\n",
      "batch 470, training loss: 4.5135: : 469it [06:52,  1.26s/it]\u001b[A\n",
      "batch 470, training loss: 4.5135: : 470it [06:52,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 4.5366: : 470it [06:53,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 4.5366: : 471it [06:53,  1.26s/it]\u001b[A\n",
      "batch 472, training loss: 4.461: : 471it [06:54,  1.26s/it] \u001b[A\n",
      "batch 472, training loss: 4.461: : 472it [06:54,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 4.4408: : 472it [06:55,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 4.4408: : 473it [06:55,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 4.5243: : 473it [06:57,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 4.5243: : 474it [06:57,  1.28s/it]\u001b[A\n",
      "batch 475, training loss: 4.4693: : 474it [06:58,  1.28s/it]\u001b[A\n",
      "batch 475, training loss: 4.4693: : 475it [06:58,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 4.5283: : 475it [06:59,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 4.5283: : 476it [06:59,  1.28s/it]\u001b[A\n",
      "batch 477, training loss: 4.3333: : 476it [07:00,  1.28s/it]\u001b[A\n",
      "batch 477, training loss: 4.3333: : 477it [07:00,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.4307: : 477it [07:02,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.4307: : 478it [07:02,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.4012: : 478it [07:03,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.4012: : 479it [07:03,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 4.3983: : 479it [07:04,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 4.3983: : 480it [07:04,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 4.4532: : 480it [07:05,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 4.4532: : 481it [07:05,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 4.4043: : 481it [07:07,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 4.4043: : 482it [07:07,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 4.3314: : 482it [07:08,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 4.3314: : 483it [07:08,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.3896: : 483it [07:09,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.3896: : 484it [07:09,  1.24s/it]\u001b[A\n",
      "batch 485, training loss: 4.3782: : 484it [07:10,  1.24s/it]\u001b[A\n",
      "batch 485, training loss: 4.3782: : 485it [07:10,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 4.441: : 485it [07:12,  1.26s/it] \u001b[A\n",
      "batch 486, training loss: 4.441: : 486it [07:12,  1.25s/it]\u001b[A\n",
      "batch 487, training loss: 4.5468: : 486it [07:13,  1.25s/it]\u001b[A\n",
      "batch 487, training loss: 4.5468: : 487it [07:13,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 4.348: : 487it [07:14,  1.26s/it] \u001b[A\n",
      "batch 488, training loss: 4.348: : 488it [07:14,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 4.2619: : 488it [07:16,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 4.2619: : 489it [07:16,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 4.3593: : 489it [07:17,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 4.3593: : 490it [07:17,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 4.3381: : 490it [07:18,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 4.3381: : 491it [07:18,  1.27s/it]\u001b[A\n",
      "batch 492, training loss: 4.2817: : 491it [07:19,  1.27s/it]\u001b[A\n",
      "batch 492, training loss: 4.2817: : 492it [07:19,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 4.4313: : 492it [07:21,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 493, training loss: 4.4313: : 493it [07:21,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 4.4613: : 493it [07:22,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 4.4613: : 494it [07:22,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 4.2406: : 494it [07:23,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 4.2406: : 495it [07:23,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 4.3099: : 495it [07:24,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 4.3099: : 496it [07:24,  1.27s/it]\u001b[A\n",
      "batch 497, training loss: 4.2206: : 496it [07:26,  1.27s/it]\u001b[A\n",
      "batch 497, training loss: 4.2206: : 497it [07:26,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 4.297: : 497it [07:27,  1.26s/it] \u001b[A\n",
      "batch 498, training loss: 4.297: : 498it [07:27,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 4.3639: : 498it [07:28,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 4.3639: : 499it [07:28,  1.14s/it]\u001b[A\n",
      "batch 500, training loss: 4.519: : 499it [07:29,  1.14s/it] \u001b[A\n",
      "batch 500, training loss: 4.519: : 500it [07:29,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 4.4074: : 500it [07:30,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 4.4074: : 501it [07:30,  1.25s/it]\u001b[A\n",
      "batch 502, training loss: 4.3424: : 501it [07:32,  1.25s/it]\u001b[A\n",
      "batch 502, training loss: 4.3424: : 502it [07:32,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 4.4523: : 502it [07:33,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 4.4523: : 503it [07:33,  1.30s/it]\u001b[A\n",
      "batch 504, training loss: 4.4315: : 503it [07:35,  1.30s/it]\u001b[A\n",
      "batch 504, training loss: 4.4315: : 504it [07:35,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 4.507: : 504it [07:36,  1.34s/it] \u001b[A\n",
      "batch 505, training loss: 4.507: : 505it [07:36,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 4.4778: : 505it [07:37,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 4.4778: : 506it [07:37,  1.27s/it]\u001b[A\n",
      "batch 507, training loss: 4.3052: : 506it [07:38,  1.27s/it]\u001b[A\n",
      "batch 507, training loss: 4.3052: : 507it [07:38,  1.31s/it]\u001b[A\n",
      "batch 508, training loss: 4.4494: : 507it [07:40,  1.31s/it]\u001b[A\n",
      "batch 508, training loss: 4.4494: : 508it [07:40,  1.33s/it]\u001b[A\n",
      "batch 509, training loss: 4.3515: : 508it [07:41,  1.33s/it]\u001b[A\n",
      "batch 509, training loss: 4.3515: : 509it [07:41,  1.34s/it]\u001b[A\n",
      "batch 510, training loss: 4.4072: : 509it [07:43,  1.34s/it]\u001b[A\n",
      "batch 510, training loss: 4.4072: : 510it [07:43,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 4.4222: : 510it [07:44,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 4.4222: : 511it [07:44,  1.36s/it]\u001b[A\n",
      "batch 512, training loss: 4.4041: : 511it [07:45,  1.36s/it]\u001b[A\n",
      "batch 512, training loss: 4.4041: : 512it [07:45,  1.37s/it]\u001b[A\n",
      "batch 513, training loss: 4.447: : 512it [07:47,  1.37s/it] \u001b[A\n",
      "batch 513, training loss: 4.447: : 513it [07:47,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 4.2726: : 513it [07:48,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 4.2726: : 514it [07:48,  1.33s/it]\u001b[A\n",
      "batch 515, training loss: 4.4684: : 514it [07:49,  1.33s/it]\u001b[A\n",
      "batch 515, training loss: 4.4684: : 515it [07:49,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 4.3654: : 515it [07:51,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 4.3654: : 516it [07:51,  1.37s/it]\u001b[A\n",
      "batch 517, training loss: 4.3144: : 516it [07:52,  1.37s/it]\u001b[A\n",
      "batch 517, training loss: 4.3144: : 517it [07:52,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 4.3734: : 517it [07:54,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 4.3734: : 518it [07:54,  1.39s/it]\u001b[A\n",
      "batch 519, training loss: 4.2292: : 518it [07:55,  1.39s/it]\u001b[A\n",
      "batch 519, training loss: 4.2292: : 519it [07:55,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 4.3982: : 519it [07:56,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 4.3982: : 520it [07:56,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 4.3924: : 520it [07:58,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 4.3924: : 521it [07:58,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 4.4834: : 521it [07:59,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 4.4834: : 522it [07:59,  1.35s/it]\u001b[A\n",
      "batch 523, training loss: 4.3: : 522it [08:00,  1.35s/it]   \u001b[A\n",
      "batch 523, training loss: 4.3: : 523it [08:00,  1.38s/it]\u001b[A\n",
      "batch 524, training loss: 4.4922: : 523it [08:02,  1.38s/it]\u001b[A\n",
      "batch 524, training loss: 4.4922: : 524it [08:02,  1.38s/it]\u001b[A\n",
      "batch 525, training loss: 4.4951: : 524it [08:03,  1.38s/it]\u001b[A\n",
      "batch 525, training loss: 4.4951: : 525it [08:03,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.3227: : 525it [08:04,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.3227: : 526it [08:04,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 4.241: : 526it [08:06,  1.37s/it] \u001b[A\n",
      "batch 527, training loss: 4.241: : 527it [08:06,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.4004: : 527it [08:07,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.4004: : 528it [08:07,  1.33s/it]\u001b[A\n",
      "batch 529, training loss: 4.3896: : 528it [08:09,  1.33s/it]\u001b[A\n",
      "batch 529, training loss: 4.3896: : 529it [08:09,  1.41s/it]\u001b[A\n",
      "batch 530, training loss: 4.4208: : 529it [08:10,  1.41s/it]\u001b[A\n",
      "batch 530, training loss: 4.4208: : 530it [08:10,  1.45s/it]\u001b[A\n",
      "batch 531, training loss: 4.3303: : 530it [08:12,  1.45s/it]\u001b[A\n",
      "batch 531, training loss: 4.3303: : 531it [08:12,  1.47s/it]\u001b[A\n",
      "batch 532, training loss: 4.1608: : 531it [08:13,  1.47s/it]\u001b[A\n",
      "batch 532, training loss: 4.1608: : 532it [08:13,  1.44s/it]\u001b[A\n",
      "batch 533, training loss: 4.3758: : 532it [08:14,  1.44s/it]\u001b[A\n",
      "batch 533, training loss: 4.3758: : 533it [08:14,  1.38s/it]\u001b[A\n",
      "batch 534, training loss: 4.4509: : 533it [08:16,  1.38s/it]\u001b[A\n",
      "batch 534, training loss: 4.4509: : 534it [08:16,  1.42s/it]\u001b[A\n",
      "batch 535, training loss: 4.3166: : 534it [08:17,  1.42s/it]\u001b[A\n",
      "batch 535, training loss: 4.3166: : 535it [08:17,  1.44s/it]\u001b[A\n",
      "batch 536, training loss: 4.1896: : 535it [08:19,  1.44s/it]\u001b[A\n",
      "batch 536, training loss: 4.1896: : 536it [08:19,  1.45s/it]\u001b[A\n",
      "batch 537, training loss: 4.2238: : 536it [08:20,  1.45s/it]\u001b[A\n",
      "batch 537, training loss: 4.2238: : 537it [08:20,  1.44s/it]\u001b[A\n",
      "batch 538, training loss: 4.3906: : 537it [08:22,  1.44s/it]\u001b[A\n",
      "batch 538, training loss: 4.3906: : 538it [08:22,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 4.2906: : 538it [08:23,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 4.2906: : 539it [08:23,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 4.2973: : 539it [08:25,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 4.2973: : 540it [08:25,  1.44s/it]\u001b[A\n",
      "batch 541, training loss: 4.3624: : 540it [08:26,  1.44s/it]\u001b[A\n",
      "batch 541, training loss: 4.3624: : 541it [08:26,  1.46s/it]\u001b[A\n",
      "batch 542, training loss: 4.3819: : 541it [08:28,  1.46s/it]\u001b[A\n",
      "batch 542, training loss: 4.3819: : 542it [08:28,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 4.2901: : 542it [08:29,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 4.2901: : 543it [08:29,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 4.3309: : 543it [08:31,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 4.3309: : 544it [08:31,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 4.3339: : 544it [08:32,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 4.3339: : 545it [08:32,  1.47s/it]\u001b[A\n",
      "batch 546, training loss: 4.2332: : 545it [08:33,  1.47s/it]\u001b[A\n",
      "batch 546, training loss: 4.2332: : 546it [08:33,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 4.132: : 546it [08:35,  1.48s/it] \u001b[A\n",
      "batch 547, training loss: 4.132: : 547it [08:35,  1.50s/it]\u001b[A\n",
      "batch 548, training loss: 4.3068: : 547it [08:36,  1.50s/it]\u001b[A\n",
      "batch 548, training loss: 4.3068: : 548it [08:36,  1.30s/it]\u001b[A\n",
      "batch 549, training loss: 4.3029: : 548it [08:37,  1.30s/it]\u001b[A\n",
      "batch 549, training loss: 4.3029: : 549it [08:37,  1.40s/it]\u001b[A\n",
      "batch 550, training loss: 4.1756: : 549it [08:39,  1.40s/it]\u001b[A\n",
      "batch 550, training loss: 4.1756: : 550it [08:39,  1.47s/it]\u001b[A\n",
      "batch 551, training loss: 4.2996: : 550it [08:41,  1.47s/it]\u001b[A\n",
      "batch 551, training loss: 4.2996: : 551it [08:41,  1.52s/it]\u001b[A\n",
      "batch 552, training loss: 4.2949: : 551it [08:42,  1.52s/it]\u001b[A\n",
      "batch 552, training loss: 4.2949: : 552it [08:42,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 4.3133: : 552it [08:44,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 4.3133: : 553it [08:44,  1.56s/it]\u001b[A\n",
      "batch 554, training loss: 4.003: : 553it [08:45,  1.56s/it] \u001b[A\n",
      "batch 554, training loss: 4.003: : 554it [08:45,  1.53s/it]\u001b[A\n",
      "batch 555, training loss: 4.21: : 554it [08:47,  1.53s/it] \u001b[A\n",
      "batch 555, training loss: 4.21: : 555it [08:47,  1.52s/it]\u001b[A\n",
      "batch 556, training loss: 4.3465: : 555it [08:49,  1.52s/it]\u001b[A\n",
      "batch 556, training loss: 4.3465: : 556it [08:49,  1.54s/it]\u001b[A\n",
      "batch 557, training loss: 4.1609: : 556it [08:50,  1.54s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 557, training loss: 4.1609: : 557it [08:50,  1.55s/it]\u001b[A\n",
      "batch 558, training loss: 4.0947: : 557it [08:52,  1.55s/it]\u001b[A\n",
      "batch 558, training loss: 4.0947: : 558it [08:52,  1.57s/it]\u001b[A\n",
      "batch 559, training loss: 4.1709: : 558it [08:53,  1.57s/it]\u001b[A\n",
      "batch 559, training loss: 4.1709: : 559it [08:53,  1.58s/it]\u001b[A\n",
      "batch 560, training loss: 4.1789: : 559it [08:55,  1.58s/it]\u001b[A\n",
      "batch 560, training loss: 4.1789: : 560it [08:55,  1.60s/it]\u001b[A\n",
      "batch 561, training loss: 4.1988: : 560it [08:57,  1.60s/it]\u001b[A\n",
      "batch 561, training loss: 4.1988: : 561it [08:57,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 4.0209: : 561it [08:58,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 4.0209: : 562it [08:58,  1.59s/it]\u001b[A\n",
      "batch 563, training loss: 4.1832: : 562it [09:00,  1.59s/it]\u001b[A\n",
      "batch 563, training loss: 4.1832: : 563it [09:00,  1.57s/it]\u001b[A\n",
      "batch 564, training loss: 4.105: : 563it [09:01,  1.57s/it] \u001b[A\n",
      "batch 564, training loss: 4.105: : 564it [09:01,  1.55s/it]\u001b[A\n",
      "batch 565, training loss: 4.1701: : 564it [09:03,  1.55s/it]\u001b[A\n",
      "batch 565, training loss: 4.1701: : 565it [09:03,  1.52s/it]\u001b[A\n",
      "batch 566, training loss: 4.3409: : 565it [09:04,  1.52s/it]\u001b[A\n",
      "batch 566, training loss: 4.3409: : 566it [09:04,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 4.2407: : 566it [09:06,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 4.2407: : 567it [09:06,  1.60s/it]\u001b[A\n",
      "batch 568, training loss: 4.3555: : 567it [09:08,  1.60s/it]\u001b[A\n",
      "batch 568, training loss: 4.3555: : 568it [09:08,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 4.2913: : 568it [09:09,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 4.2913: : 569it [09:09,  1.64s/it]\u001b[A\n",
      "batch 570, training loss: 4.503: : 569it [09:11,  1.64s/it] \u001b[A\n",
      "batch 570, training loss: 4.503: : 570it [09:11,  1.57s/it]\u001b[A\n",
      "batch 571, training loss: 4.3106: : 570it [09:12,  1.57s/it]\u001b[A\n",
      "batch 571, training loss: 4.3106: : 571it [09:12,  1.60s/it]\u001b[A\n",
      "batch 572, training loss: 4.3502: : 571it [09:14,  1.60s/it]\u001b[A\n",
      "batch 572, training loss: 4.3502: : 572it [09:14,  1.63s/it]\u001b[A\n",
      "batch 573, training loss: 4.2999: : 572it [09:16,  1.63s/it]\u001b[A\n",
      "batch 573, training loss: 4.2999: : 573it [09:16,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 4.3564: : 573it [09:17,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 4.3564: : 574it [09:17,  1.65s/it]\u001b[A\n",
      "batch 575, training loss: 4.2729: : 574it [09:18,  1.65s/it]\u001b[A\n",
      "batch 575, training loss: 4.2729: : 575it [09:18,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 4.3062: : 575it [09:20,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 4.3062: : 576it [09:20,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 4.2093: : 576it [09:22,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 4.2093: : 577it [09:22,  1.58s/it]\u001b[A\n",
      "batch 578, training loss: 4.2043: : 577it [09:23,  1.58s/it]\u001b[A\n",
      "batch 578, training loss: 4.2043: : 578it [09:23,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 4.1949: : 578it [09:25,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 4.1949: : 579it [09:25,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 4.1505: : 579it [09:27,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 4.1505: : 580it [09:27,  1.67s/it]\u001b[A\n",
      "batch 581, training loss: 4.1398: : 580it [09:29,  1.67s/it]\u001b[A\n",
      "batch 581, training loss: 4.1398: : 581it [09:29,  1.69s/it]\u001b[A\n",
      "batch 582, training loss: 4.1711: : 581it [09:30,  1.69s/it]\u001b[A\n",
      "batch 582, training loss: 4.1711: : 582it [09:30,  1.71s/it]\u001b[A\n",
      "batch 583, training loss: 4.1055: : 582it [09:31,  1.71s/it]\u001b[A\n",
      "batch 583, training loss: 4.1055: : 583it [09:31,  1.41s/it]\u001b[A\n",
      "batch 584, training loss: 4.3798: : 583it [09:33,  1.41s/it]\u001b[A\n",
      "batch 584, training loss: 4.3798: : 584it [09:33,  1.53s/it]\u001b[A\n",
      "batch 585, training loss: 4.358: : 584it [09:35,  1.53s/it] \u001b[A\n",
      "batch 585, training loss: 4.358: : 585it [09:35,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 4.453: : 585it [09:37,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 4.453: : 586it [09:37,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 4.3312: : 586it [09:39,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 4.3312: : 587it [09:39,  1.76s/it]\u001b[A\n",
      "batch 588, training loss: 4.2019: : 587it [09:41,  1.76s/it]\u001b[A\n",
      "batch 588, training loss: 4.2019: : 588it [09:41,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 4.2824: : 588it [09:42,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 4.2824: : 589it [09:42,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 4.3052: : 589it [09:44,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 4.3052: : 590it [09:44,  1.88s/it]\u001b[A\n",
      "batch 591, training loss: 4.1113: : 590it [09:46,  1.88s/it]\u001b[A\n",
      "batch 591, training loss: 4.1113: : 591it [09:46,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 4.1778: : 591it [09:48,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 4.1778: : 592it [09:48,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 4.1233: : 592it [09:50,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 4.1233: : 593it [09:50,  1.92s/it]\u001b[A\n",
      "batch 594, training loss: 4.2948: : 593it [09:52,  1.92s/it]\u001b[A\n",
      "batch 594, training loss: 4.2948: : 594it [09:52,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 4.3925: : 594it [09:54,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 4.3925: : 595it [09:54,  1.86s/it]\u001b[A\n",
      "batch 596, training loss: 4.2397: : 595it [09:56,  1.86s/it]\u001b[A\n",
      "batch 596, training loss: 4.2397: : 596it [09:56,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 4.0923: : 596it [09:58,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 4.0923: : 597it [09:58,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 4.3556: : 597it [10:00,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 4.3556: : 598it [10:00,  2.04s/it]\u001b[A\n",
      "batch 599, training loss: 4.2847: : 598it [10:02,  2.04s/it]\u001b[A\n",
      "batch 599, training loss: 4.2847: : 599it [10:02,  1.86s/it]\u001b[A\n",
      "batch 600, training loss: 4.0644: : 599it [10:04,  1.86s/it]\u001b[A\n",
      "batch 600, training loss: 4.0644: : 600it [10:04,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 3.7026: : 600it [10:05,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 3.7026: : 601it [10:05,  1.69s/it]\u001b[A\n",
      "batch 602, training loss: 4.1726: : 601it [10:07,  1.69s/it]\u001b[A\n",
      "batch 602, training loss: 4.1726: : 602it [10:07,  1.80s/it]\u001b[A\n",
      "batch 603, training loss: 4.1614: : 602it [10:09,  1.80s/it]\u001b[A\n",
      "batch 603, training loss: 4.1614: : 603it [10:09,  1.78s/it]\u001b[A\n",
      "batch 604, training loss: 4.3369: : 603it [10:11,  1.78s/it]\u001b[A\n",
      "batch 604, training loss: 4.3369: : 604it [10:11,  1.81s/it]\u001b[A\n",
      "batch 605, training loss: 4.4051: : 604it [10:12,  1.81s/it]\u001b[A\n",
      "batch 605, training loss: 4.4051: : 605it [10:12,  1.79s/it]\u001b[A\n",
      "batch 606, training loss: 4.2434: : 605it [10:14,  1.79s/it]\u001b[A\n",
      "batch 606, training loss: 4.2434: : 606it [10:14,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 4.1107: : 606it [10:16,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 4.1107: : 607it [10:16,  1.68s/it]\u001b[A\n",
      "batch 608, training loss: 4.5012: : 607it [10:17,  1.68s/it]\u001b[A\n",
      "batch 608, training loss: 4.5012: : 608it [10:17,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 4.4446: : 608it [10:18,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 4.4446: : 609it [10:18,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 3.9788: : 609it [10:19,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 3.9788: : 610it [10:19,  1.43s/it]\u001b[A\n",
      "batch 611, training loss: 3.9551: : 610it [10:21,  1.43s/it]\u001b[A\n",
      "batch 611, training loss: 3.9551: : 611it [10:21,  1.36s/it]\u001b[A\n",
      "batch 612, training loss: 3.1021: : 611it [10:22,  1.36s/it]\u001b[A\n",
      "batch 612, training loss: 3.1021: : 612it [10:22,  1.29s/it]\u001b[A\n",
      "batch 613, training loss: 4.2243: : 612it [10:23,  1.29s/it]\u001b[A\n",
      "batch 613, training loss: 4.2243: : 616it [10:23,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 4.4012: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 4.4012: : 1it [00:00,  3.80it/s]\u001b[A\n",
      "batch 1, dev loss: 4.6178: : 1it [00:00,  3.80it/s]\u001b[A\n",
      "batch 1, dev loss: 4.6178: : 2it [00:00,  4.59it/s]\u001b[A\n",
      "batch 2, dev loss: 4.2022: : 2it [00:00,  4.59it/s]\u001b[A\n",
      "batch 2, dev loss: 4.2022: : 3it [00:00,  4.78it/s]\u001b[A\n",
      "batch 3, dev loss: 4.3149: : 3it [00:00,  4.78it/s]\u001b[A\n",
      "batch 3, dev loss: 4.3149: : 4it [00:00,  4.70it/s]\u001b[A\n",
      "batch 4, dev loss: 4.3352: : 4it [00:01,  4.70it/s]\u001b[A\n",
      "batch 4, dev loss: 4.3352: : 5it [00:01,  4.61it/s]\u001b[A\n",
      "batch 5, dev loss: 4.3222: : 5it [00:01,  4.61it/s]\u001b[A\n",
      "batch 5, dev loss: 4.3222: : 6it [00:01,  4.87it/s]\u001b[A\n",
      "batch 6, dev loss: 4.4616: : 6it [00:01,  4.87it/s]\u001b[A\n",
      "batch 6, dev loss: 4.4616: : 7it [00:01,  5.17it/s]\u001b[A\n",
      "batch 7, dev loss: 4.2375: : 7it [00:01,  5.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7, dev loss: 4.2375: : 8it [00:01,  5.31it/s]\u001b[A\n",
      "batch 8, dev loss: 4.4601: : 8it [00:01,  5.31it/s]\u001b[A\n",
      "batch 8, dev loss: 4.4601: : 9it [00:01,  4.91it/s]\u001b[A\n",
      "batch 9, dev loss: 4.3125: : 9it [00:02,  4.91it/s]\u001b[A\n",
      "batch 9, dev loss: 4.3125: : 10it [00:02,  4.84it/s]\u001b[A\n",
      "batch 10, dev loss: 4.3906: : 10it [00:02,  4.84it/s]\u001b[A\n",
      "batch 10, dev loss: 4.3906: : 11it [00:02,  4.74it/s]\u001b[A\n",
      "batch 11, dev loss: 4.477: : 11it [00:02,  4.74it/s] \u001b[A\n",
      "batch 11, dev loss: 4.477: : 12it [00:02,  4.53it/s]\u001b[A\n",
      "batch 12, dev loss: 4.3263: : 12it [00:02,  4.53it/s]\u001b[A\n",
      "batch 12, dev loss: 4.3263: : 13it [00:02,  4.54it/s]\u001b[A\n",
      "batch 13, dev loss: 4.4641: : 13it [00:02,  4.54it/s]\u001b[A\n",
      "batch 13, dev loss: 4.4641: : 14it [00:02,  4.42it/s]\u001b[A\n",
      "batch 14, dev loss: 4.5264: : 14it [00:03,  4.42it/s]\u001b[A\n",
      "batch 14, dev loss: 4.5264: : 15it [00:03,  4.46it/s]\u001b[A\n",
      "batch 15, dev loss: 4.4394: : 15it [00:03,  4.46it/s]\u001b[A\n",
      "batch 15, dev loss: 4.4394: : 16it [00:03,  4.96it/s]\u001b[A\n",
      "batch 16, dev loss: 4.6798: : 16it [00:03,  4.96it/s]\u001b[A\n",
      "batch 16, dev loss: 4.6798: : 17it [00:03,  4.53it/s]\u001b[A\n",
      "batch 17, dev loss: 4.4378: : 17it [00:03,  4.53it/s]\u001b[A\n",
      "batch 17, dev loss: 4.4378: : 18it [00:03,  4.15it/s]\u001b[A\n",
      "batch 18, dev loss: 4.3647: : 18it [00:04,  4.15it/s]\u001b[A\n",
      "batch 18, dev loss: 4.3647: : 19it [00:04,  4.10it/s]\u001b[A\n",
      "batch 19, dev loss: 4.5542: : 19it [00:04,  4.10it/s]\u001b[A\n",
      "batch 19, dev loss: 4.5542: : 20it [00:04,  3.93it/s]\u001b[A\n",
      "batch 20, dev loss: 4.3308: : 20it [00:04,  3.93it/s]\u001b[A\n",
      "batch 20, dev loss: 4.3308: : 21it [00:04,  3.82it/s]\u001b[A\n",
      "batch 21, dev loss: 4.2435: : 21it [00:04,  3.82it/s]\u001b[A\n",
      "batch 21, dev loss: 4.2435: : 22it [00:04,  4.03it/s]\u001b[A\n",
      "batch 22, dev loss: 4.4232: : 22it [00:05,  4.03it/s]\u001b[A\n",
      "batch 22, dev loss: 4.4232: : 23it [00:05,  3.98it/s]\u001b[A\n",
      "batch 23, dev loss: 4.4393: : 23it [00:05,  3.98it/s]\u001b[A\n",
      "batch 23, dev loss: 4.4393: : 24it [00:05,  4.54it/s]\u001b[A\n",
      "batch 24, dev loss: 4.4079: : 24it [00:05,  4.54it/s]\u001b[A\n",
      "batch 24, dev loss: 4.4079: : 25it [00:05,  4.19it/s]\u001b[A\n",
      "batch 25, dev loss: 4.4282: : 25it [00:05,  4.19it/s]\u001b[A\n",
      "batch 25, dev loss: 4.4282: : 26it [00:05,  3.89it/s]\u001b[A\n",
      "batch 26, dev loss: 4.367: : 26it [00:06,  3.89it/s] \u001b[A\n",
      "batch 26, dev loss: 4.367: : 27it [00:06,  3.74it/s]\u001b[A\n",
      "batch 27, dev loss: 4.2944: : 27it [00:06,  3.74it/s]\u001b[A\n",
      "batch 27, dev loss: 4.2944: : 28it [00:06,  3.76it/s]\u001b[A\n",
      "batch 28, dev loss: 4.4705: : 28it [00:06,  3.76it/s]\u001b[A\n",
      "batch 28, dev loss: 4.4705: : 29it [00:06,  3.79it/s]\u001b[A\n",
      "batch 29, dev loss: 4.4184: : 29it [00:07,  3.79it/s]\u001b[A\n",
      "batch 29, dev loss: 4.4184: : 30it [00:07,  3.63it/s]\u001b[A\n",
      "batch 30, dev loss: 4.5988: : 30it [00:07,  3.63it/s]\u001b[A\n",
      "batch 30, dev loss: 4.5988: : 31it [00:07,  4.41it/s]\u001b[A\n",
      "batch 31, dev loss: 4.4962: : 31it [00:07,  4.41it/s]\u001b[A\n",
      "batch 31, dev loss: 4.4962: : 32it [00:07,  3.89it/s]\u001b[A\n",
      "batch 32, dev loss: 4.4927: : 32it [00:07,  3.89it/s]\u001b[A\n",
      "batch 32, dev loss: 4.4927: : 33it [00:07,  3.48it/s]\u001b[A\n",
      "batch 33, dev loss: 4.2678: : 33it [00:08,  3.48it/s]\u001b[A\n",
      "batch 33, dev loss: 4.2678: : 34it [00:08,  3.28it/s]\u001b[A\n",
      "batch 34, dev loss: 4.7042: : 34it [00:08,  3.28it/s]\u001b[A\n",
      "batch 34, dev loss: 4.7042: : 35it [00:08,  3.17it/s]\u001b[A\n",
      "batch 35, dev loss: 4.4925: : 35it [00:08,  3.17it/s]\u001b[A\n",
      "batch 35, dev loss: 4.4925: : 36it [00:08,  3.17it/s]\u001b[A\n",
      "batch 36, dev loss: 4.4535: : 36it [00:09,  3.17it/s]\u001b[A\n",
      "batch 36, dev loss: 4.4535: : 37it [00:09,  3.29it/s]\u001b[A\n",
      "batch 37, dev loss: 4.2672: : 37it [00:09,  3.29it/s]\u001b[A\n",
      "batch 37, dev loss: 4.2672: : 38it [00:09,  3.25it/s]\u001b[A\n",
      "batch 38, dev loss: 4.5066: : 38it [00:09,  3.25it/s]\u001b[A\n",
      "batch 38, dev loss: 4.5066: : 39it [00:09,  3.06it/s]\u001b[A\n",
      "batch 39, dev loss: 4.4951: : 39it [00:10,  3.06it/s]\u001b[A\n",
      "batch 39, dev loss: 4.4951: : 40it [00:10,  3.03it/s]\u001b[A\n",
      "batch 40, dev loss: 4.5297: : 40it [00:10,  3.03it/s]\u001b[A\n",
      "batch 40, dev loss: 4.5297: : 41it [00:10,  2.98it/s]\u001b[A\n",
      "batch 41, dev loss: 4.2908: : 41it [00:10,  2.98it/s]\u001b[A\n",
      "batch 41, dev loss: 4.2908: : 42it [00:10,  3.10it/s]\u001b[A\n",
      "batch 42, dev loss: 4.4518: : 42it [00:11,  3.10it/s]\u001b[A\n",
      "batch 42, dev loss: 4.4518: : 43it [00:11,  2.90it/s]\u001b[A\n",
      "batch 43, dev loss: 4.4739: : 43it [00:11,  2.90it/s]\u001b[A\n",
      "batch 43, dev loss: 4.4739: : 44it [00:11,  2.76it/s]\u001b[A\n",
      "batch 44, dev loss: 4.3806: : 44it [00:11,  2.76it/s]\u001b[A\n",
      "batch 44, dev loss: 4.3806: : 45it [00:11,  2.71it/s]\u001b[A\n",
      "batch 45, dev loss: 4.6703: : 45it [00:12,  2.71it/s]\u001b[A\n",
      "batch 45, dev loss: 4.6703: : 46it [00:12,  2.64it/s]\u001b[A\n",
      "batch 46, dev loss: 4.2059: : 46it [00:12,  2.64it/s]\u001b[A\n",
      "batch 46, dev loss: 4.2059: : 47it [00:12,  2.63it/s]\u001b[A\n",
      "batch 47, dev loss: 4.3876: : 47it [00:13,  2.63it/s]\u001b[A\n",
      "batch 47, dev loss: 4.3876: : 48it [00:13,  2.58it/s]\u001b[A\n",
      "batch 48, dev loss: 4.205: : 48it [00:13,  2.58it/s] \u001b[A\n",
      "batch 48, dev loss: 4.205: : 49it [00:13,  2.52it/s]\u001b[A\n",
      "batch 49, dev loss: 4.3219: : 49it [00:13,  2.52it/s]\u001b[A\n",
      "batch 49, dev loss: 4.3219: : 50it [00:13,  2.75it/s]\u001b[A\n",
      "batch 50, dev loss: 4.374: : 50it [00:14,  2.75it/s] \u001b[A\n",
      "batch 50, dev loss: 4.374: : 51it [00:14,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 4.3338: : 51it [00:14,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 4.3338: : 52it [00:14,  2.58it/s]\u001b[A\n",
      "batch 52, dev loss: 4.1618: : 52it [00:15,  2.58it/s]\u001b[A\n",
      "batch 52, dev loss: 4.1618: : 53it [00:15,  2.63it/s]\u001b[A\n",
      "batch 53, dev loss: 4.4031: : 53it [00:15,  2.63it/s]\u001b[A\n",
      "batch 53, dev loss: 4.4031: : 54it [00:15,  2.52it/s]\u001b[A\n",
      "batch 54, dev loss: 4.1885: : 54it [00:15,  2.52it/s]\u001b[A\n",
      "batch 54, dev loss: 4.1885: : 55it [00:15,  2.38it/s]\u001b[A\n",
      "batch 55, dev loss: 4.3084: : 55it [00:16,  2.38it/s]\u001b[A\n",
      "batch 55, dev loss: 4.3084: : 56it [00:16,  2.24it/s]\u001b[A\n",
      "batch 56, dev loss: 4.1738: : 56it [00:16,  2.24it/s]\u001b[A\n",
      "batch 56, dev loss: 4.1738: : 57it [00:16,  2.30it/s]\u001b[A\n",
      "batch 57, dev loss: 4.109: : 57it [00:17,  2.30it/s] \u001b[A\n",
      "batch 57, dev loss: 4.109: : 58it [00:17,  2.17it/s]\u001b[A\n",
      "batch 58, dev loss: 4.4059: : 58it [00:17,  2.17it/s]\u001b[A\n",
      "batch 58, dev loss: 4.4059: : 59it [00:17,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 4.4041: : 59it [00:18,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 4.4041: : 60it [00:18,  2.17it/s]\u001b[A\n",
      "batch 60, dev loss: 4.0457: : 60it [00:18,  2.17it/s]\u001b[A\n",
      "batch 60, dev loss: 4.0457: : 61it [00:18,  2.28it/s]\u001b[A\n",
      "batch 61, dev loss: 4.0924: : 61it [00:19,  2.28it/s]\u001b[A\n",
      "batch 61, dev loss: 4.0924: : 62it [00:19,  2.46it/s]\u001b[A\n",
      "batch 62, dev loss: 4.0257: : 62it [00:19,  2.46it/s]\u001b[A\n",
      "batch 62, dev loss: 4.0257: : 63it [00:19,  2.77it/s]\u001b[A\n",
      "batch 63, dev loss: 4.5325: : 63it [00:19,  2.77it/s]\u001b[A\n",
      "batch 63, dev loss: 4.5325: : 64it [00:19,  3.11it/s]\u001b[A\n",
      "batch 64, dev loss: 4.0654: : 64it [00:19,  3.11it/s]\u001b[A\n",
      "batch 64, dev loss: 4.0654: : 65it [00:19,  3.43it/s]\u001b[A\n",
      "batch 65, dev loss: 4.1983: : 65it [00:20,  3.43it/s]\u001b[A\n",
      "batch 65, dev loss: 4.1983: : 66it [00:20,  3.33it/s]\u001b[A\n",
      "batch 66, dev loss: 4.1766: : 66it [00:20,  3.33it/s]\u001b[A\n",
      "batch 66, dev loss: 4.1766: : 67it [00:20,  3.29it/s]\u001b[A\n",
      "batch 67, dev loss: 3.5347: : 67it [00:20,  3.29it/s]\u001b[A\n",
      "batch 67, dev loss: 3.5347: : 68it [00:20,  3.15it/s]\u001b[A\n",
      "batch 68, dev loss: 3.5986: : 68it [00:21,  3.15it/s]\u001b[A\n",
      "batch 68, dev loss: 3.5986: : 69it [00:21,  3.23it/s]\u001b[A\n",
      "batch 69, dev loss: 3.3913: : 69it [00:21,  3.23it/s]\u001b[A\n",
      "batch 69, dev loss: 3.3913: : 70it [00:21,  3.12it/s]\u001b[A\n",
      "batch 70, dev loss: 4.1614: : 70it [00:21,  3.12it/s]\u001b[A\n",
      "batch 70, dev loss: 4.1614: : 71it [00:21,  3.11it/s]\u001b[A\n",
      "batch 71, dev loss: 4.1519: : 71it [00:22,  3.11it/s]\u001b[A\n",
      "batch 71, dev loss: 4.1519: : 72it [00:22,  3.03it/s]\u001b[A\n",
      "batch 72, dev loss: 4.7119: : 72it [00:22,  3.03it/s]\u001b[A\n",
      "batch 72, dev loss: 4.7119: : 73it [00:22,  2.99it/s]\u001b[A\n",
      "batch 72, dev loss: 4.7119: : 74it [00:22,  3.76it/s]\u001b[A\n",
      "batch 72, dev loss: 4.7119: : 76it [00:22,  3.34it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.43s/it]\u001b[A\n",
      "2it [00:02,  1.43s/it]\u001b[A\n",
      "3it [00:03,  1.23s/it]\u001b[A\n",
      "4it [00:05,  1.31s/it]\u001b[A\n",
      "5it [00:06,  1.30s/it]\u001b[A\n",
      "6it [00:07,  1.30s/it]\u001b[A\n",
      "7it [00:09,  1.32s/it]\u001b[A\n",
      "8it [00:10,  1.28s/it]\u001b[A\n",
      "9it [00:12,  1.47s/it]\u001b[A\n",
      "10it [00:14,  1.58s/it]\u001b[A\n",
      "11it [00:15,  1.45s/it]\u001b[A\n",
      "12it [00:16,  1.45s/it]\u001b[A\n",
      "13it [00:18,  1.57s/it]\u001b[A\n",
      "14it [00:19,  1.51s/it]\u001b[A\n",
      "15it [00:21,  1.42s/it]\u001b[A\n",
      "16it [00:22,  1.29s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17it [00:23,  1.36s/it]\u001b[A\n",
      "18it [00:25,  1.62s/it]\u001b[A\n",
      "19it [00:27,  1.75s/it]\u001b[A\n",
      "20it [00:29,  1.73s/it]\u001b[A\n",
      "21it [00:31,  1.76s/it]\u001b[A\n",
      "22it [00:33,  1.83s/it]\u001b[A\n",
      "23it [00:35,  1.90s/it]\u001b[A\n",
      "24it [00:36,  1.47s/it]\u001b[A\n",
      "25it [00:38,  1.71s/it]\u001b[A\n",
      "26it [00:40,  1.87s/it]\u001b[A\n",
      "27it [00:42,  1.99s/it]\u001b[A\n",
      "28it [00:44,  2.03s/it]\u001b[A\n",
      "29it [00:47,  2.17s/it]\u001b[A\n",
      "30it [00:49,  2.10s/it]\u001b[A\n",
      "31it [00:51,  2.22s/it]\u001b[A\n",
      "32it [00:54,  2.39s/it]\u001b[A\n",
      "33it [00:57,  2.45s/it]\u001b[A\n",
      "34it [00:59,  2.46s/it]\u001b[A\n",
      "35it [01:02,  2.64s/it]\u001b[A\n",
      "36it [01:03,  1.95s/it]\u001b[A\n",
      "37it [01:06,  2.30s/it]\u001b[A\n",
      "38it [01:09,  2.56s/it]\u001b[A\n",
      "39it [01:10,  2.24s/it]\u001b[A\n",
      "40it [01:13,  2.34s/it]\u001b[A\n",
      "41it [01:14,  1.85s/it]\u001b[A\n",
      "42it [01:17,  2.41s/it]\u001b[A\n",
      "43it [01:21,  2.78s/it]\u001b[A\n",
      "44it [01:24,  2.95s/it]\u001b[A\n",
      "45it [01:27,  2.90s/it]\u001b[A\n",
      "46it [01:31,  3.09s/it]\u001b[A\n",
      "47it [01:34,  3.29s/it]\u001b[A\n",
      "48it [01:38,  3.48s/it]\u001b[A\n",
      "49it [01:39,  2.49s/it]\u001b[A\n",
      "50it [01:42,  2.92s/it]\u001b[A\n",
      "51it [01:47,  3.30s/it]\u001b[A\n",
      "52it [01:49,  3.06s/it]\u001b[A\n",
      "53it [01:54,  3.51s/it]\u001b[A\n",
      "54it [01:56,  3.06s/it]\u001b[A\n",
      "55it [02:00,  3.52s/it]\u001b[A\n",
      "56it [02:03,  3.13s/it]\u001b[A\n",
      "57it [02:07,  3.42s/it]\u001b[A\n",
      "58it [02:10,  3.48s/it]\u001b[A\n",
      "59it [02:13,  3.25s/it]\u001b[A\n",
      "60it [02:15,  2.90s/it]\u001b[A\n",
      "61it [02:17,  2.50s/it]\u001b[A\n",
      "62it [02:18,  2.07s/it]\u001b[A\n",
      "63it [02:19,  1.75s/it]\u001b[A\n",
      "64it [02:19,  1.37s/it]\u001b[A\n",
      "65it [02:20,  1.06s/it]\u001b[A\n",
      "66it [02:20,  1.23it/s]\u001b[A\n",
      "67it [02:20,  1.52it/s]\u001b[A\n",
      "68it [02:20,  1.77it/s]\u001b[A\n",
      "69it [02:21,  1.79it/s]\u001b[A\n",
      "70it [02:22,  2.03s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██▉                                   | 511/6740 [00:00<00:01, 5105.06it/s]\u001b[A\n",
      " 16%|█████▊                               | 1053/6740 [00:00<00:01, 5289.62it/s]\u001b[A\n",
      " 23%|████████▋                            | 1582/6740 [00:00<00:01, 5152.13it/s]\u001b[A\n",
      " 31%|███████████▌                         | 2109/6740 [00:00<00:00, 5197.69it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 2630/6740 [00:00<00:00, 5172.29it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 3154/6740 [00:00<00:00, 5194.34it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 3690/6740 [00:00<00:00, 5247.25it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 4215/6740 [00:00<00:00, 5197.36it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 4735/6740 [00:00<00:00, 5195.25it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 5255/6740 [00:01<00:00, 5139.56it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 5789/6740 [00:01<00:00, 5199.23it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5210.35it/s]\u001b[A\n",
      "Epoch: 2, tfr: 1.0, loss(train/dev): 4.4979/4.3341, ppl(dev/test): 76.2563/85.28\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 4.2674: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 4.2674: : 1it [00:01,  1.88s/it]\u001b[A\n",
      "batch 2, training loss: 4.2211: : 1it [00:02,  1.88s/it]\u001b[A\n",
      "batch 2, training loss: 4.2211: : 2it [00:02,  1.10s/it]\u001b[A\n",
      "batch 3, training loss: 4.3268: : 2it [00:03,  1.10s/it]\u001b[A\n",
      "batch 3, training loss: 4.3268: : 3it [00:03,  1.12it/s]\u001b[A\n",
      "batch 4, training loss: 4.2806: : 3it [00:03,  1.12it/s]\u001b[A\n",
      "batch 4, training loss: 4.2806: : 4it [00:03,  1.21it/s]\u001b[A\n",
      "batch 5, training loss: 4.0717: : 4it [00:04,  1.21it/s]\u001b[A\n",
      "batch 5, training loss: 4.0717: : 5it [00:04,  1.28it/s]\u001b[A\n",
      "batch 6, training loss: 4.2892: : 5it [00:05,  1.28it/s]\u001b[A\n",
      "batch 6, training loss: 4.2892: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 4.2982: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 4.2982: : 7it [00:05,  1.47it/s]\u001b[A\n",
      "batch 8, training loss: 4.3453: : 7it [00:06,  1.47it/s]\u001b[A\n",
      "batch 8, training loss: 4.3453: : 8it [00:06,  1.59it/s]\u001b[A\n",
      "batch 9, training loss: 4.1604: : 8it [00:06,  1.59it/s]\u001b[A\n",
      "batch 9, training loss: 4.1604: : 9it [00:06,  1.56it/s]\u001b[A\n",
      "batch 10, training loss: 4.1682: : 9it [00:07,  1.56it/s]\u001b[A\n",
      "batch 10, training loss: 4.1682: : 10it [00:07,  1.52it/s]\u001b[A\n",
      "batch 11, training loss: 4.2333: : 10it [00:08,  1.52it/s]\u001b[A\n",
      "batch 11, training loss: 4.2333: : 11it [00:08,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 4.2791: : 11it [00:09,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 4.2791: : 12it [00:09,  1.47it/s]\u001b[A\n",
      "batch 13, training loss: 4.1444: : 12it [00:09,  1.47it/s]\u001b[A\n",
      "batch 13, training loss: 4.1444: : 13it [00:09,  1.46it/s]\u001b[A\n",
      "batch 14, training loss: 4.4446: : 13it [00:10,  1.46it/s]\u001b[A\n",
      "batch 14, training loss: 4.4446: : 14it [00:10,  1.51it/s]\u001b[A\n",
      "batch 15, training loss: 4.2848: : 14it [00:10,  1.51it/s]\u001b[A\n",
      "batch 15, training loss: 4.2848: : 15it [00:10,  1.55it/s]\u001b[A\n",
      "batch 16, training loss: 4.2633: : 15it [00:11,  1.55it/s]\u001b[A\n",
      "batch 16, training loss: 4.2633: : 16it [00:11,  1.59it/s]\u001b[A\n",
      "batch 17, training loss: 4.4147: : 16it [00:12,  1.59it/s]\u001b[A\n",
      "batch 17, training loss: 4.4147: : 17it [00:12,  1.57it/s]\u001b[A\n",
      "batch 18, training loss: 4.2576: : 17it [00:12,  1.57it/s]\u001b[A\n",
      "batch 18, training loss: 4.2576: : 18it [00:12,  1.54it/s]\u001b[A\n",
      "batch 19, training loss: 4.0426: : 18it [00:13,  1.54it/s]\u001b[A\n",
      "batch 19, training loss: 4.0426: : 19it [00:13,  1.55it/s]\u001b[A\n",
      "batch 20, training loss: 4.1697: : 19it [00:14,  1.55it/s]\u001b[A\n",
      "batch 20, training loss: 4.1697: : 20it [00:14,  1.59it/s]\u001b[A\n",
      "batch 21, training loss: 4.2976: : 20it [00:14,  1.59it/s]\u001b[A\n",
      "batch 21, training loss: 4.2976: : 21it [00:14,  1.61it/s]\u001b[A\n",
      "batch 22, training loss: 4.0818: : 21it [00:15,  1.61it/s]\u001b[A\n",
      "batch 22, training loss: 4.0818: : 22it [00:15,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 4.2128: : 22it [00:16,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 4.2128: : 23it [00:16,  1.54it/s]\u001b[A\n",
      "batch 24, training loss: 4.1238: : 23it [00:16,  1.54it/s]\u001b[A\n",
      "batch 24, training loss: 4.1238: : 24it [00:16,  1.51it/s]\u001b[A\n",
      "batch 25, training loss: 4.2355: : 24it [00:17,  1.51it/s]\u001b[A\n",
      "batch 25, training loss: 4.2355: : 25it [00:17,  1.48it/s]\u001b[A\n",
      "batch 26, training loss: 4.0305: : 25it [00:18,  1.48it/s]\u001b[A\n",
      "batch 26, training loss: 4.0305: : 26it [00:18,  1.51it/s]\u001b[A\n",
      "batch 27, training loss: 4.1711: : 26it [00:18,  1.51it/s]\u001b[A\n",
      "batch 27, training loss: 4.1711: : 27it [00:18,  1.57it/s]\u001b[A\n",
      "batch 28, training loss: 4.0828: : 27it [00:19,  1.57it/s]\u001b[A\n",
      "batch 28, training loss: 4.0828: : 28it [00:19,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 4.1976: : 28it [00:19,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 4.1976: : 29it [00:19,  1.60it/s]\u001b[A\n",
      "batch 30, training loss: 4.2943: : 29it [00:20,  1.60it/s]\u001b[A\n",
      "batch 30, training loss: 4.2943: : 30it [00:20,  1.57it/s]\u001b[A\n",
      "batch 31, training loss: 4.0816: : 30it [00:20,  1.57it/s]\u001b[A\n",
      "batch 31, training loss: 4.0816: : 31it [00:20,  1.69it/s]\u001b[A\n",
      "batch 32, training loss: 4.2273: : 31it [00:21,  1.69it/s]\u001b[A\n",
      "batch 32, training loss: 4.2273: : 32it [00:21,  1.70it/s]\u001b[A\n",
      "batch 33, training loss: 4.157: : 32it [00:22,  1.70it/s] \u001b[A\n",
      "batch 33, training loss: 4.157: : 33it [00:22,  1.65it/s]\u001b[A\n",
      "batch 34, training loss: 4.1218: : 33it [00:22,  1.65it/s]\u001b[A\n",
      "batch 34, training loss: 4.1218: : 34it [00:22,  1.59it/s]\u001b[A\n",
      "batch 35, training loss: 4.2466: : 34it [00:23,  1.59it/s]\u001b[A\n",
      "batch 35, training loss: 4.2466: : 35it [00:23,  1.56it/s]\u001b[A\n",
      "batch 36, training loss: 4.2731: : 35it [00:24,  1.56it/s]\u001b[A\n",
      "batch 36, training loss: 4.2731: : 36it [00:24,  1.57it/s]\u001b[A\n",
      "batch 37, training loss: 4.1254: : 36it [00:24,  1.57it/s]\u001b[A\n",
      "batch 37, training loss: 4.1254: : 37it [00:24,  1.57it/s]\u001b[A\n",
      "batch 38, training loss: 3.9671: : 37it [00:25,  1.57it/s]\u001b[A\n",
      "batch 38, training loss: 3.9671: : 38it [00:25,  1.59it/s]\u001b[A\n",
      "batch 39, training loss: 4.0816: : 38it [00:26,  1.59it/s]\u001b[A\n",
      "batch 39, training loss: 4.0816: : 39it [00:26,  1.57it/s]\u001b[A\n",
      "batch 40, training loss: 4.214: : 39it [00:26,  1.57it/s] \u001b[A\n",
      "batch 40, training loss: 4.214: : 40it [00:26,  1.55it/s]\u001b[A\n",
      "batch 41, training loss: 4.3913: : 40it [00:27,  1.55it/s]\u001b[A\n",
      "batch 41, training loss: 4.3913: : 41it [00:27,  1.50it/s]\u001b[A\n",
      "batch 42, training loss: 4.2333: : 41it [00:28,  1.50it/s]\u001b[A\n",
      "batch 42, training loss: 4.2333: : 42it [00:28,  1.49it/s]\u001b[A\n",
      "batch 43, training loss: 3.9991: : 42it [00:28,  1.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 3.9991: : 43it [00:28,  1.59it/s]\u001b[A\n",
      "batch 44, training loss: 3.9826: : 43it [00:29,  1.59it/s]\u001b[A\n",
      "batch 44, training loss: 3.9826: : 44it [00:29,  1.62it/s]\u001b[A\n",
      "batch 45, training loss: 4.0108: : 44it [00:29,  1.62it/s]\u001b[A\n",
      "batch 45, training loss: 4.0108: : 45it [00:29,  1.61it/s]\u001b[A\n",
      "batch 46, training loss: 4.1471: : 45it [00:30,  1.61it/s]\u001b[A\n",
      "batch 46, training loss: 4.1471: : 46it [00:30,  1.57it/s]\u001b[A\n",
      "batch 47, training loss: 4.146: : 46it [00:31,  1.57it/s] \u001b[A\n",
      "batch 47, training loss: 4.146: : 47it [00:31,  1.54it/s]\u001b[A\n",
      "batch 48, training loss: 4.098: : 47it [00:31,  1.54it/s]\u001b[A\n",
      "batch 48, training loss: 4.098: : 48it [00:31,  1.52it/s]\u001b[A\n",
      "batch 49, training loss: 4.3318: : 48it [00:32,  1.52it/s]\u001b[A\n",
      "batch 49, training loss: 4.3318: : 49it [00:32,  1.56it/s]\u001b[A\n",
      "batch 50, training loss: 4.0743: : 49it [00:33,  1.56it/s]\u001b[A\n",
      "batch 50, training loss: 4.0743: : 50it [00:33,  1.58it/s]\u001b[A\n",
      "batch 51, training loss: 4.1737: : 50it [00:33,  1.58it/s]\u001b[A\n",
      "batch 51, training loss: 4.1737: : 51it [00:33,  1.59it/s]\u001b[A\n",
      "batch 52, training loss: 4.0618: : 51it [00:34,  1.59it/s]\u001b[A\n",
      "batch 52, training loss: 4.0618: : 52it [00:34,  1.57it/s]\u001b[A\n",
      "batch 53, training loss: 4.0704: : 52it [00:35,  1.57it/s]\u001b[A\n",
      "batch 53, training loss: 4.0704: : 53it [00:35,  1.53it/s]\u001b[A\n",
      "batch 54, training loss: 3.9699: : 53it [00:35,  1.53it/s]\u001b[A\n",
      "batch 54, training loss: 3.9699: : 54it [00:35,  1.55it/s]\u001b[A\n",
      "batch 55, training loss: 4.1228: : 54it [00:36,  1.55it/s]\u001b[A\n",
      "batch 55, training loss: 4.1228: : 55it [00:36,  1.59it/s]\u001b[A\n",
      "batch 56, training loss: 4.1564: : 55it [00:36,  1.59it/s]\u001b[A\n",
      "batch 56, training loss: 4.1564: : 56it [00:36,  1.63it/s]\u001b[A\n",
      "batch 57, training loss: 4.0937: : 56it [00:37,  1.63it/s]\u001b[A\n",
      "batch 57, training loss: 4.0937: : 57it [00:37,  1.60it/s]\u001b[A\n",
      "batch 58, training loss: 4.1942: : 57it [00:38,  1.60it/s]\u001b[A\n",
      "batch 58, training loss: 4.1942: : 58it [00:38,  1.56it/s]\u001b[A\n",
      "batch 59, training loss: 4.011: : 58it [00:38,  1.56it/s] \u001b[A\n",
      "batch 59, training loss: 4.011: : 59it [00:38,  1.67it/s]\u001b[A\n",
      "batch 60, training loss: 3.9959: : 59it [00:39,  1.67it/s]\u001b[A\n",
      "batch 60, training loss: 3.9959: : 60it [00:39,  1.69it/s]\u001b[A\n",
      "batch 61, training loss: 4.2356: : 60it [00:39,  1.69it/s]\u001b[A\n",
      "batch 61, training loss: 4.2356: : 61it [00:39,  1.80it/s]\u001b[A\n",
      "batch 62, training loss: 4.0629: : 61it [00:40,  1.80it/s]\u001b[A\n",
      "batch 62, training loss: 4.0629: : 62it [00:40,  1.97it/s]\u001b[A\n",
      "batch 63, training loss: 4.1651: : 62it [00:40,  1.97it/s]\u001b[A\n",
      "batch 63, training loss: 4.1651: : 63it [00:40,  2.10it/s]\u001b[A\n",
      "batch 64, training loss: 4.1171: : 63it [00:41,  2.10it/s]\u001b[A\n",
      "batch 64, training loss: 4.1171: : 64it [00:41,  2.09it/s]\u001b[A\n",
      "batch 65, training loss: 4.1722: : 64it [00:41,  2.09it/s]\u001b[A\n",
      "batch 65, training loss: 4.1722: : 65it [00:41,  2.12it/s]\u001b[A\n",
      "batch 66, training loss: 4.1414: : 65it [00:42,  2.12it/s]\u001b[A\n",
      "batch 66, training loss: 4.1414: : 66it [00:42,  1.97it/s]\u001b[A\n",
      "batch 67, training loss: 4.0756: : 66it [00:42,  1.97it/s]\u001b[A\n",
      "batch 67, training loss: 4.0756: : 67it [00:42,  1.91it/s]\u001b[A\n",
      "batch 68, training loss: 4.1548: : 67it [00:43,  1.91it/s]\u001b[A\n",
      "batch 68, training loss: 4.1548: : 68it [00:43,  1.82it/s]\u001b[A\n",
      "batch 69, training loss: 4.0494: : 68it [00:43,  1.82it/s]\u001b[A\n",
      "batch 69, training loss: 4.0494: : 69it [00:43,  1.75it/s]\u001b[A\n",
      "batch 70, training loss: 4.2221: : 69it [00:44,  1.75it/s]\u001b[A\n",
      "batch 70, training loss: 4.2221: : 70it [00:44,  1.66it/s]\u001b[A\n",
      "batch 71, training loss: 4.0562: : 70it [00:45,  1.66it/s]\u001b[A\n",
      "batch 71, training loss: 4.0562: : 71it [00:45,  1.59it/s]\u001b[A\n",
      "batch 72, training loss: 4.0816: : 71it [00:45,  1.59it/s]\u001b[A\n",
      "batch 72, training loss: 4.0816: : 72it [00:45,  1.53it/s]\u001b[A\n",
      "batch 73, training loss: 4.1288: : 72it [00:46,  1.53it/s]\u001b[A\n",
      "batch 73, training loss: 4.1288: : 73it [00:46,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 3.9956: : 73it [00:47,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 3.9956: : 74it [00:47,  1.58it/s]\u001b[A\n",
      "batch 75, training loss: 4.2253: : 74it [00:47,  1.58it/s]\u001b[A\n",
      "batch 75, training loss: 4.2253: : 75it [00:47,  1.62it/s]\u001b[A\n",
      "batch 76, training loss: 4.0137: : 75it [00:48,  1.62it/s]\u001b[A\n",
      "batch 76, training loss: 4.0137: : 76it [00:48,  1.59it/s]\u001b[A\n",
      "batch 77, training loss: 4.1518: : 76it [00:49,  1.59it/s]\u001b[A\n",
      "batch 77, training loss: 4.1518: : 77it [00:49,  1.53it/s]\u001b[A\n",
      "batch 78, training loss: 4.1441: : 77it [00:49,  1.53it/s]\u001b[A\n",
      "batch 78, training loss: 4.1441: : 78it [00:49,  1.50it/s]\u001b[A\n",
      "batch 79, training loss: 4.1697: : 78it [00:50,  1.50it/s]\u001b[A\n",
      "batch 79, training loss: 4.1697: : 79it [00:50,  1.50it/s]\u001b[A\n",
      "batch 80, training loss: 4.0544: : 79it [00:51,  1.50it/s]\u001b[A\n",
      "batch 80, training loss: 4.0544: : 80it [00:51,  1.61it/s]\u001b[A\n",
      "batch 81, training loss: 4.1064: : 80it [00:51,  1.61it/s]\u001b[A\n",
      "batch 81, training loss: 4.1064: : 81it [00:51,  1.65it/s]\u001b[A\n",
      "batch 82, training loss: 4.2189: : 81it [00:52,  1.65it/s]\u001b[A\n",
      "batch 82, training loss: 4.2189: : 82it [00:52,  1.62it/s]\u001b[A\n",
      "batch 83, training loss: 4.0898: : 82it [00:52,  1.62it/s]\u001b[A\n",
      "batch 83, training loss: 4.0898: : 83it [00:52,  1.58it/s]\u001b[A\n",
      "batch 84, training loss: 4.2046: : 83it [00:53,  1.58it/s]\u001b[A\n",
      "batch 84, training loss: 4.2046: : 84it [00:53,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 4.3026: : 84it [00:54,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 4.3026: : 85it [00:54,  1.59it/s]\u001b[A\n",
      "batch 86, training loss: 4.1491: : 85it [00:54,  1.59it/s]\u001b[A\n",
      "batch 86, training loss: 4.1491: : 86it [00:54,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 4.2415: : 86it [00:55,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 4.2415: : 87it [00:55,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 4.403: : 87it [00:56,  1.59it/s] \u001b[A\n",
      "batch 88, training loss: 4.403: : 88it [00:56,  1.51it/s]\u001b[A\n",
      "batch 89, training loss: 4.4857: : 88it [00:56,  1.51it/s]\u001b[A\n",
      "batch 89, training loss: 4.4857: : 89it [00:56,  1.44it/s]\u001b[A\n",
      "batch 90, training loss: 4.395: : 89it [00:57,  1.44it/s] \u001b[A\n",
      "batch 90, training loss: 4.395: : 90it [00:57,  1.40it/s]\u001b[A\n",
      "batch 91, training loss: 4.4953: : 90it [00:58,  1.40it/s]\u001b[A\n",
      "batch 91, training loss: 4.4953: : 91it [00:58,  1.36it/s]\u001b[A\n",
      "batch 92, training loss: 4.4024: : 91it [00:59,  1.36it/s]\u001b[A\n",
      "batch 92, training loss: 4.4024: : 92it [00:59,  1.35it/s]\u001b[A\n",
      "batch 93, training loss: 4.355: : 92it [00:59,  1.35it/s] \u001b[A\n",
      "batch 93, training loss: 4.355: : 93it [00:59,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 4.396: : 93it [01:00,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 4.396: : 94it [01:00,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 4.4377: : 94it [01:01,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 4.4377: : 95it [01:01,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 4.3952: : 95it [01:02,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 4.3952: : 96it [01:02,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 4.4685: : 96it [01:03,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 4.4685: : 97it [01:03,  1.33it/s]\u001b[A\n",
      "batch 98, training loss: 4.4524: : 97it [01:03,  1.33it/s]\u001b[A\n",
      "batch 98, training loss: 4.4524: : 98it [01:03,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 4.2721: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 4.2721: : 99it [01:04,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 4.0923: : 99it [01:05,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 4.0923: : 100it [01:05,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 4.3064: : 100it [01:06,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 4.3064: : 101it [01:06,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 4.2305: : 101it [01:06,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 4.2305: : 102it [01:06,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 4.2007: : 102it [01:07,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 4.2007: : 103it [01:07,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 4.0533: : 103it [01:08,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 4.0533: : 104it [01:08,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 4.2514: : 104it [01:09,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 4.2514: : 105it [01:09,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 4.3317: : 105it [01:09,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 4.3317: : 106it [01:09,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 4.0077: : 106it [01:10,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 4.0077: : 107it [01:10,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 4.2853: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 4.2853: : 108it [01:11,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 4.3429: : 108it [01:12,  1.30it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 109, training loss: 4.3429: : 109it [01:12,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 4.4981: : 109it [01:13,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 4.4981: : 110it [01:13,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 4.1712: : 110it [01:13,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 4.1712: : 111it [01:13,  1.32it/s]\u001b[A\n",
      "batch 112, training loss: 4.2294: : 111it [01:14,  1.32it/s]\u001b[A\n",
      "batch 112, training loss: 4.2294: : 112it [01:14,  1.31it/s]\u001b[A\n",
      "batch 113, training loss: 4.3071: : 112it [01:15,  1.31it/s]\u001b[A\n",
      "batch 113, training loss: 4.3071: : 113it [01:15,  1.31it/s]\u001b[A\n",
      "batch 114, training loss: 4.3132: : 113it [01:16,  1.31it/s]\u001b[A\n",
      "batch 114, training loss: 4.3132: : 114it [01:16,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 4.1985: : 114it [01:16,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 4.1985: : 115it [01:16,  1.32it/s]\u001b[A\n",
      "batch 116, training loss: 4.1688: : 115it [01:17,  1.32it/s]\u001b[A\n",
      "batch 116, training loss: 4.1688: : 116it [01:17,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 4.2342: : 116it [01:18,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 4.2342: : 117it [01:18,  1.30it/s]\u001b[A\n",
      "batch 118, training loss: 4.322: : 117it [01:19,  1.30it/s] \u001b[A\n",
      "batch 118, training loss: 4.322: : 118it [01:19,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 4.2049: : 118it [01:19,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 4.2049: : 119it [01:19,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 4.1725: : 119it [01:20,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 4.1725: : 120it [01:20,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 4.3937: : 120it [01:21,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 4.3937: : 121it [01:21,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 4.0785: : 121it [01:22,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 4.0785: : 122it [01:22,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 4.2422: : 122it [01:23,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 4.2422: : 123it [01:23,  1.29it/s]\u001b[A\n",
      "batch 124, training loss: 4.2444: : 123it [01:23,  1.29it/s]\u001b[A\n",
      "batch 124, training loss: 4.2444: : 124it [01:23,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 4.244: : 124it [01:24,  1.29it/s] \u001b[A\n",
      "batch 125, training loss: 4.244: : 125it [01:24,  1.31it/s]\u001b[A\n",
      "batch 126, training loss: 4.3158: : 125it [01:25,  1.31it/s]\u001b[A\n",
      "batch 126, training loss: 4.3158: : 126it [01:25,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 4.0312: : 126it [01:26,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 4.0312: : 127it [01:26,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 4.2423: : 127it [01:26,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 4.2423: : 128it [01:26,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 4.2076: : 128it [01:27,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 4.2076: : 129it [01:27,  1.36it/s]\u001b[A\n",
      "batch 130, training loss: 4.2227: : 129it [01:28,  1.36it/s]\u001b[A\n",
      "batch 130, training loss: 4.2227: : 130it [01:28,  1.42it/s]\u001b[A\n",
      "batch 131, training loss: 4.381: : 130it [01:28,  1.42it/s] \u001b[A\n",
      "batch 131, training loss: 4.381: : 131it [01:28,  1.42it/s]\u001b[A\n",
      "batch 132, training loss: 4.1773: : 131it [01:29,  1.42it/s]\u001b[A\n",
      "batch 132, training loss: 4.1773: : 132it [01:29,  1.39it/s]\u001b[A\n",
      "batch 133, training loss: 4.0783: : 132it [01:30,  1.39it/s]\u001b[A\n",
      "batch 133, training loss: 4.0783: : 133it [01:30,  1.37it/s]\u001b[A\n",
      "batch 134, training loss: 4.1536: : 133it [01:31,  1.37it/s]\u001b[A\n",
      "batch 134, training loss: 4.1536: : 134it [01:31,  1.33it/s]\u001b[A\n",
      "batch 135, training loss: 4.1496: : 134it [01:31,  1.33it/s]\u001b[A\n",
      "batch 135, training loss: 4.1496: : 135it [01:31,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 4.1204: : 135it [01:32,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 4.1204: : 136it [01:32,  1.33it/s]\u001b[A\n",
      "batch 137, training loss: 4.2564: : 136it [01:33,  1.33it/s]\u001b[A\n",
      "batch 137, training loss: 4.2564: : 137it [01:33,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 4.2783: : 137it [01:34,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 4.2783: : 138it [01:34,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 4.0636: : 138it [01:35,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 4.0636: : 139it [01:35,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 4.1365: : 139it [01:35,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 4.1365: : 140it [01:35,  1.33it/s]\u001b[A\n",
      "batch 141, training loss: 4.1527: : 140it [01:36,  1.33it/s]\u001b[A\n",
      "batch 141, training loss: 4.1527: : 141it [01:36,  1.32it/s]\u001b[A\n",
      "batch 142, training loss: 4.1855: : 141it [01:37,  1.32it/s]\u001b[A\n",
      "batch 142, training loss: 4.1855: : 142it [01:37,  1.31it/s]\u001b[A\n",
      "batch 143, training loss: 4.042: : 142it [01:38,  1.31it/s] \u001b[A\n",
      "batch 143, training loss: 4.042: : 143it [01:38,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 4.0675: : 143it [01:38,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 4.0675: : 144it [01:38,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 4.1501: : 144it [01:39,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 4.1501: : 145it [01:39,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 4.1612: : 145it [01:40,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 4.1612: : 146it [01:40,  1.30it/s]\u001b[A\n",
      "batch 147, training loss: 4.0439: : 146it [01:41,  1.30it/s]\u001b[A\n",
      "batch 147, training loss: 4.0439: : 147it [01:41,  1.33it/s]\u001b[A\n",
      "batch 148, training loss: 4.1511: : 147it [01:41,  1.33it/s]\u001b[A\n",
      "batch 148, training loss: 4.1511: : 148it [01:41,  1.31it/s]\u001b[A\n",
      "batch 149, training loss: 4.2352: : 148it [01:42,  1.31it/s]\u001b[A\n",
      "batch 149, training loss: 4.2352: : 149it [01:42,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 4.2362: : 149it [01:43,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 4.2362: : 150it [01:43,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 4.1867: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 4.1867: : 151it [01:44,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 4.1039: : 151it [01:44,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 4.1039: : 152it [01:44,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 4.1605: : 152it [01:45,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 4.1605: : 153it [01:45,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 4.2258: : 153it [01:46,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 4.2258: : 154it [01:46,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 4.3248: : 154it [01:47,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 4.3248: : 155it [01:47,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.9939: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.9939: : 156it [01:48,  1.30it/s]\u001b[A\n",
      "batch 157, training loss: 4.189: : 156it [01:48,  1.30it/s] \u001b[A\n",
      "batch 157, training loss: 4.189: : 157it [01:48,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 4.1674: : 157it [01:49,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 4.1674: : 158it [01:49,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 4.0697: : 158it [01:50,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 4.0697: : 159it [01:50,  1.30it/s]\u001b[A\n",
      "batch 160, training loss: 4.2358: : 159it [01:51,  1.30it/s]\u001b[A\n",
      "batch 160, training loss: 4.2358: : 160it [01:51,  1.31it/s]\u001b[A\n",
      "batch 161, training loss: 4.0786: : 160it [01:51,  1.31it/s]\u001b[A\n",
      "batch 161, training loss: 4.0786: : 161it [01:51,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 4.0783: : 161it [01:52,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 4.0783: : 162it [01:52,  1.32it/s]\u001b[A\n",
      "batch 163, training loss: 4.3137: : 162it [01:53,  1.32it/s]\u001b[A\n",
      "batch 163, training loss: 4.3137: : 163it [01:53,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 4.1093: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 4.1093: : 164it [01:54,  1.31it/s]\u001b[A\n",
      "batch 165, training loss: 4.124: : 164it [01:54,  1.31it/s] \u001b[A\n",
      "batch 165, training loss: 4.124: : 165it [01:54,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 4.1003: : 165it [01:55,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 4.1003: : 166it [01:55,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 4.2727: : 166it [01:56,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 4.2727: : 167it [01:56,  1.29it/s]\u001b[A\n",
      "batch 168, training loss: 4.1703: : 167it [01:57,  1.29it/s]\u001b[A\n",
      "batch 168, training loss: 4.1703: : 168it [01:57,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 4.2578: : 168it [01:57,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 4.2578: : 169it [01:57,  1.32it/s]\u001b[A\n",
      "batch 170, training loss: 4.1154: : 169it [01:58,  1.32it/s]\u001b[A\n",
      "batch 170, training loss: 4.1154: : 170it [01:58,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 3.6799: : 170it [01:59,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 3.6799: : 171it [01:59,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 4.5271: : 171it [01:59,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 4.5271: : 172it [01:59,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 4.3526: : 172it [02:00,  1.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 173, training loss: 4.3526: : 173it [02:00,  1.38it/s]\u001b[A\n",
      "batch 174, training loss: 4.541: : 173it [02:01,  1.38it/s] \u001b[A\n",
      "batch 174, training loss: 4.541: : 174it [02:01,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 4.4462: : 174it [02:02,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 4.4462: : 175it [02:02,  1.30it/s]\u001b[A\n",
      "batch 176, training loss: 4.3692: : 175it [02:03,  1.30it/s]\u001b[A\n",
      "batch 176, training loss: 4.3692: : 176it [02:03,  1.28it/s]\u001b[A\n",
      "batch 177, training loss: 4.4052: : 176it [02:03,  1.28it/s]\u001b[A\n",
      "batch 177, training loss: 4.4052: : 177it [02:03,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 4.4166: : 177it [02:04,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 4.4166: : 178it [02:04,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 4.4342: : 178it [02:05,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 4.4342: : 179it [02:05,  1.28it/s]\u001b[A\n",
      "batch 180, training loss: 4.5044: : 179it [02:06,  1.28it/s]\u001b[A\n",
      "batch 180, training loss: 4.5044: : 180it [02:06,  1.28it/s]\u001b[A\n",
      "batch 181, training loss: 4.5207: : 180it [02:07,  1.28it/s]\u001b[A\n",
      "batch 181, training loss: 4.5207: : 181it [02:07,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.4242: : 181it [02:07,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.4242: : 182it [02:07,  1.27it/s]\u001b[A\n",
      "batch 183, training loss: 4.5087: : 182it [02:08,  1.27it/s]\u001b[A\n",
      "batch 183, training loss: 4.5087: : 183it [02:08,  1.26it/s]\u001b[A\n",
      "batch 184, training loss: 4.298: : 183it [02:09,  1.26it/s] \u001b[A\n",
      "batch 184, training loss: 4.298: : 184it [02:09,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 4.3078: : 184it [02:10,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 4.3078: : 185it [02:10,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 4.4744: : 185it [02:11,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 4.4744: : 186it [02:11,  1.24it/s]\u001b[A\n",
      "batch 187, training loss: 4.4281: : 186it [02:11,  1.24it/s]\u001b[A\n",
      "batch 187, training loss: 4.4281: : 187it [02:11,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 4.5666: : 187it [02:12,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 4.5666: : 188it [02:12,  1.24it/s]\u001b[A\n",
      "batch 189, training loss: 4.5651: : 188it [02:13,  1.24it/s]\u001b[A\n",
      "batch 189, training loss: 4.5651: : 189it [02:13,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 4.1995: : 189it [02:14,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 4.1995: : 190it [02:14,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 4.2196: : 190it [02:15,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 4.2196: : 191it [02:15,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.3592: : 191it [02:15,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.3592: : 192it [02:15,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 4.1995: : 192it [02:16,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 4.1995: : 193it [02:16,  1.23it/s]\u001b[A\n",
      "batch 194, training loss: 4.282: : 193it [02:17,  1.23it/s] \u001b[A\n",
      "batch 194, training loss: 4.282: : 194it [02:17,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.2212: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.2212: : 195it [02:18,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 4.2261: : 195it [02:19,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 4.2261: : 196it [02:19,  1.23it/s]\u001b[A\n",
      "batch 197, training loss: 4.3286: : 196it [02:19,  1.23it/s]\u001b[A\n",
      "batch 197, training loss: 4.3286: : 197it [02:19,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.2485: : 197it [02:20,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.2485: : 198it [02:20,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 4.2137: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 4.2137: : 199it [02:21,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 4.3798: : 199it [02:22,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 4.3798: : 200it [02:22,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 4.171: : 200it [02:23,  1.24it/s] \u001b[A\n",
      "batch 201, training loss: 4.171: : 201it [02:23,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 4.0292: : 201it [02:24,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 4.0292: : 202it [02:24,  1.25it/s]\u001b[A\n",
      "batch 203, training loss: 4.2467: : 202it [02:24,  1.25it/s]\u001b[A\n",
      "batch 203, training loss: 4.2467: : 203it [02:24,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 4.2992: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 4.2992: : 204it [02:25,  1.26it/s]\u001b[A\n",
      "batch 205, training loss: 4.2485: : 204it [02:26,  1.26it/s]\u001b[A\n",
      "batch 205, training loss: 4.2485: : 205it [02:26,  1.26it/s]\u001b[A\n",
      "batch 206, training loss: 4.3948: : 205it [02:27,  1.26it/s]\u001b[A\n",
      "batch 206, training loss: 4.3948: : 206it [02:27,  1.25it/s]\u001b[A\n",
      "batch 207, training loss: 4.1863: : 206it [02:28,  1.25it/s]\u001b[A\n",
      "batch 207, training loss: 4.1863: : 207it [02:28,  1.25it/s]\u001b[A\n",
      "batch 208, training loss: 4.2493: : 207it [02:28,  1.25it/s]\u001b[A\n",
      "batch 208, training loss: 4.2493: : 208it [02:28,  1.26it/s]\u001b[A\n",
      "batch 209, training loss: 4.1891: : 208it [02:29,  1.26it/s]\u001b[A\n",
      "batch 209, training loss: 4.1891: : 209it [02:29,  1.27it/s]\u001b[A\n",
      "batch 210, training loss: 4.1798: : 209it [02:30,  1.27it/s]\u001b[A\n",
      "batch 210, training loss: 4.1798: : 210it [02:30,  1.26it/s]\u001b[A\n",
      "batch 211, training loss: 4.1765: : 210it [02:31,  1.26it/s]\u001b[A\n",
      "batch 211, training loss: 4.1765: : 211it [02:31,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 4.2007: : 211it [02:31,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 4.2007: : 212it [02:31,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 4.4144: : 212it [02:32,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 4.4144: : 213it [02:32,  1.28it/s]\u001b[A\n",
      "batch 214, training loss: 4.2608: : 213it [02:33,  1.28it/s]\u001b[A\n",
      "batch 214, training loss: 4.2608: : 214it [02:33,  1.26it/s]\u001b[A\n",
      "batch 215, training loss: 4.077: : 214it [02:34,  1.26it/s] \u001b[A\n",
      "batch 215, training loss: 4.077: : 215it [02:34,  1.26it/s]\u001b[A\n",
      "batch 216, training loss: 4.2252: : 215it [02:35,  1.26it/s]\u001b[A\n",
      "batch 216, training loss: 4.2252: : 216it [02:35,  1.26it/s]\u001b[A\n",
      "batch 217, training loss: 4.3377: : 216it [02:35,  1.26it/s]\u001b[A\n",
      "batch 217, training loss: 4.3377: : 217it [02:35,  1.28it/s]\u001b[A\n",
      "batch 218, training loss: 4.2537: : 217it [02:36,  1.28it/s]\u001b[A\n",
      "batch 218, training loss: 4.2537: : 218it [02:36,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 4.4687: : 218it [02:37,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 4.4687: : 219it [02:37,  1.26it/s]\u001b[A\n",
      "batch 220, training loss: 4.3474: : 219it [02:38,  1.26it/s]\u001b[A\n",
      "batch 220, training loss: 4.3474: : 220it [02:38,  1.26it/s]\u001b[A\n",
      "batch 221, training loss: 4.2705: : 220it [02:39,  1.26it/s]\u001b[A\n",
      "batch 221, training loss: 4.2705: : 221it [02:39,  1.23it/s]\u001b[A\n",
      "batch 222, training loss: 4.2791: : 221it [02:39,  1.23it/s]\u001b[A\n",
      "batch 222, training loss: 4.2791: : 222it [02:39,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 4.2714: : 222it [02:40,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 4.2714: : 223it [02:40,  1.25it/s]\u001b[A\n",
      "batch 224, training loss: 4.1997: : 223it [02:41,  1.25it/s]\u001b[A\n",
      "batch 224, training loss: 4.1997: : 224it [02:41,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 4.3313: : 224it [02:42,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 4.3313: : 225it [02:42,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 4.3389: : 225it [02:43,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 4.3389: : 226it [02:43,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 4.1734: : 226it [02:43,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 4.1734: : 227it [02:43,  1.23it/s]\u001b[A\n",
      "batch 228, training loss: 4.2247: : 227it [02:44,  1.23it/s]\u001b[A\n",
      "batch 228, training loss: 4.2247: : 228it [02:44,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 4.2188: : 228it [02:45,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 4.2188: : 229it [02:45,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.3489: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.3489: : 230it [02:46,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 4.4275: : 230it [02:47,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 4.4275: : 231it [02:47,  1.24it/s]\u001b[A\n",
      "batch 232, training loss: 4.1802: : 231it [02:48,  1.24it/s]\u001b[A\n",
      "batch 232, training loss: 4.1802: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 4.3044: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 4.3044: : 233it [02:48,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 4.389: : 233it [02:49,  1.23it/s] \u001b[A\n",
      "batch 234, training loss: 4.389: : 234it [02:49,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 4.2768: : 234it [02:50,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 4.2768: : 235it [02:50,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 4.2785: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 4.2785: : 236it [02:51,  1.22it/s]\u001b[A\n",
      "batch 237, training loss: 4.1409: : 236it [02:52,  1.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 237, training loss: 4.1409: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 4.2687: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 4.2687: : 238it [02:52,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 4.2013: : 238it [02:53,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 4.2013: : 239it [02:53,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 4.2397: : 239it [02:54,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 4.2397: : 240it [02:54,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 4.2845: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 4.2845: : 241it [02:55,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 4.0991: : 241it [02:56,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 4.0991: : 242it [02:56,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 4.2662: : 242it [02:56,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 4.2662: : 243it [02:56,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 4.2136: : 243it [02:57,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 4.2136: : 244it [02:57,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.2907: : 244it [02:58,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.2907: : 245it [02:58,  1.30it/s]\u001b[A\n",
      "batch 246, training loss: 4.2111: : 245it [02:59,  1.30it/s]\u001b[A\n",
      "batch 246, training loss: 4.2111: : 246it [02:59,  1.33it/s]\u001b[A\n",
      "batch 247, training loss: 4.2033: : 246it [02:59,  1.33it/s]\u001b[A\n",
      "batch 247, training loss: 4.2033: : 247it [02:59,  1.30it/s]\u001b[A\n",
      "batch 248, training loss: 4.1437: : 247it [03:00,  1.30it/s]\u001b[A\n",
      "batch 248, training loss: 4.1437: : 248it [03:00,  1.28it/s]\u001b[A\n",
      "batch 249, training loss: 4.1166: : 248it [03:01,  1.28it/s]\u001b[A\n",
      "batch 249, training loss: 4.1166: : 249it [03:01,  1.29it/s]\u001b[A\n",
      "batch 250, training loss: 4.3264: : 249it [03:02,  1.29it/s]\u001b[A\n",
      "batch 250, training loss: 4.3264: : 250it [03:02,  1.27it/s]\u001b[A\n",
      "batch 251, training loss: 4.325: : 250it [03:03,  1.27it/s] \u001b[A\n",
      "batch 251, training loss: 4.325: : 251it [03:03,  1.27it/s]\u001b[A\n",
      "batch 252, training loss: 4.0589: : 251it [03:03,  1.27it/s]\u001b[A\n",
      "batch 252, training loss: 4.0589: : 252it [03:03,  1.50it/s]\u001b[A\n",
      "batch 253, training loss: 4.3178: : 252it [03:04,  1.50it/s]\u001b[A\n",
      "batch 253, training loss: 4.3178: : 253it [03:04,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 4.5094: : 253it [03:05,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 4.5094: : 254it [03:05,  1.30it/s]\u001b[A\n",
      "batch 255, training loss: 4.3352: : 254it [03:06,  1.30it/s]\u001b[A\n",
      "batch 255, training loss: 4.3352: : 255it [03:06,  1.23it/s]\u001b[A\n",
      "batch 256, training loss: 4.272: : 255it [03:07,  1.23it/s] \u001b[A\n",
      "batch 256, training loss: 4.272: : 256it [03:07,  1.19it/s]\u001b[A\n",
      "batch 257, training loss: 4.4127: : 256it [03:07,  1.19it/s]\u001b[A\n",
      "batch 257, training loss: 4.4127: : 257it [03:07,  1.19it/s]\u001b[A\n",
      "batch 258, training loss: 4.4791: : 257it [03:08,  1.19it/s]\u001b[A\n",
      "batch 258, training loss: 4.4791: : 258it [03:08,  1.16it/s]\u001b[A\n",
      "batch 259, training loss: 4.3562: : 258it [03:09,  1.16it/s]\u001b[A\n",
      "batch 259, training loss: 4.3562: : 259it [03:09,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 4.3758: : 259it [03:10,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 4.3758: : 260it [03:10,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 4.4698: : 260it [03:11,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 4.4698: : 261it [03:11,  1.19it/s]\u001b[A\n",
      "batch 262, training loss: 4.3215: : 261it [03:12,  1.19it/s]\u001b[A\n",
      "batch 262, training loss: 4.3215: : 262it [03:12,  1.29it/s]\u001b[A\n",
      "batch 263, training loss: 4.3155: : 262it [03:12,  1.29it/s]\u001b[A\n",
      "batch 263, training loss: 4.3155: : 263it [03:12,  1.28it/s]\u001b[A\n",
      "batch 264, training loss: 4.3926: : 263it [03:13,  1.28it/s]\u001b[A\n",
      "batch 264, training loss: 4.3926: : 264it [03:13,  1.22it/s]\u001b[A\n",
      "batch 265, training loss: 4.2217: : 264it [03:14,  1.22it/s]\u001b[A\n",
      "batch 265, training loss: 4.2217: : 265it [03:14,  1.18it/s]\u001b[A\n",
      "batch 266, training loss: 4.4139: : 265it [03:15,  1.18it/s]\u001b[A\n",
      "batch 266, training loss: 4.4139: : 266it [03:15,  1.17it/s]\u001b[A\n",
      "batch 267, training loss: 4.3257: : 266it [03:16,  1.17it/s]\u001b[A\n",
      "batch 267, training loss: 4.3257: : 267it [03:16,  1.13it/s]\u001b[A\n",
      "batch 268, training loss: 4.2657: : 267it [03:17,  1.13it/s]\u001b[A\n",
      "batch 268, training loss: 4.2657: : 268it [03:17,  1.13it/s]\u001b[A\n",
      "batch 269, training loss: 4.225: : 268it [03:18,  1.13it/s] \u001b[A\n",
      "batch 269, training loss: 4.225: : 269it [03:18,  1.12it/s]\u001b[A\n",
      "batch 270, training loss: 4.2691: : 269it [03:19,  1.12it/s]\u001b[A\n",
      "batch 270, training loss: 4.2691: : 270it [03:19,  1.13it/s]\u001b[A\n",
      "batch 271, training loss: 4.2769: : 270it [03:20,  1.13it/s]\u001b[A\n",
      "batch 271, training loss: 4.2769: : 271it [03:20,  1.11it/s]\u001b[A\n",
      "batch 272, training loss: 4.2553: : 271it [03:20,  1.11it/s]\u001b[A\n",
      "batch 272, training loss: 4.2553: : 272it [03:20,  1.10it/s]\u001b[A\n",
      "batch 273, training loss: 4.37: : 272it [03:21,  1.10it/s]  \u001b[A\n",
      "batch 273, training loss: 4.37: : 273it [03:21,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 4.3067: : 273it [03:22,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 4.3067: : 274it [03:22,  1.11it/s]\u001b[A\n",
      "batch 275, training loss: 4.1949: : 274it [03:23,  1.11it/s]\u001b[A\n",
      "batch 275, training loss: 4.1949: : 275it [03:23,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 4.1123: : 275it [03:24,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 4.1123: : 276it [03:24,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 4.2529: : 276it [03:25,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 4.2529: : 277it [03:25,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 4.184: : 277it [03:26,  1.09it/s] \u001b[A\n",
      "batch 278, training loss: 4.184: : 278it [03:26,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 4.2343: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 4.2343: : 279it [03:27,  1.11it/s]\u001b[A\n",
      "batch 280, training loss: 4.0626: : 279it [03:28,  1.11it/s]\u001b[A\n",
      "batch 280, training loss: 4.0626: : 280it [03:28,  1.10it/s]\u001b[A\n",
      "batch 281, training loss: 4.1509: : 280it [03:29,  1.10it/s]\u001b[A\n",
      "batch 281, training loss: 4.1509: : 281it [03:29,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 4.1225: : 281it [03:30,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 4.1225: : 282it [03:30,  1.08it/s]\u001b[A\n",
      "batch 283, training loss: 4.1657: : 282it [03:30,  1.08it/s]\u001b[A\n",
      "batch 283, training loss: 4.1657: : 283it [03:30,  1.08it/s]\u001b[A\n",
      "batch 284, training loss: 4.1299: : 283it [03:31,  1.08it/s]\u001b[A\n",
      "batch 284, training loss: 4.1299: : 284it [03:31,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 4.1899: : 284it [03:32,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 4.1899: : 285it [03:32,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 4.3351: : 285it [03:33,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 4.3351: : 286it [03:33,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 4.0683: : 286it [03:34,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 4.0683: : 287it [03:34,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 4.0973: : 287it [03:35,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 4.0973: : 288it [03:35,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 4.2609: : 288it [03:36,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 4.2609: : 289it [03:36,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 4.0273: : 289it [03:37,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 4.0273: : 290it [03:37,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 4.3051: : 290it [03:38,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 4.3051: : 291it [03:38,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 4.0784: : 291it [03:39,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 4.0784: : 292it [03:39,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 4.192: : 292it [03:40,  1.10it/s] \u001b[A\n",
      "batch 293, training loss: 4.192: : 293it [03:40,  1.08it/s]\u001b[A\n",
      "batch 294, training loss: 4.279: : 293it [03:40,  1.08it/s]\u001b[A\n",
      "batch 294, training loss: 4.279: : 294it [03:40,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 4.1833: : 294it [03:41,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 4.1833: : 295it [03:41,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 4.0606: : 295it [03:42,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 4.0606: : 296it [03:42,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 4.2235: : 296it [03:43,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 4.2235: : 297it [03:43,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 4.117: : 297it [03:44,  1.09it/s] \u001b[A\n",
      "batch 298, training loss: 4.117: : 298it [03:44,  1.11it/s]\u001b[A\n",
      "batch 299, training loss: 4.2218: : 298it [03:45,  1.11it/s]\u001b[A\n",
      "batch 299, training loss: 4.2218: : 299it [03:45,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 4.2876: : 299it [03:46,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 4.2876: : 300it [03:46,  1.12it/s]\u001b[A\n",
      "batch 301, training loss: 4.2461: : 300it [03:47,  1.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 301, training loss: 4.2461: : 301it [03:47,  1.12it/s]\u001b[A\n",
      "batch 302, training loss: 4.1973: : 301it [03:48,  1.12it/s]\u001b[A\n",
      "batch 302, training loss: 4.1973: : 302it [03:48,  1.11it/s]\u001b[A\n",
      "batch 303, training loss: 4.1927: : 302it [03:49,  1.11it/s]\u001b[A\n",
      "batch 303, training loss: 4.1927: : 303it [03:49,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 4.2134: : 303it [03:50,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 4.2134: : 304it [03:50,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 4.1801: : 304it [03:50,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 4.1801: : 305it [03:50,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.2123: : 305it [03:51,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.2123: : 306it [03:51,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 4.3293: : 306it [03:52,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 4.3293: : 307it [03:52,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.9591: : 307it [03:53,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.9591: : 308it [03:53,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 4.3343: : 308it [03:54,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 4.3343: : 309it [03:54,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 4.11: : 309it [03:55,  1.09it/s]  \u001b[A\n",
      "batch 310, training loss: 4.11: : 310it [03:55,  1.12it/s]\u001b[A\n",
      "batch 311, training loss: 4.1993: : 310it [03:56,  1.12it/s]\u001b[A\n",
      "batch 311, training loss: 4.1993: : 311it [03:56,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 4.0234: : 311it [03:57,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 4.0234: : 312it [03:57,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 4.1521: : 312it [03:58,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 4.1521: : 313it [03:58,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 4.191: : 313it [03:59,  1.09it/s] \u001b[A\n",
      "batch 314, training loss: 4.191: : 314it [03:59,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 4.2658: : 314it [03:59,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 4.2658: : 315it [03:59,  1.23it/s]\u001b[A\n",
      "batch 316, training loss: 4.4176: : 315it [04:00,  1.23it/s]\u001b[A\n",
      "batch 316, training loss: 4.4176: : 316it [04:00,  1.35it/s]\u001b[A\n",
      "batch 317, training loss: 4.0675: : 316it [04:00,  1.35it/s]\u001b[A\n",
      "batch 317, training loss: 4.0675: : 317it [04:00,  1.55it/s]\u001b[A\n",
      "batch 318, training loss: 4.3493: : 317it [04:01,  1.55it/s]\u001b[A\n",
      "batch 318, training loss: 4.3493: : 318it [04:01,  1.47it/s]\u001b[A\n",
      "batch 319, training loss: 4.4406: : 318it [04:02,  1.47it/s]\u001b[A\n",
      "batch 319, training loss: 4.4406: : 319it [04:02,  1.35it/s]\u001b[A\n",
      "batch 320, training loss: 4.4336: : 319it [04:03,  1.35it/s]\u001b[A\n",
      "batch 320, training loss: 4.4336: : 320it [04:03,  1.21it/s]\u001b[A\n",
      "batch 321, training loss: 4.4809: : 320it [04:04,  1.21it/s]\u001b[A\n",
      "batch 321, training loss: 4.4809: : 321it [04:04,  1.13it/s]\u001b[A\n",
      "batch 322, training loss: 4.5536: : 321it [04:05,  1.13it/s]\u001b[A\n",
      "batch 322, training loss: 4.5536: : 322it [04:05,  1.09it/s]\u001b[A\n",
      "batch 323, training loss: 4.296: : 322it [04:06,  1.09it/s] \u001b[A\n",
      "batch 323, training loss: 4.296: : 323it [04:06,  1.05it/s]\u001b[A\n",
      "batch 324, training loss: 4.3788: : 323it [04:07,  1.05it/s]\u001b[A\n",
      "batch 324, training loss: 4.3788: : 324it [04:07,  1.07it/s]\u001b[A\n",
      "batch 325, training loss: 4.4433: : 324it [04:08,  1.07it/s]\u001b[A\n",
      "batch 325, training loss: 4.4433: : 325it [04:08,  1.05it/s]\u001b[A\n",
      "batch 326, training loss: 4.359: : 325it [04:09,  1.05it/s] \u001b[A\n",
      "batch 326, training loss: 4.359: : 326it [04:09,  1.03it/s]\u001b[A\n",
      "batch 327, training loss: 4.4855: : 326it [04:10,  1.03it/s]\u001b[A\n",
      "batch 327, training loss: 4.4855: : 327it [04:10,  1.01it/s]\u001b[A\n",
      "batch 328, training loss: 4.3118: : 327it [04:11,  1.01it/s]\u001b[A\n",
      "batch 328, training loss: 4.3118: : 328it [04:11,  1.00it/s]\u001b[A\n",
      "batch 329, training loss: 4.242: : 328it [04:12,  1.00it/s] \u001b[A\n",
      "batch 329, training loss: 4.242: : 329it [04:12,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.3229: : 329it [04:13,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.3229: : 330it [04:13,  1.03it/s]\u001b[A\n",
      "batch 331, training loss: 4.3849: : 330it [04:14,  1.03it/s]\u001b[A\n",
      "batch 331, training loss: 4.3849: : 331it [04:14,  1.00it/s]\u001b[A\n",
      "batch 332, training loss: 4.2581: : 331it [04:15,  1.00it/s]\u001b[A\n",
      "batch 332, training loss: 4.2581: : 332it [04:15,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 4.2237: : 332it [04:16,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 4.2237: : 333it [04:16,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 4.3394: : 333it [04:17,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 4.3394: : 334it [04:17,  1.01s/it]\u001b[A\n",
      "batch 335, training loss: 4.4538: : 334it [04:18,  1.01s/it]\u001b[A\n",
      "batch 335, training loss: 4.4538: : 335it [04:18,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.3647: : 335it [04:19,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.3647: : 336it [04:19,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.3722: : 336it [04:20,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.3722: : 337it [04:20,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 4.2646: : 337it [04:21,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 4.2646: : 338it [04:21,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 4.3307: : 338it [04:22,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 4.3307: : 339it [04:22,  1.00s/it]\u001b[A\n",
      "batch 340, training loss: 4.5403: : 339it [04:23,  1.00s/it]\u001b[A\n",
      "batch 340, training loss: 4.5403: : 340it [04:23,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 4.2023: : 340it [04:24,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 4.2023: : 341it [04:24,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 4.2972: : 341it [04:25,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 4.2972: : 342it [04:25,  1.04it/s]\u001b[A\n",
      "batch 343, training loss: 4.341: : 342it [04:25,  1.04it/s] \u001b[A\n",
      "batch 343, training loss: 4.341: : 343it [04:25,  1.11it/s]\u001b[A\n",
      "batch 344, training loss: 4.3888: : 343it [04:26,  1.11it/s]\u001b[A\n",
      "batch 344, training loss: 4.3888: : 344it [04:26,  1.09it/s]\u001b[A\n",
      "batch 345, training loss: 4.3014: : 344it [04:27,  1.09it/s]\u001b[A\n",
      "batch 345, training loss: 4.3014: : 345it [04:27,  1.05it/s]\u001b[A\n",
      "batch 346, training loss: 4.3076: : 345it [04:28,  1.05it/s]\u001b[A\n",
      "batch 346, training loss: 4.3076: : 346it [04:28,  1.03it/s]\u001b[A\n",
      "batch 347, training loss: 4.2186: : 346it [04:29,  1.03it/s]\u001b[A\n",
      "batch 347, training loss: 4.2186: : 347it [04:29,  1.01it/s]\u001b[A\n",
      "batch 348, training loss: 4.2926: : 347it [04:30,  1.01it/s]\u001b[A\n",
      "batch 348, training loss: 4.2926: : 348it [04:30,  1.04it/s]\u001b[A\n",
      "batch 349, training loss: 4.4194: : 348it [04:31,  1.04it/s]\u001b[A\n",
      "batch 349, training loss: 4.4194: : 349it [04:31,  1.09it/s]\u001b[A\n",
      "batch 350, training loss: 4.2992: : 349it [04:32,  1.09it/s]\u001b[A\n",
      "batch 350, training loss: 4.2992: : 350it [04:32,  1.05it/s]\u001b[A\n",
      "batch 351, training loss: 4.1858: : 350it [04:33,  1.05it/s]\u001b[A\n",
      "batch 351, training loss: 4.1858: : 351it [04:33,  1.02it/s]\u001b[A\n",
      "batch 352, training loss: 4.2614: : 351it [04:34,  1.02it/s]\u001b[A\n",
      "batch 352, training loss: 4.2614: : 352it [04:34,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 4.2969: : 352it [04:35,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 4.2969: : 353it [04:35,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 4.3616: : 353it [04:36,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 4.3616: : 354it [04:36,  1.01it/s]\u001b[A\n",
      "batch 355, training loss: 4.319: : 354it [04:37,  1.01it/s] \u001b[A\n",
      "batch 355, training loss: 4.319: : 355it [04:37,  1.01s/it]\u001b[A\n",
      "batch 356, training loss: 4.1464: : 355it [04:38,  1.01s/it]\u001b[A\n",
      "batch 356, training loss: 4.1464: : 356it [04:38,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 4.1302: : 356it [04:39,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 4.1302: : 357it [04:39,  1.00it/s]\u001b[A\n",
      "batch 358, training loss: 4.334: : 357it [04:40,  1.00it/s] \u001b[A\n",
      "batch 358, training loss: 4.334: : 358it [04:40,  1.02s/it]\u001b[A\n",
      "batch 359, training loss: 4.163: : 358it [04:41,  1.02s/it]\u001b[A\n",
      "batch 359, training loss: 4.163: : 359it [04:41,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 4.3099: : 359it [04:42,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 4.3099: : 360it [04:42,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 4.2675: : 360it [04:43,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 4.2675: : 361it [04:43,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 4.4036: : 361it [04:44,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 4.4036: : 362it [04:44,  1.00s/it]\u001b[A\n",
      "batch 363, training loss: 4.2578: : 362it [04:45,  1.00s/it]\u001b[A\n",
      "batch 363, training loss: 4.2578: : 363it [04:45,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 4.1463: : 363it [04:46,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 4.1463: : 364it [04:46,  1.00it/s]\u001b[A\n",
      "batch 365, training loss: 4.1471: : 364it [04:47,  1.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365, training loss: 4.1471: : 365it [04:47,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 4.3123: : 365it [04:48,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 4.3123: : 366it [04:48,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 4.2043: : 366it [04:49,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 4.2043: : 367it [04:49,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 4.3152: : 367it [04:50,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 4.3152: : 368it [04:50,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 4.3278: : 368it [04:51,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 4.3278: : 369it [04:51,  1.11it/s]\u001b[A\n",
      "batch 370, training loss: 4.1896: : 369it [04:52,  1.11it/s]\u001b[A\n",
      "batch 370, training loss: 4.1896: : 370it [04:52,  1.17it/s]\u001b[A\n",
      "batch 371, training loss: 4.2601: : 370it [04:53,  1.17it/s]\u001b[A\n",
      "batch 371, training loss: 4.2601: : 371it [04:53,  1.13it/s]\u001b[A\n",
      "batch 372, training loss: 4.2101: : 371it [04:54,  1.13it/s]\u001b[A\n",
      "batch 372, training loss: 4.2101: : 372it [04:54,  1.05it/s]\u001b[A\n",
      "batch 373, training loss: 4.2155: : 372it [04:55,  1.05it/s]\u001b[A\n",
      "batch 373, training loss: 4.2155: : 373it [04:55,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 4.2333: : 373it [04:56,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 4.2333: : 374it [04:56,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 4.181: : 374it [04:56,  1.01it/s] \u001b[A\n",
      "batch 375, training loss: 4.181: : 375it [04:56,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 4.3404: : 375it [04:57,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 4.3404: : 376it [04:57,  1.08it/s]\u001b[A\n",
      "batch 377, training loss: 4.4495: : 376it [04:59,  1.08it/s]\u001b[A\n",
      "batch 377, training loss: 4.4495: : 377it [04:59,  1.00it/s]\u001b[A\n",
      "batch 378, training loss: 4.3371: : 377it [05:00,  1.00it/s]\u001b[A\n",
      "batch 378, training loss: 4.3371: : 378it [05:00,  1.04s/it]\u001b[A\n",
      "batch 379, training loss: 4.2703: : 378it [05:01,  1.04s/it]\u001b[A\n",
      "batch 379, training loss: 4.2703: : 379it [05:01,  1.06s/it]\u001b[A\n",
      "batch 380, training loss: 4.3448: : 379it [05:02,  1.06s/it]\u001b[A\n",
      "batch 380, training loss: 4.3448: : 380it [05:02,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.3865: : 380it [05:03,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.3865: : 381it [05:03,  1.07s/it]\u001b[A\n",
      "batch 382, training loss: 4.2419: : 381it [05:04,  1.07s/it]\u001b[A\n",
      "batch 382, training loss: 4.2419: : 382it [05:04,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 4.2892: : 382it [05:05,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 4.2892: : 383it [05:05,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 4.4486: : 383it [05:06,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 4.4486: : 384it [05:06,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 4.3153: : 384it [05:07,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 4.3153: : 385it [05:07,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 4.2474: : 385it [05:08,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 4.2474: : 386it [05:08,  1.08s/it]\u001b[A\n",
      "batch 387, training loss: 4.3615: : 386it [05:10,  1.08s/it]\u001b[A\n",
      "batch 387, training loss: 4.3615: : 387it [05:10,  1.10s/it]\u001b[A\n",
      "batch 388, training loss: 4.08: : 387it [05:11,  1.10s/it]  \u001b[A\n",
      "batch 388, training loss: 4.08: : 388it [05:11,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 4.2866: : 388it [05:12,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 4.2866: : 389it [05:12,  1.08s/it]\u001b[A\n",
      "batch 390, training loss: 4.3776: : 389it [05:13,  1.08s/it]\u001b[A\n",
      "batch 390, training loss: 4.3776: : 390it [05:13,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.3646: : 390it [05:14,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.3646: : 391it [05:14,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.3267: : 391it [05:15,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.3267: : 392it [05:15,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 4.1809: : 392it [05:16,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 4.1809: : 393it [05:16,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 4.1001: : 393it [05:17,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 4.1001: : 394it [05:17,  1.03s/it]\u001b[A\n",
      "batch 395, training loss: 4.1922: : 394it [05:18,  1.03s/it]\u001b[A\n",
      "batch 395, training loss: 4.1922: : 395it [05:18,  1.05s/it]\u001b[A\n",
      "batch 396, training loss: 4.3854: : 395it [05:19,  1.05s/it]\u001b[A\n",
      "batch 396, training loss: 4.3854: : 396it [05:19,  1.07s/it]\u001b[A\n",
      "batch 397, training loss: 4.1517: : 396it [05:20,  1.07s/it]\u001b[A\n",
      "batch 397, training loss: 4.1517: : 397it [05:20,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 4.252: : 397it [05:22,  1.10s/it] \u001b[A\n",
      "batch 398, training loss: 4.252: : 398it [05:22,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.3699: : 398it [05:23,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.3699: : 399it [05:23,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 4.1332: : 399it [05:24,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 4.1332: : 400it [05:24,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 4.1095: : 400it [05:25,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 4.1095: : 401it [05:25,  1.07s/it]\u001b[A\n",
      "batch 402, training loss: 4.1824: : 401it [05:26,  1.07s/it]\u001b[A\n",
      "batch 402, training loss: 4.1824: : 402it [05:26,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.3727: : 402it [05:27,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.3727: : 403it [05:27,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.0074: : 403it [05:28,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 4.0074: : 404it [05:28,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 4.234: : 404it [05:29,  1.08s/it] \u001b[A\n",
      "batch 405, training loss: 4.234: : 405it [05:29,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 4.1229: : 405it [05:30,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 4.1229: : 406it [05:30,  1.10s/it]\u001b[A\n",
      "batch 407, training loss: 4.222: : 406it [05:31,  1.10s/it] \u001b[A\n",
      "batch 407, training loss: 4.222: : 407it [05:31,  1.10s/it]\u001b[A\n",
      "batch 408, training loss: 4.057: : 407it [05:33,  1.10s/it]\u001b[A\n",
      "batch 408, training loss: 4.057: : 408it [05:33,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 4.3056: : 408it [05:34,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 4.3056: : 409it [05:34,  1.10s/it]\u001b[A\n",
      "batch 410, training loss: 4.1153: : 409it [05:35,  1.10s/it]\u001b[A\n",
      "batch 410, training loss: 4.1153: : 410it [05:35,  1.09s/it]\u001b[A\n",
      "batch 411, training loss: 4.214: : 410it [05:36,  1.09s/it] \u001b[A\n",
      "batch 411, training loss: 4.214: : 411it [05:36,  1.11s/it]\u001b[A\n",
      "batch 412, training loss: 4.1985: : 411it [05:37,  1.11s/it]\u001b[A\n",
      "batch 412, training loss: 4.1985: : 412it [05:37,  1.11s/it]\u001b[A\n",
      "batch 413, training loss: 4.1636: : 412it [05:38,  1.11s/it]\u001b[A\n",
      "batch 413, training loss: 4.1636: : 413it [05:38,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 4.0387: : 413it [05:39,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 4.0387: : 414it [05:39,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.9629: : 414it [05:40,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.9629: : 415it [05:40,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 4.0218: : 415it [05:41,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 4.0218: : 416it [05:41,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 4.3015: : 416it [05:42,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 4.3015: : 417it [05:42,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 4.1776: : 417it [05:43,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 4.1776: : 418it [05:43,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 3.9918: : 418it [05:45,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 3.9918: : 419it [05:45,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.1103: : 419it [05:46,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 4.1103: : 420it [05:46,  1.11s/it]\u001b[A\n",
      "batch 421, training loss: 4.1877: : 420it [05:47,  1.11s/it]\u001b[A\n",
      "batch 421, training loss: 4.1877: : 421it [05:47,  1.11s/it]\u001b[A\n",
      "batch 422, training loss: 4.2059: : 421it [05:48,  1.11s/it]\u001b[A\n",
      "batch 422, training loss: 4.2059: : 422it [05:48,  1.12s/it]\u001b[A\n",
      "batch 423, training loss: 4.0583: : 422it [05:49,  1.12s/it]\u001b[A\n",
      "batch 423, training loss: 4.0583: : 423it [05:49,  1.11s/it]\u001b[A\n",
      "batch 424, training loss: 4.2788: : 423it [05:50,  1.11s/it]\u001b[A\n",
      "batch 424, training loss: 4.2788: : 424it [05:50,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.3802: : 424it [05:51,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.3802: : 425it [05:51,  1.15s/it]\u001b[A\n",
      "batch 426, training loss: 4.3021: : 425it [05:53,  1.15s/it]\u001b[A\n",
      "batch 426, training loss: 4.3021: : 426it [05:53,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 4.3345: : 426it [05:54,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 4.3345: : 427it [05:54,  1.16s/it]\u001b[A\n",
      "batch 428, training loss: 4.4777: : 427it [05:55,  1.16s/it]\u001b[A\n",
      "batch 428, training loss: 4.4777: : 428it [05:55,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 4.3543: : 428it [05:56,  1.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 429, training loss: 4.3543: : 429it [05:56,  1.18s/it]\u001b[A\n",
      "batch 430, training loss: 4.3383: : 429it [05:57,  1.18s/it]\u001b[A\n",
      "batch 430, training loss: 4.3383: : 430it [05:57,  1.19s/it]\u001b[A\n",
      "batch 431, training loss: 4.3817: : 430it [05:59,  1.19s/it]\u001b[A\n",
      "batch 431, training loss: 4.3817: : 431it [05:59,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 4.2554: : 431it [06:00,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 4.2554: : 432it [06:00,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 4.283: : 432it [06:01,  1.21s/it] \u001b[A\n",
      "batch 433, training loss: 4.283: : 433it [06:01,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.2611: : 433it [06:02,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.2611: : 434it [06:02,  1.12s/it]\u001b[A\n",
      "batch 435, training loss: 4.2485: : 434it [06:03,  1.12s/it]\u001b[A\n",
      "batch 435, training loss: 4.2485: : 435it [06:03,  1.16s/it]\u001b[A\n",
      "batch 436, training loss: 4.2819: : 435it [06:04,  1.16s/it]\u001b[A\n",
      "batch 436, training loss: 4.2819: : 436it [06:04,  1.18s/it]\u001b[A\n",
      "batch 437, training loss: 4.2283: : 436it [06:06,  1.18s/it]\u001b[A\n",
      "batch 437, training loss: 4.2283: : 437it [06:06,  1.19s/it]\u001b[A\n",
      "batch 438, training loss: 4.2929: : 437it [06:07,  1.19s/it]\u001b[A\n",
      "batch 438, training loss: 4.2929: : 438it [06:07,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 4.3546: : 438it [06:08,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 4.3546: : 439it [06:08,  1.19s/it]\u001b[A\n",
      "batch 440, training loss: 4.2021: : 439it [06:09,  1.19s/it]\u001b[A\n",
      "batch 440, training loss: 4.2021: : 440it [06:09,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 4.4211: : 440it [06:11,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 4.4211: : 441it [06:11,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 4.136: : 441it [06:12,  1.21s/it] \u001b[A\n",
      "batch 442, training loss: 4.136: : 442it [06:12,  1.20s/it]\u001b[A\n",
      "batch 443, training loss: 4.2032: : 442it [06:13,  1.20s/it]\u001b[A\n",
      "batch 443, training loss: 4.2032: : 443it [06:13,  1.18s/it]\u001b[A\n",
      "batch 444, training loss: 4.1958: : 443it [06:14,  1.18s/it]\u001b[A\n",
      "batch 444, training loss: 4.1958: : 444it [06:14,  1.18s/it]\u001b[A\n",
      "batch 445, training loss: 4.23: : 444it [06:15,  1.18s/it]  \u001b[A\n",
      "batch 445, training loss: 4.23: : 445it [06:15,  1.18s/it]\u001b[A\n",
      "batch 446, training loss: 4.3301: : 445it [06:16,  1.18s/it]\u001b[A\n",
      "batch 446, training loss: 4.3301: : 446it [06:16,  1.18s/it]\u001b[A\n",
      "batch 447, training loss: 4.2149: : 446it [06:18,  1.18s/it]\u001b[A\n",
      "batch 447, training loss: 4.2149: : 447it [06:18,  1.20s/it]\u001b[A\n",
      "batch 448, training loss: 4.1058: : 447it [06:19,  1.20s/it]\u001b[A\n",
      "batch 448, training loss: 4.1058: : 448it [06:19,  1.20s/it]\u001b[A\n",
      "batch 449, training loss: 4.2528: : 448it [06:20,  1.20s/it]\u001b[A\n",
      "batch 449, training loss: 4.2528: : 449it [06:20,  1.21s/it]\u001b[A\n",
      "batch 450, training loss: 4.2511: : 449it [06:21,  1.21s/it]\u001b[A\n",
      "batch 450, training loss: 4.2511: : 450it [06:21,  1.20s/it]\u001b[A\n",
      "batch 451, training loss: 4.2835: : 450it [06:22,  1.20s/it]\u001b[A\n",
      "batch 451, training loss: 4.2835: : 451it [06:22,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 4.2815: : 451it [06:24,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 4.2815: : 452it [06:24,  1.20s/it]\u001b[A\n",
      "batch 453, training loss: 4.3294: : 452it [06:25,  1.20s/it]\u001b[A\n",
      "batch 453, training loss: 4.3294: : 453it [06:25,  1.20s/it]\u001b[A\n",
      "batch 454, training loss: 4.2051: : 453it [06:26,  1.20s/it]\u001b[A\n",
      "batch 454, training loss: 4.2051: : 454it [06:26,  1.21s/it]\u001b[A\n",
      "batch 455, training loss: 4.0727: : 454it [06:27,  1.21s/it]\u001b[A\n",
      "batch 455, training loss: 4.0727: : 455it [06:27,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.1253: : 455it [06:29,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.1253: : 456it [06:29,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 4.278: : 456it [06:30,  1.22s/it] \u001b[A\n",
      "batch 457, training loss: 4.278: : 457it [06:30,  1.21s/it]\u001b[A\n",
      "batch 458, training loss: 4.1255: : 457it [06:31,  1.21s/it]\u001b[A\n",
      "batch 458, training loss: 4.1255: : 458it [06:31,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.1503: : 458it [06:32,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 4.1503: : 459it [06:32,  1.22s/it]\u001b[A\n",
      "batch 460, training loss: 4.1935: : 459it [06:33,  1.22s/it]\u001b[A\n",
      "batch 460, training loss: 4.1935: : 460it [06:33,  1.13s/it]\u001b[A\n",
      "batch 461, training loss: 4.2818: : 460it [06:34,  1.13s/it]\u001b[A\n",
      "batch 461, training loss: 4.2818: : 461it [06:34,  1.06s/it]\u001b[A\n",
      "batch 462, training loss: 4.189: : 461it [06:35,  1.06s/it] \u001b[A\n",
      "batch 462, training loss: 4.189: : 462it [06:35,  1.11s/it]\u001b[A\n",
      "batch 463, training loss: 4.06: : 462it [06:36,  1.11s/it] \u001b[A\n",
      "batch 463, training loss: 4.06: : 463it [06:36,  1.14s/it]\u001b[A\n",
      "batch 464, training loss: 4.18: : 463it [06:38,  1.14s/it]\u001b[A\n",
      "batch 464, training loss: 4.18: : 464it [06:38,  1.16s/it]\u001b[A\n",
      "batch 465, training loss: 4.2006: : 464it [06:39,  1.16s/it]\u001b[A\n",
      "batch 465, training loss: 4.2006: : 465it [06:39,  1.11s/it]\u001b[A\n",
      "batch 466, training loss: 4.1314: : 465it [06:40,  1.11s/it]\u001b[A\n",
      "batch 466, training loss: 4.1314: : 466it [06:40,  1.13s/it]\u001b[A\n",
      "batch 467, training loss: 4.0963: : 466it [06:41,  1.13s/it]\u001b[A\n",
      "batch 467, training loss: 4.0963: : 467it [06:41,  1.18s/it]\u001b[A\n",
      "batch 468, training loss: 4.213: : 467it [06:42,  1.18s/it] \u001b[A\n",
      "batch 468, training loss: 4.213: : 468it [06:42,  1.20s/it]\u001b[A\n",
      "batch 469, training loss: 4.1536: : 468it [06:44,  1.20s/it]\u001b[A\n",
      "batch 469, training loss: 4.1536: : 469it [06:44,  1.23s/it]\u001b[A\n",
      "batch 470, training loss: 4.2712: : 469it [06:45,  1.23s/it]\u001b[A\n",
      "batch 470, training loss: 4.2712: : 470it [06:45,  1.23s/it]\u001b[A\n",
      "batch 471, training loss: 4.2812: : 470it [06:46,  1.23s/it]\u001b[A\n",
      "batch 471, training loss: 4.2812: : 471it [06:46,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 4.1958: : 471it [06:47,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 4.1958: : 472it [06:47,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 4.2022: : 472it [06:49,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 4.2022: : 473it [06:49,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 4.2678: : 473it [06:50,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 4.2678: : 474it [06:50,  1.28s/it]\u001b[A\n",
      "batch 475, training loss: 4.2165: : 474it [06:51,  1.28s/it]\u001b[A\n",
      "batch 475, training loss: 4.2165: : 475it [06:51,  1.28s/it]\u001b[A\n",
      "batch 476, training loss: 4.2781: : 475it [06:53,  1.28s/it]\u001b[A\n",
      "batch 476, training loss: 4.2781: : 476it [06:53,  1.29s/it]\u001b[A\n",
      "batch 477, training loss: 4.0993: : 476it [06:54,  1.29s/it]\u001b[A\n",
      "batch 477, training loss: 4.0993: : 477it [06:54,  1.27s/it]\u001b[A\n",
      "batch 478, training loss: 4.1761: : 477it [06:55,  1.27s/it]\u001b[A\n",
      "batch 478, training loss: 4.1761: : 478it [06:55,  1.28s/it]\u001b[A\n",
      "batch 479, training loss: 4.1433: : 478it [06:56,  1.28s/it]\u001b[A\n",
      "batch 479, training loss: 4.1433: : 479it [06:56,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 4.1502: : 479it [06:58,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 4.1502: : 480it [06:58,  1.27s/it]\u001b[A\n",
      "batch 481, training loss: 4.2126: : 480it [06:59,  1.27s/it]\u001b[A\n",
      "batch 481, training loss: 4.2126: : 481it [06:59,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 4.1614: : 481it [07:00,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 4.1614: : 482it [07:00,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 4.0944: : 482it [07:01,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 4.0944: : 483it [07:01,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.1608: : 483it [07:03,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 4.1608: : 484it [07:03,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 4.1113: : 484it [07:04,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 4.1113: : 485it [07:04,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 4.2008: : 485it [07:05,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 4.2008: : 486it [07:05,  1.27s/it]\u001b[A\n",
      "batch 487, training loss: 4.3233: : 486it [07:07,  1.27s/it]\u001b[A\n",
      "batch 487, training loss: 4.3233: : 487it [07:07,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 4.0973: : 487it [07:08,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 4.0973: : 488it [07:08,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 4.0311: : 488it [07:09,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 4.0311: : 489it [07:09,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 4.1155: : 489it [07:10,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 4.1155: : 490it [07:10,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 4.1088: : 490it [07:12,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 4.1088: : 491it [07:12,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 4.027: : 491it [07:13,  1.26s/it] \u001b[A\n",
      "batch 492, training loss: 4.027: : 492it [07:13,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 4.1961: : 492it [07:14,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 493, training loss: 4.1961: : 493it [07:14,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 4.2182: : 493it [07:15,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 4.2182: : 494it [07:15,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 4.0119: : 494it [07:17,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 4.0119: : 495it [07:17,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 4.0582: : 495it [07:18,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 4.0582: : 496it [07:18,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.9818: : 496it [07:19,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.9818: : 497it [07:19,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 4.0589: : 497it [07:20,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 4.0589: : 498it [07:20,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 4.1293: : 498it [07:21,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 4.1293: : 499it [07:21,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 4.3028: : 499it [07:22,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 4.3028: : 500it [07:22,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 4.1664: : 500it [07:24,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 4.1664: : 501it [07:24,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 4.1355: : 501it [07:25,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 4.1355: : 502it [07:25,  1.28s/it]\u001b[A\n",
      "batch 503, training loss: 4.2454: : 502it [07:27,  1.28s/it]\u001b[A\n",
      "batch 503, training loss: 4.2454: : 503it [07:27,  1.32s/it]\u001b[A\n",
      "batch 504, training loss: 4.2009: : 503it [07:28,  1.32s/it]\u001b[A\n",
      "batch 504, training loss: 4.2009: : 504it [07:28,  1.33s/it]\u001b[A\n",
      "batch 505, training loss: 4.296: : 504it [07:29,  1.33s/it] \u001b[A\n",
      "batch 505, training loss: 4.296: : 505it [07:29,  1.33s/it]\u001b[A\n",
      "batch 506, training loss: 4.244: : 505it [07:31,  1.33s/it]\u001b[A\n",
      "batch 506, training loss: 4.244: : 506it [07:31,  1.35s/it]\u001b[A\n",
      "batch 507, training loss: 4.0585: : 506it [07:32,  1.35s/it]\u001b[A\n",
      "batch 507, training loss: 4.0585: : 507it [07:32,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 4.2341: : 507it [07:33,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 4.2341: : 508it [07:33,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 4.1251: : 508it [07:35,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 4.1251: : 509it [07:35,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 4.1745: : 509it [07:36,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 4.1745: : 510it [07:36,  1.28s/it]\u001b[A\n",
      "batch 511, training loss: 4.1979: : 510it [07:37,  1.28s/it]\u001b[A\n",
      "batch 511, training loss: 4.1979: : 511it [07:37,  1.27s/it]\u001b[A\n",
      "batch 512, training loss: 4.183: : 511it [07:39,  1.27s/it] \u001b[A\n",
      "batch 512, training loss: 4.183: : 512it [07:39,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 4.2129: : 512it [07:40,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 4.2129: : 513it [07:40,  1.32s/it]\u001b[A\n",
      "batch 514, training loss: 4.0525: : 513it [07:41,  1.32s/it]\u001b[A\n",
      "batch 514, training loss: 4.0525: : 514it [07:41,  1.34s/it]\u001b[A\n",
      "batch 515, training loss: 4.2642: : 514it [07:43,  1.34s/it]\u001b[A\n",
      "batch 515, training loss: 4.2642: : 515it [07:43,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 4.1465: : 515it [07:44,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 4.1465: : 516it [07:44,  1.35s/it]\u001b[A\n",
      "batch 517, training loss: 4.1077: : 516it [07:46,  1.35s/it]\u001b[A\n",
      "batch 517, training loss: 4.1077: : 517it [07:46,  1.39s/it]\u001b[A\n",
      "batch 518, training loss: 4.161: : 517it [07:47,  1.39s/it] \u001b[A\n",
      "batch 518, training loss: 4.161: : 518it [07:47,  1.35s/it]\u001b[A\n",
      "batch 519, training loss: 4.0055: : 518it [07:48,  1.35s/it]\u001b[A\n",
      "batch 519, training loss: 4.0055: : 519it [07:48,  1.35s/it]\u001b[A\n",
      "batch 520, training loss: 4.1769: : 519it [07:50,  1.35s/it]\u001b[A\n",
      "batch 520, training loss: 4.1769: : 520it [07:50,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 4.1615: : 520it [07:51,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 4.1615: : 521it [07:51,  1.36s/it]\u001b[A\n",
      "batch 522, training loss: 4.2521: : 521it [07:52,  1.36s/it]\u001b[A\n",
      "batch 522, training loss: 4.2521: : 522it [07:52,  1.38s/it]\u001b[A\n",
      "batch 523, training loss: 4.0756: : 522it [07:54,  1.38s/it]\u001b[A\n",
      "batch 523, training loss: 4.0756: : 523it [07:54,  1.39s/it]\u001b[A\n",
      "batch 524, training loss: 4.2783: : 523it [07:55,  1.39s/it]\u001b[A\n",
      "batch 524, training loss: 4.2783: : 524it [07:55,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 4.2895: : 524it [07:56,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 4.2895: : 525it [07:56,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.1071: : 525it [07:58,  1.39s/it]\u001b[A\n",
      "batch 526, training loss: 4.1071: : 526it [07:58,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 4.0438: : 526it [07:59,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 4.0438: : 527it [07:59,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.1777: : 527it [08:00,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 4.1777: : 528it [08:00,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 4.1835: : 528it [08:02,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 4.1835: : 529it [08:02,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 4.1857: : 529it [08:03,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 4.1857: : 530it [08:03,  1.40s/it]\u001b[A\n",
      "batch 531, training loss: 4.1145: : 530it [08:05,  1.40s/it]\u001b[A\n",
      "batch 531, training loss: 4.1145: : 531it [08:05,  1.42s/it]\u001b[A\n",
      "batch 532, training loss: 3.9364: : 531it [08:06,  1.42s/it]\u001b[A\n",
      "batch 532, training loss: 3.9364: : 532it [08:06,  1.40s/it]\u001b[A\n",
      "batch 533, training loss: 4.1417: : 532it [08:08,  1.40s/it]\u001b[A\n",
      "batch 533, training loss: 4.1417: : 533it [08:08,  1.42s/it]\u001b[A\n",
      "batch 534, training loss: 4.2376: : 533it [08:09,  1.42s/it]\u001b[A\n",
      "batch 534, training loss: 4.2376: : 534it [08:09,  1.45s/it]\u001b[A\n",
      "batch 535, training loss: 4.1017: : 534it [08:11,  1.45s/it]\u001b[A\n",
      "batch 535, training loss: 4.1017: : 535it [08:11,  1.47s/it]\u001b[A\n",
      "batch 536, training loss: 3.9576: : 535it [08:12,  1.47s/it]\u001b[A\n",
      "batch 536, training loss: 3.9576: : 536it [08:12,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.9879: : 536it [08:14,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.9879: : 537it [08:14,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 4.1766: : 537it [08:15,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 4.1766: : 538it [08:15,  1.47s/it]\u001b[A\n",
      "batch 539, training loss: 4.0713: : 538it [08:17,  1.47s/it]\u001b[A\n",
      "batch 539, training loss: 4.0713: : 539it [08:17,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 4.0694: : 539it [08:18,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 4.0694: : 540it [08:18,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.1359: : 540it [08:20,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.1359: : 541it [08:20,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.1798: : 541it [08:21,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.1798: : 542it [08:21,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 4.0574: : 542it [08:23,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 4.0574: : 543it [08:23,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 4.1024: : 543it [08:24,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 4.1024: : 544it [08:24,  1.50s/it]\u001b[A\n",
      "batch 545, training loss: 4.1051: : 544it [08:26,  1.50s/it]\u001b[A\n",
      "batch 545, training loss: 4.1051: : 545it [08:26,  1.51s/it]\u001b[A\n",
      "batch 546, training loss: 4.0067: : 545it [08:27,  1.51s/it]\u001b[A\n",
      "batch 546, training loss: 4.0067: : 546it [08:27,  1.49s/it]\u001b[A\n",
      "batch 547, training loss: 3.8983: : 546it [08:29,  1.49s/it]\u001b[A\n",
      "batch 547, training loss: 3.8983: : 547it [08:29,  1.46s/it]\u001b[A\n",
      "batch 548, training loss: 4.0671: : 547it [08:29,  1.46s/it]\u001b[A\n",
      "batch 548, training loss: 4.0671: : 548it [08:29,  1.26s/it]\u001b[A\n",
      "batch 549, training loss: 4.0691: : 548it [08:31,  1.26s/it]\u001b[A\n",
      "batch 549, training loss: 4.0691: : 549it [08:31,  1.36s/it]\u001b[A\n",
      "batch 550, training loss: 3.9646: : 549it [08:32,  1.36s/it]\u001b[A\n",
      "batch 550, training loss: 3.9646: : 550it [08:32,  1.43s/it]\u001b[A\n",
      "batch 551, training loss: 4.0777: : 550it [08:34,  1.43s/it]\u001b[A\n",
      "batch 551, training loss: 4.0777: : 551it [08:34,  1.49s/it]\u001b[A\n",
      "batch 552, training loss: 4.0781: : 551it [08:36,  1.49s/it]\u001b[A\n",
      "batch 552, training loss: 4.0781: : 552it [08:36,  1.53s/it]\u001b[A\n",
      "batch 553, training loss: 4.0702: : 552it [08:37,  1.53s/it]\u001b[A\n",
      "batch 553, training loss: 4.0702: : 553it [08:37,  1.56s/it]\u001b[A\n",
      "batch 554, training loss: 3.7877: : 553it [08:39,  1.56s/it]\u001b[A\n",
      "batch 554, training loss: 3.7877: : 554it [08:39,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 4.0001: : 554it [08:41,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 4.0001: : 555it [08:41,  1.59s/it]\u001b[A\n",
      "batch 556, training loss: 4.1175: : 555it [08:42,  1.59s/it]\u001b[A\n",
      "batch 556, training loss: 4.1175: : 556it [08:42,  1.56s/it]\u001b[A\n",
      "batch 557, training loss: 3.9738: : 556it [08:44,  1.56s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 557, training loss: 3.9738: : 557it [08:44,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.8849: : 557it [08:45,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.8849: : 558it [08:45,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.9584: : 558it [08:47,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.9584: : 559it [08:47,  1.54s/it]\u001b[A\n",
      "batch 560, training loss: 3.9816: : 559it [08:48,  1.54s/it]\u001b[A\n",
      "batch 560, training loss: 3.9816: : 560it [08:48,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 4.0027: : 560it [08:50,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 4.0027: : 561it [08:50,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 3.8041: : 561it [08:52,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 3.8041: : 562it [08:52,  1.59s/it]\u001b[A\n",
      "batch 563, training loss: 3.9876: : 562it [08:53,  1.59s/it]\u001b[A\n",
      "batch 563, training loss: 3.9876: : 563it [08:53,  1.54s/it]\u001b[A\n",
      "batch 564, training loss: 3.8989: : 563it [08:54,  1.54s/it]\u001b[A\n",
      "batch 564, training loss: 3.8989: : 564it [08:54,  1.52s/it]\u001b[A\n",
      "batch 565, training loss: 3.9787: : 564it [08:56,  1.52s/it]\u001b[A\n",
      "batch 565, training loss: 3.9787: : 565it [08:56,  1.49s/it]\u001b[A\n",
      "batch 566, training loss: 4.1256: : 565it [08:57,  1.49s/it]\u001b[A\n",
      "batch 566, training loss: 4.1256: : 566it [08:57,  1.50s/it]\u001b[A\n",
      "batch 567, training loss: 4.0314: : 566it [08:59,  1.50s/it]\u001b[A\n",
      "batch 567, training loss: 4.0314: : 567it [08:59,  1.55s/it]\u001b[A\n",
      "batch 568, training loss: 4.1498: : 567it [09:01,  1.55s/it]\u001b[A\n",
      "batch 568, training loss: 4.1498: : 568it [09:01,  1.58s/it]\u001b[A\n",
      "batch 569, training loss: 4.0842: : 568it [09:02,  1.58s/it]\u001b[A\n",
      "batch 569, training loss: 4.0842: : 569it [09:02,  1.62s/it]\u001b[A\n",
      "batch 570, training loss: 4.2957: : 569it [09:04,  1.62s/it]\u001b[A\n",
      "batch 570, training loss: 4.2957: : 570it [09:04,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 4.0991: : 570it [09:06,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 4.0991: : 571it [09:06,  1.66s/it]\u001b[A\n",
      "batch 572, training loss: 4.1479: : 571it [09:07,  1.66s/it]\u001b[A\n",
      "batch 572, training loss: 4.1479: : 572it [09:07,  1.67s/it]\u001b[A\n",
      "batch 573, training loss: 4.0786: : 572it [09:09,  1.67s/it]\u001b[A\n",
      "batch 573, training loss: 4.0786: : 573it [09:09,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 4.1567: : 573it [09:11,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 4.1567: : 574it [09:11,  1.59s/it]\u001b[A\n",
      "batch 575, training loss: 4.0673: : 574it [09:11,  1.59s/it]\u001b[A\n",
      "batch 575, training loss: 4.0673: : 575it [09:11,  1.36s/it]\u001b[A\n",
      "batch 576, training loss: 4.1034: : 575it [09:13,  1.36s/it]\u001b[A\n",
      "batch 576, training loss: 4.1034: : 576it [09:13,  1.48s/it]\u001b[A\n",
      "batch 577, training loss: 4.0058: : 576it [09:15,  1.48s/it]\u001b[A\n",
      "batch 577, training loss: 4.0058: : 577it [09:15,  1.55s/it]\u001b[A\n",
      "batch 578, training loss: 3.9873: : 577it [09:16,  1.55s/it]\u001b[A\n",
      "batch 578, training loss: 3.9873: : 578it [09:16,  1.55s/it]\u001b[A\n",
      "batch 579, training loss: 3.9968: : 578it [09:17,  1.55s/it]\u001b[A\n",
      "batch 579, training loss: 3.9968: : 579it [09:17,  1.41s/it]\u001b[A\n",
      "batch 580, training loss: 3.9467: : 579it [09:19,  1.41s/it]\u001b[A\n",
      "batch 580, training loss: 3.9467: : 580it [09:19,  1.41s/it]\u001b[A\n",
      "batch 581, training loss: 3.9391: : 580it [09:21,  1.41s/it]\u001b[A\n",
      "batch 581, training loss: 3.9391: : 581it [09:21,  1.51s/it]\u001b[A\n",
      "batch 582, training loss: 3.9707: : 581it [09:22,  1.51s/it]\u001b[A\n",
      "batch 582, training loss: 3.9707: : 582it [09:22,  1.57s/it]\u001b[A\n",
      "batch 583, training loss: 3.8427: : 582it [09:23,  1.57s/it]\u001b[A\n",
      "batch 583, training loss: 3.8427: : 583it [09:23,  1.29s/it]\u001b[A\n",
      "batch 584, training loss: 4.1741: : 583it [09:25,  1.29s/it]\u001b[A\n",
      "batch 584, training loss: 4.1741: : 584it [09:25,  1.46s/it]\u001b[A\n",
      "batch 585, training loss: 4.1614: : 584it [09:27,  1.46s/it]\u001b[A\n",
      "batch 585, training loss: 4.1614: : 585it [09:27,  1.59s/it]\u001b[A\n",
      "batch 586, training loss: 4.2529: : 585it [09:29,  1.59s/it]\u001b[A\n",
      "batch 586, training loss: 4.2529: : 586it [09:29,  1.66s/it]\u001b[A\n",
      "batch 587, training loss: 4.1337: : 586it [09:30,  1.66s/it]\u001b[A\n",
      "batch 587, training loss: 4.1337: : 587it [09:30,  1.72s/it]\u001b[A\n",
      "batch 588, training loss: 3.9967: : 587it [09:32,  1.72s/it]\u001b[A\n",
      "batch 588, training loss: 3.9967: : 588it [09:32,  1.76s/it]\u001b[A\n",
      "batch 589, training loss: 4.084: : 588it [09:34,  1.76s/it] \u001b[A\n",
      "batch 589, training loss: 4.084: : 589it [09:34,  1.83s/it]\u001b[A\n",
      "batch 590, training loss: 4.0871: : 589it [09:36,  1.83s/it]\u001b[A\n",
      "batch 590, training loss: 4.0871: : 590it [09:36,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.9008: : 590it [09:38,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.9008: : 591it [09:38,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 3.9847: : 591it [09:40,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 3.9847: : 592it [09:40,  1.86s/it]\u001b[A\n",
      "batch 593, training loss: 3.9083: : 592it [09:42,  1.86s/it]\u001b[A\n",
      "batch 593, training loss: 3.9083: : 593it [09:42,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 4.0799: : 593it [09:44,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 4.0799: : 594it [09:44,  1.88s/it]\u001b[A\n",
      "batch 595, training loss: 4.1895: : 594it [09:45,  1.88s/it]\u001b[A\n",
      "batch 595, training loss: 4.1895: : 595it [09:45,  1.75s/it]\u001b[A\n",
      "batch 596, training loss: 4.0405: : 595it [09:47,  1.75s/it]\u001b[A\n",
      "batch 596, training loss: 4.0405: : 596it [09:47,  1.87s/it]\u001b[A\n",
      "batch 597, training loss: 3.8772: : 596it [09:49,  1.87s/it]\u001b[A\n",
      "batch 597, training loss: 3.8772: : 597it [09:49,  1.91s/it]\u001b[A\n",
      "batch 598, training loss: 4.1533: : 597it [09:52,  1.91s/it]\u001b[A\n",
      "batch 598, training loss: 4.1533: : 598it [09:52,  1.99s/it]\u001b[A\n",
      "batch 599, training loss: 4.0778: : 598it [09:53,  1.99s/it]\u001b[A\n",
      "batch 599, training loss: 4.0778: : 599it [09:53,  1.79s/it]\u001b[A\n",
      "batch 600, training loss: 3.888: : 599it [09:55,  1.79s/it] \u001b[A\n",
      "batch 600, training loss: 3.888: : 600it [09:55,  1.94s/it]\u001b[A\n",
      "batch 601, training loss: 3.401: : 600it [09:56,  1.94s/it]\u001b[A\n",
      "batch 601, training loss: 3.401: : 601it [09:56,  1.64s/it]\u001b[A\n",
      "batch 602, training loss: 3.987: : 601it [09:58,  1.64s/it]\u001b[A\n",
      "batch 602, training loss: 3.987: : 602it [09:58,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.9537: : 602it [10:00,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.9537: : 603it [10:00,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 4.1147: : 603it [10:02,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 4.1147: : 604it [10:02,  1.83s/it]\u001b[A\n",
      "batch 605, training loss: 4.2162: : 604it [10:04,  1.83s/it]\u001b[A\n",
      "batch 605, training loss: 4.2162: : 605it [10:04,  1.81s/it]\u001b[A\n",
      "batch 606, training loss: 4.0026: : 605it [10:05,  1.81s/it]\u001b[A\n",
      "batch 606, training loss: 4.0026: : 606it [10:05,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.9209: : 606it [10:07,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.9209: : 607it [10:07,  1.70s/it]\u001b[A\n",
      "batch 608, training loss: 4.278: : 607it [10:08,  1.70s/it] \u001b[A\n",
      "batch 608, training loss: 4.278: : 608it [10:08,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.218: : 608it [10:10,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.218: : 609it [10:10,  1.51s/it]\u001b[A\n",
      "batch 610, training loss: 3.7303: : 609it [10:11,  1.51s/it]\u001b[A\n",
      "batch 610, training loss: 3.7303: : 610it [10:11,  1.43s/it]\u001b[A\n",
      "batch 611, training loss: 3.7608: : 610it [10:12,  1.43s/it]\u001b[A\n",
      "batch 611, training loss: 3.7608: : 611it [10:12,  1.37s/it]\u001b[A\n",
      "batch 612, training loss: 2.8847: : 611it [10:13,  1.37s/it]\u001b[A\n",
      "batch 612, training loss: 2.8847: : 612it [10:13,  1.28s/it]\u001b[A\n",
      "batch 613, training loss: 3.9069: : 612it [10:14,  1.28s/it]\u001b[A\n",
      "batch 613, training loss: 3.9069: : 613it [10:14,  1.25s/it]\u001b[A\n",
      "batch 613, training loss: 3.9069: : 616it [10:15,  1.00it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 4.2111: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 4.2111: : 1it [00:00,  2.40it/s]\u001b[A\n",
      "batch 1, dev loss: 4.4369: : 1it [00:00,  2.40it/s]\u001b[A\n",
      "batch 1, dev loss: 4.4369: : 2it [00:00,  3.37it/s]\u001b[A\n",
      "batch 2, dev loss: 4.0066: : 2it [00:00,  3.37it/s]\u001b[A\n",
      "batch 2, dev loss: 4.0066: : 3it [00:00,  4.02it/s]\u001b[A\n",
      "batch 3, dev loss: 4.1255: : 3it [00:00,  4.02it/s]\u001b[A\n",
      "batch 3, dev loss: 4.1255: : 4it [00:00,  4.57it/s]\u001b[A\n",
      "batch 4, dev loss: 4.1481: : 4it [00:01,  4.57it/s]\u001b[A\n",
      "batch 4, dev loss: 4.1481: : 5it [00:01,  5.03it/s]\u001b[A\n",
      "batch 5, dev loss: 4.1305: : 5it [00:01,  5.03it/s]\u001b[A\n",
      "batch 5, dev loss: 4.1305: : 6it [00:01,  5.08it/s]\u001b[A\n",
      "batch 6, dev loss: 4.2826: : 6it [00:01,  5.08it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6, dev loss: 4.2826: : 7it [00:01,  5.18it/s]\u001b[A\n",
      "batch 7, dev loss: 4.0407: : 7it [00:01,  5.18it/s]\u001b[A\n",
      "batch 7, dev loss: 4.0407: : 8it [00:01,  5.42it/s]\u001b[A\n",
      "batch 8, dev loss: 4.2594: : 8it [00:01,  5.42it/s]\u001b[A\n",
      "batch 8, dev loss: 4.2594: : 9it [00:01,  5.16it/s]\u001b[A\n",
      "batch 9, dev loss: 4.116: : 9it [00:02,  5.16it/s] \u001b[A\n",
      "batch 9, dev loss: 4.116: : 10it [00:02,  4.98it/s]\u001b[A\n",
      "batch 10, dev loss: 4.1861: : 10it [00:02,  4.98it/s]\u001b[A\n",
      "batch 10, dev loss: 4.1861: : 11it [00:02,  5.20it/s]\u001b[A\n",
      "batch 11, dev loss: 4.2876: : 11it [00:02,  5.20it/s]\u001b[A\n",
      "batch 11, dev loss: 4.2876: : 12it [00:02,  4.84it/s]\u001b[A\n",
      "batch 12, dev loss: 4.1252: : 12it [00:02,  4.84it/s]\u001b[A\n",
      "batch 12, dev loss: 4.1252: : 13it [00:02,  4.43it/s]\u001b[A\n",
      "batch 13, dev loss: 4.2751: : 13it [00:03,  4.43it/s]\u001b[A\n",
      "batch 13, dev loss: 4.2751: : 14it [00:03,  4.43it/s]\u001b[A\n",
      "batch 14, dev loss: 4.3486: : 14it [00:03,  4.43it/s]\u001b[A\n",
      "batch 14, dev loss: 4.3486: : 15it [00:03,  4.29it/s]\u001b[A\n",
      "batch 15, dev loss: 4.2171: : 15it [00:03,  4.29it/s]\u001b[A\n",
      "batch 15, dev loss: 4.2171: : 16it [00:03,  4.64it/s]\u001b[A\n",
      "batch 16, dev loss: 4.5154: : 16it [00:03,  4.64it/s]\u001b[A\n",
      "batch 16, dev loss: 4.5154: : 17it [00:03,  4.40it/s]\u001b[A\n",
      "batch 17, dev loss: 4.2626: : 17it [00:04,  4.40it/s]\u001b[A\n",
      "batch 17, dev loss: 4.2626: : 18it [00:04,  4.03it/s]\u001b[A\n",
      "batch 18, dev loss: 4.1813: : 18it [00:04,  4.03it/s]\u001b[A\n",
      "batch 18, dev loss: 4.1813: : 19it [00:04,  3.97it/s]\u001b[A\n",
      "batch 19, dev loss: 4.3892: : 19it [00:04,  3.97it/s]\u001b[A\n",
      "batch 19, dev loss: 4.3892: : 20it [00:04,  4.02it/s]\u001b[A\n",
      "batch 20, dev loss: 4.1366: : 20it [00:04,  4.02it/s]\u001b[A\n",
      "batch 20, dev loss: 4.1366: : 21it [00:04,  4.03it/s]\u001b[A\n",
      "batch 21, dev loss: 4.0461: : 21it [00:05,  4.03it/s]\u001b[A\n",
      "batch 21, dev loss: 4.0461: : 22it [00:05,  3.98it/s]\u001b[A\n",
      "batch 22, dev loss: 4.2435: : 22it [00:05,  3.98it/s]\u001b[A\n",
      "batch 22, dev loss: 4.2435: : 23it [00:05,  4.00it/s]\u001b[A\n",
      "batch 23, dev loss: 4.2682: : 23it [00:05,  4.00it/s]\u001b[A\n",
      "batch 23, dev loss: 4.2682: : 24it [00:05,  4.51it/s]\u001b[A\n",
      "batch 24, dev loss: 4.2068: : 24it [00:05,  4.51it/s]\u001b[A\n",
      "batch 24, dev loss: 4.2068: : 25it [00:05,  3.97it/s]\u001b[A\n",
      "batch 25, dev loss: 4.2288: : 25it [00:06,  3.97it/s]\u001b[A\n",
      "batch 25, dev loss: 4.2288: : 26it [00:06,  3.81it/s]\u001b[A\n",
      "batch 26, dev loss: 4.1713: : 26it [00:06,  3.81it/s]\u001b[A\n",
      "batch 26, dev loss: 4.1713: : 27it [00:06,  3.53it/s]\u001b[A\n",
      "batch 27, dev loss: 4.0976: : 27it [00:06,  3.53it/s]\u001b[A\n",
      "batch 27, dev loss: 4.0976: : 28it [00:06,  3.52it/s]\u001b[A\n",
      "batch 28, dev loss: 4.2836: : 28it [00:06,  3.52it/s]\u001b[A\n",
      "batch 28, dev loss: 4.2836: : 29it [00:06,  3.46it/s]\u001b[A\n",
      "batch 29, dev loss: 4.2258: : 29it [00:07,  3.46it/s]\u001b[A\n",
      "batch 29, dev loss: 4.2258: : 30it [00:07,  3.39it/s]\u001b[A\n",
      "batch 30, dev loss: 4.4611: : 30it [00:07,  3.39it/s]\u001b[A\n",
      "batch 30, dev loss: 4.4611: : 31it [00:07,  4.16it/s]\u001b[A\n",
      "batch 31, dev loss: 4.3246: : 31it [00:07,  4.16it/s]\u001b[A\n",
      "batch 31, dev loss: 4.3246: : 32it [00:07,  3.77it/s]\u001b[A\n",
      "batch 32, dev loss: 4.3243: : 32it [00:08,  3.77it/s]\u001b[A\n",
      "batch 32, dev loss: 4.3243: : 33it [00:08,  3.41it/s]\u001b[A\n",
      "batch 33, dev loss: 4.0994: : 33it [00:08,  3.41it/s]\u001b[A\n",
      "batch 33, dev loss: 4.0994: : 34it [00:08,  3.25it/s]\u001b[A\n",
      "batch 34, dev loss: 4.5474: : 34it [00:08,  3.25it/s]\u001b[A\n",
      "batch 34, dev loss: 4.5474: : 35it [00:08,  3.11it/s]\u001b[A\n",
      "batch 35, dev loss: 4.3195: : 35it [00:09,  3.11it/s]\u001b[A\n",
      "batch 35, dev loss: 4.3195: : 36it [00:09,  3.13it/s]\u001b[A\n",
      "batch 36, dev loss: 4.2803: : 36it [00:09,  3.13it/s]\u001b[A\n",
      "batch 36, dev loss: 4.2803: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 4.0746: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 4.0746: : 38it [00:09,  3.18it/s]\u001b[A\n",
      "batch 38, dev loss: 4.3058: : 38it [00:10,  3.18it/s]\u001b[A\n",
      "batch 38, dev loss: 4.3058: : 39it [00:10,  3.12it/s]\u001b[A\n",
      "batch 39, dev loss: 4.2969: : 39it [00:10,  3.12it/s]\u001b[A\n",
      "batch 39, dev loss: 4.2969: : 40it [00:10,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 4.3347: : 40it [00:10,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 4.3347: : 41it [00:10,  2.89it/s]\u001b[A\n",
      "batch 41, dev loss: 4.0994: : 41it [00:11,  2.89it/s]\u001b[A\n",
      "batch 41, dev loss: 4.0994: : 42it [00:11,  2.96it/s]\u001b[A\n",
      "batch 42, dev loss: 4.2659: : 42it [00:11,  2.96it/s]\u001b[A\n",
      "batch 42, dev loss: 4.2659: : 43it [00:11,  2.91it/s]\u001b[A\n",
      "batch 43, dev loss: 4.2815: : 43it [00:11,  2.91it/s]\u001b[A\n",
      "batch 43, dev loss: 4.2815: : 44it [00:11,  2.74it/s]\u001b[A\n",
      "batch 44, dev loss: 4.202: : 44it [00:12,  2.74it/s] \u001b[A\n",
      "batch 44, dev loss: 4.202: : 45it [00:12,  2.68it/s]\u001b[A\n",
      "batch 45, dev loss: 4.4981: : 45it [00:12,  2.68it/s]\u001b[A\n",
      "batch 45, dev loss: 4.4981: : 46it [00:12,  2.65it/s]\u001b[A\n",
      "batch 46, dev loss: 4.0204: : 46it [00:13,  2.65it/s]\u001b[A\n",
      "batch 46, dev loss: 4.0204: : 47it [00:13,  2.53it/s]\u001b[A\n",
      "batch 47, dev loss: 4.2063: : 47it [00:13,  2.53it/s]\u001b[A\n",
      "batch 47, dev loss: 4.2063: : 48it [00:13,  2.52it/s]\u001b[A\n",
      "batch 48, dev loss: 4.016: : 48it [00:13,  2.52it/s] \u001b[A\n",
      "batch 48, dev loss: 4.016: : 49it [00:13,  2.43it/s]\u001b[A\n",
      "batch 49, dev loss: 4.1793: : 49it [00:14,  2.43it/s]\u001b[A\n",
      "batch 49, dev loss: 4.1793: : 50it [00:14,  2.85it/s]\u001b[A\n",
      "batch 50, dev loss: 4.2005: : 50it [00:14,  2.85it/s]\u001b[A\n",
      "batch 50, dev loss: 4.2005: : 51it [00:14,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 4.1744: : 51it [00:14,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 4.1744: : 52it [00:14,  2.55it/s]\u001b[A\n",
      "batch 52, dev loss: 3.979: : 52it [00:15,  2.55it/s] \u001b[A\n",
      "batch 52, dev loss: 3.979: : 53it [00:15,  2.51it/s]\u001b[A\n",
      "batch 53, dev loss: 4.2161: : 53it [00:15,  2.51it/s]\u001b[A\n",
      "batch 53, dev loss: 4.2161: : 54it [00:15,  2.34it/s]\u001b[A\n",
      "batch 54, dev loss: 3.9997: : 54it [00:16,  2.34it/s]\u001b[A\n",
      "batch 54, dev loss: 3.9997: : 55it [00:16,  2.31it/s]\u001b[A\n",
      "batch 55, dev loss: 4.1344: : 55it [00:16,  2.31it/s]\u001b[A\n",
      "batch 55, dev loss: 4.1344: : 56it [00:16,  2.20it/s]\u001b[A\n",
      "batch 56, dev loss: 4.001: : 56it [00:17,  2.20it/s] \u001b[A\n",
      "batch 56, dev loss: 4.001: : 57it [00:17,  2.32it/s]\u001b[A\n",
      "batch 57, dev loss: 3.9308: : 57it [00:17,  2.32it/s]\u001b[A\n",
      "batch 57, dev loss: 3.9308: : 58it [00:17,  2.13it/s]\u001b[A\n",
      "batch 58, dev loss: 4.2527: : 58it [00:18,  2.13it/s]\u001b[A\n",
      "batch 58, dev loss: 4.2527: : 59it [00:18,  2.10it/s]\u001b[A\n",
      "batch 59, dev loss: 4.2376: : 59it [00:18,  2.10it/s]\u001b[A\n",
      "batch 59, dev loss: 4.2376: : 60it [00:18,  2.26it/s]\u001b[A\n",
      "batch 60, dev loss: 3.8502: : 60it [00:19,  2.26it/s]\u001b[A\n",
      "batch 60, dev loss: 3.8502: : 61it [00:19,  2.29it/s]\u001b[A\n",
      "batch 61, dev loss: 3.8989: : 61it [00:19,  2.29it/s]\u001b[A\n",
      "batch 61, dev loss: 3.8989: : 62it [00:19,  2.49it/s]\u001b[A\n",
      "batch 62, dev loss: 3.8259: : 62it [00:19,  2.49it/s]\u001b[A\n",
      "batch 62, dev loss: 3.8259: : 63it [00:19,  2.72it/s]\u001b[A\n",
      "batch 63, dev loss: 4.3589: : 63it [00:20,  2.72it/s]\u001b[A\n",
      "batch 63, dev loss: 4.3589: : 64it [00:20,  2.69it/s]\u001b[A\n",
      "batch 64, dev loss: 3.9417: : 64it [00:20,  2.69it/s]\u001b[A\n",
      "batch 64, dev loss: 3.9417: : 65it [00:20,  2.79it/s]\u001b[A\n",
      "batch 65, dev loss: 3.9921: : 65it [00:20,  2.79it/s]\u001b[A\n",
      "batch 65, dev loss: 3.9921: : 66it [00:20,  2.76it/s]\u001b[A\n",
      "batch 66, dev loss: 3.9914: : 66it [00:21,  2.76it/s]\u001b[A\n",
      "batch 66, dev loss: 3.9914: : 67it [00:21,  2.91it/s]\u001b[A\n",
      "batch 67, dev loss: 3.3073: : 67it [00:21,  2.91it/s]\u001b[A\n",
      "batch 67, dev loss: 3.3073: : 68it [00:21,  3.08it/s]\u001b[A\n",
      "batch 68, dev loss: 3.3153: : 68it [00:21,  3.08it/s]\u001b[A\n",
      "batch 68, dev loss: 3.3153: : 69it [00:21,  3.16it/s]\u001b[A\n",
      "batch 69, dev loss: 3.335: : 69it [00:21,  3.16it/s] \u001b[A\n",
      "batch 69, dev loss: 3.335: : 70it [00:21,  3.01it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0518: : 70it [00:22,  3.01it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0518: : 71it [00:22,  3.01it/s]\u001b[A\n",
      "batch 71, dev loss: 3.7969: : 71it [00:22,  3.01it/s]\u001b[A\n",
      "batch 71, dev loss: 3.7969: : 72it [00:22,  2.91it/s]\u001b[A\n",
      "batch 72, dev loss: 4.5569: : 72it [00:23,  2.91it/s]\u001b[A\n",
      "batch 72, dev loss: 4.5569: : 73it [00:23,  2.90it/s]\u001b[A\n",
      "batch 72, dev loss: 4.5569: : 76it [00:23,  3.28it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:00,  1.02it/s]\u001b[A\n",
      "2it [00:02,  1.24s/it]\u001b[A\n",
      "3it [00:03,  1.19s/it]\u001b[A\n",
      "4it [00:03,  1.11it/s]\u001b[A\n",
      "5it [00:05,  1.05s/it]\u001b[A\n",
      "6it [00:06,  1.21s/it]\u001b[A\n",
      "7it [00:08,  1.25s/it]\u001b[A\n",
      "8it [00:09,  1.24s/it]\u001b[A\n",
      "9it [00:10,  1.26s/it]\u001b[A\n",
      "10it [00:12,  1.35s/it]\u001b[A\n",
      "11it [00:13,  1.39s/it]\u001b[A\n",
      "12it [00:15,  1.45s/it]\u001b[A\n",
      "13it [00:17,  1.55s/it]\u001b[A\n",
      "14it [00:18,  1.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15it [00:19,  1.43s/it]\u001b[A\n",
      "16it [00:20,  1.31s/it]\u001b[A\n",
      "17it [00:22,  1.58s/it]\u001b[A\n",
      "18it [00:24,  1.72s/it]\u001b[A\n",
      "19it [00:26,  1.75s/it]\u001b[A\n",
      "20it [00:28,  1.85s/it]\u001b[A\n",
      "21it [00:30,  1.93s/it]\u001b[A\n",
      "22it [00:33,  1.97s/it]\u001b[A\n",
      "23it [00:35,  1.98s/it]\u001b[A\n",
      "24it [00:35,  1.54s/it]\u001b[A\n",
      "25it [00:37,  1.64s/it]\u001b[A\n",
      "26it [00:40,  1.91s/it]\u001b[A\n",
      "27it [00:41,  1.91s/it]\u001b[A\n",
      "28it [00:44,  1.97s/it]\u001b[A\n",
      "29it [00:45,  1.96s/it]\u001b[A\n",
      "30it [00:48,  2.05s/it]\u001b[A\n",
      "31it [00:50,  2.09s/it]\u001b[A\n",
      "32it [00:52,  2.21s/it]\u001b[A\n",
      "33it [00:55,  2.34s/it]\u001b[A\n",
      "34it [00:58,  2.52s/it]\u001b[A\n",
      "35it [01:00,  2.46s/it]\u001b[A\n",
      "36it [01:01,  1.89s/it]\u001b[A\n",
      "37it [01:04,  2.18s/it]\u001b[A\n",
      "38it [01:07,  2.51s/it]\u001b[A\n",
      "39it [01:10,  2.57s/it]\u001b[A\n",
      "40it [01:12,  2.64s/it]\u001b[A\n",
      "41it [01:13,  2.09s/it]\u001b[A\n",
      "42it [01:16,  2.27s/it]\u001b[A\n",
      "43it [01:16,  1.73s/it]\u001b[A\n",
      "44it [01:20,  2.23s/it]\u001b[A\n",
      "45it [01:23,  2.37s/it]\u001b[A\n",
      "46it [01:24,  2.10s/it]\u001b[A\n",
      "47it [01:25,  1.87s/it]\u001b[A\n",
      "48it [01:29,  2.39s/it]\u001b[A\n",
      "49it [01:29,  1.73s/it]\u001b[A\n",
      "50it [01:33,  2.38s/it]\u001b[A\n",
      "51it [01:37,  2.92s/it]\u001b[A\n",
      "52it [01:39,  2.63s/it]\u001b[A\n",
      "53it [01:43,  3.00s/it]\u001b[A\n",
      "54it [01:46,  3.11s/it]\u001b[A\n",
      "55it [01:51,  3.60s/it]\u001b[A\n",
      "56it [01:53,  3.14s/it]\u001b[A\n",
      "57it [01:57,  3.46s/it]\u001b[A\n",
      "58it [02:01,  3.42s/it]\u001b[A\n",
      "59it [02:03,  3.17s/it]\u001b[A\n",
      "60it [02:06,  2.97s/it]\u001b[A\n",
      "61it [02:07,  2.55s/it]\u001b[A\n",
      "62it [02:09,  2.21s/it]\u001b[A\n",
      "63it [02:10,  1.83s/it]\u001b[A\n",
      "64it [02:11,  1.51s/it]\u001b[A\n",
      "65it [02:11,  1.24s/it]\u001b[A\n",
      "66it [02:12,  1.01it/s]\u001b[A\n",
      "67it [02:12,  1.21it/s]\u001b[A\n",
      "68it [02:13,  1.36it/s]\u001b[A\n",
      "69it [02:13,  1.51it/s]\u001b[A\n",
      "70it [02:14,  1.92s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███▋                                  | 644/6740 [00:00<00:00, 6433.64it/s]\u001b[A\n",
      " 19%|███████                              | 1288/6740 [00:00<00:00, 6327.22it/s]\u001b[A\n",
      " 29%|██████████▌                          | 1921/6740 [00:00<00:00, 6192.04it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 2541/6740 [00:00<00:00, 6031.51it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 3145/6740 [00:00<00:00, 5933.16it/s]\u001b[A\n",
      " 55%|████████████████████▌                | 3739/6740 [00:00<00:00, 5885.84it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 4328/6740 [00:00<00:00, 5802.61it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 4909/6740 [00:00<00:00, 5759.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 5485/6740 [00:00<00:00, 5712.25it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 6064/6740 [00:01<00:00, 5733.90it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5860.99it/s]\u001b[A\n",
      "Epoch: 3, tfr: 1.0, loss(train/dev): 4.2/4.1499, ppl(dev/test): 63.4277/71.7868,\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 4.0702: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 4.0702: : 1it [00:01,  1.92s/it]\u001b[A\n",
      "batch 2, training loss: 4.0211: : 1it [00:02,  1.92s/it]\u001b[A\n",
      "batch 2, training loss: 4.0211: : 2it [00:02,  1.20s/it]\u001b[A\n",
      "batch 3, training loss: 4.1359: : 2it [00:03,  1.20s/it]\u001b[A\n",
      "batch 3, training loss: 4.1359: : 3it [00:03,  1.08it/s]\u001b[A\n",
      "batch 4, training loss: 4.0596: : 3it [00:03,  1.08it/s]\u001b[A\n",
      "batch 4, training loss: 4.0596: : 4it [00:03,  1.25it/s]\u001b[A\n",
      "batch 5, training loss: 3.8594: : 4it [00:04,  1.25it/s]\u001b[A\n",
      "batch 5, training loss: 3.8594: : 5it [00:04,  1.40it/s]\u001b[A\n",
      "batch 6, training loss: 4.1053: : 5it [00:05,  1.40it/s]\u001b[A\n",
      "batch 6, training loss: 4.1053: : 6it [00:05,  1.43it/s]\u001b[A\n",
      "batch 7, training loss: 4.1037: : 6it [00:05,  1.43it/s]\u001b[A\n",
      "batch 7, training loss: 4.1037: : 7it [00:05,  1.44it/s]\u001b[A\n",
      "batch 8, training loss: 4.1368: : 7it [00:06,  1.44it/s]\u001b[A\n",
      "batch 8, training loss: 4.1368: : 8it [00:06,  1.45it/s]\u001b[A\n",
      "batch 9, training loss: 3.9757: : 8it [00:07,  1.45it/s]\u001b[A\n",
      "batch 9, training loss: 3.9757: : 9it [00:07,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.9741: : 9it [00:07,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.9741: : 10it [00:07,  1.57it/s]\u001b[A\n",
      "batch 11, training loss: 4.0219: : 10it [00:08,  1.57it/s]\u001b[A\n",
      "batch 11, training loss: 4.0219: : 11it [00:08,  1.46it/s]\u001b[A\n",
      "batch 12, training loss: 4.0724: : 11it [00:08,  1.46it/s]\u001b[A\n",
      "batch 12, training loss: 4.0724: : 12it [00:08,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 3.9358: : 12it [00:09,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 3.9358: : 13it [00:09,  1.51it/s]\u001b[A\n",
      "batch 14, training loss: 4.2658: : 13it [00:10,  1.51it/s]\u001b[A\n",
      "batch 14, training loss: 4.2658: : 14it [00:10,  1.65it/s]\u001b[A\n",
      "batch 15, training loss: 4.0839: : 14it [00:10,  1.65it/s]\u001b[A\n",
      "batch 15, training loss: 4.0839: : 15it [00:10,  1.67it/s]\u001b[A\n",
      "batch 16, training loss: 4.0639: : 15it [00:11,  1.67it/s]\u001b[A\n",
      "batch 16, training loss: 4.0639: : 16it [00:11,  1.63it/s]\u001b[A\n",
      "batch 17, training loss: 4.2171: : 16it [00:12,  1.63it/s]\u001b[A\n",
      "batch 17, training loss: 4.2171: : 17it [00:12,  1.58it/s]\u001b[A\n",
      "batch 18, training loss: 4.0379: : 17it [00:12,  1.58it/s]\u001b[A\n",
      "batch 18, training loss: 4.0379: : 18it [00:12,  1.71it/s]\u001b[A\n",
      "batch 19, training loss: 3.8371: : 18it [00:13,  1.71it/s]\u001b[A\n",
      "batch 19, training loss: 3.8371: : 19it [00:13,  1.71it/s]\u001b[A\n",
      "batch 20, training loss: 3.9744: : 19it [00:13,  1.71it/s]\u001b[A\n",
      "batch 20, training loss: 3.9744: : 20it [00:13,  1.65it/s]\u001b[A\n",
      "batch 21, training loss: 4.1109: : 20it [00:14,  1.65it/s]\u001b[A\n",
      "batch 21, training loss: 4.1109: : 21it [00:14,  1.59it/s]\u001b[A\n",
      "batch 22, training loss: 3.8836: : 21it [00:14,  1.59it/s]\u001b[A\n",
      "batch 22, training loss: 3.8836: : 22it [00:14,  1.66it/s]\u001b[A\n",
      "batch 23, training loss: 4.0296: : 22it [00:15,  1.66it/s]\u001b[A\n",
      "batch 23, training loss: 4.0296: : 23it [00:15,  1.77it/s]\u001b[A\n",
      "batch 24, training loss: 3.9525: : 23it [00:15,  1.77it/s]\u001b[A\n",
      "batch 24, training loss: 3.9525: : 24it [00:15,  1.90it/s]\u001b[A\n",
      "batch 25, training loss: 4.0364: : 24it [00:16,  1.90it/s]\u001b[A\n",
      "batch 25, training loss: 4.0364: : 25it [00:16,  2.00it/s]\u001b[A\n",
      "batch 26, training loss: 3.8621: : 25it [00:16,  2.00it/s]\u001b[A\n",
      "batch 26, training loss: 3.8621: : 26it [00:16,  2.09it/s]\u001b[A\n",
      "batch 27, training loss: 4.0015: : 26it [00:17,  2.09it/s]\u001b[A\n",
      "batch 27, training loss: 4.0015: : 27it [00:17,  2.15it/s]\u001b[A\n",
      "batch 28, training loss: 3.8767: : 27it [00:17,  2.15it/s]\u001b[A\n",
      "batch 28, training loss: 3.8767: : 28it [00:17,  2.20it/s]\u001b[A\n",
      "batch 29, training loss: 4.0133: : 28it [00:18,  2.20it/s]\u001b[A\n",
      "batch 29, training loss: 4.0133: : 29it [00:18,  2.22it/s]\u001b[A\n",
      "batch 30, training loss: 4.1239: : 29it [00:18,  2.22it/s]\u001b[A\n",
      "batch 30, training loss: 4.1239: : 30it [00:18,  1.96it/s]\u001b[A\n",
      "batch 31, training loss: 3.878: : 30it [00:19,  1.96it/s] \u001b[A\n",
      "batch 31, training loss: 3.878: : 31it [00:19,  1.97it/s]\u001b[A\n",
      "batch 32, training loss: 4.0507: : 31it [00:19,  1.97it/s]\u001b[A\n",
      "batch 32, training loss: 4.0507: : 32it [00:19,  1.90it/s]\u001b[A\n",
      "batch 33, training loss: 3.9646: : 32it [00:20,  1.90it/s]\u001b[A\n",
      "batch 33, training loss: 3.9646: : 33it [00:20,  1.76it/s]\u001b[A\n",
      "batch 34, training loss: 3.9575: : 33it [00:21,  1.76it/s]\u001b[A\n",
      "batch 34, training loss: 3.9575: : 34it [00:21,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 4.0537: : 34it [00:21,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 4.0537: : 35it [00:21,  1.61it/s]\u001b[A\n",
      "batch 36, training loss: 4.1044: : 35it [00:22,  1.61it/s]\u001b[A\n",
      "batch 36, training loss: 4.1044: : 36it [00:22,  1.61it/s]\u001b[A\n",
      "batch 37, training loss: 3.9515: : 36it [00:23,  1.61it/s]\u001b[A\n",
      "batch 37, training loss: 3.9515: : 37it [00:23,  1.61it/s]\u001b[A\n",
      "batch 38, training loss: 3.7678: : 37it [00:23,  1.61it/s]\u001b[A\n",
      "batch 38, training loss: 3.7678: : 38it [00:23,  1.61it/s]\u001b[A\n",
      "batch 39, training loss: 3.8977: : 38it [00:24,  1.61it/s]\u001b[A\n",
      "batch 39, training loss: 3.8977: : 39it [00:24,  1.57it/s]\u001b[A\n",
      "batch 40, training loss: 4.0204: : 39it [00:25,  1.57it/s]\u001b[A\n",
      "batch 40, training loss: 4.0204: : 40it [00:25,  1.51it/s]\u001b[A\n",
      "batch 41, training loss: 4.1852: : 40it [00:25,  1.51it/s]\u001b[A\n",
      "batch 41, training loss: 4.1852: : 41it [00:25,  1.49it/s]\u001b[A\n",
      "batch 42, training loss: 4.0249: : 41it [00:26,  1.49it/s]\u001b[A\n",
      "batch 42, training loss: 4.0249: : 42it [00:26,  1.62it/s]\u001b[A\n",
      "batch 43, training loss: 3.7878: : 42it [00:26,  1.62it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 3.7878: : 43it [00:26,  1.66it/s]\u001b[A\n",
      "batch 44, training loss: 3.7948: : 43it [00:27,  1.66it/s]\u001b[A\n",
      "batch 44, training loss: 3.7948: : 44it [00:27,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 3.8007: : 44it [00:28,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 3.8007: : 45it [00:28,  1.57it/s]\u001b[A\n",
      "batch 46, training loss: 3.9504: : 45it [00:28,  1.57it/s]\u001b[A\n",
      "batch 46, training loss: 3.9504: : 46it [00:28,  1.69it/s]\u001b[A\n",
      "batch 47, training loss: 3.9636: : 46it [00:29,  1.69it/s]\u001b[A\n",
      "batch 47, training loss: 3.9636: : 47it [00:29,  1.71it/s]\u001b[A\n",
      "batch 48, training loss: 3.8974: : 47it [00:29,  1.71it/s]\u001b[A\n",
      "batch 48, training loss: 3.8974: : 48it [00:29,  1.65it/s]\u001b[A\n",
      "batch 49, training loss: 4.135: : 48it [00:30,  1.65it/s] \u001b[A\n",
      "batch 49, training loss: 4.135: : 49it [00:30,  1.58it/s]\u001b[A\n",
      "batch 50, training loss: 3.8706: : 49it [00:31,  1.58it/s]\u001b[A\n",
      "batch 50, training loss: 3.8706: : 50it [00:31,  1.54it/s]\u001b[A\n",
      "batch 51, training loss: 3.9691: : 50it [00:31,  1.54it/s]\u001b[A\n",
      "batch 51, training loss: 3.9691: : 51it [00:31,  1.57it/s]\u001b[A\n",
      "batch 52, training loss: 3.8562: : 51it [00:32,  1.57it/s]\u001b[A\n",
      "batch 52, training loss: 3.8562: : 52it [00:32,  1.61it/s]\u001b[A\n",
      "batch 53, training loss: 3.8502: : 52it [00:33,  1.61it/s]\u001b[A\n",
      "batch 53, training loss: 3.8502: : 53it [00:33,  1.61it/s]\u001b[A\n",
      "batch 54, training loss: 3.7812: : 53it [00:33,  1.61it/s]\u001b[A\n",
      "batch 54, training loss: 3.7812: : 54it [00:33,  1.55it/s]\u001b[A\n",
      "batch 55, training loss: 3.9161: : 54it [00:34,  1.55it/s]\u001b[A\n",
      "batch 55, training loss: 3.9161: : 55it [00:34,  1.51it/s]\u001b[A\n",
      "batch 56, training loss: 3.9565: : 55it [00:35,  1.51it/s]\u001b[A\n",
      "batch 56, training loss: 3.9565: : 56it [00:35,  1.53it/s]\u001b[A\n",
      "batch 57, training loss: 3.907: : 56it [00:35,  1.53it/s] \u001b[A\n",
      "batch 57, training loss: 3.907: : 57it [00:35,  1.57it/s]\u001b[A\n",
      "batch 58, training loss: 3.9998: : 57it [00:36,  1.57it/s]\u001b[A\n",
      "batch 58, training loss: 3.9998: : 58it [00:36,  1.65it/s]\u001b[A\n",
      "batch 59, training loss: 3.8252: : 58it [00:36,  1.65it/s]\u001b[A\n",
      "batch 59, training loss: 3.8252: : 59it [00:36,  1.61it/s]\u001b[A\n",
      "batch 60, training loss: 3.7805: : 59it [00:37,  1.61it/s]\u001b[A\n",
      "batch 60, training loss: 3.7805: : 60it [00:37,  1.59it/s]\u001b[A\n",
      "batch 61, training loss: 4.0412: : 60it [00:38,  1.59it/s]\u001b[A\n",
      "batch 61, training loss: 4.0412: : 61it [00:38,  1.62it/s]\u001b[A\n",
      "batch 62, training loss: 3.8687: : 61it [00:38,  1.62it/s]\u001b[A\n",
      "batch 62, training loss: 3.8687: : 62it [00:38,  1.63it/s]\u001b[A\n",
      "batch 63, training loss: 3.9591: : 62it [00:39,  1.63it/s]\u001b[A\n",
      "batch 63, training loss: 3.9591: : 63it [00:39,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.9085: : 63it [00:40,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.9085: : 64it [00:40,  1.55it/s]\u001b[A\n",
      "batch 65, training loss: 3.9774: : 64it [00:40,  1.55it/s]\u001b[A\n",
      "batch 65, training loss: 3.9774: : 65it [00:40,  1.49it/s]\u001b[A\n",
      "batch 66, training loss: 3.9659: : 65it [00:41,  1.49it/s]\u001b[A\n",
      "batch 66, training loss: 3.9659: : 66it [00:41,  1.48it/s]\u001b[A\n",
      "batch 67, training loss: 3.8808: : 66it [00:41,  1.48it/s]\u001b[A\n",
      "batch 67, training loss: 3.8808: : 67it [00:41,  1.60it/s]\u001b[A\n",
      "batch 68, training loss: 3.97: : 67it [00:42,  1.60it/s]  \u001b[A\n",
      "batch 68, training loss: 3.97: : 68it [00:42,  1.64it/s]\u001b[A\n",
      "batch 69, training loss: 3.8702: : 68it [00:43,  1.64it/s]\u001b[A\n",
      "batch 69, training loss: 3.8702: : 69it [00:43,  1.60it/s]\u001b[A\n",
      "batch 70, training loss: 4.0396: : 69it [00:43,  1.60it/s]\u001b[A\n",
      "batch 70, training loss: 4.0396: : 70it [00:43,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.8758: : 70it [00:44,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.8758: : 71it [00:44,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 3.8818: : 71it [00:45,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 3.8818: : 72it [00:45,  1.58it/s]\u001b[A\n",
      "batch 73, training loss: 3.9255: : 72it [00:45,  1.58it/s]\u001b[A\n",
      "batch 73, training loss: 3.9255: : 73it [00:45,  1.58it/s]\u001b[A\n",
      "batch 74, training loss: 3.7884: : 73it [00:46,  1.58it/s]\u001b[A\n",
      "batch 74, training loss: 3.7884: : 74it [00:46,  1.57it/s]\u001b[A\n",
      "batch 75, training loss: 4.0285: : 74it [00:47,  1.57it/s]\u001b[A\n",
      "batch 75, training loss: 4.0285: : 75it [00:47,  1.56it/s]\u001b[A\n",
      "batch 76, training loss: 3.8345: : 75it [00:47,  1.56it/s]\u001b[A\n",
      "batch 76, training loss: 3.8345: : 76it [00:47,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 3.9766: : 76it [00:48,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 3.9766: : 77it [00:48,  1.51it/s]\u001b[A\n",
      "batch 78, training loss: 3.966: : 77it [00:49,  1.51it/s] \u001b[A\n",
      "batch 78, training loss: 3.966: : 78it [00:49,  1.59it/s]\u001b[A\n",
      "batch 79, training loss: 3.9888: : 78it [00:49,  1.59it/s]\u001b[A\n",
      "batch 79, training loss: 3.9888: : 79it [00:49,  1.63it/s]\u001b[A\n",
      "batch 80, training loss: 3.8802: : 79it [00:50,  1.63it/s]\u001b[A\n",
      "batch 80, training loss: 3.8802: : 80it [00:50,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 3.9236: : 80it [00:50,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 3.9236: : 81it [00:50,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 4.0463: : 81it [00:51,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 4.0463: : 82it [00:51,  1.53it/s]\u001b[A\n",
      "batch 83, training loss: 3.9047: : 82it [00:52,  1.53it/s]\u001b[A\n",
      "batch 83, training loss: 3.9047: : 83it [00:52,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 4.0261: : 83it [00:52,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 4.0261: : 84it [00:52,  1.57it/s]\u001b[A\n",
      "batch 85, training loss: 4.1092: : 84it [00:53,  1.57it/s]\u001b[A\n",
      "batch 85, training loss: 4.1092: : 85it [00:53,  1.59it/s]\u001b[A\n",
      "batch 86, training loss: 3.9722: : 85it [00:54,  1.59it/s]\u001b[A\n",
      "batch 86, training loss: 3.9722: : 86it [00:54,  1.56it/s]\u001b[A\n",
      "batch 87, training loss: 4.0653: : 86it [00:54,  1.56it/s]\u001b[A\n",
      "batch 87, training loss: 4.0653: : 87it [00:54,  1.55it/s]\u001b[A\n",
      "batch 88, training loss: 4.2082: : 87it [00:55,  1.55it/s]\u001b[A\n",
      "batch 88, training loss: 4.2082: : 88it [00:55,  1.46it/s]\u001b[A\n",
      "batch 89, training loss: 4.2985: : 88it [00:56,  1.46it/s]\u001b[A\n",
      "batch 89, training loss: 4.2985: : 89it [00:56,  1.39it/s]\u001b[A\n",
      "batch 90, training loss: 4.1992: : 89it [00:57,  1.39it/s]\u001b[A\n",
      "batch 90, training loss: 4.1992: : 90it [00:57,  1.35it/s]\u001b[A\n",
      "batch 91, training loss: 4.3159: : 90it [00:57,  1.35it/s]\u001b[A\n",
      "batch 91, training loss: 4.3159: : 91it [00:57,  1.34it/s]\u001b[A\n",
      "batch 92, training loss: 4.1947: : 91it [00:58,  1.34it/s]\u001b[A\n",
      "batch 92, training loss: 4.1947: : 92it [00:58,  1.31it/s]\u001b[A\n",
      "batch 93, training loss: 4.1546: : 92it [00:59,  1.31it/s]\u001b[A\n",
      "batch 93, training loss: 4.1546: : 93it [00:59,  1.30it/s]\u001b[A\n",
      "batch 94, training loss: 4.209: : 93it [01:00,  1.30it/s] \u001b[A\n",
      "batch 94, training loss: 4.209: : 94it [01:00,  1.31it/s]\u001b[A\n",
      "batch 95, training loss: 4.2214: : 94it [01:01,  1.31it/s]\u001b[A\n",
      "batch 95, training loss: 4.2214: : 95it [01:01,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 4.1912: : 95it [01:01,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 4.1912: : 96it [01:01,  1.29it/s]\u001b[A\n",
      "batch 97, training loss: 4.2737: : 96it [01:02,  1.29it/s]\u001b[A\n",
      "batch 97, training loss: 4.2737: : 97it [01:02,  1.29it/s]\u001b[A\n",
      "batch 98, training loss: 4.2639: : 97it [01:03,  1.29it/s]\u001b[A\n",
      "batch 98, training loss: 4.2639: : 98it [01:03,  1.29it/s]\u001b[A\n",
      "batch 99, training loss: 4.0784: : 98it [01:04,  1.29it/s]\u001b[A\n",
      "batch 99, training loss: 4.0784: : 99it [01:04,  1.29it/s]\u001b[A\n",
      "batch 100, training loss: 3.8781: : 99it [01:04,  1.29it/s]\u001b[A\n",
      "batch 100, training loss: 3.8781: : 100it [01:04,  1.28it/s]\u001b[A\n",
      "batch 101, training loss: 4.1045: : 100it [01:05,  1.28it/s]\u001b[A\n",
      "batch 101, training loss: 4.1045: : 101it [01:05,  1.28it/s]\u001b[A\n",
      "batch 102, training loss: 4.0288: : 101it [01:06,  1.28it/s]\u001b[A\n",
      "batch 102, training loss: 4.0288: : 102it [01:06,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 4.0051: : 102it [01:07,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 4.0051: : 103it [01:07,  1.28it/s]\u001b[A\n",
      "batch 104, training loss: 3.8591: : 103it [01:08,  1.28it/s]\u001b[A\n",
      "batch 104, training loss: 3.8591: : 104it [01:08,  1.29it/s]\u001b[A\n",
      "batch 105, training loss: 4.0411: : 104it [01:08,  1.29it/s]\u001b[A\n",
      "batch 105, training loss: 4.0411: : 105it [01:08,  1.29it/s]\u001b[A\n",
      "batch 106, training loss: 4.1302: : 105it [01:09,  1.29it/s]\u001b[A\n",
      "batch 106, training loss: 4.1302: : 106it [01:09,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.8175: : 106it [01:10,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.8175: : 107it [01:10,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 4.0985: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 4.0985: : 108it [01:11,  1.28it/s]\u001b[A\n",
      "batch 109, training loss: 4.1586: : 108it [01:11,  1.28it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 109, training loss: 4.1586: : 109it [01:11,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 4.298: : 109it [01:12,  1.29it/s] \u001b[A\n",
      "batch 110, training loss: 4.298: : 110it [01:12,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.9748: : 110it [01:13,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.9748: : 111it [01:13,  1.30it/s]\u001b[A\n",
      "batch 112, training loss: 4.0349: : 111it [01:14,  1.30it/s]\u001b[A\n",
      "batch 112, training loss: 4.0349: : 112it [01:14,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 4.1129: : 112it [01:14,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 4.1129: : 113it [01:14,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 4.1219: : 113it [01:15,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 4.1219: : 114it [01:15,  1.29it/s]\u001b[A\n",
      "batch 115, training loss: 4.0096: : 114it [01:16,  1.29it/s]\u001b[A\n",
      "batch 115, training loss: 4.0096: : 115it [01:16,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.9872: : 115it [01:17,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.9872: : 116it [01:17,  1.28it/s]\u001b[A\n",
      "batch 117, training loss: 4.0491: : 116it [01:18,  1.28it/s]\u001b[A\n",
      "batch 117, training loss: 4.0491: : 117it [01:18,  1.28it/s]\u001b[A\n",
      "batch 118, training loss: 4.1339: : 117it [01:18,  1.28it/s]\u001b[A\n",
      "batch 118, training loss: 4.1339: : 118it [01:18,  1.29it/s]\u001b[A\n",
      "batch 119, training loss: 4.028: : 118it [01:19,  1.29it/s] \u001b[A\n",
      "batch 119, training loss: 4.028: : 119it [01:19,  1.28it/s]\u001b[A\n",
      "batch 120, training loss: 3.982: : 119it [01:20,  1.28it/s]\u001b[A\n",
      "batch 120, training loss: 3.982: : 120it [01:20,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 4.2102: : 120it [01:21,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 4.2102: : 121it [01:21,  1.28it/s]\u001b[A\n",
      "batch 122, training loss: 3.8905: : 121it [01:22,  1.28it/s]\u001b[A\n",
      "batch 122, training loss: 3.8905: : 122it [01:22,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 4.0612: : 122it [01:22,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 4.0612: : 123it [01:22,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 4.0584: : 123it [01:23,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 4.0584: : 124it [01:23,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 4.0676: : 124it [01:24,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 4.0676: : 125it [01:24,  1.28it/s]\u001b[A\n",
      "batch 126, training loss: 4.1353: : 125it [01:25,  1.28it/s]\u001b[A\n",
      "batch 126, training loss: 4.1353: : 126it [01:25,  1.30it/s]\u001b[A\n",
      "batch 127, training loss: 3.8555: : 126it [01:25,  1.30it/s]\u001b[A\n",
      "batch 127, training loss: 3.8555: : 127it [01:25,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 4.0502: : 127it [01:26,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 4.0502: : 128it [01:26,  1.29it/s]\u001b[A\n",
      "batch 129, training loss: 4.0306: : 128it [01:27,  1.29it/s]\u001b[A\n",
      "batch 129, training loss: 4.0306: : 129it [01:27,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 4.0298: : 129it [01:28,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 4.0298: : 130it [01:28,  1.28it/s]\u001b[A\n",
      "batch 131, training loss: 4.1978: : 130it [01:28,  1.28it/s]\u001b[A\n",
      "batch 131, training loss: 4.1978: : 131it [01:28,  1.28it/s]\u001b[A\n",
      "batch 132, training loss: 3.9833: : 131it [01:29,  1.28it/s]\u001b[A\n",
      "batch 132, training loss: 3.9833: : 132it [01:29,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.8904: : 132it [01:30,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.8904: : 133it [01:30,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.9983: : 133it [01:31,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.9983: : 134it [01:31,  1.29it/s]\u001b[A\n",
      "batch 135, training loss: 3.9546: : 134it [01:32,  1.29it/s]\u001b[A\n",
      "batch 135, training loss: 3.9546: : 135it [01:32,  1.30it/s]\u001b[A\n",
      "batch 136, training loss: 3.9346: : 135it [01:32,  1.30it/s]\u001b[A\n",
      "batch 136, training loss: 3.9346: : 136it [01:32,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 4.0669: : 136it [01:33,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 4.0669: : 137it [01:33,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 4.0986: : 137it [01:34,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 4.0986: : 138it [01:34,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.8739: : 138it [01:35,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.8739: : 139it [01:35,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 3.9601: : 139it [01:35,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 3.9601: : 140it [01:35,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.9758: : 140it [01:36,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.9758: : 141it [01:36,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.9984: : 141it [01:37,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.9984: : 142it [01:37,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.8527: : 142it [01:38,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.8527: : 143it [01:38,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.8888: : 143it [01:38,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.8888: : 144it [01:38,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 3.9802: : 144it [01:39,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 3.9802: : 145it [01:39,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 3.99: : 145it [01:40,  1.30it/s]  \u001b[A\n",
      "batch 146, training loss: 3.99: : 146it [01:40,  1.29it/s]\u001b[A\n",
      "batch 147, training loss: 3.8714: : 146it [01:41,  1.29it/s]\u001b[A\n",
      "batch 147, training loss: 3.8714: : 147it [01:41,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.9658: : 147it [01:42,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.9658: : 148it [01:42,  1.29it/s]\u001b[A\n",
      "batch 149, training loss: 4.0632: : 148it [01:42,  1.29it/s]\u001b[A\n",
      "batch 149, training loss: 4.0632: : 149it [01:42,  1.29it/s]\u001b[A\n",
      "batch 150, training loss: 4.0521: : 149it [01:43,  1.29it/s]\u001b[A\n",
      "batch 150, training loss: 4.0521: : 150it [01:43,  1.28it/s]\u001b[A\n",
      "batch 151, training loss: 4.0003: : 150it [01:44,  1.28it/s]\u001b[A\n",
      "batch 151, training loss: 4.0003: : 151it [01:44,  1.29it/s]\u001b[A\n",
      "batch 152, training loss: 3.9161: : 151it [01:45,  1.29it/s]\u001b[A\n",
      "batch 152, training loss: 3.9161: : 152it [01:45,  1.28it/s]\u001b[A\n",
      "batch 153, training loss: 3.9599: : 152it [01:45,  1.28it/s]\u001b[A\n",
      "batch 153, training loss: 3.9599: : 153it [01:45,  1.35it/s]\u001b[A\n",
      "batch 154, training loss: 4.0549: : 153it [01:46,  1.35it/s]\u001b[A\n",
      "batch 154, training loss: 4.0549: : 154it [01:46,  1.43it/s]\u001b[A\n",
      "batch 155, training loss: 4.1697: : 154it [01:47,  1.43it/s]\u001b[A\n",
      "batch 155, training loss: 4.1697: : 155it [01:47,  1.41it/s]\u001b[A\n",
      "batch 156, training loss: 3.8101: : 155it [01:47,  1.41it/s]\u001b[A\n",
      "batch 156, training loss: 3.8101: : 156it [01:47,  1.38it/s]\u001b[A\n",
      "batch 157, training loss: 4.0098: : 156it [01:48,  1.38it/s]\u001b[A\n",
      "batch 157, training loss: 4.0098: : 157it [01:48,  1.37it/s]\u001b[A\n",
      "batch 158, training loss: 3.9822: : 157it [01:49,  1.37it/s]\u001b[A\n",
      "batch 158, training loss: 3.9822: : 158it [01:49,  1.34it/s]\u001b[A\n",
      "batch 159, training loss: 3.8807: : 158it [01:50,  1.34it/s]\u001b[A\n",
      "batch 159, training loss: 3.8807: : 159it [01:50,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 4.0581: : 159it [01:50,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 4.0581: : 160it [01:50,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.8982: : 160it [01:51,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.8982: : 161it [01:51,  1.32it/s]\u001b[A\n",
      "batch 162, training loss: 3.91: : 161it [01:52,  1.32it/s]  \u001b[A\n",
      "batch 162, training loss: 3.91: : 162it [01:52,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 4.1499: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 4.1499: : 163it [01:53,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.9427: : 163it [01:54,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.9427: : 164it [01:54,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.9339: : 164it [01:54,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.9339: : 165it [01:54,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.9224: : 165it [01:55,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.9224: : 166it [01:55,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 4.1086: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 4.1086: : 167it [01:56,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 4.0146: : 167it [01:57,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 4.0146: : 168it [01:57,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 4.1027: : 168it [01:57,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 4.1027: : 169it [01:57,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.9374: : 169it [01:58,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.9374: : 170it [01:58,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 3.4614: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 3.4614: : 171it [01:59,  1.56it/s]\u001b[A\n",
      "batch 172, training loss: 4.3509: : 171it [01:59,  1.56it/s]\u001b[A\n",
      "batch 172, training loss: 4.3509: : 172it [01:59,  1.45it/s]\u001b[A\n",
      "batch 173, training loss: 4.1854: : 172it [02:00,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 173, training loss: 4.1854: : 173it [02:00,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 4.3715: : 173it [02:01,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 4.3715: : 174it [02:01,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 4.2789: : 174it [02:02,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 4.2789: : 175it [02:02,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 4.2116: : 175it [02:03,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 4.2116: : 176it [02:03,  1.31it/s]\u001b[A\n",
      "batch 177, training loss: 4.2256: : 176it [02:03,  1.31it/s]\u001b[A\n",
      "batch 177, training loss: 4.2256: : 177it [02:03,  1.29it/s]\u001b[A\n",
      "batch 178, training loss: 4.2407: : 177it [02:04,  1.29it/s]\u001b[A\n",
      "batch 178, training loss: 4.2407: : 178it [02:04,  1.31it/s]\u001b[A\n",
      "batch 179, training loss: 4.2707: : 178it [02:05,  1.31it/s]\u001b[A\n",
      "batch 179, training loss: 4.2707: : 179it [02:05,  1.29it/s]\u001b[A\n",
      "batch 180, training loss: 4.3325: : 179it [02:06,  1.29it/s]\u001b[A\n",
      "batch 180, training loss: 4.3325: : 180it [02:06,  1.26it/s]\u001b[A\n",
      "batch 181, training loss: 4.3455: : 180it [02:06,  1.26it/s]\u001b[A\n",
      "batch 181, training loss: 4.3455: : 181it [02:06,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.2441: : 181it [02:07,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.2441: : 182it [02:07,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 4.3414: : 182it [02:08,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 4.3414: : 183it [02:08,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 4.1275: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 4.1275: : 184it [02:09,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 4.1461: : 184it [02:10,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 4.1461: : 185it [02:10,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 4.3063: : 185it [02:11,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 4.3063: : 186it [02:11,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 4.2786: : 186it [02:11,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 4.2786: : 187it [02:11,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 4.3977: : 187it [02:12,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 4.3977: : 188it [02:12,  1.24it/s]\u001b[A\n",
      "batch 189, training loss: 4.4211: : 188it [02:13,  1.24it/s]\u001b[A\n",
      "batch 189, training loss: 4.4211: : 189it [02:13,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 4.0311: : 189it [02:14,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 4.0311: : 190it [02:14,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 4.0636: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 4.0636: : 191it [02:15,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.1945: : 191it [02:15,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.1945: : 192it [02:15,  1.22it/s]\u001b[A\n",
      "batch 193, training loss: 4.0317: : 192it [02:16,  1.22it/s]\u001b[A\n",
      "batch 193, training loss: 4.0317: : 193it [02:16,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 4.1185: : 193it [02:17,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 4.1185: : 194it [02:17,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.0638: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 4.0638: : 195it [02:18,  1.23it/s]\u001b[A\n",
      "batch 196, training loss: 4.04: : 195it [02:19,  1.23it/s]  \u001b[A\n",
      "batch 196, training loss: 4.04: : 196it [02:19,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.1727: : 196it [02:19,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.1727: : 197it [02:19,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.0925: : 197it [02:20,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 4.0925: : 198it [02:20,  1.23it/s]\u001b[A\n",
      "batch 199, training loss: 4.0449: : 198it [02:21,  1.23it/s]\u001b[A\n",
      "batch 199, training loss: 4.0449: : 199it [02:21,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.2288: : 199it [02:22,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.2288: : 200it [02:22,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 4.0061: : 200it [02:23,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 4.0061: : 201it [02:23,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.855: : 201it [02:23,  1.23it/s] \u001b[A\n",
      "batch 202, training loss: 3.855: : 202it [02:23,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 4.0834: : 202it [02:24,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 4.0834: : 203it [02:24,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 4.133: : 203it [02:25,  1.24it/s] \u001b[A\n",
      "batch 204, training loss: 4.133: : 204it [02:25,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 4.0721: : 204it [02:26,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 4.0721: : 205it [02:26,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 4.2258: : 205it [02:27,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 4.2258: : 206it [02:27,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 4.0155: : 206it [02:28,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 4.0155: : 207it [02:28,  1.23it/s]\u001b[A\n",
      "batch 208, training loss: 4.0894: : 207it [02:28,  1.23it/s]\u001b[A\n",
      "batch 208, training loss: 4.0894: : 208it [02:28,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 4.0117: : 208it [02:29,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 4.0117: : 209it [02:29,  1.23it/s]\u001b[A\n",
      "batch 210, training loss: 4.0226: : 209it [02:30,  1.23it/s]\u001b[A\n",
      "batch 210, training loss: 4.0226: : 210it [02:30,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 4.0123: : 210it [02:31,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 4.0123: : 211it [02:31,  1.24it/s]\u001b[A\n",
      "batch 212, training loss: 4.0223: : 211it [02:32,  1.24it/s]\u001b[A\n",
      "batch 212, training loss: 4.0223: : 212it [02:32,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 4.2576: : 212it [02:32,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 4.2576: : 213it [02:32,  1.23it/s]\u001b[A\n",
      "batch 214, training loss: 4.0952: : 213it [02:33,  1.23it/s]\u001b[A\n",
      "batch 214, training loss: 4.0952: : 214it [02:33,  1.24it/s]\u001b[A\n",
      "batch 215, training loss: 3.9234: : 214it [02:34,  1.24it/s]\u001b[A\n",
      "batch 215, training loss: 3.9234: : 215it [02:34,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 4.0681: : 215it [02:35,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 4.0681: : 216it [02:35,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 4.173: : 216it [02:36,  1.23it/s] \u001b[A\n",
      "batch 217, training loss: 4.173: : 217it [02:36,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 4.0904: : 217it [02:36,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 4.0904: : 218it [02:36,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 4.2996: : 218it [02:37,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 4.2996: : 219it [02:37,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 4.1955: : 219it [02:38,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 4.1955: : 220it [02:38,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 4.0908: : 220it [02:39,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 4.0908: : 221it [02:39,  1.24it/s]\u001b[A\n",
      "batch 222, training loss: 4.1162: : 221it [02:40,  1.24it/s]\u001b[A\n",
      "batch 222, training loss: 4.1162: : 222it [02:40,  1.23it/s]\u001b[A\n",
      "batch 223, training loss: 4.1088: : 222it [02:40,  1.23it/s]\u001b[A\n",
      "batch 223, training loss: 4.1088: : 223it [02:40,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 4.0447: : 223it [02:41,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 4.0447: : 224it [02:41,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 4.1601: : 224it [02:42,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 4.1601: : 225it [02:42,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 4.1865: : 225it [02:43,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 4.1865: : 226it [02:43,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 4.0267: : 226it [02:44,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 4.0267: : 227it [02:44,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 4.0868: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 4.0868: : 228it [02:45,  1.23it/s]\u001b[A\n",
      "batch 229, training loss: 4.0688: : 228it [02:45,  1.23it/s]\u001b[A\n",
      "batch 229, training loss: 4.0688: : 229it [02:45,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.1976: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.1976: : 230it [02:46,  1.24it/s]\u001b[A\n",
      "batch 231, training loss: 4.2674: : 230it [02:47,  1.24it/s]\u001b[A\n",
      "batch 231, training loss: 4.2674: : 231it [02:47,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 4.0339: : 231it [02:48,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 4.0339: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 4.144: : 232it [02:49,  1.24it/s] \u001b[A\n",
      "batch 233, training loss: 4.144: : 233it [02:49,  1.24it/s]\u001b[A\n",
      "batch 234, training loss: 4.2383: : 233it [02:49,  1.24it/s]\u001b[A\n",
      "batch 234, training loss: 4.2383: : 234it [02:49,  1.21it/s]\u001b[A\n",
      "batch 235, training loss: 4.0961: : 234it [02:50,  1.21it/s]\u001b[A\n",
      "batch 235, training loss: 4.0961: : 235it [02:50,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 4.1321: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 4.1321: : 236it [02:51,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.9731: : 236it [02:52,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 237, training loss: 3.9731: : 237it [02:52,  1.22it/s]\u001b[A\n",
      "batch 238, training loss: 4.1229: : 237it [02:53,  1.22it/s]\u001b[A\n",
      "batch 238, training loss: 4.1229: : 238it [02:53,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 4.054: : 238it [02:53,  1.24it/s] \u001b[A\n",
      "batch 239, training loss: 4.054: : 239it [02:53,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 4.0829: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 4.0829: : 240it [02:54,  1.23it/s]\u001b[A\n",
      "batch 241, training loss: 4.1107: : 240it [02:55,  1.23it/s]\u001b[A\n",
      "batch 241, training loss: 4.1107: : 241it [02:55,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 3.9355: : 241it [02:56,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 3.9355: : 242it [02:56,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 4.1003: : 242it [02:57,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 4.1003: : 243it [02:57,  1.23it/s]\u001b[A\n",
      "batch 244, training loss: 4.0339: : 243it [02:58,  1.23it/s]\u001b[A\n",
      "batch 244, training loss: 4.0339: : 244it [02:58,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.1409: : 244it [02:58,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.1409: : 245it [02:58,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 4.069: : 245it [02:59,  1.24it/s] \u001b[A\n",
      "batch 246, training loss: 4.069: : 246it [02:59,  1.26it/s]\u001b[A\n",
      "batch 247, training loss: 4.0645: : 246it [03:00,  1.26it/s]\u001b[A\n",
      "batch 247, training loss: 4.0645: : 247it [03:00,  1.27it/s]\u001b[A\n",
      "batch 248, training loss: 3.9889: : 247it [03:01,  1.27it/s]\u001b[A\n",
      "batch 248, training loss: 3.9889: : 248it [03:01,  1.25it/s]\u001b[A\n",
      "batch 249, training loss: 3.9691: : 248it [03:01,  1.25it/s]\u001b[A\n",
      "batch 249, training loss: 3.9691: : 249it [03:01,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 4.1665: : 249it [03:02,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 4.1665: : 250it [03:02,  1.27it/s]\u001b[A\n",
      "batch 251, training loss: 4.1703: : 250it [03:03,  1.27it/s]\u001b[A\n",
      "batch 251, training loss: 4.1703: : 251it [03:03,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 3.8463: : 251it [03:03,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 3.8463: : 252it [03:03,  1.52it/s]\u001b[A\n",
      "batch 253, training loss: 4.1598: : 252it [03:04,  1.52it/s]\u001b[A\n",
      "batch 253, training loss: 4.1598: : 253it [03:04,  1.36it/s]\u001b[A\n",
      "batch 254, training loss: 4.3445: : 253it [03:05,  1.36it/s]\u001b[A\n",
      "batch 254, training loss: 4.3445: : 254it [03:05,  1.33it/s]\u001b[A\n",
      "batch 255, training loss: 4.1532: : 254it [03:06,  1.33it/s]\u001b[A\n",
      "batch 255, training loss: 4.1532: : 255it [03:06,  1.31it/s]\u001b[A\n",
      "batch 256, training loss: 4.1182: : 255it [03:07,  1.31it/s]\u001b[A\n",
      "batch 256, training loss: 4.1182: : 256it [03:07,  1.25it/s]\u001b[A\n",
      "batch 257, training loss: 4.2586: : 256it [03:08,  1.25it/s]\u001b[A\n",
      "batch 257, training loss: 4.2586: : 257it [03:08,  1.24it/s]\u001b[A\n",
      "batch 258, training loss: 4.3096: : 257it [03:08,  1.24it/s]\u001b[A\n",
      "batch 258, training loss: 4.3096: : 258it [03:08,  1.23it/s]\u001b[A\n",
      "batch 259, training loss: 4.1908: : 258it [03:09,  1.23it/s]\u001b[A\n",
      "batch 259, training loss: 4.1908: : 259it [03:09,  1.20it/s]\u001b[A\n",
      "batch 260, training loss: 4.2217: : 259it [03:10,  1.20it/s]\u001b[A\n",
      "batch 260, training loss: 4.2217: : 260it [03:10,  1.16it/s]\u001b[A\n",
      "batch 261, training loss: 4.301: : 260it [03:11,  1.16it/s] \u001b[A\n",
      "batch 261, training loss: 4.301: : 261it [03:11,  1.14it/s]\u001b[A\n",
      "batch 262, training loss: 4.1697: : 261it [03:12,  1.14it/s]\u001b[A\n",
      "batch 262, training loss: 4.1697: : 262it [03:12,  1.12it/s]\u001b[A\n",
      "batch 263, training loss: 4.156: : 262it [03:13,  1.12it/s] \u001b[A\n",
      "batch 263, training loss: 4.156: : 263it [03:13,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 4.2395: : 263it [03:14,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 4.2395: : 264it [03:14,  1.12it/s]\u001b[A\n",
      "batch 265, training loss: 4.058: : 264it [03:15,  1.12it/s] \u001b[A\n",
      "batch 265, training loss: 4.058: : 265it [03:15,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 4.2524: : 265it [03:16,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 4.2524: : 266it [03:16,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 4.1694: : 266it [03:16,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 4.1694: : 267it [03:16,  1.17it/s]\u001b[A\n",
      "batch 268, training loss: 4.0978: : 267it [03:17,  1.17it/s]\u001b[A\n",
      "batch 268, training loss: 4.0978: : 268it [03:17,  1.33it/s]\u001b[A\n",
      "batch 269, training loss: 4.074: : 268it [03:18,  1.33it/s] \u001b[A\n",
      "batch 269, training loss: 4.074: : 269it [03:18,  1.25it/s]\u001b[A\n",
      "batch 270, training loss: 4.1054: : 269it [03:19,  1.25it/s]\u001b[A\n",
      "batch 270, training loss: 4.1054: : 270it [03:19,  1.25it/s]\u001b[A\n",
      "batch 271, training loss: 4.1171: : 270it [03:19,  1.25it/s]\u001b[A\n",
      "batch 271, training loss: 4.1171: : 271it [03:19,  1.23it/s]\u001b[A\n",
      "batch 272, training loss: 4.1034: : 271it [03:20,  1.23it/s]\u001b[A\n",
      "batch 272, training loss: 4.1034: : 272it [03:20,  1.19it/s]\u001b[A\n",
      "batch 273, training loss: 4.2062: : 272it [03:21,  1.19it/s]\u001b[A\n",
      "batch 273, training loss: 4.2062: : 273it [03:21,  1.17it/s]\u001b[A\n",
      "batch 274, training loss: 4.1579: : 273it [03:22,  1.17it/s]\u001b[A\n",
      "batch 274, training loss: 4.1579: : 274it [03:22,  1.14it/s]\u001b[A\n",
      "batch 275, training loss: 4.0404: : 274it [03:23,  1.14it/s]\u001b[A\n",
      "batch 275, training loss: 4.0404: : 275it [03:23,  1.14it/s]\u001b[A\n",
      "batch 276, training loss: 3.9527: : 275it [03:24,  1.14it/s]\u001b[A\n",
      "batch 276, training loss: 3.9527: : 276it [03:24,  1.13it/s]\u001b[A\n",
      "batch 277, training loss: 4.0876: : 276it [03:25,  1.13it/s]\u001b[A\n",
      "batch 277, training loss: 4.0876: : 277it [03:25,  1.12it/s]\u001b[A\n",
      "batch 278, training loss: 4.0309: : 277it [03:26,  1.12it/s]\u001b[A\n",
      "batch 278, training loss: 4.0309: : 278it [03:26,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 4.0613: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 4.0613: : 279it [03:27,  1.12it/s]\u001b[A\n",
      "batch 280, training loss: 3.9073: : 279it [03:28,  1.12it/s]\u001b[A\n",
      "batch 280, training loss: 3.9073: : 280it [03:28,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.9949: : 280it [03:29,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.9949: : 281it [03:29,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.9508: : 281it [03:29,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.9508: : 282it [03:29,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 4.0197: : 282it [03:30,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 4.0197: : 283it [03:30,  1.12it/s]\u001b[A\n",
      "batch 284, training loss: 3.9657: : 283it [03:31,  1.12it/s]\u001b[A\n",
      "batch 284, training loss: 3.9657: : 284it [03:31,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 4.0321: : 284it [03:32,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 4.0321: : 285it [03:32,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 4.182: : 285it [03:33,  1.11it/s] \u001b[A\n",
      "batch 286, training loss: 4.182: : 286it [03:33,  1.09it/s]\u001b[A\n",
      "batch 287, training loss: 3.9187: : 286it [03:34,  1.09it/s]\u001b[A\n",
      "batch 287, training loss: 3.9187: : 287it [03:34,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 3.9421: : 287it [03:35,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 3.9421: : 288it [03:35,  1.09it/s]\u001b[A\n",
      "batch 289, training loss: 4.1107: : 288it [03:36,  1.09it/s]\u001b[A\n",
      "batch 289, training loss: 4.1107: : 289it [03:36,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 3.8563: : 289it [03:37,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 3.8563: : 290it [03:37,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.1478: : 290it [03:38,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.1478: : 291it [03:38,  1.09it/s]\u001b[A\n",
      "batch 292, training loss: 3.9326: : 291it [03:39,  1.09it/s]\u001b[A\n",
      "batch 292, training loss: 3.9326: : 292it [03:39,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 4.0291: : 292it [03:39,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 4.0291: : 293it [03:39,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 4.1203: : 293it [03:40,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 4.1203: : 294it [03:40,  1.09it/s]\u001b[A\n",
      "batch 295, training loss: 4.0477: : 294it [03:41,  1.09it/s]\u001b[A\n",
      "batch 295, training loss: 4.0477: : 295it [03:41,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.9188: : 295it [03:42,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.9188: : 296it [03:42,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 4.0607: : 296it [03:43,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 4.0607: : 297it [03:43,  1.11it/s]\u001b[A\n",
      "batch 298, training loss: 3.9629: : 297it [03:44,  1.11it/s]\u001b[A\n",
      "batch 298, training loss: 3.9629: : 298it [03:44,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 4.0685: : 298it [03:45,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 4.0685: : 299it [03:45,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 4.1248: : 299it [03:46,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 4.1248: : 300it [03:46,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 4.0895: : 300it [03:47,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 301, training loss: 4.0895: : 301it [03:47,  1.12it/s]\u001b[A\n",
      "batch 302, training loss: 4.0364: : 301it [03:48,  1.12it/s]\u001b[A\n",
      "batch 302, training loss: 4.0364: : 302it [03:48,  1.12it/s]\u001b[A\n",
      "batch 303, training loss: 4.0248: : 302it [03:49,  1.12it/s]\u001b[A\n",
      "batch 303, training loss: 4.0248: : 303it [03:49,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 4.0693: : 303it [03:49,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 4.0693: : 304it [03:49,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 4.0068: : 304it [03:50,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 4.0068: : 305it [03:50,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.0657: : 305it [03:51,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 4.0657: : 306it [03:51,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 4.1889: : 306it [03:52,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 4.1889: : 307it [03:52,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.8094: : 307it [03:53,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.8094: : 308it [03:53,  1.11it/s]\u001b[A\n",
      "batch 309, training loss: 4.1677: : 308it [03:54,  1.11it/s]\u001b[A\n",
      "batch 309, training loss: 4.1677: : 309it [03:54,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 3.9502: : 309it [03:55,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 3.9502: : 310it [03:55,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 4.0516: : 310it [03:56,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 4.0516: : 311it [03:56,  1.12it/s]\u001b[A\n",
      "batch 312, training loss: 3.8702: : 311it [03:57,  1.12it/s]\u001b[A\n",
      "batch 312, training loss: 3.8702: : 312it [03:57,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.9967: : 312it [03:58,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.9967: : 313it [03:58,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 4.0379: : 313it [03:59,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 4.0379: : 314it [03:59,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 4.1079: : 314it [03:59,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 4.1079: : 315it [03:59,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 4.2539: : 315it [04:00,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 4.2539: : 316it [04:00,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.9268: : 316it [04:01,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.9268: : 317it [04:01,  1.13it/s]\u001b[A\n",
      "batch 318, training loss: 4.2103: : 317it [04:02,  1.13it/s]\u001b[A\n",
      "batch 318, training loss: 4.2103: : 318it [04:02,  1.08it/s]\u001b[A\n",
      "batch 319, training loss: 4.2929: : 318it [04:03,  1.08it/s]\u001b[A\n",
      "batch 319, training loss: 4.2929: : 319it [04:03,  1.08it/s]\u001b[A\n",
      "batch 320, training loss: 4.2928: : 319it [04:04,  1.08it/s]\u001b[A\n",
      "batch 320, training loss: 4.2928: : 320it [04:04,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.3248: : 320it [04:05,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.3248: : 321it [04:05,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.4025: : 321it [04:06,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.4025: : 322it [04:06,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 4.1474: : 322it [04:07,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 4.1474: : 323it [04:07,  1.01it/s]\u001b[A\n",
      "batch 324, training loss: 4.2249: : 323it [04:08,  1.01it/s]\u001b[A\n",
      "batch 324, training loss: 4.2249: : 324it [04:08,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 4.2863: : 324it [04:09,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 4.2863: : 325it [04:09,  1.00it/s]\u001b[A\n",
      "batch 326, training loss: 4.2261: : 325it [04:10,  1.00it/s]\u001b[A\n",
      "batch 326, training loss: 4.2261: : 326it [04:10,  1.01s/it]\u001b[A\n",
      "batch 327, training loss: 4.3353: : 326it [04:11,  1.01s/it]\u001b[A\n",
      "batch 327, training loss: 4.3353: : 327it [04:11,  1.02it/s]\u001b[A\n",
      "batch 328, training loss: 4.1723: : 327it [04:12,  1.02it/s]\u001b[A\n",
      "batch 328, training loss: 4.1723: : 328it [04:12,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 4.0959: : 328it [04:13,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 4.0959: : 329it [04:13,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 4.1734: : 329it [04:14,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 4.1734: : 330it [04:14,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 4.233: : 330it [04:15,  1.01s/it] \u001b[A\n",
      "batch 331, training loss: 4.233: : 331it [04:15,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 4.1019: : 331it [04:16,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 4.1019: : 332it [04:16,  1.03s/it]\u001b[A\n",
      "batch 333, training loss: 4.0607: : 332it [04:17,  1.03s/it]\u001b[A\n",
      "batch 333, training loss: 4.0607: : 333it [04:17,  1.01it/s]\u001b[A\n",
      "batch 334, training loss: 4.1919: : 333it [04:18,  1.01it/s]\u001b[A\n",
      "batch 334, training loss: 4.1919: : 334it [04:18,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.2987: : 334it [04:19,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.2987: : 335it [04:19,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 4.204: : 335it [04:20,  1.01s/it] \u001b[A\n",
      "batch 336, training loss: 4.204: : 336it [04:20,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.232: : 336it [04:21,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.232: : 337it [04:21,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 4.1207: : 337it [04:22,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 4.1207: : 338it [04:22,  1.00s/it]\u001b[A\n",
      "batch 339, training loss: 4.1693: : 338it [04:23,  1.00s/it]\u001b[A\n",
      "batch 339, training loss: 4.1693: : 339it [04:23,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 4.4009: : 339it [04:24,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 4.4009: : 340it [04:24,  1.01s/it]\u001b[A\n",
      "batch 341, training loss: 4.066: : 340it [04:25,  1.01s/it] \u001b[A\n",
      "batch 341, training loss: 4.066: : 341it [04:25,  1.01it/s]\u001b[A\n",
      "batch 342, training loss: 4.1528: : 341it [04:26,  1.01it/s]\u001b[A\n",
      "batch 342, training loss: 4.1528: : 342it [04:26,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 4.1826: : 342it [04:27,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 4.1826: : 343it [04:27,  1.01s/it]\u001b[A\n",
      "batch 344, training loss: 4.2411: : 343it [04:28,  1.01s/it]\u001b[A\n",
      "batch 344, training loss: 4.2411: : 344it [04:28,  1.02s/it]\u001b[A\n",
      "batch 345, training loss: 4.172: : 344it [04:29,  1.02s/it] \u001b[A\n",
      "batch 345, training loss: 4.172: : 345it [04:29,  1.00s/it]\u001b[A\n",
      "batch 346, training loss: 4.1647: : 345it [04:30,  1.00s/it]\u001b[A\n",
      "batch 346, training loss: 4.1647: : 346it [04:30,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 4.0715: : 346it [04:31,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 4.0715: : 347it [04:31,  1.00s/it]\u001b[A\n",
      "batch 348, training loss: 4.1443: : 347it [04:32,  1.00s/it]\u001b[A\n",
      "batch 348, training loss: 4.1443: : 348it [04:32,  1.01s/it]\u001b[A\n",
      "batch 349, training loss: 4.2693: : 348it [04:33,  1.01s/it]\u001b[A\n",
      "batch 349, training loss: 4.2693: : 349it [04:33,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.1515: : 349it [04:34,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.1515: : 350it [04:34,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 4.0292: : 350it [04:35,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 4.0292: : 351it [04:35,  1.02s/it]\u001b[A\n",
      "batch 352, training loss: 4.1148: : 351it [04:36,  1.02s/it]\u001b[A\n",
      "batch 352, training loss: 4.1148: : 352it [04:36,  1.01s/it]\u001b[A\n",
      "batch 353, training loss: 4.1509: : 352it [04:37,  1.01s/it]\u001b[A\n",
      "batch 353, training loss: 4.1509: : 353it [04:37,  1.02s/it]\u001b[A\n",
      "batch 354, training loss: 4.2303: : 353it [04:38,  1.02s/it]\u001b[A\n",
      "batch 354, training loss: 4.2303: : 354it [04:38,  1.01s/it]\u001b[A\n",
      "batch 355, training loss: 4.1695: : 354it [04:39,  1.01s/it]\u001b[A\n",
      "batch 355, training loss: 4.1695: : 355it [04:39,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 4.0116: : 355it [04:40,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 4.0116: : 356it [04:40,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.9915: : 356it [04:41,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.9915: : 357it [04:41,  1.00it/s]\u001b[A\n",
      "batch 358, training loss: 4.2011: : 357it [04:42,  1.00it/s]\u001b[A\n",
      "batch 358, training loss: 4.2011: : 358it [04:42,  1.01s/it]\u001b[A\n",
      "batch 359, training loss: 4.023: : 358it [04:43,  1.01s/it] \u001b[A\n",
      "batch 359, training loss: 4.023: : 359it [04:43,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 4.1614: : 359it [04:44,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 4.1614: : 360it [04:44,  1.03it/s]\u001b[A\n",
      "batch 361, training loss: 4.1272: : 360it [04:45,  1.03it/s]\u001b[A\n",
      "batch 361, training loss: 4.1272: : 361it [04:45,  1.02it/s]\u001b[A\n",
      "batch 362, training loss: 4.2522: : 361it [04:46,  1.02it/s]\u001b[A\n",
      "batch 362, training loss: 4.2522: : 362it [04:46,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 4.1171: : 362it [04:47,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 4.1171: : 363it [04:47,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 4.0035: : 363it [04:48,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 4.0035: : 364it [04:48,  1.07it/s]\u001b[A\n",
      "batch 365, training loss: 4.0147: : 364it [04:49,  1.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365, training loss: 4.0147: : 365it [04:49,  1.05it/s]\u001b[A\n",
      "batch 366, training loss: 4.1559: : 365it [04:50,  1.05it/s]\u001b[A\n",
      "batch 366, training loss: 4.1559: : 366it [04:50,  1.05it/s]\u001b[A\n",
      "batch 367, training loss: 4.0486: : 366it [04:51,  1.05it/s]\u001b[A\n",
      "batch 367, training loss: 4.0486: : 367it [04:51,  1.03it/s]\u001b[A\n",
      "batch 368, training loss: 4.1651: : 367it [04:52,  1.03it/s]\u001b[A\n",
      "batch 368, training loss: 4.1651: : 368it [04:52,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 4.1855: : 368it [04:53,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 4.1855: : 369it [04:53,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 4.0438: : 369it [04:54,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 4.0438: : 370it [04:54,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 4.1263: : 370it [04:55,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 4.1263: : 371it [04:55,  1.00it/s]\u001b[A\n",
      "batch 372, training loss: 4.0643: : 371it [04:56,  1.00it/s]\u001b[A\n",
      "batch 372, training loss: 4.0643: : 372it [04:56,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 4.0825: : 372it [04:57,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 4.0825: : 373it [04:57,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 4.0933: : 373it [04:58,  1.02it/s]\u001b[A\n",
      "batch 374, training loss: 4.0933: : 374it [04:58,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 4.0059: : 374it [04:59,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 4.0059: : 375it [04:59,  1.15it/s]\u001b[A\n",
      "batch 376, training loss: 4.1837: : 375it [05:00,  1.15it/s]\u001b[A\n",
      "batch 376, training loss: 4.1837: : 376it [05:00,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 4.3068: : 376it [05:01,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 4.3068: : 377it [05:01,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 4.1834: : 377it [05:02,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 4.1834: : 378it [05:02,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 4.138: : 378it [05:03,  1.03s/it] \u001b[A\n",
      "batch 379, training loss: 4.138: : 379it [05:03,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 4.2014: : 379it [05:04,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 4.2014: : 380it [05:04,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.2329: : 380it [05:05,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.2329: : 381it [05:05,  1.10s/it]\u001b[A\n",
      "batch 382, training loss: 4.0939: : 381it [05:06,  1.10s/it]\u001b[A\n",
      "batch 382, training loss: 4.0939: : 382it [05:06,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 4.1494: : 382it [05:07,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 4.1494: : 383it [05:07,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.3049: : 383it [05:09,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.3049: : 384it [05:09,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 4.1798: : 384it [05:10,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 4.1798: : 385it [05:10,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 4.0967: : 385it [05:11,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 4.0967: : 386it [05:11,  1.11s/it]\u001b[A\n",
      "batch 387, training loss: 4.2096: : 386it [05:12,  1.11s/it]\u001b[A\n",
      "batch 387, training loss: 4.2096: : 387it [05:12,  1.09s/it]\u001b[A\n",
      "batch 388, training loss: 3.9294: : 387it [05:13,  1.09s/it]\u001b[A\n",
      "batch 388, training loss: 3.9294: : 388it [05:13,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 4.1269: : 388it [05:14,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 4.1269: : 389it [05:14,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 4.2407: : 389it [05:15,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 4.2407: : 390it [05:15,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 4.2311: : 390it [05:16,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 4.2311: : 391it [05:16,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.1902: : 391it [05:17,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 4.1902: : 392it [05:17,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 4.0251: : 392it [05:19,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 4.0251: : 393it [05:19,  1.10s/it]\u001b[A\n",
      "batch 394, training loss: 3.959: : 393it [05:20,  1.10s/it] \u001b[A\n",
      "batch 394, training loss: 3.959: : 394it [05:20,  1.09s/it]\u001b[A\n",
      "batch 395, training loss: 4.0391: : 394it [05:21,  1.09s/it]\u001b[A\n",
      "batch 395, training loss: 4.0391: : 395it [05:21,  1.09s/it]\u001b[A\n",
      "batch 396, training loss: 4.249: : 395it [05:22,  1.09s/it] \u001b[A\n",
      "batch 396, training loss: 4.249: : 396it [05:22,  1.09s/it]\u001b[A\n",
      "batch 397, training loss: 4.0018: : 396it [05:23,  1.09s/it]\u001b[A\n",
      "batch 397, training loss: 4.0018: : 397it [05:23,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 4.1118: : 397it [05:24,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 4.1118: : 398it [05:24,  1.05s/it]\u001b[A\n",
      "batch 399, training loss: 4.224: : 398it [05:25,  1.05s/it] \u001b[A\n",
      "batch 399, training loss: 4.224: : 399it [05:25,  1.07s/it]\u001b[A\n",
      "batch 400, training loss: 3.9964: : 399it [05:26,  1.07s/it]\u001b[A\n",
      "batch 400, training loss: 3.9964: : 400it [05:26,  1.08s/it]\u001b[A\n",
      "batch 401, training loss: 3.964: : 400it [05:27,  1.08s/it] \u001b[A\n",
      "batch 401, training loss: 3.964: : 401it [05:27,  1.10s/it]\u001b[A\n",
      "batch 402, training loss: 4.0425: : 401it [05:28,  1.10s/it]\u001b[A\n",
      "batch 402, training loss: 4.0425: : 402it [05:28,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 4.2234: : 402it [05:29,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 4.2234: : 403it [05:29,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 3.8407: : 403it [05:30,  1.10s/it]\u001b[A\n",
      "batch 404, training loss: 3.8407: : 404it [05:30,  1.07s/it]\u001b[A\n",
      "batch 405, training loss: 4.1037: : 404it [05:32,  1.07s/it]\u001b[A\n",
      "batch 405, training loss: 4.1037: : 405it [05:32,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 3.989: : 405it [05:33,  1.10s/it] \u001b[A\n",
      "batch 406, training loss: 3.989: : 406it [05:33,  1.07s/it]\u001b[A\n",
      "batch 407, training loss: 4.0758: : 406it [05:34,  1.07s/it]\u001b[A\n",
      "batch 407, training loss: 4.0758: : 407it [05:34,  1.08s/it]\u001b[A\n",
      "batch 408, training loss: 3.9175: : 407it [05:35,  1.08s/it]\u001b[A\n",
      "batch 408, training loss: 3.9175: : 408it [05:35,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 4.1572: : 408it [05:36,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 4.1572: : 409it [05:36,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.9723: : 409it [05:37,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.9723: : 410it [05:37,  1.09s/it]\u001b[A\n",
      "batch 411, training loss: 4.0683: : 410it [05:38,  1.09s/it]\u001b[A\n",
      "batch 411, training loss: 4.0683: : 411it [05:38,  1.10s/it]\u001b[A\n",
      "batch 412, training loss: 4.0691: : 411it [05:39,  1.10s/it]\u001b[A\n",
      "batch 412, training loss: 4.0691: : 412it [05:39,  1.09s/it]\u001b[A\n",
      "batch 413, training loss: 4.0288: : 412it [05:40,  1.09s/it]\u001b[A\n",
      "batch 413, training loss: 4.0288: : 413it [05:40,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.9186: : 413it [05:41,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.9186: : 414it [05:41,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.8187: : 414it [05:42,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.8187: : 415it [05:42,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 3.8738: : 415it [05:44,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 3.8738: : 416it [05:44,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 4.1615: : 416it [05:45,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 4.1615: : 417it [05:45,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 4.0499: : 417it [05:46,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 4.0499: : 418it [05:46,  1.11s/it]\u001b[A\n",
      "batch 419, training loss: 3.8576: : 418it [05:47,  1.11s/it]\u001b[A\n",
      "batch 419, training loss: 3.8576: : 419it [05:47,  1.11s/it]\u001b[A\n",
      "batch 420, training loss: 3.9611: : 419it [05:48,  1.11s/it]\u001b[A\n",
      "batch 420, training loss: 3.9611: : 420it [05:48,  1.11s/it]\u001b[A\n",
      "batch 421, training loss: 4.0536: : 420it [05:49,  1.11s/it]\u001b[A\n",
      "batch 421, training loss: 4.0536: : 421it [05:49,  1.08s/it]\u001b[A\n",
      "batch 422, training loss: 4.0763: : 421it [05:50,  1.08s/it]\u001b[A\n",
      "batch 422, training loss: 4.0763: : 422it [05:50,  1.09s/it]\u001b[A\n",
      "batch 423, training loss: 3.9044: : 422it [05:51,  1.09s/it]\u001b[A\n",
      "batch 423, training loss: 3.9044: : 423it [05:51,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 4.1338: : 423it [05:52,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 4.1338: : 424it [05:52,  1.10s/it]\u001b[A\n",
      "batch 425, training loss: 4.2224: : 424it [05:54,  1.10s/it]\u001b[A\n",
      "batch 425, training loss: 4.2224: : 425it [05:54,  1.13s/it]\u001b[A\n",
      "batch 426, training loss: 4.1676: : 425it [05:55,  1.13s/it]\u001b[A\n",
      "batch 426, training loss: 4.1676: : 426it [05:55,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 4.1883: : 426it [05:56,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 4.1883: : 427it [05:56,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.3532: : 427it [05:57,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.3532: : 428it [05:57,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 4.2051: : 428it [05:58,  1.18s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 429, training loss: 4.2051: : 429it [05:58,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 4.2055: : 429it [06:00,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 4.2055: : 430it [06:00,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.2389: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.2389: : 431it [06:01,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 4.104: : 431it [06:02,  1.20s/it] \u001b[A\n",
      "batch 432, training loss: 4.104: : 432it [06:02,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 4.1387: : 432it [06:03,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 4.1387: : 433it [06:03,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.1276: : 433it [06:05,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.1276: : 434it [06:05,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 4.1136: : 434it [06:06,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 4.1136: : 435it [06:06,  1.21s/it]\u001b[A\n",
      "batch 436, training loss: 4.134: : 435it [06:07,  1.21s/it] \u001b[A\n",
      "batch 436, training loss: 4.134: : 436it [06:07,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 4.0814: : 436it [06:08,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 4.0814: : 437it [06:08,  1.20s/it]\u001b[A\n",
      "batch 438, training loss: 4.1486: : 437it [06:09,  1.20s/it]\u001b[A\n",
      "batch 438, training loss: 4.1486: : 438it [06:09,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 4.2047: : 438it [06:11,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 4.2047: : 439it [06:11,  1.20s/it]\u001b[A\n",
      "batch 440, training loss: 4.0519: : 439it [06:12,  1.20s/it]\u001b[A\n",
      "batch 440, training loss: 4.0519: : 440it [06:12,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 4.3019: : 440it [06:13,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 4.3019: : 441it [06:13,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.9947: : 441it [06:14,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.9947: : 442it [06:14,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 4.0639: : 442it [06:15,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 4.0639: : 443it [06:15,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 4.0635: : 443it [06:17,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 4.0635: : 444it [06:17,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 4.1018: : 444it [06:18,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 4.1018: : 445it [06:18,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 4.1928: : 445it [06:19,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 4.1928: : 446it [06:19,  1.21s/it]\u001b[A\n",
      "batch 447, training loss: 4.088: : 446it [06:20,  1.21s/it] \u001b[A\n",
      "batch 447, training loss: 4.088: : 447it [06:20,  1.21s/it]\u001b[A\n",
      "batch 448, training loss: 3.9737: : 447it [06:21,  1.21s/it]\u001b[A\n",
      "batch 448, training loss: 3.9737: : 448it [06:21,  1.10s/it]\u001b[A\n",
      "batch 449, training loss: 4.1249: : 448it [06:22,  1.10s/it]\u001b[A\n",
      "batch 449, training loss: 4.1249: : 449it [06:22,  1.12s/it]\u001b[A\n",
      "batch 450, training loss: 4.1232: : 449it [06:23,  1.12s/it]\u001b[A\n",
      "batch 450, training loss: 4.1232: : 450it [06:23,  1.15s/it]\u001b[A\n",
      "batch 451, training loss: 4.1477: : 450it [06:25,  1.15s/it]\u001b[A\n",
      "batch 451, training loss: 4.1477: : 451it [06:25,  1.16s/it]\u001b[A\n",
      "batch 452, training loss: 4.1501: : 451it [06:26,  1.16s/it]\u001b[A\n",
      "batch 452, training loss: 4.1501: : 452it [06:26,  1.17s/it]\u001b[A\n",
      "batch 453, training loss: 4.2004: : 452it [06:27,  1.17s/it]\u001b[A\n",
      "batch 453, training loss: 4.2004: : 453it [06:27,  1.19s/it]\u001b[A\n",
      "batch 454, training loss: 4.0554: : 453it [06:28,  1.19s/it]\u001b[A\n",
      "batch 454, training loss: 4.0554: : 454it [06:28,  1.20s/it]\u001b[A\n",
      "batch 455, training loss: 3.9414: : 454it [06:30,  1.20s/it]\u001b[A\n",
      "batch 455, training loss: 3.9414: : 455it [06:30,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.0031: : 455it [06:31,  1.21s/it]\u001b[A\n",
      "batch 456, training loss: 4.0031: : 456it [06:31,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 4.1399: : 456it [06:32,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 4.1399: : 457it [06:32,  1.20s/it]\u001b[A\n",
      "batch 458, training loss: 3.9947: : 457it [06:33,  1.20s/it]\u001b[A\n",
      "batch 458, training loss: 3.9947: : 458it [06:33,  1.21s/it]\u001b[A\n",
      "batch 459, training loss: 4.0265: : 458it [06:34,  1.21s/it]\u001b[A\n",
      "batch 459, training loss: 4.0265: : 459it [06:34,  1.20s/it]\u001b[A\n",
      "batch 460, training loss: 4.0469: : 459it [06:36,  1.20s/it]\u001b[A\n",
      "batch 460, training loss: 4.0469: : 460it [06:36,  1.21s/it]\u001b[A\n",
      "batch 461, training loss: 4.1392: : 460it [06:37,  1.21s/it]\u001b[A\n",
      "batch 461, training loss: 4.1392: : 461it [06:37,  1.22s/it]\u001b[A\n",
      "batch 462, training loss: 4.0705: : 461it [06:38,  1.22s/it]\u001b[A\n",
      "batch 462, training loss: 4.0705: : 462it [06:38,  1.21s/it]\u001b[A\n",
      "batch 463, training loss: 3.9208: : 462it [06:39,  1.21s/it]\u001b[A\n",
      "batch 463, training loss: 3.9208: : 463it [06:39,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 4.0531: : 463it [06:40,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 4.0531: : 464it [06:40,  1.22s/it]\u001b[A\n",
      "batch 465, training loss: 4.0715: : 464it [06:41,  1.22s/it]\u001b[A\n",
      "batch 465, training loss: 4.0715: : 465it [06:41,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.9922: : 465it [06:43,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.9922: : 466it [06:43,  1.17s/it]\u001b[A\n",
      "batch 467, training loss: 3.951: : 466it [06:44,  1.17s/it] \u001b[A\n",
      "batch 467, training loss: 3.951: : 467it [06:44,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 4.0892: : 467it [06:45,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 4.0892: : 468it [06:45,  1.21s/it]\u001b[A\n",
      "batch 469, training loss: 4.0223: : 468it [06:46,  1.21s/it]\u001b[A\n",
      "batch 469, training loss: 4.0223: : 469it [06:46,  1.24s/it]\u001b[A\n",
      "batch 470, training loss: 4.1231: : 469it [06:48,  1.24s/it]\u001b[A\n",
      "batch 470, training loss: 4.1231: : 470it [06:48,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 4.1437: : 470it [06:49,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 4.1437: : 471it [06:49,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 4.0637: : 471it [06:50,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 4.0637: : 472it [06:50,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 4.0726: : 472it [06:52,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 4.0726: : 473it [06:52,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 4.1422: : 473it [06:53,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 4.1422: : 474it [06:53,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 4.0662: : 474it [06:54,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 4.0662: : 475it [06:54,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 4.1371: : 475it [06:55,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 4.1371: : 476it [06:55,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.9553: : 476it [06:57,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.9553: : 477it [06:57,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.0312: : 477it [06:58,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 4.0312: : 478it [06:58,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.0023: : 478it [06:59,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 4.0023: : 479it [06:59,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 4.0251: : 479it [07:00,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 4.0251: : 480it [07:00,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 4.0649: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 4.0649: : 481it [07:02,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 4.0471: : 481it [07:03,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 4.0471: : 482it [07:03,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.9588: : 482it [07:04,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.9588: : 483it [07:04,  1.27s/it]\u001b[A\n",
      "batch 484, training loss: 4.0267: : 483it [07:05,  1.27s/it]\u001b[A\n",
      "batch 484, training loss: 4.0267: : 484it [07:05,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.97: : 484it [07:07,  1.26s/it]  \u001b[A\n",
      "batch 485, training loss: 3.97: : 485it [07:07,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 4.0617: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 4.0617: : 486it [07:08,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 4.1752: : 486it [07:09,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 4.1752: : 487it [07:09,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.9401: : 487it [07:10,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.9401: : 488it [07:10,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.8946: : 488it [07:12,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.8946: : 489it [07:12,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.9709: : 489it [07:13,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.9709: : 490it [07:13,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.9722: : 490it [07:14,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.9722: : 491it [07:14,  1.27s/it]\u001b[A\n",
      "batch 492, training loss: 3.8926: : 491it [07:15,  1.27s/it]\u001b[A\n",
      "batch 492, training loss: 3.8926: : 492it [07:15,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 4.0716: : 492it [07:17,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 493, training loss: 4.0716: : 493it [07:17,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 4.0913: : 493it [07:18,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 4.0913: : 494it [07:18,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.9004: : 494it [07:19,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.9004: : 495it [07:19,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.9127: : 495it [07:20,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.9127: : 496it [07:20,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.8462: : 496it [07:22,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.8462: : 497it [07:22,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.9201: : 497it [07:23,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.9201: : 498it [07:23,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.9943: : 498it [07:24,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.9943: : 499it [07:24,  1.13s/it]\u001b[A\n",
      "batch 500, training loss: 4.1768: : 499it [07:25,  1.13s/it]\u001b[A\n",
      "batch 500, training loss: 4.1768: : 500it [07:25,  1.21s/it]\u001b[A\n",
      "batch 501, training loss: 4.0282: : 500it [07:27,  1.21s/it]\u001b[A\n",
      "batch 501, training loss: 4.0282: : 501it [07:27,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 4.0047: : 501it [07:28,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 4.0047: : 502it [07:28,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 4.108: : 502it [07:29,  1.29s/it] \u001b[A\n",
      "batch 503, training loss: 4.108: : 503it [07:29,  1.32s/it]\u001b[A\n",
      "batch 504, training loss: 4.0646: : 503it [07:31,  1.32s/it]\u001b[A\n",
      "batch 504, training loss: 4.0646: : 504it [07:31,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 4.1558: : 504it [07:32,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 4.1558: : 505it [07:32,  1.35s/it]\u001b[A\n",
      "batch 506, training loss: 4.1072: : 505it [07:34,  1.35s/it]\u001b[A\n",
      "batch 506, training loss: 4.1072: : 506it [07:34,  1.38s/it]\u001b[A\n",
      "batch 507, training loss: 3.9366: : 506it [07:35,  1.38s/it]\u001b[A\n",
      "batch 507, training loss: 3.9366: : 507it [07:35,  1.34s/it]\u001b[A\n",
      "batch 508, training loss: 4.1102: : 507it [07:36,  1.34s/it]\u001b[A\n",
      "batch 508, training loss: 4.1102: : 508it [07:36,  1.35s/it]\u001b[A\n",
      "batch 509, training loss: 3.9934: : 508it [07:37,  1.35s/it]\u001b[A\n",
      "batch 509, training loss: 3.9934: : 509it [07:37,  1.33s/it]\u001b[A\n",
      "batch 510, training loss: 4.043: : 509it [07:39,  1.33s/it] \u001b[A\n",
      "batch 510, training loss: 4.043: : 510it [07:39,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 4.0689: : 510it [07:40,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 4.0689: : 511it [07:40,  1.25s/it]\u001b[A\n",
      "batch 512, training loss: 4.0391: : 511it [07:41,  1.25s/it]\u001b[A\n",
      "batch 512, training loss: 4.0391: : 512it [07:41,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 4.0946: : 512it [07:43,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 4.0946: : 513it [07:43,  1.33s/it]\u001b[A\n",
      "batch 514, training loss: 3.9304: : 513it [07:44,  1.33s/it]\u001b[A\n",
      "batch 514, training loss: 3.9304: : 514it [07:44,  1.33s/it]\u001b[A\n",
      "batch 515, training loss: 4.1298: : 514it [07:45,  1.33s/it]\u001b[A\n",
      "batch 515, training loss: 4.1298: : 515it [07:45,  1.35s/it]\u001b[A\n",
      "batch 516, training loss: 4.0211: : 515it [07:47,  1.35s/it]\u001b[A\n",
      "batch 516, training loss: 4.0211: : 516it [07:47,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.9978: : 516it [07:48,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.9978: : 517it [07:48,  1.35s/it]\u001b[A\n",
      "batch 518, training loss: 4.0381: : 517it [07:50,  1.35s/it]\u001b[A\n",
      "batch 518, training loss: 4.0381: : 518it [07:50,  1.36s/it]\u001b[A\n",
      "batch 519, training loss: 3.8796: : 518it [07:51,  1.36s/it]\u001b[A\n",
      "batch 519, training loss: 3.8796: : 519it [07:51,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 4.0488: : 519it [07:52,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 4.0488: : 520it [07:52,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 4.0363: : 520it [07:54,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 4.0363: : 521it [07:54,  1.34s/it]\u001b[A\n",
      "batch 522, training loss: 4.1237: : 521it [07:55,  1.34s/it]\u001b[A\n",
      "batch 522, training loss: 4.1237: : 522it [07:55,  1.25s/it]\u001b[A\n",
      "batch 523, training loss: 3.9512: : 522it [07:56,  1.25s/it]\u001b[A\n",
      "batch 523, training loss: 3.9512: : 523it [07:56,  1.30s/it]\u001b[A\n",
      "batch 524, training loss: 4.1446: : 523it [07:57,  1.30s/it]\u001b[A\n",
      "batch 524, training loss: 4.1446: : 524it [07:57,  1.32s/it]\u001b[A\n",
      "batch 525, training loss: 4.1614: : 524it [07:59,  1.32s/it]\u001b[A\n",
      "batch 525, training loss: 4.1614: : 525it [07:59,  1.34s/it]\u001b[A\n",
      "batch 526, training loss: 4.0032: : 525it [08:00,  1.34s/it]\u001b[A\n",
      "batch 526, training loss: 4.0032: : 526it [08:00,  1.36s/it]\u001b[A\n",
      "batch 527, training loss: 3.9274: : 526it [08:01,  1.36s/it]\u001b[A\n",
      "batch 527, training loss: 3.9274: : 527it [08:01,  1.31s/it]\u001b[A\n",
      "batch 528, training loss: 4.0376: : 527it [08:03,  1.31s/it]\u001b[A\n",
      "batch 528, training loss: 4.0376: : 528it [08:03,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 4.059: : 528it [08:04,  1.37s/it] \u001b[A\n",
      "batch 529, training loss: 4.059: : 529it [08:04,  1.35s/it]\u001b[A\n",
      "batch 530, training loss: 4.05: : 529it [08:06,  1.35s/it] \u001b[A\n",
      "batch 530, training loss: 4.05: : 530it [08:06,  1.39s/it]\u001b[A\n",
      "batch 531, training loss: 3.9734: : 530it [08:07,  1.39s/it]\u001b[A\n",
      "batch 531, training loss: 3.9734: : 531it [08:07,  1.44s/it]\u001b[A\n",
      "batch 532, training loss: 3.806: : 531it [08:09,  1.44s/it] \u001b[A\n",
      "batch 532, training loss: 3.806: : 532it [08:09,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 4.0041: : 532it [08:10,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 4.0041: : 533it [08:10,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.1074: : 533it [08:12,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.1074: : 534it [08:12,  1.44s/it]\u001b[A\n",
      "batch 535, training loss: 3.9845: : 534it [08:13,  1.44s/it]\u001b[A\n",
      "batch 535, training loss: 3.9845: : 535it [08:13,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.8154: : 535it [08:15,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.8154: : 536it [08:15,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.8711: : 536it [08:16,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.8711: : 537it [08:16,  1.47s/it]\u001b[A\n",
      "batch 538, training loss: 4.0324: : 537it [08:18,  1.47s/it]\u001b[A\n",
      "batch 538, training loss: 4.0324: : 538it [08:18,  1.47s/it]\u001b[A\n",
      "batch 539, training loss: 3.9579: : 538it [08:19,  1.47s/it]\u001b[A\n",
      "batch 539, training loss: 3.9579: : 539it [08:19,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 3.9395: : 539it [08:21,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 3.9395: : 540it [08:21,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.0095: : 540it [08:22,  1.50s/it]\u001b[A\n",
      "batch 541, training loss: 4.0095: : 541it [08:22,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.0413: : 541it [08:23,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 4.0413: : 542it [08:23,  1.45s/it]\u001b[A\n",
      "batch 543, training loss: 3.9211: : 542it [08:25,  1.45s/it]\u001b[A\n",
      "batch 543, training loss: 3.9211: : 543it [08:25,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 3.9686: : 543it [08:26,  1.48s/it]\u001b[A\n",
      "batch 544, training loss: 3.9686: : 544it [08:26,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 3.9885: : 544it [08:28,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 3.9885: : 545it [08:28,  1.48s/it]\u001b[A\n",
      "batch 546, training loss: 3.871: : 545it [08:29,  1.48s/it] \u001b[A\n",
      "batch 546, training loss: 3.871: : 546it [08:29,  1.47s/it]\u001b[A\n",
      "batch 547, training loss: 3.7767: : 546it [08:31,  1.47s/it]\u001b[A\n",
      "batch 547, training loss: 3.7767: : 547it [08:31,  1.49s/it]\u001b[A\n",
      "batch 548, training loss: 3.9152: : 547it [08:32,  1.49s/it]\u001b[A\n",
      "batch 548, training loss: 3.9152: : 548it [08:32,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.9517: : 548it [08:33,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.9517: : 549it [08:33,  1.39s/it]\u001b[A\n",
      "batch 550, training loss: 3.8319: : 549it [08:35,  1.39s/it]\u001b[A\n",
      "batch 550, training loss: 3.8319: : 550it [08:35,  1.47s/it]\u001b[A\n",
      "batch 551, training loss: 3.9381: : 550it [08:37,  1.47s/it]\u001b[A\n",
      "batch 551, training loss: 3.9381: : 551it [08:37,  1.51s/it]\u001b[A\n",
      "batch 552, training loss: 3.9454: : 551it [08:38,  1.51s/it]\u001b[A\n",
      "batch 552, training loss: 3.9454: : 552it [08:38,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.9414: : 552it [08:40,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.9414: : 553it [08:40,  1.53s/it]\u001b[A\n",
      "batch 554, training loss: 3.6644: : 553it [08:41,  1.53s/it]\u001b[A\n",
      "batch 554, training loss: 3.6644: : 554it [08:41,  1.51s/it]\u001b[A\n",
      "batch 555, training loss: 3.8753: : 554it [08:43,  1.51s/it]\u001b[A\n",
      "batch 555, training loss: 3.8753: : 555it [08:43,  1.54s/it]\u001b[A\n",
      "batch 556, training loss: 3.9957: : 555it [08:44,  1.54s/it]\u001b[A\n",
      "batch 556, training loss: 3.9957: : 556it [08:44,  1.55s/it]\u001b[A\n",
      "batch 557, training loss: 3.8448: : 556it [08:46,  1.55s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 557, training loss: 3.8448: : 557it [08:46,  1.56s/it]\u001b[A\n",
      "batch 558, training loss: 3.7489: : 557it [08:48,  1.56s/it]\u001b[A\n",
      "batch 558, training loss: 3.7489: : 558it [08:48,  1.59s/it]\u001b[A\n",
      "batch 559, training loss: 3.8456: : 558it [08:49,  1.59s/it]\u001b[A\n",
      "batch 559, training loss: 3.8456: : 559it [08:49,  1.60s/it]\u001b[A\n",
      "batch 560, training loss: 3.8552: : 559it [08:51,  1.60s/it]\u001b[A\n",
      "batch 560, training loss: 3.8552: : 560it [08:51,  1.59s/it]\u001b[A\n",
      "batch 561, training loss: 3.8937: : 560it [08:52,  1.59s/it]\u001b[A\n",
      "batch 561, training loss: 3.8937: : 561it [08:52,  1.57s/it]\u001b[A\n",
      "batch 562, training loss: 3.68: : 561it [08:54,  1.57s/it]  \u001b[A\n",
      "batch 562, training loss: 3.68: : 562it [08:54,  1.56s/it]\u001b[A\n",
      "batch 563, training loss: 3.8554: : 562it [08:55,  1.56s/it]\u001b[A\n",
      "batch 563, training loss: 3.8554: : 563it [08:55,  1.56s/it]\u001b[A\n",
      "batch 564, training loss: 3.7634: : 563it [08:57,  1.56s/it]\u001b[A\n",
      "batch 564, training loss: 3.7634: : 564it [08:57,  1.56s/it]\u001b[A\n",
      "batch 565, training loss: 3.8435: : 564it [08:58,  1.56s/it]\u001b[A\n",
      "batch 565, training loss: 3.8435: : 565it [08:58,  1.53s/it]\u001b[A\n",
      "batch 566, training loss: 3.9916: : 565it [09:00,  1.53s/it]\u001b[A\n",
      "batch 566, training loss: 3.9916: : 566it [09:00,  1.56s/it]\u001b[A\n",
      "batch 567, training loss: 3.9164: : 566it [09:02,  1.56s/it]\u001b[A\n",
      "batch 567, training loss: 3.9164: : 567it [09:02,  1.56s/it]\u001b[A\n",
      "batch 568, training loss: 4.019: : 567it [09:03,  1.56s/it] \u001b[A\n",
      "batch 568, training loss: 4.019: : 568it [09:03,  1.59s/it]\u001b[A\n",
      "batch 569, training loss: 3.9692: : 568it [09:05,  1.59s/it]\u001b[A\n",
      "batch 569, training loss: 3.9692: : 569it [09:05,  1.62s/it]\u001b[A\n",
      "batch 570, training loss: 4.1796: : 569it [09:07,  1.62s/it]\u001b[A\n",
      "batch 570, training loss: 4.1796: : 570it [09:07,  1.64s/it]\u001b[A\n",
      "batch 571, training loss: 3.9725: : 570it [09:08,  1.64s/it]\u001b[A\n",
      "batch 571, training loss: 3.9725: : 571it [09:08,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 4.0352: : 571it [09:10,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 4.0352: : 572it [09:10,  1.66s/it]\u001b[A\n",
      "batch 573, training loss: 3.9607: : 572it [09:12,  1.66s/it]\u001b[A\n",
      "batch 573, training loss: 3.9607: : 573it [09:12,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 4.0445: : 573it [09:13,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 4.0445: : 574it [09:13,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 3.9156: : 574it [09:14,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 3.9156: : 575it [09:14,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.9928: : 575it [09:16,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.9928: : 576it [09:16,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.8864: : 576it [09:18,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.8864: : 577it [09:18,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.87: : 577it [09:19,  1.59s/it]  \u001b[A\n",
      "batch 578, training loss: 3.87: : 578it [09:19,  1.64s/it]\u001b[A\n",
      "batch 579, training loss: 3.8846: : 578it [09:21,  1.64s/it]\u001b[A\n",
      "batch 579, training loss: 3.8846: : 579it [09:21,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 3.8066: : 579it [09:23,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 3.8066: : 580it [09:23,  1.68s/it]\u001b[A\n",
      "batch 581, training loss: 3.8095: : 580it [09:25,  1.68s/it]\u001b[A\n",
      "batch 581, training loss: 3.8095: : 581it [09:25,  1.71s/it]\u001b[A\n",
      "batch 582, training loss: 3.8409: : 581it [09:26,  1.71s/it]\u001b[A\n",
      "batch 582, training loss: 3.8409: : 582it [09:26,  1.72s/it]\u001b[A\n",
      "batch 583, training loss: 3.6719: : 582it [09:27,  1.72s/it]\u001b[A\n",
      "batch 583, training loss: 3.6719: : 583it [09:27,  1.36s/it]\u001b[A\n",
      "batch 584, training loss: 4.069: : 583it [09:29,  1.36s/it] \u001b[A\n",
      "batch 584, training loss: 4.069: : 584it [09:29,  1.49s/it]\u001b[A\n",
      "batch 585, training loss: 4.0542: : 584it [09:31,  1.49s/it]\u001b[A\n",
      "batch 585, training loss: 4.0542: : 585it [09:31,  1.61s/it]\u001b[A\n",
      "batch 586, training loss: 4.1362: : 585it [09:33,  1.61s/it]\u001b[A\n",
      "batch 586, training loss: 4.1362: : 586it [09:33,  1.69s/it]\u001b[A\n",
      "batch 587, training loss: 4.0087: : 586it [09:34,  1.69s/it]\u001b[A\n",
      "batch 587, training loss: 4.0087: : 587it [09:34,  1.76s/it]\u001b[A\n",
      "batch 588, training loss: 3.8721: : 587it [09:36,  1.76s/it]\u001b[A\n",
      "batch 588, training loss: 3.8721: : 588it [09:36,  1.78s/it]\u001b[A\n",
      "batch 589, training loss: 3.9636: : 588it [09:38,  1.78s/it]\u001b[A\n",
      "batch 589, training loss: 3.9636: : 589it [09:38,  1.84s/it]\u001b[A\n",
      "batch 590, training loss: 3.9698: : 589it [09:40,  1.84s/it]\u001b[A\n",
      "batch 590, training loss: 3.9698: : 590it [09:40,  1.88s/it]\u001b[A\n",
      "batch 591, training loss: 3.7987: : 590it [09:42,  1.88s/it]\u001b[A\n",
      "batch 591, training loss: 3.7987: : 591it [09:42,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.8729: : 591it [09:44,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.8729: : 592it [09:44,  1.86s/it]\u001b[A\n",
      "batch 593, training loss: 3.7824: : 592it [09:46,  1.86s/it]\u001b[A\n",
      "batch 593, training loss: 3.7824: : 593it [09:46,  1.91s/it]\u001b[A\n",
      "batch 594, training loss: 3.9658: : 593it [09:48,  1.91s/it]\u001b[A\n",
      "batch 594, training loss: 3.9658: : 594it [09:48,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 4.0832: : 594it [09:50,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 4.0832: : 595it [09:50,  1.88s/it]\u001b[A\n",
      "batch 596, training loss: 3.9288: : 595it [09:52,  1.88s/it]\u001b[A\n",
      "batch 596, training loss: 3.9288: : 596it [09:52,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.7517: : 596it [09:54,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.7517: : 597it [09:54,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 4.0095: : 597it [09:56,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 4.0095: : 598it [09:56,  2.06s/it]\u001b[A\n",
      "batch 599, training loss: 3.9642: : 598it [09:57,  2.06s/it]\u001b[A\n",
      "batch 599, training loss: 3.9642: : 599it [09:57,  1.84s/it]\u001b[A\n",
      "batch 600, training loss: 3.7828: : 599it [10:00,  1.84s/it]\u001b[A\n",
      "batch 600, training loss: 3.7828: : 600it [10:00,  2.00s/it]\u001b[A\n",
      "batch 601, training loss: 3.2596: : 600it [10:01,  2.00s/it]\u001b[A\n",
      "batch 601, training loss: 3.2596: : 601it [10:01,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 3.852: : 601it [10:03,  1.68s/it] \u001b[A\n",
      "batch 602, training loss: 3.852: : 602it [10:03,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.8407: : 602it [10:05,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.8407: : 603it [10:05,  1.83s/it]\u001b[A\n",
      "batch 604, training loss: 3.9735: : 603it [10:07,  1.83s/it]\u001b[A\n",
      "batch 604, training loss: 3.9735: : 604it [10:07,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 4.0709: : 604it [10:08,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 4.0709: : 605it [10:08,  1.80s/it]\u001b[A\n",
      "batch 606, training loss: 3.8887: : 605it [10:10,  1.80s/it]\u001b[A\n",
      "batch 606, training loss: 3.8887: : 606it [10:10,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 3.8065: : 606it [10:11,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 3.8065: : 607it [10:11,  1.70s/it]\u001b[A\n",
      "batch 608, training loss: 4.123: : 607it [10:13,  1.70s/it] \u001b[A\n",
      "batch 608, training loss: 4.123: : 608it [10:13,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.056: : 608it [10:14,  1.59s/it]\u001b[A\n",
      "batch 609, training loss: 4.056: : 609it [10:14,  1.50s/it]\u001b[A\n",
      "batch 610, training loss: 3.5448: : 609it [10:15,  1.50s/it]\u001b[A\n",
      "batch 610, training loss: 3.5448: : 610it [10:15,  1.45s/it]\u001b[A\n",
      "batch 611, training loss: 3.5719: : 610it [10:17,  1.45s/it]\u001b[A\n",
      "batch 611, training loss: 3.5719: : 611it [10:17,  1.36s/it]\u001b[A\n",
      "batch 612, training loss: 2.78: : 611it [10:18,  1.36s/it]  \u001b[A\n",
      "batch 612, training loss: 2.78: : 612it [10:18,  1.30s/it]\u001b[A\n",
      "batch 613, training loss: 3.6794: : 612it [10:19,  1.30s/it]\u001b[A\n",
      "batch 613, training loss: 3.6794: : 613it [10:19,  1.22s/it]\u001b[A\n",
      "batch 613, training loss: 3.6794: : 616it [10:19,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 4.0588: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 4.0588: : 1it [00:00,  4.49it/s]\u001b[A\n",
      "batch 1, dev loss: 4.278: : 1it [00:00,  4.49it/s] \u001b[A\n",
      "batch 1, dev loss: 4.278: : 2it [00:00,  4.56it/s]\u001b[A\n",
      "batch 2, dev loss: 3.8635: : 2it [00:00,  4.56it/s]\u001b[A\n",
      "batch 2, dev loss: 3.8635: : 3it [00:00,  4.79it/s]\u001b[A\n",
      "batch 3, dev loss: 3.9822: : 3it [00:00,  4.79it/s]\u001b[A\n",
      "batch 3, dev loss: 3.9822: : 4it [00:00,  5.11it/s]\u001b[A\n",
      "batch 4, dev loss: 4.0104: : 4it [00:00,  5.11it/s]\u001b[A\n",
      "batch 4, dev loss: 4.0104: : 5it [00:00,  5.42it/s]\u001b[A\n",
      "batch 5, dev loss: 3.9843: : 5it [00:01,  5.42it/s]\u001b[A\n",
      "batch 5, dev loss: 3.9843: : 6it [00:01,  5.27it/s]\u001b[A\n",
      "batch 6, dev loss: 4.119: : 6it [00:01,  5.27it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6, dev loss: 4.119: : 7it [00:01,  5.00it/s]\u001b[A\n",
      "batch 7, dev loss: 3.887: : 7it [00:01,  5.00it/s]\u001b[A\n",
      "batch 7, dev loss: 3.887: : 8it [00:01,  5.10it/s]\u001b[A\n",
      "batch 8, dev loss: 4.1012: : 8it [00:01,  5.10it/s]\u001b[A\n",
      "batch 8, dev loss: 4.1012: : 9it [00:01,  4.69it/s]\u001b[A\n",
      "batch 9, dev loss: 3.9812: : 9it [00:02,  4.69it/s]\u001b[A\n",
      "batch 9, dev loss: 3.9812: : 10it [00:02,  3.96it/s]\u001b[A\n",
      "batch 10, dev loss: 4.0395: : 10it [00:02,  3.96it/s]\u001b[A\n",
      "batch 10, dev loss: 4.0395: : 11it [00:02,  4.22it/s]\u001b[A\n",
      "batch 11, dev loss: 4.1391: : 11it [00:02,  4.22it/s]\u001b[A\n",
      "batch 11, dev loss: 4.1391: : 12it [00:02,  4.30it/s]\u001b[A\n",
      "batch 12, dev loss: 3.9808: : 12it [00:02,  4.30it/s]\u001b[A\n",
      "batch 12, dev loss: 3.9808: : 13it [00:02,  4.10it/s]\u001b[A\n",
      "batch 13, dev loss: 4.117: : 13it [00:03,  4.10it/s] \u001b[A\n",
      "batch 13, dev loss: 4.117: : 14it [00:03,  3.96it/s]\u001b[A\n",
      "batch 14, dev loss: 4.2057: : 14it [00:03,  3.96it/s]\u001b[A\n",
      "batch 14, dev loss: 4.2057: : 15it [00:03,  3.85it/s]\u001b[A\n",
      "batch 15, dev loss: 4.0615: : 15it [00:03,  3.85it/s]\u001b[A\n",
      "batch 15, dev loss: 4.0615: : 16it [00:03,  4.38it/s]\u001b[A\n",
      "batch 16, dev loss: 4.3738: : 16it [00:03,  4.38it/s]\u001b[A\n",
      "batch 16, dev loss: 4.3738: : 17it [00:03,  4.00it/s]\u001b[A\n",
      "batch 17, dev loss: 4.1317: : 17it [00:04,  4.00it/s]\u001b[A\n",
      "batch 17, dev loss: 4.1317: : 18it [00:04,  3.82it/s]\u001b[A\n",
      "batch 18, dev loss: 4.0399: : 18it [00:04,  3.82it/s]\u001b[A\n",
      "batch 18, dev loss: 4.0399: : 19it [00:04,  3.91it/s]\u001b[A\n",
      "batch 19, dev loss: 4.2386: : 19it [00:04,  3.91it/s]\u001b[A\n",
      "batch 19, dev loss: 4.2386: : 20it [00:04,  3.85it/s]\u001b[A\n",
      "batch 20, dev loss: 4.0024: : 20it [00:04,  3.85it/s]\u001b[A\n",
      "batch 20, dev loss: 4.0024: : 21it [00:04,  3.78it/s]\u001b[A\n",
      "batch 21, dev loss: 3.9093: : 21it [00:05,  3.78it/s]\u001b[A\n",
      "batch 21, dev loss: 3.9093: : 22it [00:05,  3.58it/s]\u001b[A\n",
      "batch 22, dev loss: 4.115: : 22it [00:05,  3.58it/s] \u001b[A\n",
      "batch 22, dev loss: 4.115: : 23it [00:05,  3.52it/s]\u001b[A\n",
      "batch 23, dev loss: 4.1482: : 23it [00:05,  3.52it/s]\u001b[A\n",
      "batch 23, dev loss: 4.1482: : 24it [00:05,  4.18it/s]\u001b[A\n",
      "batch 24, dev loss: 4.0728: : 24it [00:05,  4.18it/s]\u001b[A\n",
      "batch 24, dev loss: 4.0728: : 25it [00:05,  3.97it/s]\u001b[A\n",
      "batch 25, dev loss: 4.0739: : 25it [00:06,  3.97it/s]\u001b[A\n",
      "batch 25, dev loss: 4.0739: : 26it [00:06,  3.67it/s]\u001b[A\n",
      "batch 26, dev loss: 4.0246: : 26it [00:06,  3.67it/s]\u001b[A\n",
      "batch 26, dev loss: 4.0246: : 27it [00:06,  3.76it/s]\u001b[A\n",
      "batch 27, dev loss: 3.9569: : 27it [00:06,  3.76it/s]\u001b[A\n",
      "batch 27, dev loss: 3.9569: : 28it [00:06,  3.69it/s]\u001b[A\n",
      "batch 28, dev loss: 4.1498: : 28it [00:07,  3.69it/s]\u001b[A\n",
      "batch 28, dev loss: 4.1498: : 29it [00:07,  3.56it/s]\u001b[A\n",
      "batch 29, dev loss: 4.091: : 29it [00:07,  3.56it/s] \u001b[A\n",
      "batch 29, dev loss: 4.091: : 30it [00:07,  3.38it/s]\u001b[A\n",
      "batch 30, dev loss: 4.331: : 30it [00:07,  3.38it/s]\u001b[A\n",
      "batch 30, dev loss: 4.331: : 31it [00:07,  4.08it/s]\u001b[A\n",
      "batch 31, dev loss: 4.1752: : 31it [00:07,  4.08it/s]\u001b[A\n",
      "batch 31, dev loss: 4.1752: : 32it [00:07,  3.74it/s]\u001b[A\n",
      "batch 32, dev loss: 4.1822: : 32it [00:08,  3.74it/s]\u001b[A\n",
      "batch 32, dev loss: 4.1822: : 33it [00:08,  3.31it/s]\u001b[A\n",
      "batch 33, dev loss: 3.9709: : 33it [00:08,  3.31it/s]\u001b[A\n",
      "batch 33, dev loss: 3.9709: : 34it [00:08,  3.13it/s]\u001b[A\n",
      "batch 34, dev loss: 4.4059: : 34it [00:08,  3.13it/s]\u001b[A\n",
      "batch 34, dev loss: 4.4059: : 35it [00:08,  3.15it/s]\u001b[A\n",
      "batch 35, dev loss: 4.1745: : 35it [00:09,  3.15it/s]\u001b[A\n",
      "batch 35, dev loss: 4.1745: : 36it [00:09,  3.05it/s]\u001b[A\n",
      "batch 36, dev loss: 4.1516: : 36it [00:09,  3.05it/s]\u001b[A\n",
      "batch 36, dev loss: 4.1516: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 3.9312: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 3.9312: : 38it [00:09,  3.25it/s]\u001b[A\n",
      "batch 38, dev loss: 4.1558: : 38it [00:10,  3.25it/s]\u001b[A\n",
      "batch 38, dev loss: 4.1558: : 39it [00:10,  3.22it/s]\u001b[A\n",
      "batch 39, dev loss: 4.1483: : 39it [00:10,  3.22it/s]\u001b[A\n",
      "batch 39, dev loss: 4.1483: : 40it [00:10,  3.10it/s]\u001b[A\n",
      "batch 40, dev loss: 4.1938: : 40it [00:10,  3.10it/s]\u001b[A\n",
      "batch 40, dev loss: 4.1938: : 41it [00:10,  2.96it/s]\u001b[A\n",
      "batch 41, dev loss: 3.9651: : 41it [00:11,  2.96it/s]\u001b[A\n",
      "batch 41, dev loss: 3.9651: : 42it [00:11,  3.13it/s]\u001b[A\n",
      "batch 42, dev loss: 4.1252: : 42it [00:11,  3.13it/s]\u001b[A\n",
      "batch 42, dev loss: 4.1252: : 43it [00:11,  2.95it/s]\u001b[A\n",
      "batch 43, dev loss: 4.139: : 43it [00:12,  2.95it/s] \u001b[A\n",
      "batch 43, dev loss: 4.139: : 44it [00:12,  2.74it/s]\u001b[A\n",
      "batch 44, dev loss: 4.0581: : 44it [00:12,  2.74it/s]\u001b[A\n",
      "batch 44, dev loss: 4.0581: : 45it [00:12,  2.69it/s]\u001b[A\n",
      "batch 45, dev loss: 4.355: : 45it [00:12,  2.69it/s] \u001b[A\n",
      "batch 45, dev loss: 4.355: : 46it [00:12,  2.68it/s]\u001b[A\n",
      "batch 46, dev loss: 3.8946: : 46it [00:13,  2.68it/s]\u001b[A\n",
      "batch 46, dev loss: 3.8946: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 4.0746: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 4.0746: : 48it [00:13,  2.54it/s]\u001b[A\n",
      "batch 48, dev loss: 3.8847: : 48it [00:13,  2.54it/s]\u001b[A\n",
      "batch 48, dev loss: 3.8847: : 49it [00:13,  2.56it/s]\u001b[A\n",
      "batch 49, dev loss: 4.0714: : 49it [00:14,  2.56it/s]\u001b[A\n",
      "batch 49, dev loss: 4.0714: : 50it [00:14,  2.85it/s]\u001b[A\n",
      "batch 50, dev loss: 4.0643: : 50it [00:14,  2.85it/s]\u001b[A\n",
      "batch 50, dev loss: 4.0643: : 51it [00:14,  2.63it/s]\u001b[A\n",
      "batch 51, dev loss: 4.0552: : 51it [00:15,  2.63it/s]\u001b[A\n",
      "batch 51, dev loss: 4.0552: : 52it [00:15,  2.51it/s]\u001b[A\n",
      "batch 52, dev loss: 3.8477: : 52it [00:15,  2.51it/s]\u001b[A\n",
      "batch 52, dev loss: 3.8477: : 53it [00:15,  2.57it/s]\u001b[A\n",
      "batch 53, dev loss: 4.0708: : 53it [00:15,  2.57it/s]\u001b[A\n",
      "batch 53, dev loss: 4.0708: : 54it [00:15,  2.40it/s]\u001b[A\n",
      "batch 54, dev loss: 3.8451: : 54it [00:16,  2.40it/s]\u001b[A\n",
      "batch 54, dev loss: 3.8451: : 55it [00:16,  2.42it/s]\u001b[A\n",
      "batch 55, dev loss: 3.9972: : 55it [00:16,  2.42it/s]\u001b[A\n",
      "batch 55, dev loss: 3.9972: : 56it [00:16,  2.23it/s]\u001b[A\n",
      "batch 56, dev loss: 3.8655: : 56it [00:17,  2.23it/s]\u001b[A\n",
      "batch 56, dev loss: 3.8655: : 57it [00:17,  2.36it/s]\u001b[A\n",
      "batch 57, dev loss: 3.8007: : 57it [00:17,  2.36it/s]\u001b[A\n",
      "batch 57, dev loss: 3.8007: : 58it [00:17,  2.21it/s]\u001b[A\n",
      "batch 58, dev loss: 4.1346: : 58it [00:18,  2.21it/s]\u001b[A\n",
      "batch 58, dev loss: 4.1346: : 59it [00:18,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 4.0924: : 59it [00:18,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 4.0924: : 60it [00:18,  2.35it/s]\u001b[A\n",
      "batch 60, dev loss: 3.7217: : 60it [00:19,  2.35it/s]\u001b[A\n",
      "batch 60, dev loss: 3.7217: : 61it [00:19,  2.40it/s]\u001b[A\n",
      "batch 61, dev loss: 3.7641: : 61it [00:19,  2.40it/s]\u001b[A\n",
      "batch 61, dev loss: 3.7641: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 3.6741: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 3.6741: : 63it [00:19,  2.69it/s]\u001b[A\n",
      "batch 63, dev loss: 4.2061: : 63it [00:20,  2.69it/s]\u001b[A\n",
      "batch 63, dev loss: 4.2061: : 64it [00:20,  2.67it/s]\u001b[A\n",
      "batch 64, dev loss: 3.8556: : 64it [00:20,  2.67it/s]\u001b[A\n",
      "batch 64, dev loss: 3.8556: : 65it [00:20,  2.89it/s]\u001b[A\n",
      "batch 65, dev loss: 3.8316: : 65it [00:20,  2.89it/s]\u001b[A\n",
      "batch 65, dev loss: 3.8316: : 66it [00:20,  2.83it/s]\u001b[A\n",
      "batch 66, dev loss: 3.8374: : 66it [00:21,  2.83it/s]\u001b[A\n",
      "batch 66, dev loss: 3.8374: : 67it [00:21,  2.84it/s]\u001b[A\n",
      "batch 67, dev loss: 3.1536: : 67it [00:21,  2.84it/s]\u001b[A\n",
      "batch 67, dev loss: 3.1536: : 68it [00:21,  2.88it/s]\u001b[A\n",
      "batch 68, dev loss: 3.1101: : 68it [00:21,  2.88it/s]\u001b[A\n",
      "batch 68, dev loss: 3.1101: : 69it [00:21,  3.14it/s]\u001b[A\n",
      "batch 69, dev loss: 3.3127: : 69it [00:21,  3.14it/s]\u001b[A\n",
      "batch 69, dev loss: 3.3127: : 70it [00:21,  3.13it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0181: : 70it [00:22,  3.13it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0181: : 71it [00:22,  3.09it/s]\u001b[A\n",
      "batch 71, dev loss: 3.6466: : 71it [00:22,  3.09it/s]\u001b[A\n",
      "batch 71, dev loss: 3.6466: : 72it [00:22,  2.94it/s]\u001b[A\n",
      "batch 72, dev loss: 4.4143: : 72it [00:22,  2.94it/s]\u001b[A\n",
      "batch 72, dev loss: 4.4143: : 73it [00:22,  3.02it/s]\u001b[A\n",
      "batch 72, dev loss: 4.4143: : 74it [00:23,  3.67it/s]\u001b[A\n",
      "batch 72, dev loss: 4.4143: : 75it [00:23,  4.48it/s]\u001b[A\n",
      "batch 72, dev loss: 4.4143: : 76it [00:23,  3.25it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.71s/it]\u001b[A\n",
      "2it [00:03,  1.61s/it]\u001b[A\n",
      "3it [00:04,  1.44s/it]\u001b[A\n",
      "4it [00:05,  1.36s/it]\u001b[A\n",
      "5it [00:07,  1.39s/it]\u001b[A\n",
      "6it [00:08,  1.28s/it]\u001b[A\n",
      "7it [00:09,  1.35s/it]\u001b[A\n",
      "8it [00:10,  1.28s/it]\u001b[A\n",
      "9it [00:11,  1.07s/it]\u001b[A\n",
      "10it [00:13,  1.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11it [00:14,  1.40s/it]\u001b[A\n",
      "12it [00:16,  1.51s/it]\u001b[A\n",
      "13it [00:18,  1.48s/it]\u001b[A\n",
      "14it [00:19,  1.55s/it]\u001b[A\n",
      "15it [00:21,  1.59s/it]\u001b[A\n",
      "16it [00:21,  1.26s/it]\u001b[A\n",
      "17it [00:24,  1.58s/it]\u001b[A\n",
      "18it [00:26,  1.72s/it]\u001b[A\n",
      "19it [00:28,  1.79s/it]\u001b[A\n",
      "20it [00:30,  1.95s/it]\u001b[A\n",
      "21it [00:32,  1.94s/it]\u001b[A\n",
      "22it [00:34,  1.98s/it]\u001b[A\n",
      "23it [00:36,  1.97s/it]\u001b[A\n",
      "24it [00:37,  1.52s/it]\u001b[A\n",
      "25it [00:38,  1.63s/it]\u001b[A\n",
      "26it [00:41,  1.89s/it]\u001b[A\n",
      "27it [00:43,  1.89s/it]\u001b[A\n",
      "28it [00:45,  2.10s/it]\u001b[A\n",
      "29it [00:47,  2.10s/it]\u001b[A\n",
      "30it [00:49,  2.00s/it]\u001b[A\n",
      "31it [00:52,  2.22s/it]\u001b[A\n",
      "32it [00:55,  2.32s/it]\u001b[A\n",
      "33it [00:57,  2.49s/it]\u001b[A\n",
      "34it [01:00,  2.43s/it]\u001b[A\n",
      "35it [01:03,  2.61s/it]\u001b[A\n",
      "36it [01:03,  1.96s/it]\u001b[A\n",
      "37it [01:07,  2.38s/it]\u001b[A\n",
      "38it [01:10,  2.72s/it]\u001b[A\n",
      "39it [01:13,  2.89s/it]\u001b[A\n",
      "40it [01:17,  3.03s/it]\u001b[A\n",
      "41it [01:17,  2.35s/it]\u001b[A\n",
      "42it [01:21,  2.70s/it]\u001b[A\n",
      "43it [01:24,  2.68s/it]\u001b[A\n",
      "44it [01:28,  3.05s/it]\u001b[A\n",
      "45it [01:30,  2.91s/it]\u001b[A\n",
      "46it [01:34,  3.13s/it]\u001b[A\n",
      "47it [01:38,  3.35s/it]\u001b[A\n",
      "48it [01:42,  3.66s/it]\u001b[A\n",
      "49it [01:42,  2.63s/it]\u001b[A\n",
      "50it [01:46,  2.97s/it]\u001b[A\n",
      "51it [01:50,  3.27s/it]\u001b[A\n",
      "52it [01:52,  3.05s/it]\u001b[A\n",
      "53it [01:57,  3.41s/it]\u001b[A\n",
      "54it [02:00,  3.41s/it]\u001b[A\n",
      "55it [02:05,  3.85s/it]\u001b[A\n",
      "56it [02:07,  3.21s/it]\u001b[A\n",
      "57it [02:11,  3.56s/it]\u001b[A\n",
      "58it [02:15,  3.55s/it]\u001b[A\n",
      "59it [02:17,  3.33s/it]\u001b[A\n",
      "60it [02:20,  3.08s/it]\u001b[A\n",
      "61it [02:21,  2.53s/it]\u001b[A\n",
      "62it [02:23,  2.22s/it]\u001b[A\n",
      "63it [02:24,  1.93s/it]\u001b[A\n",
      "64it [02:25,  1.63s/it]\u001b[A\n",
      "65it [02:26,  1.36s/it]\u001b[A\n",
      "66it [02:26,  1.07s/it]\u001b[A\n",
      "67it [02:26,  1.13it/s]\u001b[A\n",
      "68it [02:27,  1.29it/s]\u001b[A\n",
      "69it [02:28,  1.41it/s]\u001b[A\n",
      "70it [02:28,  2.12s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|██▋                                   | 487/6740 [00:00<00:01, 4862.98it/s]\u001b[A\n",
      " 14%|█████▍                                | 974/6740 [00:00<00:02, 2844.84it/s]\u001b[A\n",
      " 21%|███████▊                             | 1416/6740 [00:00<00:01, 3363.21it/s]\u001b[A\n",
      " 28%|██████████▏                          | 1867/6740 [00:00<00:01, 3727.62it/s]\u001b[A\n",
      " 34%|████████████▌                        | 2296/6740 [00:00<00:01, 3903.41it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 2712/6740 [00:00<00:01, 3948.36it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 3124/6740 [00:00<00:00, 4000.23it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 3536/6740 [00:00<00:00, 4020.21it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 3947/6740 [00:01<00:00, 3949.38it/s]\u001b[A\n",
      " 65%|███████████████████████▊             | 4348/6740 [00:01<00:00, 3932.76it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 4746/6740 [00:01<00:00, 3915.32it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 5141/6740 [00:01<00:00, 3885.35it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 5532/6740 [00:01<00:00, 3841.53it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 5938/6740 [00:01<00:00, 3903.93it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 6339/6740 [00:01<00:00, 3934.50it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 3838.87it/s]\u001b[A\n",
      "Epoch: 4, tfr: 1.0, loss(train/dev): 4.0431/4.0116, ppl(dev/test): 55.2352/62.85\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.9287: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.9287: : 1it [00:01,  1.95s/it]\u001b[A\n",
      "batch 2, training loss: 3.884: : 1it [00:02,  1.95s/it] \u001b[A\n",
      "batch 2, training loss: 3.884: : 2it [00:02,  1.21s/it]\u001b[A\n",
      "batch 3, training loss: 4.013: : 2it [00:03,  1.21s/it]\u001b[A\n",
      "batch 3, training loss: 4.013: : 3it [00:03,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 3.9312: : 3it [00:04,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 3.9312: : 4it [00:04,  1.16it/s]\u001b[A\n",
      "batch 5, training loss: 3.7549: : 4it [00:04,  1.16it/s]\u001b[A\n",
      "batch 5, training loss: 3.7549: : 5it [00:04,  1.37it/s]\u001b[A\n",
      "batch 6, training loss: 3.9764: : 5it [00:05,  1.37it/s]\u001b[A\n",
      "batch 6, training loss: 3.9764: : 6it [00:05,  1.48it/s]\u001b[A\n",
      "batch 7, training loss: 3.9571: : 6it [00:05,  1.48it/s]\u001b[A\n",
      "batch 7, training loss: 3.9571: : 7it [00:05,  1.49it/s]\u001b[A\n",
      "batch 8, training loss: 3.9964: : 7it [00:06,  1.49it/s]\u001b[A\n",
      "batch 8, training loss: 3.9964: : 8it [00:06,  1.49it/s]\u001b[A\n",
      "batch 9, training loss: 3.8427: : 8it [00:06,  1.49it/s]\u001b[A\n",
      "batch 9, training loss: 3.8427: : 9it [00:06,  1.67it/s]\u001b[A\n",
      "batch 10, training loss: 3.8563: : 9it [00:07,  1.67it/s]\u001b[A\n",
      "batch 10, training loss: 3.8563: : 10it [00:07,  1.94it/s]\u001b[A\n",
      "batch 11, training loss: 3.9202: : 10it [00:07,  1.94it/s]\u001b[A\n",
      "batch 11, training loss: 3.9202: : 11it [00:07,  1.86it/s]\u001b[A\n",
      "batch 12, training loss: 3.9371: : 11it [00:08,  1.86it/s]\u001b[A\n",
      "batch 12, training loss: 3.9371: : 12it [00:08,  1.73it/s]\u001b[A\n",
      "batch 13, training loss: 3.8135: : 12it [00:09,  1.73it/s]\u001b[A\n",
      "batch 13, training loss: 3.8135: : 13it [00:09,  1.61it/s]\u001b[A\n",
      "batch 14, training loss: 4.1203: : 13it [00:09,  1.61it/s]\u001b[A\n",
      "batch 14, training loss: 4.1203: : 14it [00:09,  1.57it/s]\u001b[A\n",
      "batch 15, training loss: 3.956: : 14it [00:10,  1.57it/s] \u001b[A\n",
      "batch 15, training loss: 3.956: : 15it [00:10,  1.68it/s]\u001b[A\n",
      "batch 16, training loss: 3.9465: : 15it [00:10,  1.68it/s]\u001b[A\n",
      "batch 16, training loss: 3.9465: : 16it [00:10,  1.69it/s]\u001b[A\n",
      "batch 17, training loss: 4.1108: : 16it [00:11,  1.69it/s]\u001b[A\n",
      "batch 17, training loss: 4.1108: : 17it [00:11,  1.64it/s]\u001b[A\n",
      "batch 18, training loss: 3.9053: : 17it [00:12,  1.64it/s]\u001b[A\n",
      "batch 18, training loss: 3.9053: : 18it [00:12,  1.60it/s]\u001b[A\n",
      "batch 19, training loss: 3.7065: : 18it [00:12,  1.60it/s]\u001b[A\n",
      "batch 19, training loss: 3.7065: : 19it [00:12,  1.58it/s]\u001b[A\n",
      "batch 20, training loss: 3.8415: : 19it [00:13,  1.58it/s]\u001b[A\n",
      "batch 20, training loss: 3.8415: : 20it [00:13,  1.62it/s]\u001b[A\n",
      "batch 21, training loss: 3.9952: : 20it [00:14,  1.62it/s]\u001b[A\n",
      "batch 21, training loss: 3.9952: : 21it [00:14,  1.69it/s]\u001b[A\n",
      "batch 22, training loss: 3.7602: : 21it [00:14,  1.69it/s]\u001b[A\n",
      "batch 22, training loss: 3.7602: : 22it [00:14,  1.63it/s]\u001b[A\n",
      "batch 23, training loss: 3.9189: : 22it [00:15,  1.63it/s]\u001b[A\n",
      "batch 23, training loss: 3.9189: : 23it [00:15,  1.55it/s]\u001b[A\n",
      "batch 24, training loss: 3.837: : 23it [00:16,  1.55it/s] \u001b[A\n",
      "batch 24, training loss: 3.837: : 24it [00:16,  1.52it/s]\u001b[A\n",
      "batch 25, training loss: 3.9209: : 24it [00:16,  1.52it/s]\u001b[A\n",
      "batch 25, training loss: 3.9209: : 25it [00:16,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.7469: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.7469: : 26it [00:17,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.8753: : 26it [00:17,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.8753: : 27it [00:17,  1.64it/s]\u001b[A\n",
      "batch 28, training loss: 3.7633: : 27it [00:18,  1.64it/s]\u001b[A\n",
      "batch 28, training loss: 3.7633: : 28it [00:18,  1.59it/s]\u001b[A\n",
      "batch 29, training loss: 3.8882: : 28it [00:19,  1.59it/s]\u001b[A\n",
      "batch 29, training loss: 3.8882: : 29it [00:19,  1.52it/s]\u001b[A\n",
      "batch 30, training loss: 4.0089: : 29it [00:19,  1.52it/s]\u001b[A\n",
      "batch 30, training loss: 4.0089: : 30it [00:19,  1.52it/s]\u001b[A\n",
      "batch 31, training loss: 3.7573: : 30it [00:20,  1.52it/s]\u001b[A\n",
      "batch 31, training loss: 3.7573: : 31it [00:20,  1.63it/s]\u001b[A\n",
      "batch 32, training loss: 3.9314: : 31it [00:20,  1.63it/s]\u001b[A\n",
      "batch 32, training loss: 3.9314: : 32it [00:20,  1.66it/s]\u001b[A\n",
      "batch 33, training loss: 3.853: : 32it [00:21,  1.66it/s] \u001b[A\n",
      "batch 33, training loss: 3.853: : 33it [00:21,  1.62it/s]\u001b[A\n",
      "batch 34, training loss: 3.8349: : 33it [00:22,  1.62it/s]\u001b[A\n",
      "batch 34, training loss: 3.8349: : 34it [00:22,  1.57it/s]\u001b[A\n",
      "batch 35, training loss: 3.9271: : 34it [00:22,  1.57it/s]\u001b[A\n",
      "batch 35, training loss: 3.9271: : 35it [00:22,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 3.9792: : 35it [00:23,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 3.9792: : 36it [00:23,  1.60it/s]\u001b[A\n",
      "batch 37, training loss: 3.8348: : 36it [00:24,  1.60it/s]\u001b[A\n",
      "batch 37, training loss: 3.8348: : 37it [00:24,  1.59it/s]\u001b[A\n",
      "batch 38, training loss: 3.6448: : 37it [00:24,  1.59it/s]\u001b[A\n",
      "batch 38, training loss: 3.6448: : 38it [00:24,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 3.7829: : 38it [00:25,  1.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 39, training loss: 3.7829: : 39it [00:25,  1.55it/s]\u001b[A\n",
      "batch 40, training loss: 3.8923: : 39it [00:26,  1.55it/s]\u001b[A\n",
      "batch 40, training loss: 3.8923: : 40it [00:26,  1.49it/s]\u001b[A\n",
      "batch 41, training loss: 4.0438: : 40it [00:26,  1.49it/s]\u001b[A\n",
      "batch 41, training loss: 4.0438: : 41it [00:26,  1.48it/s]\u001b[A\n",
      "batch 42, training loss: 3.8935: : 41it [00:27,  1.48it/s]\u001b[A\n",
      "batch 42, training loss: 3.8935: : 42it [00:27,  1.60it/s]\u001b[A\n",
      "batch 43, training loss: 3.6778: : 42it [00:28,  1.60it/s]\u001b[A\n",
      "batch 43, training loss: 3.6778: : 43it [00:28,  1.64it/s]\u001b[A\n",
      "batch 44, training loss: 3.673: : 43it [00:28,  1.64it/s] \u001b[A\n",
      "batch 44, training loss: 3.673: : 44it [00:28,  1.60it/s]\u001b[A\n",
      "batch 45, training loss: 3.6792: : 44it [00:29,  1.60it/s]\u001b[A\n",
      "batch 45, training loss: 3.6792: : 45it [00:29,  1.57it/s]\u001b[A\n",
      "batch 46, training loss: 3.8312: : 45it [00:29,  1.57it/s]\u001b[A\n",
      "batch 46, training loss: 3.8312: : 46it [00:29,  1.68it/s]\u001b[A\n",
      "batch 47, training loss: 3.8328: : 46it [00:30,  1.68it/s]\u001b[A\n",
      "batch 47, training loss: 3.8328: : 47it [00:30,  1.70it/s]\u001b[A\n",
      "batch 48, training loss: 3.7646: : 47it [00:31,  1.70it/s]\u001b[A\n",
      "batch 48, training loss: 3.7646: : 48it [00:31,  1.64it/s]\u001b[A\n",
      "batch 49, training loss: 4.0104: : 48it [00:31,  1.64it/s]\u001b[A\n",
      "batch 49, training loss: 4.0104: : 49it [00:31,  1.59it/s]\u001b[A\n",
      "batch 50, training loss: 3.7432: : 49it [00:32,  1.59it/s]\u001b[A\n",
      "batch 50, training loss: 3.7432: : 50it [00:32,  1.71it/s]\u001b[A\n",
      "batch 51, training loss: 3.8361: : 50it [00:32,  1.71it/s]\u001b[A\n",
      "batch 51, training loss: 3.8361: : 51it [00:32,  1.72it/s]\u001b[A\n",
      "batch 52, training loss: 3.7192: : 51it [00:33,  1.72it/s]\u001b[A\n",
      "batch 52, training loss: 3.7192: : 52it [00:33,  1.66it/s]\u001b[A\n",
      "batch 53, training loss: 3.7263: : 52it [00:34,  1.66it/s]\u001b[A\n",
      "batch 53, training loss: 3.7263: : 53it [00:34,  1.61it/s]\u001b[A\n",
      "batch 54, training loss: 3.6675: : 53it [00:34,  1.61it/s]\u001b[A\n",
      "batch 54, training loss: 3.6675: : 54it [00:34,  1.72it/s]\u001b[A\n",
      "batch 55, training loss: 3.7975: : 54it [00:35,  1.72it/s]\u001b[A\n",
      "batch 55, training loss: 3.7975: : 55it [00:35,  1.72it/s]\u001b[A\n",
      "batch 56, training loss: 3.8339: : 55it [00:35,  1.72it/s]\u001b[A\n",
      "batch 56, training loss: 3.8339: : 56it [00:35,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.7876: : 56it [00:36,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.7876: : 57it [00:36,  1.60it/s]\u001b[A\n",
      "batch 58, training loss: 3.8647: : 57it [00:36,  1.60it/s]\u001b[A\n",
      "batch 58, training loss: 3.8647: : 58it [00:36,  1.71it/s]\u001b[A\n",
      "batch 59, training loss: 3.7094: : 58it [00:37,  1.71it/s]\u001b[A\n",
      "batch 59, training loss: 3.7094: : 59it [00:37,  1.73it/s]\u001b[A\n",
      "batch 60, training loss: 3.657: : 59it [00:38,  1.73it/s] \u001b[A\n",
      "batch 60, training loss: 3.657: : 60it [00:38,  1.67it/s]\u001b[A\n",
      "batch 61, training loss: 3.915: : 60it [00:38,  1.67it/s]\u001b[A\n",
      "batch 61, training loss: 3.915: : 61it [00:38,  1.59it/s]\u001b[A\n",
      "batch 62, training loss: 3.7454: : 61it [00:39,  1.59it/s]\u001b[A\n",
      "batch 62, training loss: 3.7454: : 62it [00:39,  1.56it/s]\u001b[A\n",
      "batch 63, training loss: 3.8337: : 62it [00:40,  1.56it/s]\u001b[A\n",
      "batch 63, training loss: 3.8337: : 63it [00:40,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.7848: : 63it [00:40,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.7848: : 64it [00:40,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 3.8559: : 64it [00:41,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 3.8559: : 65it [00:41,  1.60it/s]\u001b[A\n",
      "batch 66, training loss: 3.8557: : 65it [00:42,  1.60it/s]\u001b[A\n",
      "batch 66, training loss: 3.8557: : 66it [00:42,  1.56it/s]\u001b[A\n",
      "batch 67, training loss: 3.7511: : 66it [00:42,  1.56it/s]\u001b[A\n",
      "batch 67, training loss: 3.7511: : 67it [00:42,  1.51it/s]\u001b[A\n",
      "batch 68, training loss: 3.8567: : 67it [00:43,  1.51it/s]\u001b[A\n",
      "batch 68, training loss: 3.8567: : 68it [00:43,  1.52it/s]\u001b[A\n",
      "batch 69, training loss: 3.7579: : 68it [00:44,  1.52it/s]\u001b[A\n",
      "batch 69, training loss: 3.7579: : 69it [00:44,  1.57it/s]\u001b[A\n",
      "batch 70, training loss: 3.9168: : 69it [00:44,  1.57it/s]\u001b[A\n",
      "batch 70, training loss: 3.9168: : 70it [00:44,  1.66it/s]\u001b[A\n",
      "batch 71, training loss: 3.7562: : 70it [00:45,  1.66it/s]\u001b[A\n",
      "batch 71, training loss: 3.7562: : 71it [00:45,  1.62it/s]\u001b[A\n",
      "batch 72, training loss: 3.746: : 71it [00:45,  1.62it/s] \u001b[A\n",
      "batch 72, training loss: 3.746: : 72it [00:45,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.8071: : 72it [00:46,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.8071: : 73it [00:46,  1.54it/s]\u001b[A\n",
      "batch 74, training loss: 3.6635: : 73it [00:47,  1.54it/s]\u001b[A\n",
      "batch 74, training loss: 3.6635: : 74it [00:47,  1.56it/s]\u001b[A\n",
      "batch 75, training loss: 3.9041: : 74it [00:47,  1.56it/s]\u001b[A\n",
      "batch 75, training loss: 3.9041: : 75it [00:47,  1.58it/s]\u001b[A\n",
      "batch 76, training loss: 3.7238: : 75it [00:48,  1.58it/s]\u001b[A\n",
      "batch 76, training loss: 3.7238: : 76it [00:48,  1.59it/s]\u001b[A\n",
      "batch 77, training loss: 3.8502: : 76it [00:49,  1.59it/s]\u001b[A\n",
      "batch 77, training loss: 3.8502: : 77it [00:49,  1.57it/s]\u001b[A\n",
      "batch 78, training loss: 3.8513: : 77it [00:49,  1.57it/s]\u001b[A\n",
      "batch 78, training loss: 3.8513: : 78it [00:49,  1.52it/s]\u001b[A\n",
      "batch 79, training loss: 3.8879: : 78it [00:50,  1.52it/s]\u001b[A\n",
      "batch 79, training loss: 3.8879: : 79it [00:50,  1.51it/s]\u001b[A\n",
      "batch 80, training loss: 3.7678: : 79it [00:51,  1.51it/s]\u001b[A\n",
      "batch 80, training loss: 3.7678: : 80it [00:51,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.8037: : 80it [00:51,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.8037: : 81it [00:51,  1.64it/s]\u001b[A\n",
      "batch 82, training loss: 3.9407: : 81it [00:52,  1.64it/s]\u001b[A\n",
      "batch 82, training loss: 3.9407: : 82it [00:52,  1.60it/s]\u001b[A\n",
      "batch 83, training loss: 3.7883: : 82it [00:52,  1.60it/s]\u001b[A\n",
      "batch 83, training loss: 3.7883: : 83it [00:52,  1.57it/s]\u001b[A\n",
      "batch 84, training loss: 3.9238: : 83it [00:53,  1.57it/s]\u001b[A\n",
      "batch 84, training loss: 3.9238: : 84it [00:53,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 3.9884: : 84it [00:54,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 3.9884: : 85it [00:54,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.8815: : 85it [00:54,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.8815: : 86it [00:54,  1.57it/s]\u001b[A\n",
      "batch 87, training loss: 3.9469: : 86it [00:55,  1.57it/s]\u001b[A\n",
      "batch 87, training loss: 3.9469: : 87it [00:55,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 4.0803: : 87it [00:56,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 4.0803: : 88it [00:56,  1.52it/s]\u001b[A\n",
      "batch 89, training loss: 4.1583: : 88it [00:56,  1.52it/s]\u001b[A\n",
      "batch 89, training loss: 4.1583: : 89it [00:56,  1.45it/s]\u001b[A\n",
      "batch 90, training loss: 4.0659: : 89it [00:57,  1.45it/s]\u001b[A\n",
      "batch 90, training loss: 4.0659: : 90it [00:57,  1.41it/s]\u001b[A\n",
      "batch 91, training loss: 4.1939: : 90it [00:58,  1.41it/s]\u001b[A\n",
      "batch 91, training loss: 4.1939: : 91it [00:58,  1.36it/s]\u001b[A\n",
      "batch 92, training loss: 4.0557: : 91it [00:59,  1.36it/s]\u001b[A\n",
      "batch 92, training loss: 4.0557: : 92it [00:59,  1.35it/s]\u001b[A\n",
      "batch 93, training loss: 4.0108: : 92it [00:59,  1.35it/s]\u001b[A\n",
      "batch 93, training loss: 4.0108: : 93it [00:59,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 4.0922: : 93it [01:00,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 4.0922: : 94it [01:00,  1.33it/s]\u001b[A\n",
      "batch 95, training loss: 4.0769: : 94it [01:01,  1.33it/s]\u001b[A\n",
      "batch 95, training loss: 4.0769: : 95it [01:01,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 4.064: : 95it [01:02,  1.32it/s] \u001b[A\n",
      "batch 96, training loss: 4.064: : 96it [01:02,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.1447: : 96it [01:03,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.1447: : 97it [01:03,  1.29it/s]\u001b[A\n",
      "batch 98, training loss: 4.1449: : 97it [01:03,  1.29it/s]\u001b[A\n",
      "batch 98, training loss: 4.1449: : 98it [01:03,  1.29it/s]\u001b[A\n",
      "batch 99, training loss: 3.9547: : 98it [01:04,  1.29it/s]\u001b[A\n",
      "batch 99, training loss: 3.9547: : 99it [01:04,  1.29it/s]\u001b[A\n",
      "batch 100, training loss: 3.7499: : 99it [01:05,  1.29it/s]\u001b[A\n",
      "batch 100, training loss: 3.7499: : 100it [01:05,  1.29it/s]\u001b[A\n",
      "batch 101, training loss: 3.9692: : 100it [01:06,  1.29it/s]\u001b[A\n",
      "batch 101, training loss: 3.9692: : 101it [01:06,  1.29it/s]\u001b[A\n",
      "batch 102, training loss: 3.9105: : 101it [01:06,  1.29it/s]\u001b[A\n",
      "batch 102, training loss: 3.9105: : 102it [01:06,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 3.8785: : 102it [01:07,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 3.8785: : 103it [01:07,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.7164: : 103it [01:08,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.7164: : 104it [01:08,  1.29it/s]\u001b[A\n",
      "batch 105, training loss: 3.9076: : 104it [01:09,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 105, training loss: 3.9076: : 105it [01:09,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 4.004: : 105it [01:10,  1.30it/s] \u001b[A\n",
      "batch 106, training loss: 4.004: : 106it [01:10,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 3.6934: : 106it [01:10,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 3.6934: : 107it [01:10,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.9849: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.9849: : 108it [01:11,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 4.0332: : 108it [01:12,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 4.0332: : 109it [01:12,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 4.1645: : 109it [01:13,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 4.1645: : 110it [01:13,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 3.8509: : 110it [01:13,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 3.8509: : 111it [01:13,  1.29it/s]\u001b[A\n",
      "batch 112, training loss: 3.911: : 111it [01:14,  1.29it/s] \u001b[A\n",
      "batch 112, training loss: 3.911: : 112it [01:14,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.9977: : 112it [01:15,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.9977: : 113it [01:15,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 4.0025: : 113it [01:16,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 4.0025: : 114it [01:16,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.8836: : 114it [01:17,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.8836: : 115it [01:17,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.8688: : 115it [01:17,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.8688: : 116it [01:17,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 3.9472: : 116it [01:18,  1.31it/s]\u001b[A\n",
      "batch 117, training loss: 3.9472: : 117it [01:18,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 4.0092: : 117it [01:19,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 4.0092: : 118it [01:19,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.9149: : 118it [01:20,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.9149: : 119it [01:20,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 3.8611: : 119it [01:20,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 3.8611: : 120it [01:20,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 4.0973: : 120it [01:21,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 4.0973: : 121it [01:21,  1.29it/s]\u001b[A\n",
      "batch 122, training loss: 3.7677: : 121it [01:22,  1.29it/s]\u001b[A\n",
      "batch 122, training loss: 3.7677: : 122it [01:22,  1.28it/s]\u001b[A\n",
      "batch 123, training loss: 3.9393: : 122it [01:23,  1.28it/s]\u001b[A\n",
      "batch 123, training loss: 3.9393: : 123it [01:23,  1.29it/s]\u001b[A\n",
      "batch 124, training loss: 3.9362: : 123it [01:23,  1.29it/s]\u001b[A\n",
      "batch 124, training loss: 3.9362: : 124it [01:23,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.9606: : 124it [01:24,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.9606: : 125it [01:24,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 4.008: : 125it [01:25,  1.29it/s] \u001b[A\n",
      "batch 126, training loss: 4.008: : 126it [01:25,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 3.7342: : 126it [01:26,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 3.7342: : 127it [01:26,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 3.9327: : 127it [01:27,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 3.9327: : 128it [01:27,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.9036: : 128it [01:27,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.9036: : 129it [01:27,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 3.9156: : 129it [01:28,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 3.9156: : 130it [01:28,  1.30it/s]\u001b[A\n",
      "batch 131, training loss: 4.0782: : 130it [01:29,  1.30it/s]\u001b[A\n",
      "batch 131, training loss: 4.0782: : 131it [01:29,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.8696: : 131it [01:30,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.8696: : 132it [01:30,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.7626: : 132it [01:30,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.7626: : 133it [01:30,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 3.8939: : 133it [01:31,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 3.8939: : 134it [01:31,  1.30it/s]\u001b[A\n",
      "batch 135, training loss: 3.8332: : 134it [01:32,  1.30it/s]\u001b[A\n",
      "batch 135, training loss: 3.8332: : 135it [01:32,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.8098: : 135it [01:33,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.8098: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.951: : 136it [01:33,  1.31it/s] \u001b[A\n",
      "batch 137, training loss: 3.951: : 137it [01:33,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 3.9776: : 137it [01:34,  1.31it/s]\u001b[A\n",
      "batch 138, training loss: 3.9776: : 138it [01:34,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.7544: : 138it [01:35,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.7544: : 139it [01:35,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 3.839: : 139it [01:35,  1.30it/s] \u001b[A\n",
      "batch 140, training loss: 3.839: : 140it [01:35,  1.46it/s]\u001b[A\n",
      "batch 141, training loss: 3.8586: : 140it [01:36,  1.46it/s]\u001b[A\n",
      "batch 141, training loss: 3.8586: : 141it [01:36,  1.45it/s]\u001b[A\n",
      "batch 142, training loss: 3.8705: : 141it [01:37,  1.45it/s]\u001b[A\n",
      "batch 142, training loss: 3.8705: : 142it [01:37,  1.43it/s]\u001b[A\n",
      "batch 143, training loss: 3.7242: : 142it [01:38,  1.43it/s]\u001b[A\n",
      "batch 143, training loss: 3.7242: : 143it [01:38,  1.38it/s]\u001b[A\n",
      "batch 144, training loss: 3.7655: : 143it [01:38,  1.38it/s]\u001b[A\n",
      "batch 144, training loss: 3.7655: : 144it [01:38,  1.37it/s]\u001b[A\n",
      "batch 145, training loss: 3.8606: : 144it [01:39,  1.37it/s]\u001b[A\n",
      "batch 145, training loss: 3.8606: : 145it [01:39,  1.35it/s]\u001b[A\n",
      "batch 146, training loss: 3.8725: : 145it [01:40,  1.35it/s]\u001b[A\n",
      "batch 146, training loss: 3.8725: : 146it [01:40,  1.33it/s]\u001b[A\n",
      "batch 147, training loss: 3.7584: : 146it [01:41,  1.33it/s]\u001b[A\n",
      "batch 147, training loss: 3.7584: : 147it [01:41,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.8477: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.8477: : 148it [01:42,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.9503: : 148it [01:42,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.9503: : 149it [01:42,  1.33it/s]\u001b[A\n",
      "batch 150, training loss: 3.9292: : 149it [01:43,  1.33it/s]\u001b[A\n",
      "batch 150, training loss: 3.9292: : 150it [01:43,  1.31it/s]\u001b[A\n",
      "batch 151, training loss: 3.8749: : 150it [01:44,  1.31it/s]\u001b[A\n",
      "batch 151, training loss: 3.8749: : 151it [01:44,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 3.7804: : 151it [01:45,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 3.7804: : 152it [01:45,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 3.8481: : 152it [01:45,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 3.8481: : 153it [01:45,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.9414: : 153it [01:46,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.9414: : 154it [01:46,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 4.0496: : 154it [01:47,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 4.0496: : 155it [01:47,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.673: : 155it [01:48,  1.30it/s] \u001b[A\n",
      "batch 156, training loss: 3.673: : 156it [01:48,  1.31it/s]\u001b[A\n",
      "batch 157, training loss: 3.8807: : 156it [01:48,  1.31it/s]\u001b[A\n",
      "batch 157, training loss: 3.8807: : 157it [01:48,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.8579: : 157it [01:49,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.8579: : 158it [01:49,  1.30it/s]\u001b[A\n",
      "batch 159, training loss: 3.7617: : 158it [01:50,  1.30it/s]\u001b[A\n",
      "batch 159, training loss: 3.7617: : 159it [01:50,  1.30it/s]\u001b[A\n",
      "batch 160, training loss: 3.945: : 159it [01:51,  1.30it/s] \u001b[A\n",
      "batch 160, training loss: 3.945: : 160it [01:51,  1.30it/s]\u001b[A\n",
      "batch 161, training loss: 3.7812: : 160it [01:52,  1.30it/s]\u001b[A\n",
      "batch 161, training loss: 3.7812: : 161it [01:52,  1.29it/s]\u001b[A\n",
      "batch 162, training loss: 3.7922: : 161it [01:52,  1.29it/s]\u001b[A\n",
      "batch 162, training loss: 3.7922: : 162it [01:52,  1.29it/s]\u001b[A\n",
      "batch 163, training loss: 4.0415: : 162it [01:53,  1.29it/s]\u001b[A\n",
      "batch 163, training loss: 4.0415: : 163it [01:53,  1.29it/s]\u001b[A\n",
      "batch 164, training loss: 3.8304: : 163it [01:54,  1.29it/s]\u001b[A\n",
      "batch 164, training loss: 3.8304: : 164it [01:54,  1.28it/s]\u001b[A\n",
      "batch 165, training loss: 3.8228: : 164it [01:55,  1.28it/s]\u001b[A\n",
      "batch 165, training loss: 3.8228: : 165it [01:55,  1.28it/s]\u001b[A\n",
      "batch 166, training loss: 3.8012: : 165it [01:55,  1.28it/s]\u001b[A\n",
      "batch 166, training loss: 3.8012: : 166it [01:55,  1.28it/s]\u001b[A\n",
      "batch 167, training loss: 3.9896: : 166it [01:56,  1.28it/s]\u001b[A\n",
      "batch 167, training loss: 3.9896: : 167it [01:56,  1.28it/s]\u001b[A\n",
      "batch 168, training loss: 3.8939: : 167it [01:57,  1.28it/s]\u001b[A\n",
      "batch 168, training loss: 3.8939: : 168it [01:57,  1.28it/s]\u001b[A\n",
      "batch 169, training loss: 3.997: : 168it [01:58,  1.28it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 169, training loss: 3.997: : 169it [01:58,  1.28it/s]\u001b[A\n",
      "batch 170, training loss: 3.828: : 169it [01:59,  1.28it/s]\u001b[A\n",
      "batch 170, training loss: 3.828: : 170it [01:59,  1.28it/s]\u001b[A\n",
      "batch 171, training loss: 3.2523: : 170it [01:59,  1.28it/s]\u001b[A\n",
      "batch 171, training loss: 3.2523: : 171it [01:59,  1.60it/s]\u001b[A\n",
      "batch 172, training loss: 4.2287: : 171it [02:00,  1.60it/s]\u001b[A\n",
      "batch 172, training loss: 4.2287: : 172it [02:00,  1.47it/s]\u001b[A\n",
      "batch 173, training loss: 4.0909: : 172it [02:00,  1.47it/s]\u001b[A\n",
      "batch 173, training loss: 4.0909: : 173it [02:00,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 4.2637: : 173it [02:01,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 4.2637: : 174it [02:01,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 4.1678: : 174it [02:02,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 4.1678: : 175it [02:02,  1.27it/s]\u001b[A\n",
      "batch 176, training loss: 4.1051: : 175it [02:03,  1.27it/s]\u001b[A\n",
      "batch 176, training loss: 4.1051: : 176it [02:03,  1.25it/s]\u001b[A\n",
      "batch 177, training loss: 4.1183: : 176it [02:04,  1.25it/s]\u001b[A\n",
      "batch 177, training loss: 4.1183: : 177it [02:04,  1.25it/s]\u001b[A\n",
      "batch 178, training loss: 4.1298: : 177it [02:05,  1.25it/s]\u001b[A\n",
      "batch 178, training loss: 4.1298: : 178it [02:05,  1.28it/s]\u001b[A\n",
      "batch 179, training loss: 4.1501: : 178it [02:05,  1.28it/s]\u001b[A\n",
      "batch 179, training loss: 4.1501: : 179it [02:05,  1.29it/s]\u001b[A\n",
      "batch 180, training loss: 4.2005: : 179it [02:06,  1.29it/s]\u001b[A\n",
      "batch 180, training loss: 4.2005: : 180it [02:06,  1.28it/s]\u001b[A\n",
      "batch 181, training loss: 4.2371: : 180it [02:07,  1.28it/s]\u001b[A\n",
      "batch 181, training loss: 4.2371: : 181it [02:07,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.1329: : 181it [02:08,  1.26it/s]\u001b[A\n",
      "batch 182, training loss: 4.1329: : 182it [02:08,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 4.2331: : 182it [02:08,  1.25it/s]\u001b[A\n",
      "batch 183, training loss: 4.2331: : 183it [02:08,  1.27it/s]\u001b[A\n",
      "batch 184, training loss: 4.0215: : 183it [02:09,  1.27it/s]\u001b[A\n",
      "batch 184, training loss: 4.0215: : 184it [02:09,  1.26it/s]\u001b[A\n",
      "batch 185, training loss: 4.033: : 184it [02:10,  1.26it/s] \u001b[A\n",
      "batch 185, training loss: 4.033: : 185it [02:10,  1.26it/s]\u001b[A\n",
      "batch 186, training loss: 4.1802: : 185it [02:11,  1.26it/s]\u001b[A\n",
      "batch 186, training loss: 4.1802: : 186it [02:11,  1.25it/s]\u001b[A\n",
      "batch 187, training loss: 4.1616: : 186it [02:12,  1.25it/s]\u001b[A\n",
      "batch 187, training loss: 4.1616: : 187it [02:12,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 4.272: : 187it [02:12,  1.24it/s] \u001b[A\n",
      "batch 188, training loss: 4.272: : 188it [02:12,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 4.3141: : 188it [02:13,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 4.3141: : 189it [02:13,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 3.9379: : 189it [02:14,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 3.9379: : 190it [02:14,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 3.9523: : 190it [02:15,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 3.9523: : 191it [02:15,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.0751: : 191it [02:16,  1.24it/s]\u001b[A\n",
      "batch 192, training loss: 4.0751: : 192it [02:16,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 3.9229: : 192it [02:17,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 3.9229: : 193it [02:17,  1.23it/s]\u001b[A\n",
      "batch 194, training loss: 4.0155: : 193it [02:17,  1.23it/s]\u001b[A\n",
      "batch 194, training loss: 4.0155: : 194it [02:17,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 3.9442: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 3.9442: : 195it [02:18,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.9226: : 195it [02:19,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.9226: : 196it [02:19,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.0573: : 196it [02:20,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 4.0573: : 197it [02:20,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 3.9766: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 3.9766: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.9387: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.9387: : 199it [02:21,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 4.1089: : 199it [02:22,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 4.1089: : 200it [02:22,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.8922: : 200it [02:23,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.8922: : 201it [02:23,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.755: : 201it [02:24,  1.24it/s] \u001b[A\n",
      "batch 202, training loss: 3.755: : 202it [02:24,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.9723: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.9723: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 4.024: : 203it [02:25,  1.24it/s] \u001b[A\n",
      "batch 204, training loss: 4.024: : 204it [02:25,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.9572: : 204it [02:26,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.9572: : 205it [02:26,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 4.1138: : 205it [02:27,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 4.1138: : 206it [02:27,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 3.8961: : 206it [02:28,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 3.8961: : 207it [02:28,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.9735: : 207it [02:29,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.9735: : 208it [02:29,  1.23it/s]\u001b[A\n",
      "batch 209, training loss: 3.9034: : 208it [02:29,  1.23it/s]\u001b[A\n",
      "batch 209, training loss: 3.9034: : 209it [02:29,  1.24it/s]\u001b[A\n",
      "batch 210, training loss: 3.9195: : 209it [02:30,  1.24it/s]\u001b[A\n",
      "batch 210, training loss: 3.9195: : 210it [02:30,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.9089: : 210it [02:31,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.9089: : 211it [02:31,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 3.9173: : 211it [02:32,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 3.9173: : 212it [02:32,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 4.133: : 212it [02:33,  1.24it/s] \u001b[A\n",
      "batch 213, training loss: 4.133: : 213it [02:33,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.9852: : 213it [02:34,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.9852: : 214it [02:34,  1.24it/s]\u001b[A\n",
      "batch 215, training loss: 3.8068: : 214it [02:34,  1.24it/s]\u001b[A\n",
      "batch 215, training loss: 3.8068: : 215it [02:34,  1.35it/s]\u001b[A\n",
      "batch 216, training loss: 3.9587: : 215it [02:35,  1.35it/s]\u001b[A\n",
      "batch 216, training loss: 3.9587: : 216it [02:35,  1.37it/s]\u001b[A\n",
      "batch 217, training loss: 4.0567: : 216it [02:36,  1.37it/s]\u001b[A\n",
      "batch 217, training loss: 4.0567: : 217it [02:36,  1.30it/s]\u001b[A\n",
      "batch 218, training loss: 3.9927: : 217it [02:37,  1.30it/s]\u001b[A\n",
      "batch 218, training loss: 3.9927: : 218it [02:37,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 4.1878: : 218it [02:37,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 4.1878: : 219it [02:37,  1.22it/s]\u001b[A\n",
      "batch 220, training loss: 4.0886: : 219it [02:38,  1.22it/s]\u001b[A\n",
      "batch 220, training loss: 4.0886: : 220it [02:38,  1.21it/s]\u001b[A\n",
      "batch 221, training loss: 3.9917: : 220it [02:39,  1.21it/s]\u001b[A\n",
      "batch 221, training loss: 3.9917: : 221it [02:39,  1.21it/s]\u001b[A\n",
      "batch 222, training loss: 4.0176: : 221it [02:40,  1.21it/s]\u001b[A\n",
      "batch 222, training loss: 4.0176: : 222it [02:40,  1.22it/s]\u001b[A\n",
      "batch 223, training loss: 4.0115: : 222it [02:41,  1.22it/s]\u001b[A\n",
      "batch 223, training loss: 4.0115: : 223it [02:41,  1.22it/s]\u001b[A\n",
      "batch 224, training loss: 3.9181: : 223it [02:41,  1.22it/s]\u001b[A\n",
      "batch 224, training loss: 3.9181: : 224it [02:41,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 4.0468: : 224it [02:42,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 4.0468: : 225it [02:42,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 4.0685: : 225it [02:43,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 4.0685: : 226it [02:43,  1.22it/s]\u001b[A\n",
      "batch 227, training loss: 3.923: : 226it [02:44,  1.22it/s] \u001b[A\n",
      "batch 227, training loss: 3.923: : 227it [02:44,  1.22it/s]\u001b[A\n",
      "batch 228, training loss: 3.9892: : 227it [02:45,  1.22it/s]\u001b[A\n",
      "batch 228, training loss: 3.9892: : 228it [02:45,  1.23it/s]\u001b[A\n",
      "batch 229, training loss: 3.968: : 228it [02:46,  1.23it/s] \u001b[A\n",
      "batch 229, training loss: 3.968: : 229it [02:46,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 4.0922: : 229it [02:46,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 4.0922: : 230it [02:46,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 4.1551: : 230it [02:47,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 4.1551: : 231it [02:47,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.9313: : 231it [02:48,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.9313: : 232it [02:48,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 4.0212: : 232it [02:49,  1.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 233, training loss: 4.0212: : 233it [02:49,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 4.1338: : 233it [02:50,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 4.1338: : 234it [02:50,  1.23it/s]\u001b[A\n",
      "batch 235, training loss: 3.9783: : 234it [02:50,  1.23it/s]\u001b[A\n",
      "batch 235, training loss: 3.9783: : 235it [02:50,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 4.0203: : 235it [02:51,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 4.0203: : 236it [02:51,  1.22it/s]\u001b[A\n",
      "batch 237, training loss: 3.8621: : 236it [02:52,  1.22it/s]\u001b[A\n",
      "batch 237, training loss: 3.8621: : 237it [02:52,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 4.0251: : 237it [02:53,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 4.0251: : 238it [02:53,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 3.9451: : 238it [02:54,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 3.9451: : 239it [02:54,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 3.9784: : 239it [02:54,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 3.9784: : 240it [02:54,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 4.0066: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 4.0066: : 241it [02:55,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 3.8376: : 241it [02:56,  1.24it/s]\u001b[A\n",
      "batch 242, training loss: 3.8376: : 242it [02:56,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 4.0015: : 242it [02:57,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 4.0015: : 243it [02:57,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 3.9343: : 243it [02:58,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 3.9343: : 244it [02:58,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.0354: : 244it [02:59,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 4.0354: : 245it [02:59,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.9508: : 245it [02:59,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.9508: : 246it [02:59,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 3.9493: : 246it [03:00,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 3.9493: : 247it [03:00,  1.24it/s]\u001b[A\n",
      "batch 248, training loss: 3.8926: : 247it [03:01,  1.24it/s]\u001b[A\n",
      "batch 248, training loss: 3.8926: : 248it [03:01,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.8501: : 248it [03:02,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.8501: : 249it [03:02,  1.24it/s]\u001b[A\n",
      "batch 250, training loss: 4.0552: : 249it [03:03,  1.24it/s]\u001b[A\n",
      "batch 250, training loss: 4.0552: : 250it [03:03,  1.24it/s]\u001b[A\n",
      "batch 251, training loss: 4.0504: : 250it [03:03,  1.24it/s]\u001b[A\n",
      "batch 251, training loss: 4.0504: : 251it [03:03,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 3.7297: : 251it [03:04,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 3.7297: : 252it [03:04,  1.46it/s]\u001b[A\n",
      "batch 253, training loss: 4.0563: : 252it [03:05,  1.46it/s]\u001b[A\n",
      "batch 253, training loss: 4.0563: : 253it [03:05,  1.33it/s]\u001b[A\n",
      "batch 254, training loss: 4.2372: : 253it [03:05,  1.33it/s]\u001b[A\n",
      "batch 254, training loss: 4.2372: : 254it [03:05,  1.36it/s]\u001b[A\n",
      "batch 255, training loss: 4.0366: : 254it [03:06,  1.36it/s]\u001b[A\n",
      "batch 255, training loss: 4.0366: : 255it [03:06,  1.28it/s]\u001b[A\n",
      "batch 256, training loss: 4.0165: : 255it [03:07,  1.28it/s]\u001b[A\n",
      "batch 256, training loss: 4.0165: : 256it [03:07,  1.25it/s]\u001b[A\n",
      "batch 257, training loss: 4.1399: : 256it [03:08,  1.25it/s]\u001b[A\n",
      "batch 257, training loss: 4.1399: : 257it [03:08,  1.20it/s]\u001b[A\n",
      "batch 258, training loss: 4.2054: : 257it [03:09,  1.20it/s]\u001b[A\n",
      "batch 258, training loss: 4.2054: : 258it [03:09,  1.17it/s]\u001b[A\n",
      "batch 259, training loss: 4.0873: : 258it [03:10,  1.17it/s]\u001b[A\n",
      "batch 259, training loss: 4.0873: : 259it [03:10,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 4.1068: : 259it [03:11,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 4.1068: : 260it [03:11,  1.13it/s]\u001b[A\n",
      "batch 261, training loss: 4.1748: : 260it [03:12,  1.13it/s]\u001b[A\n",
      "batch 261, training loss: 4.1748: : 261it [03:12,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 4.0571: : 261it [03:13,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 4.0571: : 262it [03:13,  1.12it/s]\u001b[A\n",
      "batch 263, training loss: 4.0369: : 262it [03:13,  1.12it/s]\u001b[A\n",
      "batch 263, training loss: 4.0369: : 263it [03:13,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 4.1395: : 263it [03:14,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 4.1395: : 264it [03:14,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.9421: : 264it [03:15,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.9421: : 265it [03:15,  1.10it/s]\u001b[A\n",
      "batch 266, training loss: 4.1331: : 265it [03:16,  1.10it/s]\u001b[A\n",
      "batch 266, training loss: 4.1331: : 266it [03:16,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 4.0643: : 266it [03:17,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 4.0643: : 267it [03:17,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.9799: : 267it [03:18,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.9799: : 268it [03:18,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.9739: : 268it [03:19,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.9739: : 269it [03:19,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 4.0024: : 269it [03:20,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 4.0024: : 270it [03:20,  1.11it/s]\u001b[A\n",
      "batch 271, training loss: 4.006: : 270it [03:21,  1.11it/s] \u001b[A\n",
      "batch 271, training loss: 4.006: : 271it [03:21,  1.10it/s]\u001b[A\n",
      "batch 272, training loss: 3.9893: : 271it [03:22,  1.10it/s]\u001b[A\n",
      "batch 272, training loss: 3.9893: : 272it [03:22,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 4.0981: : 272it [03:23,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 4.0981: : 273it [03:23,  1.09it/s]\u001b[A\n",
      "batch 274, training loss: 4.058: : 273it [03:23,  1.09it/s] \u001b[A\n",
      "batch 274, training loss: 4.058: : 274it [03:23,  1.11it/s]\u001b[A\n",
      "batch 275, training loss: 3.9223: : 274it [03:24,  1.11it/s]\u001b[A\n",
      "batch 275, training loss: 3.9223: : 275it [03:24,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.8377: : 275it [03:25,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.8377: : 276it [03:25,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 3.9722: : 276it [03:26,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 3.9722: : 277it [03:26,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.9231: : 277it [03:27,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.9231: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.9469: : 278it [03:28,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.9469: : 279it [03:28,  1.11it/s]\u001b[A\n",
      "batch 280, training loss: 3.802: : 279it [03:29,  1.11it/s] \u001b[A\n",
      "batch 280, training loss: 3.802: : 280it [03:29,  1.10it/s]\u001b[A\n",
      "batch 281, training loss: 3.9028: : 280it [03:30,  1.10it/s]\u001b[A\n",
      "batch 281, training loss: 3.9028: : 281it [03:30,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.8254: : 281it [03:31,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.8254: : 282it [03:31,  1.09it/s]\u001b[A\n",
      "batch 283, training loss: 3.9102: : 282it [03:32,  1.09it/s]\u001b[A\n",
      "batch 283, training loss: 3.9102: : 283it [03:32,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.8727: : 283it [03:33,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.8727: : 284it [03:33,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 3.931: : 284it [03:33,  1.11it/s] \u001b[A\n",
      "batch 285, training loss: 3.931: : 285it [03:33,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 4.0845: : 285it [03:34,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 4.0845: : 286it [03:34,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.8097: : 286it [03:35,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.8097: : 287it [03:35,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 3.8284: : 287it [03:36,  1.09it/s]\u001b[A\n",
      "batch 288, training loss: 3.8284: : 288it [03:36,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 4.0045: : 288it [03:37,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 4.0045: : 289it [03:37,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 3.7509: : 289it [03:38,  1.11it/s]\u001b[A\n",
      "batch 290, training loss: 3.7509: : 290it [03:38,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.0327: : 290it [03:39,  1.10it/s]\u001b[A\n",
      "batch 291, training loss: 4.0327: : 291it [03:39,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 3.8211: : 291it [03:40,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 3.8211: : 292it [03:40,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 3.9257: : 292it [03:41,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 3.9257: : 293it [03:41,  1.08it/s]\u001b[A\n",
      "batch 294, training loss: 4.0078: : 293it [03:42,  1.08it/s]\u001b[A\n",
      "batch 294, training loss: 4.0078: : 294it [03:42,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.9443: : 294it [03:43,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.9443: : 295it [03:43,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.8127: : 295it [03:43,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.8127: : 296it [03:43,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 3.9601: : 296it [03:44,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 297, training loss: 3.9601: : 297it [03:44,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.8516: : 297it [03:45,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.8516: : 298it [03:45,  1.11it/s]\u001b[A\n",
      "batch 299, training loss: 3.962: : 298it [03:46,  1.11it/s] \u001b[A\n",
      "batch 299, training loss: 3.962: : 299it [03:46,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 4.0209: : 299it [03:47,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 4.0209: : 300it [03:47,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.9897: : 300it [03:48,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.9897: : 301it [03:48,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.9443: : 301it [03:49,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.9443: : 302it [03:49,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.9104: : 302it [03:50,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.9104: : 303it [03:50,  1.08it/s]\u001b[A\n",
      "batch 304, training loss: 3.9719: : 303it [03:51,  1.08it/s]\u001b[A\n",
      "batch 304, training loss: 3.9719: : 304it [03:51,  1.11it/s]\u001b[A\n",
      "batch 305, training loss: 3.8957: : 304it [03:52,  1.11it/s]\u001b[A\n",
      "batch 305, training loss: 3.8957: : 305it [03:52,  1.11it/s]\u001b[A\n",
      "batch 306, training loss: 3.9655: : 305it [03:53,  1.11it/s]\u001b[A\n",
      "batch 306, training loss: 3.9655: : 306it [03:53,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 4.0752: : 306it [03:54,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 4.0752: : 307it [03:54,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.6969: : 307it [03:54,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.6969: : 308it [03:54,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 4.0645: : 308it [03:55,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 4.0645: : 309it [03:55,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.8434: : 309it [03:56,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.8434: : 310it [03:56,  1.08it/s]\u001b[A\n",
      "batch 311, training loss: 3.9265: : 310it [03:57,  1.08it/s]\u001b[A\n",
      "batch 311, training loss: 3.9265: : 311it [03:57,  1.08it/s]\u001b[A\n",
      "batch 312, training loss: 3.7778: : 311it [03:58,  1.08it/s]\u001b[A\n",
      "batch 312, training loss: 3.7778: : 312it [03:58,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.9012: : 312it [03:59,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.9012: : 313it [03:59,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.9274: : 313it [04:00,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.9274: : 314it [04:00,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.9996: : 314it [04:01,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.9996: : 315it [04:01,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 4.1424: : 315it [04:02,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 4.1424: : 316it [04:02,  1.11it/s]\u001b[A\n",
      "batch 317, training loss: 3.8009: : 316it [04:02,  1.11it/s]\u001b[A\n",
      "batch 317, training loss: 3.8009: : 317it [04:02,  1.22it/s]\u001b[A\n",
      "batch 318, training loss: 4.1071: : 317it [04:03,  1.22it/s]\u001b[A\n",
      "batch 318, training loss: 4.1071: : 318it [04:03,  1.13it/s]\u001b[A\n",
      "batch 319, training loss: 4.1827: : 318it [04:04,  1.13it/s]\u001b[A\n",
      "batch 319, training loss: 4.1827: : 319it [04:04,  1.13it/s]\u001b[A\n",
      "batch 320, training loss: 4.1925: : 319it [04:05,  1.13it/s]\u001b[A\n",
      "batch 320, training loss: 4.1925: : 320it [04:05,  1.09it/s]\u001b[A\n",
      "batch 321, training loss: 4.2173: : 320it [04:06,  1.09it/s]\u001b[A\n",
      "batch 321, training loss: 4.2173: : 321it [04:06,  1.07it/s]\u001b[A\n",
      "batch 322, training loss: 4.2881: : 321it [04:07,  1.07it/s]\u001b[A\n",
      "batch 322, training loss: 4.2881: : 322it [04:07,  1.05it/s]\u001b[A\n",
      "batch 323, training loss: 4.0371: : 322it [04:08,  1.05it/s]\u001b[A\n",
      "batch 323, training loss: 4.0371: : 323it [04:08,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 4.1208: : 323it [04:09,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 4.1208: : 324it [04:09,  1.03it/s]\u001b[A\n",
      "batch 325, training loss: 4.1902: : 324it [04:10,  1.03it/s]\u001b[A\n",
      "batch 325, training loss: 4.1902: : 325it [04:10,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 4.1219: : 325it [04:11,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 4.1219: : 326it [04:11,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 4.2291: : 326it [04:12,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 4.2291: : 327it [04:12,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 4.093: : 327it [04:13,  1.00it/s] \u001b[A\n",
      "batch 328, training loss: 4.093: : 328it [04:13,  1.00s/it]\u001b[A\n",
      "batch 329, training loss: 3.9971: : 328it [04:14,  1.00s/it]\u001b[A\n",
      "batch 329, training loss: 3.9971: : 329it [04:14,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.0764: : 329it [04:15,  1.00it/s]\u001b[A\n",
      "batch 330, training loss: 4.0764: : 330it [04:15,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 4.1311: : 330it [04:16,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 4.1311: : 331it [04:16,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 4.0106: : 331it [04:17,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 4.0106: : 332it [04:17,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.9636: : 332it [04:18,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.9636: : 333it [04:18,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 4.077: : 333it [04:19,  1.00s/it] \u001b[A\n",
      "batch 334, training loss: 4.077: : 334it [04:19,  1.01s/it]\u001b[A\n",
      "batch 335, training loss: 4.1912: : 334it [04:20,  1.01s/it]\u001b[A\n",
      "batch 335, training loss: 4.1912: : 335it [04:20,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.0928: : 335it [04:21,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 4.0928: : 336it [04:21,  1.00s/it]\u001b[A\n",
      "batch 337, training loss: 4.1277: : 336it [04:22,  1.00s/it]\u001b[A\n",
      "batch 337, training loss: 4.1277: : 337it [04:22,  1.03it/s]\u001b[A\n",
      "batch 338, training loss: 4.0231: : 337it [04:23,  1.03it/s]\u001b[A\n",
      "batch 338, training loss: 4.0231: : 338it [04:23,  1.02it/s]\u001b[A\n",
      "batch 339, training loss: 4.0669: : 338it [04:24,  1.02it/s]\u001b[A\n",
      "batch 339, training loss: 4.0669: : 339it [04:24,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 4.2937: : 339it [04:25,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 4.2937: : 340it [04:25,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 3.9526: : 340it [04:26,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 3.9526: : 341it [04:26,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 4.0561: : 341it [04:27,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 4.0561: : 342it [04:27,  1.02it/s]\u001b[A\n",
      "batch 343, training loss: 4.0618: : 342it [04:28,  1.02it/s]\u001b[A\n",
      "batch 343, training loss: 4.0618: : 343it [04:28,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 4.1512: : 343it [04:29,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 4.1512: : 344it [04:29,  1.00s/it]\u001b[A\n",
      "batch 345, training loss: 4.08: : 344it [04:30,  1.00s/it]  \u001b[A\n",
      "batch 345, training loss: 4.08: : 345it [04:30,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 4.0574: : 345it [04:31,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 4.0574: : 346it [04:31,  1.00s/it]\u001b[A\n",
      "batch 347, training loss: 3.9649: : 346it [04:32,  1.00s/it]\u001b[A\n",
      "batch 347, training loss: 3.9649: : 347it [04:32,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 4.0381: : 347it [04:33,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 4.0381: : 348it [04:33,  1.01it/s]\u001b[A\n",
      "batch 349, training loss: 4.176: : 348it [04:34,  1.01it/s] \u001b[A\n",
      "batch 349, training loss: 4.176: : 349it [04:34,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.0455: : 349it [04:35,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 4.0455: : 350it [04:35,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 3.9319: : 350it [04:36,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 3.9319: : 351it [04:36,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 4.0154: : 351it [04:37,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 4.0154: : 352it [04:37,  1.07it/s]\u001b[A\n",
      "batch 353, training loss: 4.0562: : 352it [04:38,  1.07it/s]\u001b[A\n",
      "batch 353, training loss: 4.0562: : 353it [04:38,  1.04it/s]\u001b[A\n",
      "batch 354, training loss: 4.1093: : 353it [04:39,  1.04it/s]\u001b[A\n",
      "batch 354, training loss: 4.1093: : 354it [04:39,  1.03it/s]\u001b[A\n",
      "batch 355, training loss: 4.0615: : 354it [04:40,  1.03it/s]\u001b[A\n",
      "batch 355, training loss: 4.0615: : 355it [04:40,  1.01it/s]\u001b[A\n",
      "batch 356, training loss: 3.8992: : 355it [04:41,  1.01it/s]\u001b[A\n",
      "batch 356, training loss: 3.8992: : 356it [04:41,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.8906: : 356it [04:42,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.8906: : 357it [04:42,  1.00it/s]\u001b[A\n",
      "batch 358, training loss: 4.1051: : 357it [04:43,  1.00it/s]\u001b[A\n",
      "batch 358, training loss: 4.1051: : 358it [04:43,  1.01s/it]\u001b[A\n",
      "batch 359, training loss: 3.919: : 358it [04:44,  1.01s/it] \u001b[A\n",
      "batch 359, training loss: 3.919: : 359it [04:44,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 4.0504: : 359it [04:45,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 4.0504: : 360it [04:45,  1.01s/it]\u001b[A\n",
      "batch 361, training loss: 4.0261: : 360it [04:46,  1.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 361, training loss: 4.0261: : 361it [04:46,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 4.1408: : 361it [04:47,  1.01s/it]\u001b[A\n",
      "batch 362, training loss: 4.1408: : 362it [04:47,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 4.0218: : 362it [04:48,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 4.0218: : 363it [04:48,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 3.9035: : 363it [04:49,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 3.9035: : 364it [04:49,  1.02it/s]\u001b[A\n",
      "batch 365, training loss: 3.9067: : 364it [04:50,  1.02it/s]\u001b[A\n",
      "batch 365, training loss: 3.9067: : 365it [04:50,  1.01it/s]\u001b[A\n",
      "batch 366, training loss: 4.0569: : 365it [04:51,  1.01it/s]\u001b[A\n",
      "batch 366, training loss: 4.0569: : 366it [04:51,  1.00it/s]\u001b[A\n",
      "batch 367, training loss: 3.9516: : 366it [04:52,  1.00it/s]\u001b[A\n",
      "batch 367, training loss: 3.9516: : 367it [04:52,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 4.0506: : 367it [04:53,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 4.0506: : 368it [04:53,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 4.0805: : 368it [04:54,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 4.0805: : 369it [04:54,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.9387: : 369it [04:55,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.9387: : 370it [04:55,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 4.0149: : 370it [04:56,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 4.0149: : 371it [04:56,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.9623: : 371it [04:57,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.9623: : 372it [04:57,  1.00s/it]\u001b[A\n",
      "batch 373, training loss: 3.982: : 372it [04:58,  1.00s/it] \u001b[A\n",
      "batch 373, training loss: 3.982: : 373it [04:58,  1.00it/s]\u001b[A\n",
      "batch 374, training loss: 3.9863: : 373it [04:59,  1.00it/s]\u001b[A\n",
      "batch 374, training loss: 3.9863: : 374it [04:59,  1.02it/s]\u001b[A\n",
      "batch 375, training loss: 3.8792: : 374it [05:00,  1.02it/s]\u001b[A\n",
      "batch 375, training loss: 3.8792: : 375it [05:00,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 4.0877: : 375it [05:01,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 4.0877: : 376it [05:01,  1.04it/s]\u001b[A\n",
      "batch 377, training loss: 4.1954: : 376it [05:02,  1.04it/s]\u001b[A\n",
      "batch 377, training loss: 4.1954: : 377it [05:02,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 4.0897: : 377it [05:03,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 4.0897: : 378it [05:03,  1.02s/it]\u001b[A\n",
      "batch 379, training loss: 4.0288: : 378it [05:04,  1.02s/it]\u001b[A\n",
      "batch 379, training loss: 4.0288: : 379it [05:04,  1.06s/it]\u001b[A\n",
      "batch 380, training loss: 4.0738: : 379it [05:05,  1.06s/it]\u001b[A\n",
      "batch 380, training loss: 4.0738: : 380it [05:05,  1.03s/it]\u001b[A\n",
      "batch 381, training loss: 4.1474: : 380it [05:06,  1.03s/it]\u001b[A\n",
      "batch 381, training loss: 4.1474: : 381it [05:06,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 4.0003: : 381it [05:07,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 4.0003: : 382it [05:07,  1.07s/it]\u001b[A\n",
      "batch 383, training loss: 4.0412: : 382it [05:08,  1.07s/it]\u001b[A\n",
      "batch 383, training loss: 4.0412: : 383it [05:08,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.1757: : 383it [05:09,  1.07s/it]\u001b[A\n",
      "batch 384, training loss: 4.1757: : 384it [05:09,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 4.0672: : 384it [05:11,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 4.0672: : 385it [05:11,  1.10s/it]\u001b[A\n",
      "batch 386, training loss: 4.0031: : 385it [05:12,  1.10s/it]\u001b[A\n",
      "batch 386, training loss: 4.0031: : 386it [05:12,  1.08s/it]\u001b[A\n",
      "batch 387, training loss: 4.104: : 386it [05:13,  1.08s/it] \u001b[A\n",
      "batch 387, training loss: 4.104: : 387it [05:13,  1.10s/it]\u001b[A\n",
      "batch 388, training loss: 3.8327: : 387it [05:14,  1.10s/it]\u001b[A\n",
      "batch 388, training loss: 3.8327: : 388it [05:14,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 4.0341: : 388it [05:15,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 4.0341: : 389it [05:15,  1.13s/it]\u001b[A\n",
      "batch 390, training loss: 4.1428: : 389it [05:16,  1.13s/it]\u001b[A\n",
      "batch 390, training loss: 4.1428: : 390it [05:16,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 4.134: : 390it [05:17,  1.11s/it] \u001b[A\n",
      "batch 391, training loss: 4.134: : 391it [05:17,  1.11s/it]\u001b[A\n",
      "batch 392, training loss: 4.0815: : 391it [05:18,  1.11s/it]\u001b[A\n",
      "batch 392, training loss: 4.0815: : 392it [05:18,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.9184: : 392it [05:19,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.9184: : 393it [05:19,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 3.8547: : 393it [05:21,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 3.8547: : 394it [05:21,  1.11s/it]\u001b[A\n",
      "batch 395, training loss: 3.9419: : 394it [05:22,  1.11s/it]\u001b[A\n",
      "batch 395, training loss: 3.9419: : 395it [05:22,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 4.1387: : 395it [05:23,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 4.1387: : 396it [05:23,  1.10s/it]\u001b[A\n",
      "batch 397, training loss: 3.8962: : 396it [05:24,  1.10s/it]\u001b[A\n",
      "batch 397, training loss: 3.8962: : 397it [05:24,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 4.0191: : 397it [05:25,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 4.0191: : 398it [05:25,  1.11s/it]\u001b[A\n",
      "batch 399, training loss: 4.1179: : 398it [05:26,  1.11s/it]\u001b[A\n",
      "batch 399, training loss: 4.1179: : 399it [05:26,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.8825: : 399it [05:27,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.8825: : 400it [05:27,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 3.8523: : 400it [05:28,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 3.8523: : 401it [05:28,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.9237: : 401it [05:29,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.9237: : 402it [05:29,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 4.1098: : 402it [05:31,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 4.1098: : 403it [05:31,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.7173: : 403it [05:32,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.7173: : 404it [05:32,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 3.9929: : 404it [05:33,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 3.9929: : 405it [05:33,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.9008: : 405it [05:34,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.9008: : 406it [05:34,  1.10s/it]\u001b[A\n",
      "batch 407, training loss: 3.9687: : 406it [05:35,  1.10s/it]\u001b[A\n",
      "batch 407, training loss: 3.9687: : 407it [05:35,  1.11s/it]\u001b[A\n",
      "batch 408, training loss: 3.8118: : 407it [05:36,  1.11s/it]\u001b[A\n",
      "batch 408, training loss: 3.8118: : 408it [05:36,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 4.0347: : 408it [05:37,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 4.0347: : 409it [05:37,  1.10s/it]\u001b[A\n",
      "batch 410, training loss: 3.8703: : 409it [05:38,  1.10s/it]\u001b[A\n",
      "batch 410, training loss: 3.8703: : 410it [05:38,  1.08s/it]\u001b[A\n",
      "batch 411, training loss: 3.9613: : 410it [05:39,  1.08s/it]\u001b[A\n",
      "batch 411, training loss: 3.9613: : 411it [05:39,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.9652: : 411it [05:40,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.9652: : 412it [05:40,  1.08s/it]\u001b[A\n",
      "batch 413, training loss: 3.9272: : 412it [05:41,  1.08s/it]\u001b[A\n",
      "batch 413, training loss: 3.9272: : 413it [05:41,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.8165: : 413it [05:43,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.8165: : 414it [05:43,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.732: : 414it [05:44,  1.11s/it] \u001b[A\n",
      "batch 415, training loss: 3.732: : 415it [05:44,  1.10s/it]\u001b[A\n",
      "batch 416, training loss: 3.7646: : 415it [05:45,  1.10s/it]\u001b[A\n",
      "batch 416, training loss: 3.7646: : 416it [05:45,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 4.0664: : 416it [05:46,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 4.0664: : 417it [05:46,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.9457: : 417it [05:47,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.9457: : 418it [05:47,  1.08s/it]\u001b[A\n",
      "batch 419, training loss: 3.76: : 418it [05:48,  1.08s/it]  \u001b[A\n",
      "batch 419, training loss: 3.76: : 419it [05:48,  1.08s/it]\u001b[A\n",
      "batch 420, training loss: 3.8659: : 419it [05:49,  1.08s/it]\u001b[A\n",
      "batch 420, training loss: 3.8659: : 420it [05:49,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.9546: : 420it [05:50,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.9546: : 421it [05:50,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.9698: : 421it [05:51,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.9698: : 422it [05:51,  1.09s/it]\u001b[A\n",
      "batch 423, training loss: 3.7925: : 422it [05:52,  1.09s/it]\u001b[A\n",
      "batch 423, training loss: 3.7925: : 423it [05:52,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 4.0341: : 423it [05:54,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 4.0341: : 424it [05:54,  1.12s/it]\u001b[A\n",
      "batch 425, training loss: 4.1255: : 424it [05:55,  1.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 425, training loss: 4.1255: : 425it [05:55,  1.17s/it]\u001b[A\n",
      "batch 426, training loss: 4.0743: : 425it [05:56,  1.17s/it]\u001b[A\n",
      "batch 426, training loss: 4.0743: : 426it [05:56,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 4.0751: : 426it [05:57,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 4.0751: : 427it [05:57,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 4.2658: : 427it [05:58,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 4.2658: : 428it [05:58,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 4.0915: : 428it [06:00,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 4.0915: : 429it [06:00,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 4.1114: : 429it [06:01,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 4.1114: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.1599: : 430it [06:02,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 4.1599: : 431it [06:02,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 3.9904: : 431it [06:03,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 3.9904: : 432it [06:03,  1.21s/it]\u001b[A\n",
      "batch 433, training loss: 4.055: : 432it [06:04,  1.21s/it] \u001b[A\n",
      "batch 433, training loss: 4.055: : 433it [06:04,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.0224: : 433it [06:06,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 4.0224: : 434it [06:06,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 4.0105: : 434it [06:07,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 4.0105: : 435it [06:07,  1.23s/it]\u001b[A\n",
      "batch 436, training loss: 4.0397: : 435it [06:08,  1.23s/it]\u001b[A\n",
      "batch 436, training loss: 4.0397: : 436it [06:08,  1.23s/it]\u001b[A\n",
      "batch 437, training loss: 3.969: : 436it [06:09,  1.23s/it] \u001b[A\n",
      "batch 437, training loss: 3.969: : 437it [06:09,  1.11s/it]\u001b[A\n",
      "batch 438, training loss: 4.0472: : 437it [06:10,  1.11s/it]\u001b[A\n",
      "batch 438, training loss: 4.0472: : 438it [06:10,  1.15s/it]\u001b[A\n",
      "batch 439, training loss: 4.1062: : 438it [06:11,  1.15s/it]\u001b[A\n",
      "batch 439, training loss: 4.1062: : 439it [06:11,  1.14s/it]\u001b[A\n",
      "batch 440, training loss: 3.9533: : 439it [06:12,  1.14s/it]\u001b[A\n",
      "batch 440, training loss: 3.9533: : 440it [06:12,  1.13s/it]\u001b[A\n",
      "batch 441, training loss: 4.2069: : 440it [06:14,  1.13s/it]\u001b[A\n",
      "batch 441, training loss: 4.2069: : 441it [06:14,  1.16s/it]\u001b[A\n",
      "batch 442, training loss: 3.9039: : 441it [06:15,  1.16s/it]\u001b[A\n",
      "batch 442, training loss: 3.9039: : 442it [06:15,  1.17s/it]\u001b[A\n",
      "batch 443, training loss: 3.9704: : 442it [06:16,  1.17s/it]\u001b[A\n",
      "batch 443, training loss: 3.9704: : 443it [06:16,  1.20s/it]\u001b[A\n",
      "batch 444, training loss: 3.9636: : 443it [06:17,  1.20s/it]\u001b[A\n",
      "batch 444, training loss: 3.9636: : 444it [06:17,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 4.0127: : 444it [06:19,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 4.0127: : 445it [06:19,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 4.0878: : 445it [06:20,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 4.0878: : 446it [06:20,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.9776: : 446it [06:21,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.9776: : 447it [06:21,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.8699: : 447it [06:22,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.8699: : 448it [06:22,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 4.0125: : 448it [06:24,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 4.0125: : 449it [06:24,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 4.0187: : 449it [06:25,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 4.0187: : 450it [06:25,  1.23s/it]\u001b[A\n",
      "batch 451, training loss: 4.0444: : 450it [06:26,  1.23s/it]\u001b[A\n",
      "batch 451, training loss: 4.0444: : 451it [06:26,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 4.0404: : 451it [06:27,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 4.0404: : 452it [06:27,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.1161: : 452it [06:29,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.1161: : 453it [06:29,  1.24s/it]\u001b[A\n",
      "batch 454, training loss: 3.9748: : 453it [06:30,  1.24s/it]\u001b[A\n",
      "batch 454, training loss: 3.9748: : 454it [06:30,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.8474: : 454it [06:31,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.8474: : 455it [06:31,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.9029: : 455it [06:32,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.9029: : 456it [06:32,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 4.0325: : 456it [06:33,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 4.0325: : 457it [06:33,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.9068: : 457it [06:35,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.9068: : 458it [06:35,  1.22s/it]\u001b[A\n",
      "batch 459, training loss: 3.941: : 458it [06:36,  1.22s/it] \u001b[A\n",
      "batch 459, training loss: 3.941: : 459it [06:36,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.9487: : 459it [06:37,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.9487: : 460it [06:37,  1.21s/it]\u001b[A\n",
      "batch 461, training loss: 4.0389: : 460it [06:38,  1.21s/it]\u001b[A\n",
      "batch 461, training loss: 4.0389: : 461it [06:38,  1.21s/it]\u001b[A\n",
      "batch 462, training loss: 3.9573: : 461it [06:39,  1.21s/it]\u001b[A\n",
      "batch 462, training loss: 3.9573: : 462it [06:39,  1.21s/it]\u001b[A\n",
      "batch 463, training loss: 3.8157: : 462it [06:41,  1.21s/it]\u001b[A\n",
      "batch 463, training loss: 3.8157: : 463it [06:41,  1.21s/it]\u001b[A\n",
      "batch 464, training loss: 3.96: : 463it [06:42,  1.21s/it]  \u001b[A\n",
      "batch 464, training loss: 3.96: : 464it [06:42,  1.21s/it]\u001b[A\n",
      "batch 465, training loss: 3.9601: : 464it [06:43,  1.21s/it]\u001b[A\n",
      "batch 465, training loss: 3.9601: : 465it [06:43,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 3.9111: : 465it [06:44,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 3.9111: : 466it [06:44,  1.19s/it]\u001b[A\n",
      "batch 467, training loss: 3.8421: : 466it [06:45,  1.19s/it]\u001b[A\n",
      "batch 467, training loss: 3.8421: : 467it [06:45,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 3.9968: : 467it [06:47,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 3.9968: : 468it [06:47,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.9164: : 468it [06:48,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.9164: : 469it [06:48,  1.22s/it]\u001b[A\n",
      "batch 470, training loss: 4.0379: : 469it [06:49,  1.22s/it]\u001b[A\n",
      "batch 470, training loss: 4.0379: : 470it [06:49,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 4.0291: : 470it [06:50,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 4.0291: : 471it [06:50,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 3.9606: : 471it [06:52,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 3.9606: : 472it [06:52,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 3.9714: : 472it [06:53,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 3.9714: : 473it [06:53,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 4.0382: : 473it [06:54,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 4.0382: : 474it [06:54,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 3.9604: : 474it [06:55,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 3.9604: : 475it [06:55,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 4.0343: : 475it [06:57,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 4.0343: : 476it [06:57,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.8591: : 476it [06:58,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.8591: : 477it [06:58,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.9213: : 477it [06:59,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.9213: : 478it [06:59,  1.22s/it]\u001b[A\n",
      "batch 479, training loss: 3.8928: : 478it [07:00,  1.22s/it]\u001b[A\n",
      "batch 479, training loss: 3.8928: : 479it [07:00,  1.24s/it]\u001b[A\n",
      "batch 480, training loss: 3.9117: : 479it [07:02,  1.24s/it]\u001b[A\n",
      "batch 480, training loss: 3.9117: : 480it [07:02,  1.24s/it]\u001b[A\n",
      "batch 481, training loss: 3.957: : 480it [07:03,  1.24s/it] \u001b[A\n",
      "batch 481, training loss: 3.957: : 481it [07:03,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.9421: : 481it [07:04,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.9421: : 482it [07:04,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.8633: : 482it [07:05,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.8633: : 483it [07:05,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.9268: : 483it [07:07,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.9268: : 484it [07:07,  1.24s/it]\u001b[A\n",
      "batch 485, training loss: 3.8718: : 484it [07:08,  1.24s/it]\u001b[A\n",
      "batch 485, training loss: 3.8718: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.9546: : 485it [07:09,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.9546: : 486it [07:09,  1.25s/it]\u001b[A\n",
      "batch 487, training loss: 4.0811: : 486it [07:10,  1.25s/it]\u001b[A\n",
      "batch 487, training loss: 4.0811: : 487it [07:10,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.8453: : 487it [07:12,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.8453: : 488it [07:12,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 3.7822: : 488it [07:13,  1.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 489, training loss: 3.7822: : 489it [07:13,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 3.8757: : 489it [07:14,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 3.8757: : 490it [07:14,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.8759: : 490it [07:16,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.8759: : 491it [07:16,  1.27s/it]\u001b[A\n",
      "batch 492, training loss: 3.788: : 491it [07:17,  1.27s/it] \u001b[A\n",
      "batch 492, training loss: 3.788: : 492it [07:17,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.9715: : 492it [07:18,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.9715: : 493it [07:18,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 3.9799: : 493it [07:19,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 3.9799: : 494it [07:19,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.7978: : 494it [07:21,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.7978: : 495it [07:21,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 3.7988: : 495it [07:22,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 3.7988: : 496it [07:22,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.7376: : 496it [07:23,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.7376: : 497it [07:23,  1.27s/it]\u001b[A\n",
      "batch 498, training loss: 3.8267: : 497it [07:24,  1.27s/it]\u001b[A\n",
      "batch 498, training loss: 3.8267: : 498it [07:24,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.8727: : 498it [07:25,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.8727: : 499it [07:25,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 4.085: : 499it [07:26,  1.12s/it] \u001b[A\n",
      "batch 500, training loss: 4.085: : 500it [07:26,  1.19s/it]\u001b[A\n",
      "batch 501, training loss: 3.922: : 500it [07:28,  1.19s/it]\u001b[A\n",
      "batch 501, training loss: 3.922: : 501it [07:28,  1.21s/it]\u001b[A\n",
      "batch 502, training loss: 3.9192: : 501it [07:29,  1.21s/it]\u001b[A\n",
      "batch 502, training loss: 3.9192: : 502it [07:29,  1.24s/it]\u001b[A\n",
      "batch 503, training loss: 4.0033: : 502it [07:30,  1.24s/it]\u001b[A\n",
      "batch 503, training loss: 4.0033: : 503it [07:30,  1.25s/it]\u001b[A\n",
      "batch 504, training loss: 3.9752: : 503it [07:32,  1.25s/it]\u001b[A\n",
      "batch 504, training loss: 3.9752: : 504it [07:32,  1.28s/it]\u001b[A\n",
      "batch 505, training loss: 4.0715: : 504it [07:33,  1.28s/it]\u001b[A\n",
      "batch 505, training loss: 4.0715: : 505it [07:33,  1.27s/it]\u001b[A\n",
      "batch 506, training loss: 3.9934: : 505it [07:34,  1.27s/it]\u001b[A\n",
      "batch 506, training loss: 3.9934: : 506it [07:34,  1.28s/it]\u001b[A\n",
      "batch 507, training loss: 3.8362: : 506it [07:35,  1.28s/it]\u001b[A\n",
      "batch 507, training loss: 3.8362: : 507it [07:35,  1.27s/it]\u001b[A\n",
      "batch 508, training loss: 4.0234: : 507it [07:37,  1.27s/it]\u001b[A\n",
      "batch 508, training loss: 4.0234: : 508it [07:37,  1.31s/it]\u001b[A\n",
      "batch 509, training loss: 3.8968: : 508it [07:38,  1.31s/it]\u001b[A\n",
      "batch 509, training loss: 3.8968: : 509it [07:38,  1.34s/it]\u001b[A\n",
      "batch 510, training loss: 3.9424: : 509it [07:40,  1.34s/it]\u001b[A\n",
      "batch 510, training loss: 3.9424: : 510it [07:40,  1.34s/it]\u001b[A\n",
      "batch 511, training loss: 3.9582: : 510it [07:41,  1.34s/it]\u001b[A\n",
      "batch 511, training loss: 3.9582: : 511it [07:41,  1.36s/it]\u001b[A\n",
      "batch 512, training loss: 3.9507: : 511it [07:42,  1.36s/it]\u001b[A\n",
      "batch 512, training loss: 3.9507: : 512it [07:42,  1.35s/it]\u001b[A\n",
      "batch 513, training loss: 3.9883: : 512it [07:44,  1.35s/it]\u001b[A\n",
      "batch 513, training loss: 3.9883: : 513it [07:44,  1.32s/it]\u001b[A\n",
      "batch 514, training loss: 3.8464: : 513it [07:45,  1.32s/it]\u001b[A\n",
      "batch 514, training loss: 3.8464: : 514it [07:45,  1.34s/it]\u001b[A\n",
      "batch 515, training loss: 4.0365: : 514it [07:46,  1.34s/it]\u001b[A\n",
      "batch 515, training loss: 4.0365: : 515it [07:46,  1.34s/it]\u001b[A\n",
      "batch 516, training loss: 3.9175: : 515it [07:48,  1.34s/it]\u001b[A\n",
      "batch 516, training loss: 3.9175: : 516it [07:48,  1.37s/it]\u001b[A\n",
      "batch 517, training loss: 3.903: : 516it [07:49,  1.37s/it] \u001b[A\n",
      "batch 517, training loss: 3.903: : 517it [07:49,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.9492: : 517it [07:51,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.9492: : 518it [07:51,  1.37s/it]\u001b[A\n",
      "batch 519, training loss: 3.7762: : 518it [07:52,  1.37s/it]\u001b[A\n",
      "batch 519, training loss: 3.7762: : 519it [07:52,  1.38s/it]\u001b[A\n",
      "batch 520, training loss: 3.9473: : 519it [07:53,  1.38s/it]\u001b[A\n",
      "batch 520, training loss: 3.9473: : 520it [07:53,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.9418: : 520it [07:55,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.9418: : 521it [07:55,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 4.0086: : 521it [07:56,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 4.0086: : 522it [07:56,  1.34s/it]\u001b[A\n",
      "batch 523, training loss: 3.8337: : 522it [07:57,  1.34s/it]\u001b[A\n",
      "batch 523, training loss: 3.8337: : 523it [07:57,  1.35s/it]\u001b[A\n",
      "batch 524, training loss: 4.049: : 523it [07:59,  1.35s/it] \u001b[A\n",
      "batch 524, training loss: 4.049: : 524it [07:59,  1.36s/it]\u001b[A\n",
      "batch 525, training loss: 4.0632: : 524it [08:00,  1.36s/it]\u001b[A\n",
      "batch 525, training loss: 4.0632: : 525it [08:00,  1.36s/it]\u001b[A\n",
      "batch 526, training loss: 3.9057: : 525it [08:01,  1.36s/it]\u001b[A\n",
      "batch 526, training loss: 3.9057: : 526it [08:01,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.8493: : 526it [08:03,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.8493: : 527it [08:03,  1.31s/it]\u001b[A\n",
      "batch 528, training loss: 3.9428: : 527it [08:04,  1.31s/it]\u001b[A\n",
      "batch 528, training loss: 3.9428: : 528it [08:04,  1.34s/it]\u001b[A\n",
      "batch 529, training loss: 3.9502: : 528it [08:05,  1.34s/it]\u001b[A\n",
      "batch 529, training loss: 3.9502: : 529it [08:05,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 3.9416: : 529it [08:07,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 3.9416: : 530it [08:07,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 3.8801: : 530it [08:08,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 3.8801: : 531it [08:08,  1.40s/it]\u001b[A\n",
      "batch 532, training loss: 3.7055: : 531it [08:10,  1.40s/it]\u001b[A\n",
      "batch 532, training loss: 3.7055: : 532it [08:10,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.8989: : 532it [08:11,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.8989: : 533it [08:11,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.0194: : 533it [08:13,  1.45s/it]\u001b[A\n",
      "batch 534, training loss: 4.0194: : 534it [08:13,  1.46s/it]\u001b[A\n",
      "batch 535, training loss: 3.883: : 534it [08:14,  1.46s/it] \u001b[A\n",
      "batch 535, training loss: 3.883: : 535it [08:14,  1.42s/it]\u001b[A\n",
      "batch 536, training loss: 3.7145: : 535it [08:16,  1.42s/it]\u001b[A\n",
      "batch 536, training loss: 3.7145: : 536it [08:16,  1.45s/it]\u001b[A\n",
      "batch 537, training loss: 3.7773: : 536it [08:17,  1.45s/it]\u001b[A\n",
      "batch 537, training loss: 3.7773: : 537it [08:17,  1.47s/it]\u001b[A\n",
      "batch 538, training loss: 3.923: : 537it [08:19,  1.47s/it] \u001b[A\n",
      "batch 538, training loss: 3.923: : 538it [08:19,  1.45s/it]\u001b[A\n",
      "batch 539, training loss: 3.8827: : 538it [08:20,  1.45s/it]\u001b[A\n",
      "batch 539, training loss: 3.8827: : 539it [08:20,  1.43s/it]\u001b[A\n",
      "batch 540, training loss: 3.8407: : 539it [08:22,  1.43s/it]\u001b[A\n",
      "batch 540, training loss: 3.8407: : 540it [08:22,  1.46s/it]\u001b[A\n",
      "batch 541, training loss: 3.9102: : 540it [08:23,  1.46s/it]\u001b[A\n",
      "batch 541, training loss: 3.9102: : 541it [08:23,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.9573: : 541it [08:25,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.9573: : 542it [08:25,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 3.825: : 542it [08:26,  1.48s/it] \u001b[A\n",
      "batch 543, training loss: 3.825: : 543it [08:26,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 3.8777: : 543it [08:27,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 3.8777: : 544it [08:27,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.8916: : 544it [08:29,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.8916: : 545it [08:29,  1.48s/it]\u001b[A\n",
      "batch 546, training loss: 3.786: : 545it [08:30,  1.48s/it] \u001b[A\n",
      "batch 546, training loss: 3.786: : 546it [08:30,  1.49s/it]\u001b[A\n",
      "batch 547, training loss: 3.6664: : 546it [08:32,  1.49s/it]\u001b[A\n",
      "batch 547, training loss: 3.6664: : 547it [08:32,  1.48s/it]\u001b[A\n",
      "batch 548, training loss: 3.8148: : 547it [08:33,  1.48s/it]\u001b[A\n",
      "batch 548, training loss: 3.8148: : 548it [08:33,  1.28s/it]\u001b[A\n",
      "batch 549, training loss: 3.8541: : 548it [08:34,  1.28s/it]\u001b[A\n",
      "batch 549, training loss: 3.8541: : 549it [08:34,  1.33s/it]\u001b[A\n",
      "batch 550, training loss: 3.7503: : 549it [08:36,  1.33s/it]\u001b[A\n",
      "batch 550, training loss: 3.7503: : 550it [08:36,  1.40s/it]\u001b[A\n",
      "batch 551, training loss: 3.8374: : 550it [08:37,  1.40s/it]\u001b[A\n",
      "batch 551, training loss: 3.8374: : 551it [08:37,  1.46s/it]\u001b[A\n",
      "batch 552, training loss: 3.8644: : 551it [08:39,  1.46s/it]\u001b[A\n",
      "batch 552, training loss: 3.8644: : 552it [08:39,  1.51s/it]\u001b[A\n",
      "batch 553, training loss: 3.8357: : 552it [08:41,  1.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 553, training loss: 3.8357: : 553it [08:41,  1.55s/it]\u001b[A\n",
      "batch 554, training loss: 3.5862: : 553it [08:42,  1.55s/it]\u001b[A\n",
      "batch 554, training loss: 3.5862: : 554it [08:42,  1.56s/it]\u001b[A\n",
      "batch 555, training loss: 3.7736: : 554it [08:44,  1.56s/it]\u001b[A\n",
      "batch 555, training loss: 3.7736: : 555it [08:44,  1.56s/it]\u001b[A\n",
      "batch 556, training loss: 3.8995: : 555it [08:45,  1.56s/it]\u001b[A\n",
      "batch 556, training loss: 3.8995: : 556it [08:45,  1.55s/it]\u001b[A\n",
      "batch 557, training loss: 3.7393: : 556it [08:47,  1.55s/it]\u001b[A\n",
      "batch 557, training loss: 3.7393: : 557it [08:47,  1.51s/it]\u001b[A\n",
      "batch 558, training loss: 3.6604: : 557it [08:48,  1.51s/it]\u001b[A\n",
      "batch 558, training loss: 3.6604: : 558it [08:48,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.7586: : 558it [08:50,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.7586: : 559it [08:50,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.7778: : 559it [08:52,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.7778: : 560it [08:52,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 3.7846: : 560it [08:53,  1.58s/it]\u001b[A\n",
      "batch 561, training loss: 3.7846: : 561it [08:53,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 3.5825: : 561it [08:55,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 3.5825: : 562it [08:55,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.7633: : 562it [08:56,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.7633: : 563it [08:56,  1.60s/it]\u001b[A\n",
      "batch 564, training loss: 3.6725: : 563it [08:58,  1.60s/it]\u001b[A\n",
      "batch 564, training loss: 3.6725: : 564it [08:58,  1.59s/it]\u001b[A\n",
      "batch 565, training loss: 3.7645: : 564it [08:59,  1.59s/it]\u001b[A\n",
      "batch 565, training loss: 3.7645: : 565it [08:59,  1.54s/it]\u001b[A\n",
      "batch 566, training loss: 3.8978: : 565it [09:01,  1.54s/it]\u001b[A\n",
      "batch 566, training loss: 3.8978: : 566it [09:01,  1.58s/it]\u001b[A\n",
      "batch 567, training loss: 3.8335: : 566it [09:03,  1.58s/it]\u001b[A\n",
      "batch 567, training loss: 3.8335: : 567it [09:03,  1.57s/it]\u001b[A\n",
      "batch 568, training loss: 3.9357: : 567it [09:04,  1.57s/it]\u001b[A\n",
      "batch 568, training loss: 3.9357: : 568it [09:04,  1.60s/it]\u001b[A\n",
      "batch 569, training loss: 3.8787: : 568it [09:06,  1.60s/it]\u001b[A\n",
      "batch 569, training loss: 3.8787: : 569it [09:06,  1.61s/it]\u001b[A\n",
      "batch 570, training loss: 4.0582: : 569it [09:08,  1.61s/it]\u001b[A\n",
      "batch 570, training loss: 4.0582: : 570it [09:08,  1.63s/it]\u001b[A\n",
      "batch 571, training loss: 3.8859: : 570it [09:09,  1.63s/it]\u001b[A\n",
      "batch 571, training loss: 3.8859: : 571it [09:09,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 3.9458: : 571it [09:11,  1.65s/it]\u001b[A\n",
      "batch 572, training loss: 3.9458: : 572it [09:11,  1.66s/it]\u001b[A\n",
      "batch 573, training loss: 3.8629: : 572it [09:13,  1.66s/it]\u001b[A\n",
      "batch 573, training loss: 3.8629: : 573it [09:13,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 3.9539: : 573it [09:14,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 3.9539: : 574it [09:14,  1.66s/it]\u001b[A\n",
      "batch 575, training loss: 3.7525: : 574it [09:15,  1.66s/it]\u001b[A\n",
      "batch 575, training loss: 3.7525: : 575it [09:15,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.8993: : 575it [09:17,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.8993: : 576it [09:17,  1.48s/it]\u001b[A\n",
      "batch 577, training loss: 3.7967: : 576it [09:18,  1.48s/it]\u001b[A\n",
      "batch 577, training loss: 3.7967: : 577it [09:18,  1.44s/it]\u001b[A\n",
      "batch 578, training loss: 3.764: : 577it [09:20,  1.44s/it] \u001b[A\n",
      "batch 578, training loss: 3.764: : 578it [09:20,  1.55s/it]\u001b[A\n",
      "batch 579, training loss: 3.8005: : 578it [09:22,  1.55s/it]\u001b[A\n",
      "batch 579, training loss: 3.8005: : 579it [09:22,  1.61s/it]\u001b[A\n",
      "batch 580, training loss: 3.7235: : 579it [09:23,  1.61s/it]\u001b[A\n",
      "batch 580, training loss: 3.7235: : 580it [09:23,  1.63s/it]\u001b[A\n",
      "batch 581, training loss: 3.7118: : 580it [09:25,  1.63s/it]\u001b[A\n",
      "batch 581, training loss: 3.7118: : 581it [09:25,  1.67s/it]\u001b[A\n",
      "batch 582, training loss: 3.7424: : 581it [09:27,  1.67s/it]\u001b[A\n",
      "batch 582, training loss: 3.7424: : 582it [09:27,  1.69s/it]\u001b[A\n",
      "batch 583, training loss: 3.6062: : 582it [09:27,  1.69s/it]\u001b[A\n",
      "batch 583, training loss: 3.6062: : 583it [09:27,  1.36s/it]\u001b[A\n",
      "batch 584, training loss: 3.9675: : 583it [09:29,  1.36s/it]\u001b[A\n",
      "batch 584, training loss: 3.9675: : 584it [09:29,  1.51s/it]\u001b[A\n",
      "batch 585, training loss: 3.9526: : 584it [09:31,  1.51s/it]\u001b[A\n",
      "batch 585, training loss: 3.9526: : 585it [09:31,  1.62s/it]\u001b[A\n",
      "batch 586, training loss: 4.0374: : 585it [09:33,  1.62s/it]\u001b[A\n",
      "batch 586, training loss: 4.0374: : 586it [09:33,  1.70s/it]\u001b[A\n",
      "batch 587, training loss: 3.9138: : 586it [09:35,  1.70s/it]\u001b[A\n",
      "batch 587, training loss: 3.9138: : 587it [09:35,  1.75s/it]\u001b[A\n",
      "batch 588, training loss: 3.7731: : 587it [09:37,  1.75s/it]\u001b[A\n",
      "batch 588, training loss: 3.7731: : 588it [09:37,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.8662: : 588it [09:39,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.8662: : 589it [09:39,  1.83s/it]\u001b[A\n",
      "batch 590, training loss: 3.8641: : 589it [09:41,  1.83s/it]\u001b[A\n",
      "batch 590, training loss: 3.8641: : 590it [09:41,  1.86s/it]\u001b[A\n",
      "batch 591, training loss: 3.716: : 590it [09:43,  1.86s/it] \u001b[A\n",
      "batch 591, training loss: 3.716: : 591it [09:43,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 3.7784: : 591it [09:44,  1.89s/it]\u001b[A\n",
      "batch 592, training loss: 3.7784: : 592it [09:44,  1.88s/it]\u001b[A\n",
      "batch 593, training loss: 3.6961: : 592it [09:46,  1.88s/it]\u001b[A\n",
      "batch 593, training loss: 3.6961: : 593it [09:46,  1.92s/it]\u001b[A\n",
      "batch 594, training loss: 3.8574: : 593it [09:49,  1.92s/it]\u001b[A\n",
      "batch 594, training loss: 3.8574: : 594it [09:49,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.9882: : 594it [09:50,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.9882: : 595it [09:50,  1.88s/it]\u001b[A\n",
      "batch 596, training loss: 3.8491: : 595it [09:52,  1.88s/it]\u001b[A\n",
      "batch 596, training loss: 3.8491: : 596it [09:52,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.6648: : 596it [09:54,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.6648: : 597it [09:54,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.9101: : 597it [09:57,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.9101: : 598it [09:57,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.8591: : 598it [09:58,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.8591: : 599it [09:58,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.6923: : 599it [10:00,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.6923: : 600it [10:00,  1.99s/it]\u001b[A\n",
      "batch 601, training loss: 3.0717: : 600it [10:01,  1.99s/it]\u001b[A\n",
      "batch 601, training loss: 3.0717: : 601it [10:01,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 3.7541: : 601it [10:03,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 3.7541: : 602it [10:03,  1.76s/it]\u001b[A\n",
      "batch 603, training loss: 3.7548: : 602it [10:05,  1.76s/it]\u001b[A\n",
      "batch 603, training loss: 3.7548: : 603it [10:05,  1.80s/it]\u001b[A\n",
      "batch 604, training loss: 3.8556: : 603it [10:07,  1.80s/it]\u001b[A\n",
      "batch 604, training loss: 3.8556: : 604it [10:07,  1.78s/it]\u001b[A\n",
      "batch 605, training loss: 3.9677: : 604it [10:09,  1.78s/it]\u001b[A\n",
      "batch 605, training loss: 3.9677: : 605it [10:09,  1.76s/it]\u001b[A\n",
      "batch 606, training loss: 3.772: : 605it [10:10,  1.76s/it] \u001b[A\n",
      "batch 606, training loss: 3.772: : 606it [10:10,  1.70s/it]\u001b[A\n",
      "batch 607, training loss: 3.6832: : 606it [10:12,  1.70s/it]\u001b[A\n",
      "batch 607, training loss: 3.6832: : 607it [10:12,  1.67s/it]\u001b[A\n",
      "batch 608, training loss: 3.9591: : 607it [10:13,  1.67s/it]\u001b[A\n",
      "batch 608, training loss: 3.9591: : 608it [10:13,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 3.9073: : 608it [10:14,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 3.9073: : 609it [10:14,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 3.4154: : 609it [10:16,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 3.4154: : 610it [10:16,  1.44s/it]\u001b[A\n",
      "batch 611, training loss: 3.4113: : 610it [10:17,  1.44s/it]\u001b[A\n",
      "batch 611, training loss: 3.4113: : 611it [10:17,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.6705: : 611it [10:18,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.6705: : 612it [10:18,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.5375: : 612it [10:19,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.5375: : 613it [10:19,  1.23s/it]\u001b[A\n",
      "batch 613, training loss: 3.5375: : 614it [10:19,  1.12it/s]\u001b[A\n",
      "batch 613, training loss: 3.5375: : 615it [10:19,  1.51it/s]\u001b[A\n",
      "batch 613, training loss: 3.5375: : 616it [10:20,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.9597: : 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, dev loss: 3.9597: : 1it [00:00,  2.87it/s]\u001b[A\n",
      "batch 1, dev loss: 4.1718: : 1it [00:00,  2.87it/s]\u001b[A\n",
      "batch 1, dev loss: 4.1718: : 2it [00:00,  3.62it/s]\u001b[A\n",
      "batch 2, dev loss: 3.7707: : 2it [00:00,  3.62it/s]\u001b[A\n",
      "batch 2, dev loss: 3.7707: : 3it [00:00,  4.12it/s]\u001b[A\n",
      "batch 3, dev loss: 3.8839: : 3it [00:00,  4.12it/s]\u001b[A\n",
      "batch 3, dev loss: 3.8839: : 4it [00:00,  4.37it/s]\u001b[A\n",
      "batch 4, dev loss: 3.9251: : 4it [00:01,  4.37it/s]\u001b[A\n",
      "batch 4, dev loss: 3.9251: : 5it [00:01,  4.42it/s]\u001b[A\n",
      "batch 5, dev loss: 3.8913: : 5it [00:01,  4.42it/s]\u001b[A\n",
      "batch 5, dev loss: 3.8913: : 6it [00:01,  4.42it/s]\u001b[A\n",
      "batch 6, dev loss: 4.0167: : 6it [00:01,  4.42it/s]\u001b[A\n",
      "batch 6, dev loss: 4.0167: : 7it [00:01,  4.44it/s]\u001b[A\n",
      "batch 7, dev loss: 3.7839: : 7it [00:01,  4.44it/s]\u001b[A\n",
      "batch 7, dev loss: 3.7839: : 8it [00:01,  4.97it/s]\u001b[A\n",
      "batch 8, dev loss: 4.0039: : 8it [00:02,  4.97it/s]\u001b[A\n",
      "batch 8, dev loss: 4.0039: : 9it [00:02,  4.81it/s]\u001b[A\n",
      "batch 9, dev loss: 3.8951: : 9it [00:02,  4.81it/s]\u001b[A\n",
      "batch 9, dev loss: 3.8951: : 10it [00:02,  4.62it/s]\u001b[A\n",
      "batch 10, dev loss: 3.9413: : 10it [00:02,  4.62it/s]\u001b[A\n",
      "batch 10, dev loss: 3.9413: : 11it [00:02,  4.76it/s]\u001b[A\n",
      "batch 11, dev loss: 4.0419: : 11it [00:02,  4.76it/s]\u001b[A\n",
      "batch 11, dev loss: 4.0419: : 12it [00:02,  4.66it/s]\u001b[A\n",
      "batch 12, dev loss: 3.8748: : 12it [00:02,  4.66it/s]\u001b[A\n",
      "batch 12, dev loss: 3.8748: : 13it [00:02,  4.43it/s]\u001b[A\n",
      "batch 13, dev loss: 4.0093: : 13it [00:03,  4.43it/s]\u001b[A\n",
      "batch 13, dev loss: 4.0093: : 14it [00:03,  4.43it/s]\u001b[A\n",
      "batch 14, dev loss: 4.1089: : 14it [00:03,  4.43it/s]\u001b[A\n",
      "batch 14, dev loss: 4.1089: : 15it [00:03,  4.23it/s]\u001b[A\n",
      "batch 15, dev loss: 3.9404: : 15it [00:03,  4.23it/s]\u001b[A\n",
      "batch 15, dev loss: 3.9404: : 16it [00:03,  4.60it/s]\u001b[A\n",
      "batch 16, dev loss: 4.2805: : 16it [00:03,  4.60it/s]\u001b[A\n",
      "batch 16, dev loss: 4.2805: : 17it [00:03,  4.23it/s]\u001b[A\n",
      "batch 17, dev loss: 4.0395: : 17it [00:04,  4.23it/s]\u001b[A\n",
      "batch 17, dev loss: 4.0395: : 18it [00:04,  4.01it/s]\u001b[A\n",
      "batch 18, dev loss: 3.9399: : 18it [00:04,  4.01it/s]\u001b[A\n",
      "batch 18, dev loss: 3.9399: : 19it [00:04,  3.74it/s]\u001b[A\n",
      "batch 19, dev loss: 4.142: : 19it [00:04,  3.74it/s] \u001b[A\n",
      "batch 19, dev loss: 4.142: : 20it [00:04,  3.83it/s]\u001b[A\n",
      "batch 20, dev loss: 3.9142: : 20it [00:04,  3.83it/s]\u001b[A\n",
      "batch 20, dev loss: 3.9142: : 21it [00:04,  3.84it/s]\u001b[A\n",
      "batch 21, dev loss: 3.8215: : 21it [00:05,  3.84it/s]\u001b[A\n",
      "batch 21, dev loss: 3.8215: : 22it [00:05,  3.65it/s]\u001b[A\n",
      "batch 22, dev loss: 4.0319: : 22it [00:05,  3.65it/s]\u001b[A\n",
      "batch 22, dev loss: 4.0319: : 23it [00:05,  3.53it/s]\u001b[A\n",
      "batch 23, dev loss: 4.0681: : 23it [00:05,  3.53it/s]\u001b[A\n",
      "batch 23, dev loss: 4.0681: : 24it [00:05,  4.03it/s]\u001b[A\n",
      "batch 24, dev loss: 3.9872: : 24it [00:06,  4.03it/s]\u001b[A\n",
      "batch 24, dev loss: 3.9872: : 25it [00:06,  3.98it/s]\u001b[A\n",
      "batch 25, dev loss: 3.9731: : 25it [00:06,  3.98it/s]\u001b[A\n",
      "batch 25, dev loss: 3.9731: : 26it [00:06,  3.88it/s]\u001b[A\n",
      "batch 26, dev loss: 3.9244: : 26it [00:06,  3.88it/s]\u001b[A\n",
      "batch 26, dev loss: 3.9244: : 27it [00:06,  3.53it/s]\u001b[A\n",
      "batch 27, dev loss: 3.8567: : 27it [00:06,  3.53it/s]\u001b[A\n",
      "batch 27, dev loss: 3.8567: : 28it [00:06,  3.36it/s]\u001b[A\n",
      "batch 28, dev loss: 4.0579: : 28it [00:07,  3.36it/s]\u001b[A\n",
      "batch 28, dev loss: 4.0579: : 29it [00:07,  3.22it/s]\u001b[A\n",
      "batch 29, dev loss: 3.9978: : 29it [00:07,  3.22it/s]\u001b[A\n",
      "batch 29, dev loss: 3.9978: : 30it [00:07,  3.37it/s]\u001b[A\n",
      "batch 30, dev loss: 4.232: : 30it [00:07,  3.37it/s] \u001b[A\n",
      "batch 30, dev loss: 4.232: : 31it [00:07,  3.94it/s]\u001b[A\n",
      "batch 31, dev loss: 4.0837: : 31it [00:08,  3.94it/s]\u001b[A\n",
      "batch 31, dev loss: 4.0837: : 32it [00:08,  3.56it/s]\u001b[A\n",
      "batch 32, dev loss: 4.0929: : 32it [00:08,  3.56it/s]\u001b[A\n",
      "batch 32, dev loss: 4.0929: : 33it [00:08,  3.32it/s]\u001b[A\n",
      "batch 33, dev loss: 3.8823: : 33it [00:08,  3.32it/s]\u001b[A\n",
      "batch 33, dev loss: 3.8823: : 34it [00:08,  3.19it/s]\u001b[A\n",
      "batch 34, dev loss: 4.3109: : 34it [00:09,  3.19it/s]\u001b[A\n",
      "batch 34, dev loss: 4.3109: : 35it [00:09,  3.23it/s]\u001b[A\n",
      "batch 35, dev loss: 4.0824: : 35it [00:09,  3.23it/s]\u001b[A\n",
      "batch 35, dev loss: 4.0824: : 36it [00:09,  3.07it/s]\u001b[A\n",
      "batch 36, dev loss: 4.0602: : 36it [00:09,  3.07it/s]\u001b[A\n",
      "batch 36, dev loss: 4.0602: : 37it [00:09,  3.40it/s]\u001b[A\n",
      "batch 37, dev loss: 3.8287: : 37it [00:09,  3.40it/s]\u001b[A\n",
      "batch 37, dev loss: 3.8287: : 38it [00:09,  3.17it/s]\u001b[A\n",
      "batch 38, dev loss: 4.0664: : 38it [00:10,  3.17it/s]\u001b[A\n",
      "batch 38, dev loss: 4.0664: : 39it [00:10,  3.03it/s]\u001b[A\n",
      "batch 39, dev loss: 4.0517: : 39it [00:10,  3.03it/s]\u001b[A\n",
      "batch 39, dev loss: 4.0517: : 40it [00:10,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 4.1032: : 40it [00:11,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 4.1032: : 41it [00:11,  2.89it/s]\u001b[A\n",
      "batch 41, dev loss: 3.8745: : 41it [00:11,  2.89it/s]\u001b[A\n",
      "batch 41, dev loss: 3.8745: : 42it [00:11,  2.96it/s]\u001b[A\n",
      "batch 42, dev loss: 4.0396: : 42it [00:11,  2.96it/s]\u001b[A\n",
      "batch 42, dev loss: 4.0396: : 43it [00:11,  2.79it/s]\u001b[A\n",
      "batch 43, dev loss: 4.0474: : 43it [00:12,  2.79it/s]\u001b[A\n",
      "batch 43, dev loss: 4.0474: : 44it [00:12,  2.70it/s]\u001b[A\n",
      "batch 44, dev loss: 3.9727: : 44it [00:12,  2.70it/s]\u001b[A\n",
      "batch 44, dev loss: 3.9727: : 45it [00:12,  2.65it/s]\u001b[A\n",
      "batch 45, dev loss: 4.2645: : 45it [00:12,  2.65it/s]\u001b[A\n",
      "batch 45, dev loss: 4.2645: : 46it [00:12,  2.66it/s]\u001b[A\n",
      "batch 46, dev loss: 3.8078: : 46it [00:13,  2.66it/s]\u001b[A\n",
      "batch 46, dev loss: 3.8078: : 47it [00:13,  2.53it/s]\u001b[A\n",
      "batch 47, dev loss: 3.9837: : 47it [00:13,  2.53it/s]\u001b[A\n",
      "batch 47, dev loss: 3.9837: : 48it [00:13,  2.46it/s]\u001b[A\n",
      "batch 48, dev loss: 3.8009: : 48it [00:14,  2.46it/s]\u001b[A\n",
      "batch 48, dev loss: 3.8009: : 49it [00:14,  2.53it/s]\u001b[A\n",
      "batch 49, dev loss: 3.9751: : 49it [00:14,  2.53it/s]\u001b[A\n",
      "batch 49, dev loss: 3.9751: : 50it [00:14,  2.82it/s]\u001b[A\n",
      "batch 50, dev loss: 3.9758: : 50it [00:14,  2.82it/s]\u001b[A\n",
      "batch 50, dev loss: 3.9758: : 51it [00:14,  2.63it/s]\u001b[A\n",
      "batch 51, dev loss: 3.9693: : 51it [00:15,  2.63it/s]\u001b[A\n",
      "batch 51, dev loss: 3.9693: : 52it [00:15,  2.53it/s]\u001b[A\n",
      "batch 52, dev loss: 3.7524: : 52it [00:15,  2.53it/s]\u001b[A\n",
      "batch 52, dev loss: 3.7524: : 53it [00:15,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.9765: : 53it [00:16,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.9765: : 54it [00:16,  2.33it/s]\u001b[A\n",
      "batch 54, dev loss: 3.742: : 54it [00:16,  2.33it/s] \u001b[A\n",
      "batch 54, dev loss: 3.742: : 55it [00:16,  2.29it/s]\u001b[A\n",
      "batch 55, dev loss: 3.8985: : 55it [00:17,  2.29it/s]\u001b[A\n",
      "batch 55, dev loss: 3.8985: : 56it [00:17,  2.17it/s]\u001b[A\n",
      "batch 56, dev loss: 3.7797: : 56it [00:17,  2.17it/s]\u001b[A\n",
      "batch 56, dev loss: 3.7797: : 57it [00:17,  2.30it/s]\u001b[A\n",
      "batch 57, dev loss: 3.7045: : 57it [00:18,  2.30it/s]\u001b[A\n",
      "batch 57, dev loss: 3.7045: : 58it [00:18,  2.24it/s]\u001b[A\n",
      "batch 58, dev loss: 4.0446: : 58it [00:18,  2.24it/s]\u001b[A\n",
      "batch 58, dev loss: 4.0446: : 59it [00:18,  2.18it/s]\u001b[A\n",
      "batch 59, dev loss: 3.9913: : 59it [00:18,  2.18it/s]\u001b[A\n",
      "batch 59, dev loss: 3.9913: : 60it [00:18,  2.24it/s]\u001b[A\n",
      "batch 60, dev loss: 3.6401: : 60it [00:19,  2.24it/s]\u001b[A\n",
      "batch 60, dev loss: 3.6401: : 61it [00:19,  2.28it/s]\u001b[A\n",
      "batch 61, dev loss: 3.6621: : 61it [00:19,  2.28it/s]\u001b[A\n",
      "batch 61, dev loss: 3.6621: : 62it [00:19,  2.56it/s]\u001b[A\n",
      "batch 62, dev loss: 3.5453: : 62it [00:19,  2.56it/s]\u001b[A\n",
      "batch 62, dev loss: 3.5453: : 63it [00:19,  2.68it/s]\u001b[A\n",
      "batch 63, dev loss: 4.1203: : 63it [00:20,  2.68it/s]\u001b[A\n",
      "batch 63, dev loss: 4.1203: : 64it [00:20,  2.81it/s]\u001b[A\n",
      "batch 64, dev loss: 3.7849: : 64it [00:20,  2.81it/s]\u001b[A\n",
      "batch 64, dev loss: 3.7849: : 65it [00:20,  2.74it/s]\u001b[A\n",
      "batch 65, dev loss: 3.7217: : 65it [00:21,  2.74it/s]\u001b[A\n",
      "batch 65, dev loss: 3.7217: : 66it [00:21,  2.81it/s]\u001b[A\n",
      "batch 66, dev loss: 3.7589: : 66it [00:21,  2.81it/s]\u001b[A\n",
      "batch 66, dev loss: 3.7589: : 67it [00:21,  2.96it/s]\u001b[A\n",
      "batch 67, dev loss: 3.046: : 67it [00:21,  2.96it/s] \u001b[A\n",
      "batch 67, dev loss: 3.046: : 68it [00:21,  2.96it/s]\u001b[A\n",
      "batch 68, dev loss: 2.996: : 68it [00:21,  2.96it/s]\u001b[A\n",
      "batch 68, dev loss: 2.996: : 69it [00:21,  3.11it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2359: : 69it [00:22,  3.11it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2359: : 70it [00:22,  3.04it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0248: : 70it [00:22,  3.04it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0248: : 71it [00:22,  3.03it/s]\u001b[A\n",
      "batch 71, dev loss: 3.5498: : 71it [00:22,  3.03it/s]\u001b[A\n",
      "batch 71, dev loss: 3.5498: : 72it [00:22,  3.10it/s]\u001b[A\n",
      "batch 72, dev loss: 4.3298: : 72it [00:23,  3.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 72, dev loss: 4.3298: : 73it [00:23,  3.14it/s]\u001b[A\n",
      "batch 72, dev loss: 4.3298: : 74it [00:23,  3.84it/s]\u001b[A\n",
      "batch 72, dev loss: 4.3298: : 75it [00:23,  4.65it/s]\u001b[A\n",
      "batch 72, dev loss: 4.3298: : 76it [00:23,  3.22it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.02s/it]\u001b[A\n",
      "2it [00:02,  1.26s/it]\u001b[A\n",
      "3it [00:03,  1.15s/it]\u001b[A\n",
      "4it [00:04,  1.04s/it]\u001b[A\n",
      "5it [00:05,  1.13s/it]\u001b[A\n",
      "6it [00:06,  1.11s/it]\u001b[A\n",
      "7it [00:08,  1.23s/it]\u001b[A\n",
      "8it [00:09,  1.17s/it]\u001b[A\n",
      "9it [00:11,  1.40s/it]\u001b[A\n",
      "10it [00:12,  1.43s/it]\u001b[A\n",
      "11it [00:14,  1.52s/it]\u001b[A\n",
      "12it [00:16,  1.59s/it]\u001b[A\n",
      "13it [00:17,  1.52s/it]\u001b[A\n",
      "14it [00:18,  1.52s/it]\u001b[A\n",
      "15it [00:20,  1.58s/it]\u001b[A\n",
      "16it [00:21,  1.43s/it]\u001b[A\n",
      "17it [00:23,  1.49s/it]\u001b[A\n",
      "18it [00:25,  1.69s/it]\u001b[A\n",
      "19it [00:27,  1.83s/it]\u001b[A\n",
      "20it [00:29,  1.78s/it]\u001b[A\n",
      "21it [00:31,  1.89s/it]\u001b[A\n",
      "22it [00:33,  1.84s/it]\u001b[A\n",
      "23it [00:35,  1.89s/it]\u001b[A\n",
      "24it [00:35,  1.47s/it]\u001b[A\n",
      "25it [00:38,  1.76s/it]\u001b[A\n",
      "26it [00:40,  1.85s/it]\u001b[A\n",
      "27it [00:41,  1.76s/it]\u001b[A\n",
      "28it [00:43,  1.77s/it]\u001b[A\n",
      "29it [00:46,  2.07s/it]\u001b[A\n",
      "30it [00:47,  1.90s/it]\u001b[A\n",
      "31it [00:50,  2.09s/it]\u001b[A\n",
      "32it [00:52,  2.16s/it]\u001b[A\n",
      "33it [00:55,  2.24s/it]\u001b[A\n",
      "34it [00:57,  2.32s/it]\u001b[A\n",
      "35it [01:00,  2.33s/it]\u001b[A\n",
      "36it [01:00,  1.79s/it]\u001b[A\n",
      "37it [01:03,  2.17s/it]\u001b[A\n",
      "38it [01:06,  2.39s/it]\u001b[A\n",
      "39it [01:09,  2.60s/it]\u001b[A\n",
      "40it [01:12,  2.74s/it]\u001b[A\n",
      "41it [01:13,  2.12s/it]\u001b[A\n",
      "42it [01:16,  2.40s/it]\u001b[A\n",
      "43it [01:19,  2.64s/it]\u001b[A\n",
      "44it [01:23,  2.94s/it]\u001b[A\n",
      "45it [01:25,  2.81s/it]\u001b[A\n",
      "46it [01:29,  3.05s/it]\u001b[A\n",
      "47it [01:32,  3.23s/it]\u001b[A\n",
      "48it [01:36,  3.38s/it]\u001b[A\n",
      "49it [01:36,  2.43s/it]\u001b[A\n",
      "50it [01:40,  2.87s/it]\u001b[A\n",
      "51it [01:45,  3.35s/it]\u001b[A\n",
      "52it [01:47,  3.14s/it]\u001b[A\n",
      "53it [01:52,  3.62s/it]\u001b[A\n",
      "54it [01:56,  3.55s/it]\u001b[A\n",
      "55it [02:00,  3.89s/it]\u001b[A\n",
      "56it [02:03,  3.49s/it]\u001b[A\n",
      "57it [02:06,  3.54s/it]\u001b[A\n",
      "58it [02:10,  3.45s/it]\u001b[A\n",
      "59it [02:12,  3.17s/it]\u001b[A\n",
      "60it [02:14,  2.83s/it]\u001b[A\n",
      "61it [02:16,  2.41s/it]\u001b[A\n",
      "62it [02:17,  2.09s/it]\u001b[A\n",
      "63it [02:18,  1.84s/it]\u001b[A\n",
      "64it [02:19,  1.55s/it]\u001b[A\n",
      "65it [02:20,  1.29s/it]\u001b[A\n",
      "66it [02:20,  1.05s/it]\u001b[A\n",
      "67it [02:21,  1.17it/s]\u001b[A\n",
      "68it [02:21,  1.33it/s]\u001b[A\n",
      "69it [02:22,  1.47it/s]\u001b[A\n",
      "70it [02:22,  2.04s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▍                                   | 425/6740 [00:00<00:01, 4227.44it/s]\u001b[A\n",
      " 13%|████▉                                 | 878/6740 [00:00<00:01, 4400.94it/s]\u001b[A\n",
      " 20%|███████▏                             | 1319/6740 [00:00<00:01, 4183.46it/s]\u001b[A\n",
      " 26%|█████████▌                           | 1739/6740 [00:00<00:01, 4086.44it/s]\u001b[A\n",
      " 32%|███████████▊                         | 2149/6740 [00:00<00:01, 4079.50it/s]\u001b[A\n",
      " 38%|██████████████                       | 2558/6740 [00:00<00:01, 3904.86it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 2950/6740 [00:00<00:00, 3848.51it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 3336/6740 [00:00<00:00, 3756.86it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 3715/6740 [00:00<00:00, 3765.62it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 4093/6740 [00:01<00:00, 3602.16it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 4455/6740 [00:01<00:00, 3597.79it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 4816/6740 [00:01<00:00, 3590.79it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 5176/6740 [00:01<00:00, 3514.95it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 5529/6740 [00:01<00:00, 3467.13it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 5893/6740 [00:01<00:00, 3517.02it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 6246/6740 [00:01<00:00, 3506.55it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 3696.01it/s]\u001b[A\n",
      "Epoch: 5, tfr: 1.0, loss(train/dev): 3.9338/3.9187, ppl(dev/test): 50.335/57.575\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.826: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.826: : 1it [00:01,  1.87s/it]\u001b[A\n",
      "batch 2, training loss: 3.7699: : 1it [00:02,  1.87s/it]\u001b[A\n",
      "batch 2, training loss: 3.7699: : 2it [00:02,  1.16s/it]\u001b[A\n",
      "batch 3, training loss: 3.9081: : 2it [00:03,  1.16s/it]\u001b[A\n",
      "batch 3, training loss: 3.9081: : 3it [00:03,  1.05it/s]\u001b[A\n",
      "batch 4, training loss: 3.8323: : 3it [00:03,  1.05it/s]\u001b[A\n",
      "batch 4, training loss: 3.8323: : 4it [00:03,  1.17it/s]\u001b[A\n",
      "batch 5, training loss: 3.6612: : 4it [00:04,  1.17it/s]\u001b[A\n",
      "batch 5, training loss: 3.6612: : 5it [00:04,  1.32it/s]\u001b[A\n",
      "batch 6, training loss: 3.8722: : 5it [00:04,  1.32it/s]\u001b[A\n",
      "batch 6, training loss: 3.8722: : 6it [00:04,  1.52it/s]\u001b[A\n",
      "batch 7, training loss: 3.858: : 6it [00:05,  1.52it/s] \u001b[A\n",
      "batch 7, training loss: 3.858: : 7it [00:05,  1.53it/s]\u001b[A\n",
      "batch 8, training loss: 3.8955: : 7it [00:06,  1.53it/s]\u001b[A\n",
      "batch 8, training loss: 3.8955: : 8it [00:06,  1.61it/s]\u001b[A\n",
      "batch 9, training loss: 3.739: : 8it [00:06,  1.61it/s] \u001b[A\n",
      "batch 9, training loss: 3.739: : 9it [00:06,  1.69it/s]\u001b[A\n",
      "batch 10, training loss: 3.751: : 9it [00:07,  1.69it/s]\u001b[A\n",
      "batch 10, training loss: 3.751: : 10it [00:07,  1.63it/s]\u001b[A\n",
      "batch 11, training loss: 3.8139: : 10it [00:08,  1.63it/s]\u001b[A\n",
      "batch 11, training loss: 3.8139: : 11it [00:08,  1.56it/s]\u001b[A\n",
      "batch 12, training loss: 3.8372: : 11it [00:08,  1.56it/s]\u001b[A\n",
      "batch 12, training loss: 3.8372: : 12it [00:08,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 3.7234: : 12it [00:09,  1.52it/s]\u001b[A\n",
      "batch 13, training loss: 3.7234: : 13it [00:09,  1.57it/s]\u001b[A\n",
      "batch 14, training loss: 4.0181: : 13it [00:09,  1.57it/s]\u001b[A\n",
      "batch 14, training loss: 4.0181: : 14it [00:09,  1.60it/s]\u001b[A\n",
      "batch 15, training loss: 3.8543: : 14it [00:10,  1.60it/s]\u001b[A\n",
      "batch 15, training loss: 3.8543: : 15it [00:10,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.8538: : 15it [00:11,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.8538: : 16it [00:11,  1.57it/s]\u001b[A\n",
      "batch 17, training loss: 4.0154: : 16it [00:11,  1.57it/s]\u001b[A\n",
      "batch 17, training loss: 4.0154: : 17it [00:11,  1.52it/s]\u001b[A\n",
      "batch 18, training loss: 3.8015: : 17it [00:12,  1.52it/s]\u001b[A\n",
      "batch 18, training loss: 3.8015: : 18it [00:12,  1.50it/s]\u001b[A\n",
      "batch 19, training loss: 3.598: : 18it [00:13,  1.50it/s] \u001b[A\n",
      "batch 19, training loss: 3.598: : 19it [00:13,  1.61it/s]\u001b[A\n",
      "batch 20, training loss: 3.7512: : 19it [00:13,  1.61it/s]\u001b[A\n",
      "batch 20, training loss: 3.7512: : 20it [00:13,  1.64it/s]\u001b[A\n",
      "batch 21, training loss: 3.8899: : 20it [00:14,  1.64it/s]\u001b[A\n",
      "batch 21, training loss: 3.8899: : 21it [00:14,  1.60it/s]\u001b[A\n",
      "batch 22, training loss: 3.6744: : 21it [00:15,  1.60it/s]\u001b[A\n",
      "batch 22, training loss: 3.6744: : 22it [00:15,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 3.8245: : 22it [00:15,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 3.8245: : 23it [00:15,  1.58it/s]\u001b[A\n",
      "batch 24, training loss: 3.7533: : 23it [00:16,  1.58it/s]\u001b[A\n",
      "batch 24, training loss: 3.7533: : 24it [00:16,  1.66it/s]\u001b[A\n",
      "batch 25, training loss: 3.8298: : 24it [00:16,  1.66it/s]\u001b[A\n",
      "batch 25, training loss: 3.8298: : 25it [00:16,  1.63it/s]\u001b[A\n",
      "batch 26, training loss: 3.6537: : 25it [00:17,  1.63it/s]\u001b[A\n",
      "batch 26, training loss: 3.6537: : 26it [00:17,  1.59it/s]\u001b[A\n",
      "batch 27, training loss: 3.7634: : 26it [00:18,  1.59it/s]\u001b[A\n",
      "batch 27, training loss: 3.7634: : 27it [00:18,  1.53it/s]\u001b[A\n",
      "batch 28, training loss: 3.6566: : 27it [00:18,  1.53it/s]\u001b[A\n",
      "batch 28, training loss: 3.6566: : 28it [00:18,  1.53it/s]\u001b[A\n",
      "batch 29, training loss: 3.7921: : 28it [00:19,  1.53it/s]\u001b[A\n",
      "batch 29, training loss: 3.7921: : 29it [00:19,  1.48it/s]\u001b[A\n",
      "batch 30, training loss: 3.9181: : 29it [00:20,  1.48it/s]\u001b[A\n",
      "batch 30, training loss: 3.9181: : 30it [00:20,  1.47it/s]\u001b[A\n",
      "batch 31, training loss: 3.6619: : 30it [00:20,  1.47it/s]\u001b[A\n",
      "batch 31, training loss: 3.6619: : 31it [00:20,  1.58it/s]\u001b[A\n",
      "batch 32, training loss: 3.8415: : 31it [00:21,  1.58it/s]\u001b[A\n",
      "batch 32, training loss: 3.8415: : 32it [00:21,  1.62it/s]\u001b[A\n",
      "batch 33, training loss: 3.7471: : 32it [00:22,  1.62it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 33, training loss: 3.7471: : 33it [00:22,  1.59it/s]\u001b[A\n",
      "batch 34, training loss: 3.7349: : 33it [00:22,  1.59it/s]\u001b[A\n",
      "batch 34, training loss: 3.7349: : 34it [00:22,  1.57it/s]\u001b[A\n",
      "batch 35, training loss: 3.8243: : 34it [00:23,  1.57it/s]\u001b[A\n",
      "batch 35, training loss: 3.8243: : 35it [00:23,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 3.893: : 35it [00:23,  1.57it/s] \u001b[A\n",
      "batch 36, training loss: 3.893: : 36it [00:23,  1.61it/s]\u001b[A\n",
      "batch 37, training loss: 3.7463: : 36it [00:24,  1.61it/s]\u001b[A\n",
      "batch 37, training loss: 3.7463: : 37it [00:24,  1.68it/s]\u001b[A\n",
      "batch 38, training loss: 3.557: : 37it [00:25,  1.68it/s] \u001b[A\n",
      "batch 38, training loss: 3.557: : 38it [00:25,  1.63it/s]\u001b[A\n",
      "batch 39, training loss: 3.6999: : 38it [00:25,  1.63it/s]\u001b[A\n",
      "batch 39, training loss: 3.6999: : 39it [00:25,  1.56it/s]\u001b[A\n",
      "batch 40, training loss: 3.7978: : 39it [00:26,  1.56it/s]\u001b[A\n",
      "batch 40, training loss: 3.7978: : 40it [00:26,  1.51it/s]\u001b[A\n",
      "batch 41, training loss: 3.9436: : 40it [00:27,  1.51it/s]\u001b[A\n",
      "batch 41, training loss: 3.9436: : 41it [00:27,  1.52it/s]\u001b[A\n",
      "batch 42, training loss: 3.791: : 41it [00:27,  1.52it/s] \u001b[A\n",
      "batch 42, training loss: 3.791: : 42it [00:27,  1.56it/s]\u001b[A\n",
      "batch 43, training loss: 3.5825: : 42it [00:28,  1.56it/s]\u001b[A\n",
      "batch 43, training loss: 3.5825: : 43it [00:28,  1.62it/s]\u001b[A\n",
      "batch 44, training loss: 3.5765: : 43it [00:29,  1.62it/s]\u001b[A\n",
      "batch 44, training loss: 3.5765: : 44it [00:29,  1.60it/s]\u001b[A\n",
      "batch 45, training loss: 3.5915: : 44it [00:29,  1.60it/s]\u001b[A\n",
      "batch 45, training loss: 3.5915: : 45it [00:29,  1.55it/s]\u001b[A\n",
      "batch 46, training loss: 3.7384: : 45it [00:30,  1.55it/s]\u001b[A\n",
      "batch 46, training loss: 3.7384: : 46it [00:30,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.7401: : 46it [00:31,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.7401: : 47it [00:31,  1.53it/s]\u001b[A\n",
      "batch 48, training loss: 3.6698: : 47it [00:31,  1.53it/s]\u001b[A\n",
      "batch 48, training loss: 3.6698: : 48it [00:31,  1.58it/s]\u001b[A\n",
      "batch 49, training loss: 3.9159: : 48it [00:32,  1.58it/s]\u001b[A\n",
      "batch 49, training loss: 3.9159: : 49it [00:32,  1.67it/s]\u001b[A\n",
      "batch 50, training loss: 3.6604: : 49it [00:32,  1.67it/s]\u001b[A\n",
      "batch 50, training loss: 3.6604: : 50it [00:32,  1.64it/s]\u001b[A\n",
      "batch 51, training loss: 3.7453: : 50it [00:33,  1.64it/s]\u001b[A\n",
      "batch 51, training loss: 3.7453: : 51it [00:33,  1.57it/s]\u001b[A\n",
      "batch 52, training loss: 3.6245: : 51it [00:34,  1.57it/s]\u001b[A\n",
      "batch 52, training loss: 3.6245: : 52it [00:34,  1.55it/s]\u001b[A\n",
      "batch 53, training loss: 3.6331: : 52it [00:34,  1.55it/s]\u001b[A\n",
      "batch 53, training loss: 3.6331: : 53it [00:34,  1.58it/s]\u001b[A\n",
      "batch 54, training loss: 3.5816: : 53it [00:35,  1.58it/s]\u001b[A\n",
      "batch 54, training loss: 3.5816: : 54it [00:35,  1.59it/s]\u001b[A\n",
      "batch 55, training loss: 3.7182: : 54it [00:35,  1.59it/s]\u001b[A\n",
      "batch 55, training loss: 3.7182: : 55it [00:35,  1.60it/s]\u001b[A\n",
      "batch 56, training loss: 3.7501: : 55it [00:36,  1.60it/s]\u001b[A\n",
      "batch 56, training loss: 3.7501: : 56it [00:36,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 3.7013: : 56it [00:37,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 3.7013: : 57it [00:37,  1.54it/s]\u001b[A\n",
      "batch 58, training loss: 3.7646: : 57it [00:38,  1.54it/s]\u001b[A\n",
      "batch 58, training loss: 3.7646: : 58it [00:38,  1.50it/s]\u001b[A\n",
      "batch 59, training loss: 3.6145: : 58it [00:38,  1.50it/s]\u001b[A\n",
      "batch 59, training loss: 3.6145: : 59it [00:38,  1.52it/s]\u001b[A\n",
      "batch 60, training loss: 3.5708: : 59it [00:39,  1.52it/s]\u001b[A\n",
      "batch 60, training loss: 3.5708: : 60it [00:39,  1.60it/s]\u001b[A\n",
      "batch 61, training loss: 3.8346: : 60it [00:39,  1.60it/s]\u001b[A\n",
      "batch 61, training loss: 3.8346: : 61it [00:39,  1.64it/s]\u001b[A\n",
      "batch 62, training loss: 3.6447: : 61it [00:40,  1.64it/s]\u001b[A\n",
      "batch 62, training loss: 3.6447: : 62it [00:40,  1.60it/s]\u001b[A\n",
      "batch 63, training loss: 3.752: : 62it [00:41,  1.60it/s] \u001b[A\n",
      "batch 63, training loss: 3.752: : 63it [00:41,  1.56it/s]\u001b[A\n",
      "batch 64, training loss: 3.699: : 63it [00:41,  1.56it/s]\u001b[A\n",
      "batch 64, training loss: 3.699: : 64it [00:41,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.7779: : 64it [00:42,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.7779: : 65it [00:42,  1.48it/s]\u001b[A\n",
      "batch 66, training loss: 3.7785: : 65it [00:43,  1.48it/s]\u001b[A\n",
      "batch 66, training loss: 3.7785: : 66it [00:43,  1.50it/s]\u001b[A\n",
      "batch 67, training loss: 3.6586: : 66it [00:43,  1.50it/s]\u001b[A\n",
      "batch 67, training loss: 3.6586: : 67it [00:43,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 3.7674: : 67it [00:44,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 3.7674: : 68it [00:44,  1.62it/s]\u001b[A\n",
      "batch 69, training loss: 3.6542: : 68it [00:44,  1.62it/s]\u001b[A\n",
      "batch 69, training loss: 3.6542: : 69it [00:44,  1.59it/s]\u001b[A\n",
      "batch 70, training loss: 3.8333: : 69it [00:45,  1.59it/s]\u001b[A\n",
      "batch 70, training loss: 3.8333: : 70it [00:45,  1.54it/s]\u001b[A\n",
      "batch 71, training loss: 3.6747: : 70it [00:46,  1.54it/s]\u001b[A\n",
      "batch 71, training loss: 3.6747: : 71it [00:46,  1.51it/s]\u001b[A\n",
      "batch 72, training loss: 3.6578: : 71it [00:47,  1.51it/s]\u001b[A\n",
      "batch 72, training loss: 3.6578: : 72it [00:47,  1.50it/s]\u001b[A\n",
      "batch 73, training loss: 3.6973: : 72it [00:47,  1.50it/s]\u001b[A\n",
      "batch 73, training loss: 3.6973: : 73it [00:47,  1.61it/s]\u001b[A\n",
      "batch 74, training loss: 3.581: : 73it [00:48,  1.61it/s] \u001b[A\n",
      "batch 74, training loss: 3.581: : 74it [00:48,  1.64it/s]\u001b[A\n",
      "batch 75, training loss: 3.8157: : 74it [00:48,  1.64it/s]\u001b[A\n",
      "batch 75, training loss: 3.8157: : 75it [00:48,  1.62it/s]\u001b[A\n",
      "batch 76, training loss: 3.6293: : 75it [00:49,  1.62it/s]\u001b[A\n",
      "batch 76, training loss: 3.6293: : 76it [00:49,  1.58it/s]\u001b[A\n",
      "batch 77, training loss: 3.7624: : 76it [00:49,  1.58it/s]\u001b[A\n",
      "batch 77, training loss: 3.7624: : 77it [00:49,  1.69it/s]\u001b[A\n",
      "batch 78, training loss: 3.7583: : 77it [00:50,  1.69it/s]\u001b[A\n",
      "batch 78, training loss: 3.7583: : 78it [00:50,  1.70it/s]\u001b[A\n",
      "batch 79, training loss: 3.7807: : 78it [00:51,  1.70it/s]\u001b[A\n",
      "batch 79, training loss: 3.7807: : 79it [00:51,  1.66it/s]\u001b[A\n",
      "batch 80, training loss: 3.6816: : 79it [00:51,  1.66it/s]\u001b[A\n",
      "batch 80, training loss: 3.6816: : 80it [00:51,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.7028: : 80it [00:52,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.7028: : 81it [00:52,  1.57it/s]\u001b[A\n",
      "batch 82, training loss: 3.8449: : 81it [00:53,  1.57it/s]\u001b[A\n",
      "batch 82, training loss: 3.8449: : 82it [00:53,  1.59it/s]\u001b[A\n",
      "batch 83, training loss: 3.6983: : 82it [00:53,  1.59it/s]\u001b[A\n",
      "batch 83, training loss: 3.6983: : 83it [00:53,  1.59it/s]\u001b[A\n",
      "batch 84, training loss: 3.839: : 83it [00:54,  1.59it/s] \u001b[A\n",
      "batch 84, training loss: 3.839: : 84it [00:54,  1.58it/s]\u001b[A\n",
      "batch 85, training loss: 3.8862: : 84it [00:55,  1.58it/s]\u001b[A\n",
      "batch 85, training loss: 3.8862: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.7882: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.7882: : 86it [00:55,  1.51it/s]\u001b[A\n",
      "batch 87, training loss: 3.8544: : 86it [00:56,  1.51it/s]\u001b[A\n",
      "batch 87, training loss: 3.8544: : 87it [00:56,  1.51it/s]\u001b[A\n",
      "batch 88, training loss: 3.962: : 87it [00:57,  1.51it/s] \u001b[A\n",
      "batch 88, training loss: 3.962: : 88it [00:57,  1.45it/s]\u001b[A\n",
      "batch 89, training loss: 4.061: : 88it [00:57,  1.45it/s]\u001b[A\n",
      "batch 89, training loss: 4.061: : 89it [00:57,  1.38it/s]\u001b[A\n",
      "batch 90, training loss: 3.97: : 89it [00:58,  1.38it/s] \u001b[A\n",
      "batch 90, training loss: 3.97: : 90it [00:58,  1.35it/s]\u001b[A\n",
      "batch 91, training loss: 4.0967: : 90it [00:59,  1.35it/s]\u001b[A\n",
      "batch 91, training loss: 4.0967: : 91it [00:59,  1.35it/s]\u001b[A\n",
      "batch 92, training loss: 3.9432: : 91it [01:00,  1.35it/s]\u001b[A\n",
      "batch 92, training loss: 3.9432: : 92it [01:00,  1.33it/s]\u001b[A\n",
      "batch 93, training loss: 3.8962: : 92it [01:01,  1.33it/s]\u001b[A\n",
      "batch 93, training loss: 3.8962: : 93it [01:01,  1.32it/s]\u001b[A\n",
      "batch 94, training loss: 3.9869: : 93it [01:01,  1.32it/s]\u001b[A\n",
      "batch 94, training loss: 3.9869: : 94it [01:01,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.9736: : 94it [01:02,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.9736: : 95it [01:02,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 3.9547: : 95it [01:03,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 3.9547: : 96it [01:03,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.0303: : 96it [01:04,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 4.0303: : 97it [01:04,  1.30it/s]\u001b[A\n",
      "batch 98, training loss: 4.03: : 97it [01:04,  1.30it/s]  \u001b[A\n",
      "batch 98, training loss: 4.03: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.8409: : 98it [01:05,  1.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99, training loss: 3.8409: : 99it [01:05,  1.32it/s]\u001b[A\n",
      "batch 100, training loss: 3.666: : 99it [01:06,  1.32it/s]\u001b[A\n",
      "batch 100, training loss: 3.666: : 100it [01:06,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.8608: : 100it [01:07,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.8608: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.8201: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.8201: : 102it [01:07,  1.33it/s]\u001b[A\n",
      "batch 103, training loss: 3.7791: : 102it [01:08,  1.33it/s]\u001b[A\n",
      "batch 103, training loss: 3.7791: : 103it [01:08,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 3.618: : 103it [01:09,  1.31it/s] \u001b[A\n",
      "batch 104, training loss: 3.618: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.7939: : 104it [01:10,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.7939: : 105it [01:10,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.8992: : 105it [01:10,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.8992: : 106it [01:10,  1.33it/s]\u001b[A\n",
      "batch 107, training loss: 3.6112: : 106it [01:11,  1.33it/s]\u001b[A\n",
      "batch 107, training loss: 3.6112: : 107it [01:11,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.8827: : 107it [01:12,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.8827: : 108it [01:12,  1.31it/s]\u001b[A\n",
      "batch 109, training loss: 3.9426: : 108it [01:13,  1.31it/s]\u001b[A\n",
      "batch 109, training loss: 3.9426: : 109it [01:13,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 4.0621: : 109it [01:13,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 4.0621: : 110it [01:13,  1.32it/s]\u001b[A\n",
      "batch 111, training loss: 3.7496: : 110it [01:14,  1.32it/s]\u001b[A\n",
      "batch 111, training loss: 3.7496: : 111it [01:14,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.8125: : 111it [01:15,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.8125: : 112it [01:15,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.913: : 112it [01:16,  1.30it/s] \u001b[A\n",
      "batch 113, training loss: 3.913: : 113it [01:16,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.8915: : 113it [01:17,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.8915: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 3.7885: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 3.7885: : 115it [01:17,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.7865: : 115it [01:18,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.7865: : 116it [01:18,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.8479: : 116it [01:19,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.8479: : 117it [01:19,  1.32it/s]\u001b[A\n",
      "batch 118, training loss: 3.9226: : 117it [01:20,  1.32it/s]\u001b[A\n",
      "batch 118, training loss: 3.9226: : 118it [01:20,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 3.8248: : 118it [01:20,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 3.8248: : 119it [01:20,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.7716: : 119it [01:21,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.7716: : 120it [01:21,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 4.0119: : 120it [01:22,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 4.0119: : 121it [01:22,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.6757: : 121it [01:23,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.6757: : 122it [01:23,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.8578: : 122it [01:24,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.8578: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.8438: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.8438: : 124it [01:24,  1.32it/s]\u001b[A\n",
      "batch 125, training loss: 3.8664: : 124it [01:25,  1.32it/s]\u001b[A\n",
      "batch 125, training loss: 3.8664: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.9188: : 125it [01:26,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.9188: : 126it [01:26,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 3.6388: : 126it [01:27,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 3.6388: : 127it [01:27,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 3.8434: : 127it [01:27,  1.29it/s]\u001b[A\n",
      "batch 128, training loss: 3.8434: : 128it [01:27,  1.29it/s]\u001b[A\n",
      "batch 129, training loss: 3.8046: : 128it [01:28,  1.29it/s]\u001b[A\n",
      "batch 129, training loss: 3.8046: : 129it [01:28,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 3.8344: : 129it [01:29,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 3.8344: : 130it [01:29,  1.28it/s]\u001b[A\n",
      "batch 131, training loss: 3.98: : 130it [01:30,  1.28it/s]  \u001b[A\n",
      "batch 131, training loss: 3.98: : 131it [01:30,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.7665: : 131it [01:30,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.7665: : 132it [01:30,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.6822: : 132it [01:31,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.6822: : 133it [01:31,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.8111: : 133it [01:32,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.8111: : 134it [01:32,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.7453: : 134it [01:33,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.7453: : 135it [01:33,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.7007: : 135it [01:33,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.7007: : 136it [01:33,  1.41it/s]\u001b[A\n",
      "batch 137, training loss: 3.8542: : 136it [01:34,  1.41it/s]\u001b[A\n",
      "batch 137, training loss: 3.8542: : 137it [01:34,  1.46it/s]\u001b[A\n",
      "batch 138, training loss: 3.8797: : 137it [01:35,  1.46it/s]\u001b[A\n",
      "batch 138, training loss: 3.8797: : 138it [01:35,  1.42it/s]\u001b[A\n",
      "batch 139, training loss: 3.662: : 138it [01:35,  1.42it/s] \u001b[A\n",
      "batch 139, training loss: 3.662: : 139it [01:35,  1.40it/s]\u001b[A\n",
      "batch 140, training loss: 3.7577: : 139it [01:36,  1.40it/s]\u001b[A\n",
      "batch 140, training loss: 3.7577: : 140it [01:36,  1.36it/s]\u001b[A\n",
      "batch 141, training loss: 3.7567: : 140it [01:37,  1.36it/s]\u001b[A\n",
      "batch 141, training loss: 3.7567: : 141it [01:37,  1.34it/s]\u001b[A\n",
      "batch 142, training loss: 3.7918: : 141it [01:38,  1.34it/s]\u001b[A\n",
      "batch 142, training loss: 3.7918: : 142it [01:38,  1.33it/s]\u001b[A\n",
      "batch 143, training loss: 3.6235: : 142it [01:38,  1.33it/s]\u001b[A\n",
      "batch 143, training loss: 3.6235: : 143it [01:38,  1.34it/s]\u001b[A\n",
      "batch 144, training loss: 3.6725: : 143it [01:39,  1.34it/s]\u001b[A\n",
      "batch 144, training loss: 3.6725: : 144it [01:39,  1.33it/s]\u001b[A\n",
      "batch 145, training loss: 3.7796: : 144it [01:40,  1.33it/s]\u001b[A\n",
      "batch 145, training loss: 3.7796: : 145it [01:40,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.7789: : 145it [01:41,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.7789: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.6643: : 146it [01:42,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.6643: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.7466: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.7466: : 148it [01:42,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.8576: : 148it [01:43,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.8576: : 149it [01:43,  1.29it/s]\u001b[A\n",
      "batch 150, training loss: 3.839: : 149it [01:44,  1.29it/s] \u001b[A\n",
      "batch 150, training loss: 3.839: : 150it [01:44,  1.31it/s]\u001b[A\n",
      "batch 151, training loss: 3.7698: : 150it [01:45,  1.31it/s]\u001b[A\n",
      "batch 151, training loss: 3.7698: : 151it [01:45,  1.30it/s]\u001b[A\n",
      "batch 152, training loss: 3.7039: : 151it [01:45,  1.30it/s]\u001b[A\n",
      "batch 152, training loss: 3.7039: : 152it [01:45,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 3.7379: : 152it [01:46,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 3.7379: : 153it [01:46,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.849: : 153it [01:47,  1.30it/s] \u001b[A\n",
      "batch 154, training loss: 3.849: : 154it [01:47,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 3.9652: : 154it [01:48,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 3.9652: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.5876: : 155it [01:49,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.5876: : 156it [01:49,  1.29it/s]\u001b[A\n",
      "batch 157, training loss: 3.809: : 156it [01:49,  1.29it/s] \u001b[A\n",
      "batch 157, training loss: 3.809: : 157it [01:49,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.7741: : 157it [01:50,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.7741: : 158it [01:50,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.6785: : 158it [01:51,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.6785: : 159it [01:51,  1.29it/s]\u001b[A\n",
      "batch 160, training loss: 3.851: : 159it [01:52,  1.29it/s] \u001b[A\n",
      "batch 160, training loss: 3.851: : 160it [01:52,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.6962: : 160it [01:52,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.6962: : 161it [01:52,  1.37it/s]\u001b[A\n",
      "batch 162, training loss: 3.7097: : 161it [01:53,  1.37it/s]\u001b[A\n",
      "batch 162, training loss: 3.7097: : 162it [01:53,  1.37it/s]\u001b[A\n",
      "batch 163, training loss: 3.9502: : 162it [01:54,  1.37it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 163, training loss: 3.9502: : 163it [01:54,  1.35it/s]\u001b[A\n",
      "batch 164, training loss: 3.7382: : 163it [01:55,  1.35it/s]\u001b[A\n",
      "batch 164, training loss: 3.7382: : 164it [01:55,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.7297: : 164it [01:55,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.7297: : 165it [01:55,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.7174: : 165it [01:56,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.7174: : 166it [01:56,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 3.8824: : 166it [01:57,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 3.8824: : 167it [01:57,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 3.8179: : 167it [01:57,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 3.8179: : 168it [01:57,  1.40it/s]\u001b[A\n",
      "batch 169, training loss: 3.9027: : 168it [01:58,  1.40it/s]\u001b[A\n",
      "batch 169, training loss: 3.9027: : 169it [01:58,  1.42it/s]\u001b[A\n",
      "batch 170, training loss: 3.7464: : 169it [01:59,  1.42it/s]\u001b[A\n",
      "batch 170, training loss: 3.7464: : 170it [01:59,  1.36it/s]\u001b[A\n",
      "batch 171, training loss: 3.0978: : 170it [01:59,  1.36it/s]\u001b[A\n",
      "batch 171, training loss: 3.0978: : 171it [01:59,  1.68it/s]\u001b[A\n",
      "batch 172, training loss: 4.144: : 171it [02:00,  1.68it/s] \u001b[A\n",
      "batch 172, training loss: 4.144: : 172it [02:00,  1.51it/s]\u001b[A\n",
      "batch 173, training loss: 3.9913: : 172it [02:01,  1.51it/s]\u001b[A\n",
      "batch 173, training loss: 3.9913: : 173it [02:01,  1.42it/s]\u001b[A\n",
      "batch 174, training loss: 4.169: : 173it [02:02,  1.42it/s] \u001b[A\n",
      "batch 174, training loss: 4.169: : 174it [02:02,  1.39it/s]\u001b[A\n",
      "batch 175, training loss: 4.0697: : 174it [02:02,  1.39it/s]\u001b[A\n",
      "batch 175, training loss: 4.0697: : 175it [02:02,  1.34it/s]\u001b[A\n",
      "batch 176, training loss: 4.0292: : 175it [02:03,  1.34it/s]\u001b[A\n",
      "batch 176, training loss: 4.0292: : 176it [02:03,  1.32it/s]\u001b[A\n",
      "batch 177, training loss: 4.0271: : 176it [02:04,  1.32it/s]\u001b[A\n",
      "batch 177, training loss: 4.0271: : 177it [02:04,  1.30it/s]\u001b[A\n",
      "batch 178, training loss: 4.0393: : 177it [02:05,  1.30it/s]\u001b[A\n",
      "batch 178, training loss: 4.0393: : 178it [02:05,  1.28it/s]\u001b[A\n",
      "batch 179, training loss: 4.0629: : 178it [02:06,  1.28it/s]\u001b[A\n",
      "batch 179, training loss: 4.0629: : 179it [02:06,  1.27it/s]\u001b[A\n",
      "batch 180, training loss: 4.122: : 179it [02:06,  1.27it/s] \u001b[A\n",
      "batch 180, training loss: 4.122: : 180it [02:06,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.1345: : 180it [02:07,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.1345: : 181it [02:07,  1.27it/s]\u001b[A\n",
      "batch 182, training loss: 4.033: : 181it [02:08,  1.27it/s] \u001b[A\n",
      "batch 182, training loss: 4.033: : 182it [02:08,  1.24it/s]\u001b[A\n",
      "batch 183, training loss: 4.1439: : 182it [02:09,  1.24it/s]\u001b[A\n",
      "batch 183, training loss: 4.1439: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.9209: : 183it [02:10,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.9209: : 184it [02:10,  1.26it/s]\u001b[A\n",
      "batch 185, training loss: 3.9571: : 184it [02:10,  1.26it/s]\u001b[A\n",
      "batch 185, training loss: 3.9571: : 185it [02:10,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 4.0966: : 185it [02:11,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 4.0966: : 186it [02:11,  1.26it/s]\u001b[A\n",
      "batch 187, training loss: 4.0752: : 186it [02:12,  1.26it/s]\u001b[A\n",
      "batch 187, training loss: 4.0752: : 187it [02:12,  1.25it/s]\u001b[A\n",
      "batch 188, training loss: 4.1829: : 187it [02:13,  1.25it/s]\u001b[A\n",
      "batch 188, training loss: 4.1829: : 188it [02:13,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 4.2135: : 188it [02:14,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 4.2135: : 189it [02:14,  1.25it/s]\u001b[A\n",
      "batch 190, training loss: 3.8526: : 189it [02:14,  1.25it/s]\u001b[A\n",
      "batch 190, training loss: 3.8526: : 190it [02:14,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.8618: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.8618: : 191it [02:15,  1.23it/s]\u001b[A\n",
      "batch 192, training loss: 3.9717: : 191it [02:16,  1.23it/s]\u001b[A\n",
      "batch 192, training loss: 3.9717: : 192it [02:16,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 3.8318: : 192it [02:17,  1.24it/s]\u001b[A\n",
      "batch 193, training loss: 3.8318: : 193it [02:17,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 3.923: : 193it [02:18,  1.24it/s] \u001b[A\n",
      "batch 194, training loss: 3.923: : 194it [02:18,  1.22it/s]\u001b[A\n",
      "batch 195, training loss: 3.8653: : 194it [02:18,  1.22it/s]\u001b[A\n",
      "batch 195, training loss: 3.8653: : 195it [02:18,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.818: : 195it [02:19,  1.24it/s] \u001b[A\n",
      "batch 196, training loss: 3.818: : 196it [02:19,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 3.9741: : 196it [02:20,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 3.9741: : 197it [02:20,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.8836: : 197it [02:21,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.8836: : 198it [02:21,  1.25it/s]\u001b[A\n",
      "batch 199, training loss: 3.8398: : 198it [02:22,  1.25it/s]\u001b[A\n",
      "batch 199, training loss: 3.8398: : 199it [02:22,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.0177: : 199it [02:23,  1.24it/s]\u001b[A\n",
      "batch 200, training loss: 4.0177: : 200it [02:23,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.8034: : 200it [02:23,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.8034: : 201it [02:23,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.6743: : 201it [02:24,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.6743: : 202it [02:24,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 3.8825: : 202it [02:25,  1.24it/s]\u001b[A\n",
      "batch 203, training loss: 3.8825: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.9479: : 203it [02:26,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.9479: : 204it [02:26,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.8597: : 204it [02:27,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.8597: : 205it [02:27,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 4.0224: : 205it [02:27,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 4.0224: : 206it [02:27,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.8262: : 206it [02:28,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.8262: : 207it [02:28,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.8606: : 207it [02:29,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.8606: : 208it [02:29,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 3.8258: : 208it [02:30,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 3.8258: : 209it [02:30,  1.26it/s]\u001b[A\n",
      "batch 210, training loss: 3.8335: : 209it [02:31,  1.26it/s]\u001b[A\n",
      "batch 210, training loss: 3.8335: : 210it [02:31,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.8366: : 210it [02:31,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.8366: : 211it [02:31,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 3.8119: : 211it [02:32,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 3.8119: : 212it [02:32,  1.25it/s]\u001b[A\n",
      "batch 213, training loss: 4.0345: : 212it [02:33,  1.25it/s]\u001b[A\n",
      "batch 213, training loss: 4.0345: : 213it [02:33,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.8941: : 213it [02:34,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.8941: : 214it [02:34,  1.25it/s]\u001b[A\n",
      "batch 215, training loss: 3.73: : 214it [02:35,  1.25it/s]  \u001b[A\n",
      "batch 215, training loss: 3.73: : 215it [02:35,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 3.8679: : 215it [02:35,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 3.8679: : 216it [02:35,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.9847: : 216it [02:36,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.9847: : 217it [02:36,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.9022: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.9022: : 218it [02:37,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 4.1015: : 218it [02:38,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 4.1015: : 219it [02:38,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 4.0067: : 219it [02:39,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 4.0067: : 220it [02:39,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.8823: : 220it [02:39,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.8823: : 221it [02:39,  1.24it/s]\u001b[A\n",
      "batch 222, training loss: 3.922: : 221it [02:40,  1.24it/s] \u001b[A\n",
      "batch 222, training loss: 3.922: : 222it [02:40,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.9153: : 222it [02:41,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.9153: : 223it [02:41,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.8244: : 223it [02:42,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.8244: : 224it [02:42,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.9505: : 224it [02:43,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.9505: : 225it [02:43,  1.26it/s]\u001b[A\n",
      "batch 226, training loss: 3.9858: : 225it [02:43,  1.26it/s]\u001b[A\n",
      "batch 226, training loss: 3.9858: : 226it [02:43,  1.23it/s]\u001b[A\n",
      "batch 227, training loss: 3.8272: : 226it [02:44,  1.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 227, training loss: 3.8272: : 227it [02:44,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.8953: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.8953: : 228it [02:45,  1.26it/s]\u001b[A\n",
      "batch 229, training loss: 3.8767: : 228it [02:46,  1.26it/s]\u001b[A\n",
      "batch 229, training loss: 3.8767: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.0054: : 229it [02:47,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 4.0054: : 230it [02:47,  1.26it/s]\u001b[A\n",
      "batch 231, training loss: 4.0554: : 230it [02:47,  1.26it/s]\u001b[A\n",
      "batch 231, training loss: 4.0554: : 231it [02:47,  1.25it/s]\u001b[A\n",
      "batch 232, training loss: 3.8551: : 231it [02:48,  1.25it/s]\u001b[A\n",
      "batch 232, training loss: 3.8551: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.9434: : 232it [02:49,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.9434: : 233it [02:49,  1.25it/s]\u001b[A\n",
      "batch 234, training loss: 4.0398: : 233it [02:50,  1.25it/s]\u001b[A\n",
      "batch 234, training loss: 4.0398: : 234it [02:50,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 3.8751: : 234it [02:51,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 3.8751: : 235it [02:51,  1.22it/s]\u001b[A\n",
      "batch 236, training loss: 3.9356: : 235it [02:52,  1.22it/s]\u001b[A\n",
      "batch 236, training loss: 3.9356: : 236it [02:52,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.7866: : 236it [02:52,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.7866: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.9398: : 237it [02:53,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.9398: : 238it [02:53,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 3.8507: : 238it [02:54,  1.23it/s]\u001b[A\n",
      "batch 239, training loss: 3.8507: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.8859: : 239it [02:55,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.8859: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.9102: : 240it [02:56,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.9102: : 241it [02:56,  1.23it/s]\u001b[A\n",
      "batch 242, training loss: 3.7428: : 241it [02:56,  1.23it/s]\u001b[A\n",
      "batch 242, training loss: 3.7428: : 242it [02:56,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 3.9153: : 242it [02:57,  1.24it/s]\u001b[A\n",
      "batch 243, training loss: 3.9153: : 243it [02:57,  1.23it/s]\u001b[A\n",
      "batch 244, training loss: 3.839: : 243it [02:58,  1.23it/s] \u001b[A\n",
      "batch 244, training loss: 3.839: : 244it [02:58,  1.23it/s]\u001b[A\n",
      "batch 245, training loss: 3.9436: : 244it [02:59,  1.23it/s]\u001b[A\n",
      "batch 245, training loss: 3.9436: : 245it [02:59,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 3.8746: : 245it [03:00,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 3.8746: : 246it [03:00,  1.23it/s]\u001b[A\n",
      "batch 247, training loss: 3.8597: : 246it [03:00,  1.23it/s]\u001b[A\n",
      "batch 247, training loss: 3.8597: : 247it [03:00,  1.23it/s]\u001b[A\n",
      "batch 248, training loss: 3.7998: : 247it [03:01,  1.23it/s]\u001b[A\n",
      "batch 248, training loss: 3.7998: : 248it [03:01,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.7714: : 248it [03:02,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.7714: : 249it [03:02,  1.24it/s]\u001b[A\n",
      "batch 250, training loss: 3.9623: : 249it [03:03,  1.24it/s]\u001b[A\n",
      "batch 250, training loss: 3.9623: : 250it [03:03,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 3.9593: : 250it [03:04,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 3.9593: : 251it [03:04,  1.24it/s]\u001b[A\n",
      "batch 252, training loss: 3.567: : 251it [03:04,  1.24it/s] \u001b[A\n",
      "batch 252, training loss: 3.567: : 252it [03:04,  1.57it/s]\u001b[A\n",
      "batch 253, training loss: 3.9548: : 252it [03:04,  1.57it/s]\u001b[A\n",
      "batch 253, training loss: 3.9548: : 253it [03:04,  1.66it/s]\u001b[A\n",
      "batch 254, training loss: 4.1408: : 253it [03:05,  1.66it/s]\u001b[A\n",
      "batch 254, training loss: 4.1408: : 254it [03:05,  1.43it/s]\u001b[A\n",
      "batch 255, training loss: 3.9396: : 254it [03:06,  1.43it/s]\u001b[A\n",
      "batch 255, training loss: 3.9396: : 255it [03:06,  1.32it/s]\u001b[A\n",
      "batch 256, training loss: 3.9253: : 255it [03:07,  1.32it/s]\u001b[A\n",
      "batch 256, training loss: 3.9253: : 256it [03:07,  1.24it/s]\u001b[A\n",
      "batch 257, training loss: 4.047: : 256it [03:08,  1.24it/s] \u001b[A\n",
      "batch 257, training loss: 4.047: : 257it [03:08,  1.19it/s]\u001b[A\n",
      "batch 258, training loss: 4.1176: : 257it [03:09,  1.19it/s]\u001b[A\n",
      "batch 258, training loss: 4.1176: : 258it [03:09,  1.15it/s]\u001b[A\n",
      "batch 259, training loss: 3.9934: : 258it [03:10,  1.15it/s]\u001b[A\n",
      "batch 259, training loss: 3.9934: : 259it [03:10,  1.16it/s]\u001b[A\n",
      "batch 260, training loss: 4.0175: : 259it [03:11,  1.16it/s]\u001b[A\n",
      "batch 260, training loss: 4.0175: : 260it [03:11,  1.14it/s]\u001b[A\n",
      "batch 261, training loss: 4.0984: : 260it [03:12,  1.14it/s]\u001b[A\n",
      "batch 261, training loss: 4.0984: : 261it [03:12,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.9724: : 261it [03:13,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.9724: : 262it [03:13,  1.11it/s]\u001b[A\n",
      "batch 263, training loss: 3.9536: : 262it [03:14,  1.11it/s]\u001b[A\n",
      "batch 263, training loss: 3.9536: : 263it [03:14,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 4.0474: : 263it [03:14,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 4.0474: : 264it [03:14,  1.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.8445: : 264it [03:15,  1.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.8445: : 265it [03:15,  1.09it/s]\u001b[A\n",
      "batch 266, training loss: 4.038: : 265it [03:16,  1.09it/s] \u001b[A\n",
      "batch 266, training loss: 4.038: : 266it [03:16,  1.08it/s]\u001b[A\n",
      "batch 267, training loss: 3.9845: : 266it [03:17,  1.08it/s]\u001b[A\n",
      "batch 267, training loss: 3.9845: : 267it [03:17,  1.11it/s]\u001b[A\n",
      "batch 268, training loss: 3.8845: : 267it [03:18,  1.11it/s]\u001b[A\n",
      "batch 268, training loss: 3.8845: : 268it [03:18,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.9022: : 268it [03:19,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.9022: : 269it [03:19,  1.14it/s]\u001b[A\n",
      "batch 270, training loss: 3.9215: : 269it [03:20,  1.14it/s]\u001b[A\n",
      "batch 270, training loss: 3.9215: : 270it [03:20,  1.12it/s]\u001b[A\n",
      "batch 271, training loss: 3.913: : 270it [03:21,  1.12it/s] \u001b[A\n",
      "batch 271, training loss: 3.913: : 271it [03:21,  1.13it/s]\u001b[A\n",
      "batch 272, training loss: 3.907: : 271it [03:22,  1.13it/s]\u001b[A\n",
      "batch 272, training loss: 3.907: : 272it [03:22,  1.12it/s]\u001b[A\n",
      "batch 273, training loss: 3.9974: : 272it [03:23,  1.12it/s]\u001b[A\n",
      "batch 273, training loss: 3.9974: : 273it [03:23,  1.11it/s]\u001b[A\n",
      "batch 274, training loss: 3.9651: : 273it [03:23,  1.11it/s]\u001b[A\n",
      "batch 274, training loss: 3.9651: : 274it [03:23,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.8369: : 274it [03:24,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.8369: : 275it [03:24,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.7584: : 275it [03:25,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.7584: : 276it [03:25,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 3.892: : 276it [03:26,  1.11it/s] \u001b[A\n",
      "batch 277, training loss: 3.892: : 277it [03:26,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.8306: : 277it [03:27,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.8306: : 278it [03:27,  1.10it/s]\u001b[A\n",
      "batch 279, training loss: 3.8535: : 278it [03:28,  1.10it/s]\u001b[A\n",
      "batch 279, training loss: 3.8535: : 279it [03:28,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.7125: : 279it [03:29,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.7125: : 280it [03:29,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.8124: : 280it [03:30,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.8124: : 281it [03:30,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 3.7259: : 281it [03:31,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 3.7259: : 282it [03:31,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.8201: : 282it [03:32,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.8201: : 283it [03:32,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.7899: : 283it [03:33,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.7899: : 284it [03:33,  1.10it/s]\u001b[A\n",
      "batch 285, training loss: 3.8376: : 284it [03:33,  1.10it/s]\u001b[A\n",
      "batch 285, training loss: 3.8376: : 285it [03:33,  1.12it/s]\u001b[A\n",
      "batch 286, training loss: 4.0034: : 285it [03:34,  1.12it/s]\u001b[A\n",
      "batch 286, training loss: 4.0034: : 286it [03:34,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.7151: : 286it [03:35,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.7151: : 287it [03:35,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.7353: : 287it [03:36,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.7353: : 288it [03:36,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.9214: : 288it [03:37,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.9214: : 289it [03:37,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.657: : 289it [03:38,  1.09it/s] \u001b[A\n",
      "batch 290, training loss: 3.657: : 290it [03:38,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.9385: : 290it [03:39,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 291, training loss: 3.9385: : 291it [03:39,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.7453: : 291it [03:40,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.7453: : 292it [03:40,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.8319: : 292it [03:41,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.8319: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.9266: : 293it [03:42,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.9266: : 294it [03:42,  1.09it/s]\u001b[A\n",
      "batch 295, training loss: 3.86: : 294it [03:42,  1.09it/s]  \u001b[A\n",
      "batch 295, training loss: 3.86: : 295it [03:42,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.7244: : 295it [03:43,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.7244: : 296it [03:43,  1.09it/s]\u001b[A\n",
      "batch 297, training loss: 3.888: : 296it [03:44,  1.09it/s] \u001b[A\n",
      "batch 297, training loss: 3.888: : 297it [03:44,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.7675: : 297it [03:45,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.7675: : 298it [03:45,  1.08it/s]\u001b[A\n",
      "batch 299, training loss: 3.8683: : 298it [03:46,  1.08it/s]\u001b[A\n",
      "batch 299, training loss: 3.8683: : 299it [03:46,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 3.9295: : 299it [03:47,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 3.9295: : 300it [03:47,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.8945: : 300it [03:48,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.8945: : 301it [03:48,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.8693: : 301it [03:49,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.8693: : 302it [03:49,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.808: : 302it [03:50,  1.09it/s] \u001b[A\n",
      "batch 303, training loss: 3.808: : 303it [03:50,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 3.8783: : 303it [03:51,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 3.8783: : 304it [03:51,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 3.806: : 304it [03:52,  1.09it/s] \u001b[A\n",
      "batch 305, training loss: 3.806: : 305it [03:52,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.8776: : 305it [03:53,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.8776: : 306it [03:53,  1.08it/s]\u001b[A\n",
      "batch 307, training loss: 3.9871: : 306it [03:53,  1.08it/s]\u001b[A\n",
      "batch 307, training loss: 3.9871: : 307it [03:53,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.6097: : 307it [03:54,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.6097: : 308it [03:54,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 3.9857: : 308it [03:55,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 3.9857: : 309it [03:55,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.7552: : 309it [03:56,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.7552: : 310it [03:56,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 3.8628: : 310it [03:57,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 3.8628: : 311it [03:57,  1.12it/s]\u001b[A\n",
      "batch 312, training loss: 3.6976: : 311it [03:58,  1.12it/s]\u001b[A\n",
      "batch 312, training loss: 3.6976: : 312it [03:58,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.8147: : 312it [03:59,  1.11it/s]\u001b[A\n",
      "batch 313, training loss: 3.8147: : 313it [03:59,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.8425: : 313it [04:00,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.8425: : 314it [04:00,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.9074: : 314it [04:01,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.9074: : 315it [04:01,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 4.0425: : 315it [04:02,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 4.0425: : 316it [04:02,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.7119: : 316it [04:02,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.7119: : 317it [04:02,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 4.0224: : 317it [04:03,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 4.0224: : 318it [04:03,  1.15it/s]\u001b[A\n",
      "batch 319, training loss: 4.0909: : 318it [04:04,  1.15it/s]\u001b[A\n",
      "batch 319, training loss: 4.0909: : 319it [04:04,  1.10it/s]\u001b[A\n",
      "batch 320, training loss: 4.1107: : 319it [04:05,  1.10it/s]\u001b[A\n",
      "batch 320, training loss: 4.1107: : 320it [04:05,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.1395: : 320it [04:06,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.1395: : 321it [04:06,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 4.2017: : 321it [04:07,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 4.2017: : 322it [04:07,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 3.9508: : 322it [04:08,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 3.9508: : 323it [04:08,  1.04it/s]\u001b[A\n",
      "batch 324, training loss: 4.0374: : 323it [04:09,  1.04it/s]\u001b[A\n",
      "batch 324, training loss: 4.0374: : 324it [04:09,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.0992: : 324it [04:10,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.0992: : 325it [04:10,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 4.0343: : 325it [04:11,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 4.0343: : 326it [04:11,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 4.1364: : 326it [04:12,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 4.1364: : 327it [04:12,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 4.0056: : 327it [04:13,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 4.0056: : 328it [04:13,  1.03it/s]\u001b[A\n",
      "batch 329, training loss: 3.9068: : 328it [04:14,  1.03it/s]\u001b[A\n",
      "batch 329, training loss: 3.9068: : 329it [04:14,  1.02it/s]\u001b[A\n",
      "batch 330, training loss: 3.9844: : 329it [04:15,  1.02it/s]\u001b[A\n",
      "batch 330, training loss: 3.9844: : 330it [04:15,  1.01it/s]\u001b[A\n",
      "batch 331, training loss: 4.0337: : 330it [04:16,  1.01it/s]\u001b[A\n",
      "batch 331, training loss: 4.0337: : 331it [04:16,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.9192: : 331it [04:17,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.9192: : 332it [04:17,  1.00s/it]\u001b[A\n",
      "batch 333, training loss: 3.8873: : 332it [04:18,  1.00s/it]\u001b[A\n",
      "batch 333, training loss: 3.8873: : 333it [04:18,  1.03it/s]\u001b[A\n",
      "batch 334, training loss: 4.0066: : 333it [04:19,  1.03it/s]\u001b[A\n",
      "batch 334, training loss: 4.0066: : 334it [04:19,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.0991: : 334it [04:20,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.0991: : 335it [04:20,  1.00it/s]\u001b[A\n",
      "batch 336, training loss: 4.0034: : 335it [04:21,  1.00it/s]\u001b[A\n",
      "batch 336, training loss: 4.0034: : 336it [04:21,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.0423: : 336it [04:22,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 4.0423: : 337it [04:22,  1.00s/it]\u001b[A\n",
      "batch 338, training loss: 3.9304: : 337it [04:23,  1.00s/it]\u001b[A\n",
      "batch 338, training loss: 3.9304: : 338it [04:23,  1.03it/s]\u001b[A\n",
      "batch 339, training loss: 3.9845: : 338it [04:24,  1.03it/s]\u001b[A\n",
      "batch 339, training loss: 3.9845: : 339it [04:24,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 4.2137: : 339it [04:25,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 4.2137: : 340it [04:25,  1.00it/s]\u001b[A\n",
      "batch 341, training loss: 3.879: : 340it [04:26,  1.00it/s] \u001b[A\n",
      "batch 341, training loss: 3.879: : 341it [04:26,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.9669: : 341it [04:27,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.9669: : 342it [04:27,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.9779: : 342it [04:28,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.9779: : 343it [04:28,  1.02s/it]\u001b[A\n",
      "batch 344, training loss: 4.0589: : 343it [04:29,  1.02s/it]\u001b[A\n",
      "batch 344, training loss: 4.0589: : 344it [04:29,  1.00it/s]\u001b[A\n",
      "batch 345, training loss: 4.0081: : 344it [04:30,  1.00it/s]\u001b[A\n",
      "batch 345, training loss: 4.0081: : 345it [04:30,  1.00it/s]\u001b[A\n",
      "batch 346, training loss: 3.9944: : 345it [04:31,  1.00it/s]\u001b[A\n",
      "batch 346, training loss: 3.9944: : 346it [04:31,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 3.8876: : 346it [04:32,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 3.8876: : 347it [04:32,  1.01s/it]\u001b[A\n",
      "batch 348, training loss: 3.9533: : 347it [04:33,  1.01s/it]\u001b[A\n",
      "batch 348, training loss: 3.9533: : 348it [04:33,  1.01s/it]\u001b[A\n",
      "batch 349, training loss: 4.0914: : 348it [04:34,  1.01s/it]\u001b[A\n",
      "batch 349, training loss: 4.0914: : 349it [04:34,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 3.9599: : 349it [04:35,  1.01s/it]\u001b[A\n",
      "batch 350, training loss: 3.9599: : 350it [04:35,  1.00s/it]\u001b[A\n",
      "batch 351, training loss: 3.836: : 350it [04:36,  1.00s/it] \u001b[A\n",
      "batch 351, training loss: 3.836: : 351it [04:36,  1.04it/s]\u001b[A\n",
      "batch 352, training loss: 3.9303: : 351it [04:37,  1.04it/s]\u001b[A\n",
      "batch 352, training loss: 3.9303: : 352it [04:37,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 3.9627: : 352it [04:38,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 3.9627: : 353it [04:38,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 4.0252: : 353it [04:39,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 4.0252: : 354it [04:39,  1.01it/s]\u001b[A\n",
      "batch 355, training loss: 3.9685: : 354it [04:40,  1.01it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 355, training loss: 3.9685: : 355it [04:40,  1.03it/s]\u001b[A\n",
      "batch 356, training loss: 3.8159: : 355it [04:41,  1.03it/s]\u001b[A\n",
      "batch 356, training loss: 3.8159: : 356it [04:41,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.8246: : 356it [04:42,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.8246: : 357it [04:42,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 4.0189: : 357it [04:43,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 4.0189: : 358it [04:43,  1.01s/it]\u001b[A\n",
      "batch 359, training loss: 3.8274: : 358it [04:44,  1.01s/it]\u001b[A\n",
      "batch 359, training loss: 3.8274: : 359it [04:44,  1.00it/s]\u001b[A\n",
      "batch 360, training loss: 3.9641: : 359it [04:45,  1.00it/s]\u001b[A\n",
      "batch 360, training loss: 3.9641: : 360it [04:45,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.9419: : 360it [04:46,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.9419: : 361it [04:46,  1.02s/it]\u001b[A\n",
      "batch 362, training loss: 4.045: : 361it [04:47,  1.02s/it] \u001b[A\n",
      "batch 362, training loss: 4.045: : 362it [04:47,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.9345: : 362it [04:48,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.9345: : 363it [04:48,  1.02s/it]\u001b[A\n",
      "batch 364, training loss: 3.8268: : 363it [04:49,  1.02s/it]\u001b[A\n",
      "batch 364, training loss: 3.8268: : 364it [04:49,  1.02s/it]\u001b[A\n",
      "batch 365, training loss: 3.836: : 364it [04:50,  1.02s/it] \u001b[A\n",
      "batch 365, training loss: 3.836: : 365it [04:50,  1.02s/it]\u001b[A\n",
      "batch 366, training loss: 3.9729: : 365it [04:51,  1.02s/it]\u001b[A\n",
      "batch 366, training loss: 3.9729: : 366it [04:51,  1.02s/it]\u001b[A\n",
      "batch 367, training loss: 3.8709: : 366it [04:52,  1.02s/it]\u001b[A\n",
      "batch 367, training loss: 3.8709: : 367it [04:52,  1.01it/s]\u001b[A\n",
      "batch 368, training loss: 3.9697: : 367it [04:53,  1.01it/s]\u001b[A\n",
      "batch 368, training loss: 3.9697: : 368it [04:53,  1.00s/it]\u001b[A\n",
      "batch 369, training loss: 4.0044: : 368it [04:54,  1.00s/it]\u001b[A\n",
      "batch 369, training loss: 4.0044: : 369it [04:54,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.8618: : 369it [04:55,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.8618: : 370it [04:55,  1.01s/it]\u001b[A\n",
      "batch 371, training loss: 3.9356: : 370it [04:56,  1.01s/it]\u001b[A\n",
      "batch 371, training loss: 3.9356: : 371it [04:56,  1.01s/it]\u001b[A\n",
      "batch 372, training loss: 3.8695: : 371it [04:57,  1.01s/it]\u001b[A\n",
      "batch 372, training loss: 3.8695: : 372it [04:57,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 3.9068: : 372it [04:58,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 3.9068: : 373it [04:58,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 3.9016: : 373it [04:59,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 3.9016: : 374it [04:59,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 3.7925: : 374it [05:00,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 3.7925: : 375it [05:00,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.9777: : 375it [05:01,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.9777: : 376it [05:01,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 4.108: : 376it [05:02,  1.05it/s] \u001b[A\n",
      "batch 377, training loss: 4.108: : 377it [05:02,  1.01it/s]\u001b[A\n",
      "batch 378, training loss: 3.998: : 377it [05:03,  1.01it/s]\u001b[A\n",
      "batch 378, training loss: 3.998: : 378it [05:03,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.9587: : 378it [05:04,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.9587: : 379it [05:04,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 3.975: : 379it [05:06,  1.05s/it] \u001b[A\n",
      "batch 380, training loss: 3.975: : 380it [05:06,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.0539: : 380it [05:07,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 4.0539: : 381it [05:07,  1.08s/it]\u001b[A\n",
      "batch 382, training loss: 3.9148: : 381it [05:08,  1.08s/it]\u001b[A\n",
      "batch 382, training loss: 3.9148: : 382it [05:08,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.9619: : 382it [05:09,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.9619: : 383it [05:09,  1.09s/it]\u001b[A\n",
      "batch 384, training loss: 4.091: : 383it [05:10,  1.09s/it] \u001b[A\n",
      "batch 384, training loss: 4.091: : 384it [05:10,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 3.9758: : 384it [05:11,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 3.9758: : 385it [05:11,  1.10s/it]\u001b[A\n",
      "batch 386, training loss: 3.9228: : 385it [05:12,  1.10s/it]\u001b[A\n",
      "batch 386, training loss: 3.9228: : 386it [05:12,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.9994: : 386it [05:13,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.9994: : 387it [05:13,  1.09s/it]\u001b[A\n",
      "batch 388, training loss: 3.7523: : 387it [05:14,  1.09s/it]\u001b[A\n",
      "batch 388, training loss: 3.7523: : 388it [05:14,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.9521: : 388it [05:15,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.9521: : 389it [05:15,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 4.0569: : 389it [05:17,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 4.0569: : 390it [05:17,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.0528: : 390it [05:18,  1.10s/it]\u001b[A\n",
      "batch 391, training loss: 4.0528: : 391it [05:18,  1.11s/it]\u001b[A\n",
      "batch 392, training loss: 3.994: : 391it [05:19,  1.11s/it] \u001b[A\n",
      "batch 392, training loss: 3.994: : 392it [05:19,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 3.8328: : 392it [05:20,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 3.8328: : 393it [05:20,  1.09s/it]\u001b[A\n",
      "batch 394, training loss: 3.7712: : 393it [05:21,  1.09s/it]\u001b[A\n",
      "batch 394, training loss: 3.7712: : 394it [05:21,  1.08s/it]\u001b[A\n",
      "batch 395, training loss: 3.8706: : 394it [05:22,  1.08s/it]\u001b[A\n",
      "batch 395, training loss: 3.8706: : 395it [05:22,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 4.0397: : 395it [05:23,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 4.0397: : 396it [05:23,  1.07s/it]\u001b[A\n",
      "batch 397, training loss: 3.8178: : 396it [05:24,  1.07s/it]\u001b[A\n",
      "batch 397, training loss: 3.8178: : 397it [05:24,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 3.9339: : 397it [05:25,  1.10s/it]\u001b[A\n",
      "batch 398, training loss: 3.9339: : 398it [05:25,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.0406: : 398it [05:26,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 4.0406: : 399it [05:26,  1.09s/it]\u001b[A\n",
      "batch 400, training loss: 3.811: : 399it [05:27,  1.09s/it] \u001b[A\n",
      "batch 400, training loss: 3.811: : 400it [05:27,  1.06s/it]\u001b[A\n",
      "batch 401, training loss: 3.7796: : 400it [05:28,  1.06s/it]\u001b[A\n",
      "batch 401, training loss: 3.7796: : 401it [05:28,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.8424: : 401it [05:30,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.8424: : 402it [05:30,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.0196: : 402it [05:31,  1.09s/it]\u001b[A\n",
      "batch 403, training loss: 4.0196: : 403it [05:31,  1.08s/it]\u001b[A\n",
      "batch 404, training loss: 3.6467: : 403it [05:32,  1.08s/it]\u001b[A\n",
      "batch 404, training loss: 3.6467: : 404it [05:32,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 3.9011: : 404it [05:33,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 3.9011: : 405it [05:33,  1.09s/it]\u001b[A\n",
      "batch 406, training loss: 3.8105: : 405it [05:34,  1.09s/it]\u001b[A\n",
      "batch 406, training loss: 3.8105: : 406it [05:34,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 3.8955: : 406it [05:35,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 3.8955: : 407it [05:35,  1.10s/it]\u001b[A\n",
      "batch 408, training loss: 3.739: : 407it [05:36,  1.10s/it] \u001b[A\n",
      "batch 408, training loss: 3.739: : 408it [05:36,  1.08s/it]\u001b[A\n",
      "batch 409, training loss: 3.945: : 408it [05:37,  1.08s/it]\u001b[A\n",
      "batch 409, training loss: 3.945: : 409it [05:37,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.7697: : 409it [05:38,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.7697: : 410it [05:38,  1.10s/it]\u001b[A\n",
      "batch 411, training loss: 3.8725: : 410it [05:39,  1.10s/it]\u001b[A\n",
      "batch 411, training loss: 3.8725: : 411it [05:39,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.8878: : 411it [05:41,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.8878: : 412it [05:41,  1.11s/it]\u001b[A\n",
      "batch 413, training loss: 3.84: : 412it [05:42,  1.11s/it]  \u001b[A\n",
      "batch 413, training loss: 3.84: : 413it [05:42,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.7341: : 413it [05:43,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.7341: : 414it [05:43,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.6466: : 414it [05:44,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.6466: : 415it [05:44,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.6831: : 415it [05:45,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.6831: : 416it [05:45,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.9777: : 416it [05:46,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.9777: : 417it [05:46,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 3.8528: : 417it [05:47,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 3.8528: : 418it [05:47,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.6654: : 418it [05:48,  1.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 419, training loss: 3.6654: : 419it [05:48,  1.05s/it]\u001b[A\n",
      "batch 420, training loss: 3.769: : 419it [05:49,  1.05s/it] \u001b[A\n",
      "batch 420, training loss: 3.769: : 420it [05:49,  1.07s/it]\u001b[A\n",
      "batch 421, training loss: 3.8759: : 420it [05:50,  1.07s/it]\u001b[A\n",
      "batch 421, training loss: 3.8759: : 421it [05:50,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.8992: : 421it [05:52,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.8992: : 422it [05:52,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.6904: : 422it [05:53,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.6904: : 423it [05:53,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 3.9464: : 423it [05:54,  1.10s/it]\u001b[A\n",
      "batch 424, training loss: 3.9464: : 424it [05:54,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.0243: : 424it [05:55,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 4.0243: : 425it [05:55,  1.15s/it]\u001b[A\n",
      "batch 426, training loss: 3.9906: : 425it [05:56,  1.15s/it]\u001b[A\n",
      "batch 426, training loss: 3.9906: : 426it [05:56,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.9942: : 426it [05:57,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.9942: : 427it [05:57,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.1693: : 427it [05:59,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.1693: : 428it [05:59,  1.16s/it]\u001b[A\n",
      "batch 429, training loss: 4.0127: : 428it [06:00,  1.16s/it]\u001b[A\n",
      "batch 429, training loss: 4.0127: : 429it [06:00,  1.14s/it]\u001b[A\n",
      "batch 430, training loss: 4.0358: : 429it [06:01,  1.14s/it]\u001b[A\n",
      "batch 430, training loss: 4.0358: : 430it [06:01,  1.17s/it]\u001b[A\n",
      "batch 431, training loss: 4.0769: : 430it [06:02,  1.17s/it]\u001b[A\n",
      "batch 431, training loss: 4.0769: : 431it [06:02,  1.18s/it]\u001b[A\n",
      "batch 432, training loss: 3.9088: : 431it [06:03,  1.18s/it]\u001b[A\n",
      "batch 432, training loss: 3.9088: : 432it [06:03,  1.20s/it]\u001b[A\n",
      "batch 433, training loss: 3.9477: : 432it [06:04,  1.20s/it]\u001b[A\n",
      "batch 433, training loss: 3.9477: : 433it [06:04,  1.19s/it]\u001b[A\n",
      "batch 434, training loss: 3.9371: : 433it [06:06,  1.19s/it]\u001b[A\n",
      "batch 434, training loss: 3.9371: : 434it [06:06,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.9355: : 434it [06:07,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.9355: : 435it [06:07,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.9495: : 435it [06:08,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.9495: : 436it [06:08,  1.17s/it]\u001b[A\n",
      "batch 437, training loss: 3.891: : 436it [06:09,  1.17s/it] \u001b[A\n",
      "batch 437, training loss: 3.891: : 437it [06:09,  1.16s/it]\u001b[A\n",
      "batch 438, training loss: 3.9616: : 437it [06:10,  1.16s/it]\u001b[A\n",
      "batch 438, training loss: 3.9616: : 438it [06:10,  1.15s/it]\u001b[A\n",
      "batch 439, training loss: 4.0358: : 438it [06:11,  1.15s/it]\u001b[A\n",
      "batch 439, training loss: 4.0358: : 439it [06:11,  1.13s/it]\u001b[A\n",
      "batch 440, training loss: 3.8541: : 439it [06:13,  1.13s/it]\u001b[A\n",
      "batch 440, training loss: 3.8541: : 440it [06:13,  1.16s/it]\u001b[A\n",
      "batch 441, training loss: 4.1301: : 440it [06:14,  1.16s/it]\u001b[A\n",
      "batch 441, training loss: 4.1301: : 441it [06:14,  1.18s/it]\u001b[A\n",
      "batch 442, training loss: 3.8141: : 441it [06:15,  1.18s/it]\u001b[A\n",
      "batch 442, training loss: 3.8141: : 442it [06:15,  1.18s/it]\u001b[A\n",
      "batch 443, training loss: 3.8777: : 442it [06:16,  1.18s/it]\u001b[A\n",
      "batch 443, training loss: 3.8777: : 443it [06:16,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.8832: : 443it [06:18,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.8832: : 444it [06:18,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.9347: : 444it [06:19,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.9347: : 445it [06:19,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.9784: : 445it [06:20,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.9784: : 446it [06:20,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.8975: : 446it [06:21,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.8975: : 447it [06:21,  1.22s/it]\u001b[A\n",
      "batch 448, training loss: 3.79: : 447it [06:22,  1.22s/it]  \u001b[A\n",
      "batch 448, training loss: 3.79: : 448it [06:22,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.934: : 448it [06:24,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.934: : 449it [06:24,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.9317: : 449it [06:25,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.9317: : 450it [06:25,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.9525: : 450it [06:26,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.9525: : 451it [06:26,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.9658: : 451it [06:27,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.9658: : 452it [06:27,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.0326: : 452it [06:29,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 4.0326: : 453it [06:29,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.8806: : 453it [06:30,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.8806: : 454it [06:30,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.7622: : 454it [06:31,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.7622: : 455it [06:31,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.8181: : 455it [06:32,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.8181: : 456it [06:32,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.9416: : 456it [06:34,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.9416: : 457it [06:34,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.8275: : 457it [06:35,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.8275: : 458it [06:35,  1.24s/it]\u001b[A\n",
      "batch 459, training loss: 3.8518: : 458it [06:36,  1.24s/it]\u001b[A\n",
      "batch 459, training loss: 3.8518: : 459it [06:36,  1.22s/it]\u001b[A\n",
      "batch 460, training loss: 3.8557: : 459it [06:37,  1.22s/it]\u001b[A\n",
      "batch 460, training loss: 3.8557: : 460it [06:37,  1.11s/it]\u001b[A\n",
      "batch 461, training loss: 3.9499: : 460it [06:38,  1.11s/it]\u001b[A\n",
      "batch 461, training loss: 3.9499: : 461it [06:38,  1.15s/it]\u001b[A\n",
      "batch 462, training loss: 3.8838: : 461it [06:39,  1.15s/it]\u001b[A\n",
      "batch 462, training loss: 3.8838: : 462it [06:39,  1.18s/it]\u001b[A\n",
      "batch 463, training loss: 3.7254: : 462it [06:41,  1.18s/it]\u001b[A\n",
      "batch 463, training loss: 3.7254: : 463it [06:41,  1.19s/it]\u001b[A\n",
      "batch 464, training loss: 3.8715: : 463it [06:42,  1.19s/it]\u001b[A\n",
      "batch 464, training loss: 3.8715: : 464it [06:42,  1.20s/it]\u001b[A\n",
      "batch 465, training loss: 3.8621: : 464it [06:43,  1.20s/it]\u001b[A\n",
      "batch 465, training loss: 3.8621: : 465it [06:43,  1.16s/it]\u001b[A\n",
      "batch 466, training loss: 3.8142: : 465it [06:44,  1.16s/it]\u001b[A\n",
      "batch 466, training loss: 3.8142: : 466it [06:44,  1.18s/it]\u001b[A\n",
      "batch 467, training loss: 3.7546: : 466it [06:45,  1.18s/it]\u001b[A\n",
      "batch 467, training loss: 3.7546: : 467it [06:45,  1.22s/it]\u001b[A\n",
      "batch 468, training loss: 3.9296: : 467it [06:47,  1.22s/it]\u001b[A\n",
      "batch 468, training loss: 3.9296: : 468it [06:47,  1.24s/it]\u001b[A\n",
      "batch 469, training loss: 3.8464: : 468it [06:48,  1.24s/it]\u001b[A\n",
      "batch 469, training loss: 3.8464: : 469it [06:48,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.9503: : 469it [06:49,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.9503: : 470it [06:49,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 3.9453: : 470it [06:50,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 3.9453: : 471it [06:50,  1.27s/it]\u001b[A\n",
      "batch 472, training loss: 3.8886: : 471it [06:52,  1.27s/it]\u001b[A\n",
      "batch 472, training loss: 3.8886: : 472it [06:52,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.8957: : 472it [06:53,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.8957: : 473it [06:53,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 3.9465: : 473it [06:54,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 3.9465: : 474it [06:54,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.8732: : 474it [06:56,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.8732: : 475it [06:56,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.9566: : 475it [06:57,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.9566: : 476it [06:57,  1.27s/it]\u001b[A\n",
      "batch 477, training loss: 3.7638: : 476it [06:58,  1.27s/it]\u001b[A\n",
      "batch 477, training loss: 3.7638: : 477it [06:58,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.8436: : 477it [06:59,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.8436: : 478it [06:59,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.8065: : 478it [07:01,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.8065: : 479it [07:01,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.8259: : 479it [07:02,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.8259: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.8595: : 480it [07:03,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.8595: : 481it [07:03,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.8553: : 481it [07:04,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.8553: : 482it [07:04,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 3.7656: : 482it [07:06,  1.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 483, training loss: 3.7656: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.8506: : 483it [07:07,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.8506: : 484it [07:07,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.7875: : 484it [07:08,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.7875: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.865: : 485it [07:09,  1.26s/it] \u001b[A\n",
      "batch 486, training loss: 3.865: : 486it [07:09,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.9899: : 486it [07:11,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.9899: : 487it [07:11,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.754: : 487it [07:12,  1.26s/it] \u001b[A\n",
      "batch 488, training loss: 3.754: : 488it [07:12,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.693: : 488it [07:13,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.693: : 489it [07:13,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.7732: : 489it [07:14,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.7732: : 490it [07:14,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.8029: : 490it [07:16,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.8029: : 491it [07:16,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.7094: : 491it [07:17,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.7094: : 492it [07:17,  1.25s/it]\u001b[A\n",
      "batch 493, training loss: 3.8915: : 492it [07:18,  1.25s/it]\u001b[A\n",
      "batch 493, training loss: 3.8915: : 493it [07:18,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.9003: : 493it [07:19,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.9003: : 494it [07:19,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.7235: : 494it [07:21,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.7235: : 495it [07:21,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.7123: : 495it [07:22,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.7123: : 496it [07:22,  1.28s/it]\u001b[A\n",
      "batch 497, training loss: 3.6429: : 496it [07:23,  1.28s/it]\u001b[A\n",
      "batch 497, training loss: 3.6429: : 497it [07:23,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.7281: : 497it [07:25,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.7281: : 498it [07:25,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.7756: : 498it [07:25,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.7756: : 499it [07:25,  1.14s/it]\u001b[A\n",
      "batch 500, training loss: 4.0042: : 499it [07:27,  1.14s/it]\u001b[A\n",
      "batch 500, training loss: 4.0042: : 500it [07:27,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 3.8358: : 500it [07:28,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 3.8358: : 501it [07:28,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.8318: : 501it [07:29,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.8318: : 502it [07:29,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 3.9126: : 502it [07:31,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 3.9126: : 503it [07:31,  1.30s/it]\u001b[A\n",
      "batch 504, training loss: 3.8906: : 503it [07:32,  1.30s/it]\u001b[A\n",
      "batch 504, training loss: 3.8906: : 504it [07:32,  1.33s/it]\u001b[A\n",
      "batch 505, training loss: 3.9775: : 504it [07:34,  1.33s/it]\u001b[A\n",
      "batch 505, training loss: 3.9775: : 505it [07:34,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.9084: : 505it [07:35,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.9084: : 506it [07:35,  1.34s/it]\u001b[A\n",
      "batch 507, training loss: 3.7601: : 506it [07:36,  1.34s/it]\u001b[A\n",
      "batch 507, training loss: 3.7601: : 507it [07:36,  1.35s/it]\u001b[A\n",
      "batch 508, training loss: 3.941: : 507it [07:38,  1.35s/it] \u001b[A\n",
      "batch 508, training loss: 3.941: : 508it [07:38,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.8116: : 508it [07:39,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.8116: : 509it [07:39,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.8818: : 509it [07:40,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.8818: : 510it [07:40,  1.39s/it]\u001b[A\n",
      "batch 511, training loss: 3.8925: : 510it [07:41,  1.39s/it]\u001b[A\n",
      "batch 511, training loss: 3.8925: : 511it [07:41,  1.26s/it]\u001b[A\n",
      "batch 512, training loss: 3.8767: : 511it [07:43,  1.26s/it]\u001b[A\n",
      "batch 512, training loss: 3.8767: : 512it [07:43,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 3.9158: : 512it [07:44,  1.30s/it]\u001b[A\n",
      "batch 513, training loss: 3.9158: : 513it [07:44,  1.33s/it]\u001b[A\n",
      "batch 514, training loss: 3.7507: : 513it [07:46,  1.33s/it]\u001b[A\n",
      "batch 514, training loss: 3.7507: : 514it [07:46,  1.35s/it]\u001b[A\n",
      "batch 515, training loss: 3.9539: : 514it [07:47,  1.35s/it]\u001b[A\n",
      "batch 515, training loss: 3.9539: : 515it [07:47,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.8449: : 515it [07:48,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.8449: : 516it [07:48,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.8166: : 516it [07:50,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.8166: : 517it [07:50,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 3.8757: : 517it [07:51,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 3.8757: : 518it [07:51,  1.39s/it]\u001b[A\n",
      "batch 519, training loss: 3.6903: : 518it [07:53,  1.39s/it]\u001b[A\n",
      "batch 519, training loss: 3.6903: : 519it [07:53,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 3.8677: : 519it [07:54,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 3.8677: : 520it [07:54,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.853: : 520it [07:55,  1.37s/it] \u001b[A\n",
      "batch 521, training loss: 3.853: : 521it [07:55,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 3.9282: : 521it [07:57,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 3.9282: : 522it [07:57,  1.35s/it]\u001b[A\n",
      "batch 523, training loss: 3.7598: : 522it [07:58,  1.35s/it]\u001b[A\n",
      "batch 523, training loss: 3.7598: : 523it [07:58,  1.37s/it]\u001b[A\n",
      "batch 524, training loss: 3.9636: : 523it [07:59,  1.37s/it]\u001b[A\n",
      "batch 524, training loss: 3.9636: : 524it [07:59,  1.38s/it]\u001b[A\n",
      "batch 525, training loss: 3.9811: : 524it [08:01,  1.38s/it]\u001b[A\n",
      "batch 525, training loss: 3.9811: : 525it [08:01,  1.35s/it]\u001b[A\n",
      "batch 526, training loss: 3.8113: : 525it [08:02,  1.35s/it]\u001b[A\n",
      "batch 526, training loss: 3.8113: : 526it [08:02,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 3.751: : 526it [08:03,  1.37s/it] \u001b[A\n",
      "batch 527, training loss: 3.751: : 527it [08:03,  1.34s/it]\u001b[A\n",
      "batch 528, training loss: 3.8555: : 527it [08:05,  1.34s/it]\u001b[A\n",
      "batch 528, training loss: 3.8555: : 528it [08:05,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 3.8695: : 528it [08:06,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 3.8695: : 529it [08:06,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.8715: : 529it [08:08,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.8715: : 530it [08:08,  1.45s/it]\u001b[A\n",
      "batch 531, training loss: 3.7918: : 530it [08:09,  1.45s/it]\u001b[A\n",
      "batch 531, training loss: 3.7918: : 531it [08:09,  1.44s/it]\u001b[A\n",
      "batch 532, training loss: 3.6314: : 531it [08:11,  1.44s/it]\u001b[A\n",
      "batch 532, training loss: 3.6314: : 532it [08:11,  1.44s/it]\u001b[A\n",
      "batch 533, training loss: 3.804: : 532it [08:12,  1.44s/it] \u001b[A\n",
      "batch 533, training loss: 3.804: : 533it [08:12,  1.47s/it]\u001b[A\n",
      "batch 534, training loss: 3.9413: : 533it [08:14,  1.47s/it]\u001b[A\n",
      "batch 534, training loss: 3.9413: : 534it [08:14,  1.49s/it]\u001b[A\n",
      "batch 535, training loss: 3.7916: : 534it [08:15,  1.49s/it]\u001b[A\n",
      "batch 535, training loss: 3.7916: : 535it [08:15,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.6308: : 535it [08:17,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.6308: : 536it [08:17,  1.43s/it]\u001b[A\n",
      "batch 537, training loss: 3.6907: : 536it [08:18,  1.43s/it]\u001b[A\n",
      "batch 537, training loss: 3.6907: : 537it [08:18,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.8317: : 537it [08:20,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.8317: : 538it [08:20,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.7842: : 538it [08:21,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.7842: : 539it [08:21,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 3.7468: : 539it [08:23,  1.48s/it]\u001b[A\n",
      "batch 540, training loss: 3.7468: : 540it [08:23,  1.47s/it]\u001b[A\n",
      "batch 541, training loss: 3.8447: : 540it [08:24,  1.47s/it]\u001b[A\n",
      "batch 541, training loss: 3.8447: : 541it [08:24,  1.47s/it]\u001b[A\n",
      "batch 542, training loss: 3.8564: : 541it [08:25,  1.47s/it]\u001b[A\n",
      "batch 542, training loss: 3.8564: : 542it [08:25,  1.48s/it]\u001b[A\n",
      "batch 543, training loss: 3.735: : 542it [08:27,  1.48s/it] \u001b[A\n",
      "batch 543, training loss: 3.735: : 543it [08:27,  1.49s/it]\u001b[A\n",
      "batch 544, training loss: 3.8003: : 543it [08:28,  1.49s/it]\u001b[A\n",
      "batch 544, training loss: 3.8003: : 544it [08:28,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.7914: : 544it [08:30,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.7914: : 545it [08:30,  1.47s/it]\u001b[A\n",
      "batch 546, training loss: 3.694: : 545it [08:31,  1.47s/it] \u001b[A\n",
      "batch 546, training loss: 3.694: : 546it [08:31,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 3.5989: : 546it [08:33,  1.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 547, training loss: 3.5989: : 547it [08:33,  1.51s/it]\u001b[A\n",
      "batch 548, training loss: 3.6767: : 547it [08:34,  1.51s/it]\u001b[A\n",
      "batch 548, training loss: 3.6767: : 548it [08:34,  1.32s/it]\u001b[A\n",
      "batch 549, training loss: 3.7722: : 548it [08:35,  1.32s/it]\u001b[A\n",
      "batch 549, training loss: 3.7722: : 549it [08:35,  1.41s/it]\u001b[A\n",
      "batch 550, training loss: 3.6722: : 549it [08:37,  1.41s/it]\u001b[A\n",
      "batch 550, training loss: 3.6722: : 550it [08:37,  1.48s/it]\u001b[A\n",
      "batch 551, training loss: 3.7517: : 550it [08:39,  1.48s/it]\u001b[A\n",
      "batch 551, training loss: 3.7517: : 551it [08:39,  1.52s/it]\u001b[A\n",
      "batch 552, training loss: 3.7808: : 551it [08:40,  1.52s/it]\u001b[A\n",
      "batch 552, training loss: 3.7808: : 552it [08:40,  1.55s/it]\u001b[A\n",
      "batch 553, training loss: 3.7538: : 552it [08:42,  1.55s/it]\u001b[A\n",
      "batch 553, training loss: 3.7538: : 553it [08:42,  1.58s/it]\u001b[A\n",
      "batch 554, training loss: 3.5191: : 553it [08:44,  1.58s/it]\u001b[A\n",
      "batch 554, training loss: 3.5191: : 554it [08:44,  1.59s/it]\u001b[A\n",
      "batch 555, training loss: 3.6794: : 554it [08:45,  1.59s/it]\u001b[A\n",
      "batch 555, training loss: 3.6794: : 555it [08:45,  1.60s/it]\u001b[A\n",
      "batch 556, training loss: 3.8078: : 555it [08:47,  1.60s/it]\u001b[A\n",
      "batch 556, training loss: 3.8078: : 556it [08:47,  1.59s/it]\u001b[A\n",
      "batch 557, training loss: 3.6603: : 556it [08:48,  1.59s/it]\u001b[A\n",
      "batch 557, training loss: 3.6603: : 557it [08:48,  1.60s/it]\u001b[A\n",
      "batch 558, training loss: 3.5872: : 557it [08:50,  1.60s/it]\u001b[A\n",
      "batch 558, training loss: 3.5872: : 558it [08:50,  1.60s/it]\u001b[A\n",
      "batch 559, training loss: 3.6804: : 558it [08:51,  1.60s/it]\u001b[A\n",
      "batch 559, training loss: 3.6804: : 559it [08:51,  1.56s/it]\u001b[A\n",
      "batch 560, training loss: 3.6976: : 559it [08:53,  1.56s/it]\u001b[A\n",
      "batch 560, training loss: 3.6976: : 560it [08:53,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.7028: : 560it [08:55,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.7028: : 561it [08:55,  1.54s/it]\u001b[A\n",
      "batch 562, training loss: 3.5103: : 561it [08:56,  1.54s/it]\u001b[A\n",
      "batch 562, training loss: 3.5103: : 562it [08:56,  1.55s/it]\u001b[A\n",
      "batch 563, training loss: 3.6812: : 562it [08:58,  1.55s/it]\u001b[A\n",
      "batch 563, training loss: 3.6812: : 563it [08:58,  1.57s/it]\u001b[A\n",
      "batch 564, training loss: 3.5844: : 563it [08:59,  1.57s/it]\u001b[A\n",
      "batch 564, training loss: 3.5844: : 564it [08:59,  1.58s/it]\u001b[A\n",
      "batch 565, training loss: 3.6792: : 564it [09:01,  1.58s/it]\u001b[A\n",
      "batch 565, training loss: 3.6792: : 565it [09:01,  1.55s/it]\u001b[A\n",
      "batch 566, training loss: 3.8064: : 565it [09:03,  1.55s/it]\u001b[A\n",
      "batch 566, training loss: 3.8064: : 566it [09:03,  1.62s/it]\u001b[A\n",
      "batch 567, training loss: 3.7505: : 566it [09:04,  1.62s/it]\u001b[A\n",
      "batch 567, training loss: 3.7505: : 567it [09:04,  1.64s/it]\u001b[A\n",
      "batch 568, training loss: 3.837: : 567it [09:06,  1.64s/it] \u001b[A\n",
      "batch 568, training loss: 3.837: : 568it [09:06,  1.66s/it]\u001b[A\n",
      "batch 569, training loss: 3.8044: : 568it [09:08,  1.66s/it]\u001b[A\n",
      "batch 569, training loss: 3.8044: : 569it [09:08,  1.67s/it]\u001b[A\n",
      "batch 570, training loss: 3.9738: : 569it [09:09,  1.67s/it]\u001b[A\n",
      "batch 570, training loss: 3.9738: : 570it [09:09,  1.66s/it]\u001b[A\n",
      "batch 571, training loss: 3.793: : 570it [09:11,  1.66s/it] \u001b[A\n",
      "batch 571, training loss: 3.793: : 571it [09:11,  1.67s/it]\u001b[A\n",
      "batch 572, training loss: 3.8651: : 571it [09:13,  1.67s/it]\u001b[A\n",
      "batch 572, training loss: 3.8651: : 572it [09:13,  1.69s/it]\u001b[A\n",
      "batch 573, training loss: 3.7912: : 572it [09:14,  1.69s/it]\u001b[A\n",
      "batch 573, training loss: 3.7912: : 573it [09:14,  1.68s/it]\u001b[A\n",
      "batch 574, training loss: 3.8754: : 573it [09:16,  1.68s/it]\u001b[A\n",
      "batch 574, training loss: 3.8754: : 574it [09:16,  1.59s/it]\u001b[A\n",
      "batch 575, training loss: 3.6458: : 574it [09:17,  1.59s/it]\u001b[A\n",
      "batch 575, training loss: 3.6458: : 575it [09:17,  1.36s/it]\u001b[A\n",
      "batch 576, training loss: 3.8271: : 575it [09:18,  1.36s/it]\u001b[A\n",
      "batch 576, training loss: 3.8271: : 576it [09:18,  1.47s/it]\u001b[A\n",
      "batch 577, training loss: 3.7083: : 576it [09:20,  1.47s/it]\u001b[A\n",
      "batch 577, training loss: 3.7083: : 577it [09:20,  1.56s/it]\u001b[A\n",
      "batch 578, training loss: 3.6871: : 577it [09:22,  1.56s/it]\u001b[A\n",
      "batch 578, training loss: 3.6871: : 578it [09:22,  1.61s/it]\u001b[A\n",
      "batch 579, training loss: 3.7354: : 578it [09:24,  1.61s/it]\u001b[A\n",
      "batch 579, training loss: 3.7354: : 579it [09:24,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 3.6395: : 579it [09:25,  1.66s/it]\u001b[A\n",
      "batch 580, training loss: 3.6395: : 580it [09:25,  1.69s/it]\u001b[A\n",
      "batch 581, training loss: 3.6484: : 580it [09:27,  1.69s/it]\u001b[A\n",
      "batch 581, training loss: 3.6484: : 581it [09:27,  1.72s/it]\u001b[A\n",
      "batch 582, training loss: 3.6654: : 581it [09:29,  1.72s/it]\u001b[A\n",
      "batch 582, training loss: 3.6654: : 582it [09:29,  1.71s/it]\u001b[A\n",
      "batch 583, training loss: 3.3636: : 582it [09:29,  1.71s/it]\u001b[A\n",
      "batch 583, training loss: 3.3636: : 583it [09:29,  1.39s/it]\u001b[A\n",
      "batch 584, training loss: 3.8932: : 583it [09:31,  1.39s/it]\u001b[A\n",
      "batch 584, training loss: 3.8932: : 584it [09:31,  1.55s/it]\u001b[A\n",
      "batch 585, training loss: 3.857: : 584it [09:33,  1.55s/it] \u001b[A\n",
      "batch 585, training loss: 3.857: : 585it [09:33,  1.65s/it]\u001b[A\n",
      "batch 586, training loss: 3.9438: : 585it [09:35,  1.65s/it]\u001b[A\n",
      "batch 586, training loss: 3.9438: : 586it [09:35,  1.73s/it]\u001b[A\n",
      "batch 587, training loss: 3.8299: : 586it [09:37,  1.73s/it]\u001b[A\n",
      "batch 587, training loss: 3.8299: : 587it [09:37,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.6989: : 587it [09:39,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.6989: : 588it [09:39,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 3.7976: : 588it [09:41,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 3.7976: : 589it [09:41,  1.86s/it]\u001b[A\n",
      "batch 590, training loss: 3.7831: : 589it [09:43,  1.86s/it]\u001b[A\n",
      "batch 590, training loss: 3.7831: : 590it [09:43,  1.90s/it]\u001b[A\n",
      "batch 591, training loss: 3.645: : 590it [09:45,  1.90s/it] \u001b[A\n",
      "batch 591, training loss: 3.645: : 591it [09:45,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.6981: : 591it [09:47,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.6981: : 592it [09:47,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.6107: : 592it [09:49,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.6107: : 593it [09:49,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.7815: : 593it [09:51,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.7815: : 594it [09:51,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.8867: : 594it [09:52,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.8867: : 595it [09:52,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.7694: : 595it [09:55,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.7694: : 596it [09:55,  1.96s/it]\u001b[A\n",
      "batch 597, training loss: 3.5873: : 596it [09:57,  1.96s/it]\u001b[A\n",
      "batch 597, training loss: 3.5873: : 597it [09:57,  1.95s/it]\u001b[A\n",
      "batch 598, training loss: 3.8384: : 597it [09:59,  1.95s/it]\u001b[A\n",
      "batch 598, training loss: 3.8384: : 598it [09:59,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.7909: : 598it [10:00,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.7909: : 599it [10:00,  1.83s/it]\u001b[A\n",
      "batch 600, training loss: 3.6308: : 599it [10:02,  1.83s/it]\u001b[A\n",
      "batch 600, training loss: 3.6308: : 600it [10:02,  1.97s/it]\u001b[A\n",
      "batch 601, training loss: 2.9708: : 600it [10:03,  1.97s/it]\u001b[A\n",
      "batch 601, training loss: 2.9708: : 601it [10:03,  1.65s/it]\u001b[A\n",
      "batch 602, training loss: 3.6814: : 601it [10:05,  1.65s/it]\u001b[A\n",
      "batch 602, training loss: 3.6814: : 602it [10:05,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.6613: : 602it [10:07,  1.79s/it]\u001b[A\n",
      "batch 603, training loss: 3.6613: : 603it [10:07,  1.82s/it]\u001b[A\n",
      "batch 604, training loss: 3.7487: : 603it [10:09,  1.82s/it]\u001b[A\n",
      "batch 604, training loss: 3.7487: : 604it [10:09,  1.80s/it]\u001b[A\n",
      "batch 605, training loss: 3.8828: : 604it [10:11,  1.80s/it]\u001b[A\n",
      "batch 605, training loss: 3.8828: : 605it [10:11,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 3.6689: : 605it [10:12,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 3.6689: : 606it [10:12,  1.71s/it]\u001b[A\n",
      "batch 607, training loss: 3.5521: : 606it [10:14,  1.71s/it]\u001b[A\n",
      "batch 607, training loss: 3.5521: : 607it [10:14,  1.67s/it]\u001b[A\n",
      "batch 608, training loss: 3.8427: : 607it [10:15,  1.67s/it]\u001b[A\n",
      "batch 608, training loss: 3.8427: : 608it [10:15,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 3.7784: : 608it [10:17,  1.56s/it]\u001b[A\n",
      "batch 609, training loss: 3.7784: : 609it [10:17,  1.47s/it]\u001b[A\n",
      "batch 610, training loss: 3.2499: : 609it [10:18,  1.47s/it]\u001b[A\n",
      "batch 610, training loss: 3.2499: : 610it [10:18,  1.45s/it]\u001b[A\n",
      "batch 611, training loss: 3.2801: : 610it [10:19,  1.45s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 611, training loss: 3.2801: : 611it [10:19,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.5766: : 611it [10:20,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.5766: : 612it [10:20,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.3696: : 612it [10:21,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.3696: : 613it [10:21,  1.24s/it]\u001b[A\n",
      "batch 613, training loss: 3.3696: : 615it [10:21,  1.44it/s]\u001b[A\n",
      "batch 613, training loss: 3.3696: : 616it [10:22,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.9053: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.9053: : 1it [00:00,  2.48it/s]\u001b[A\n",
      "batch 1, dev loss: 4.1038: : 1it [00:00,  2.48it/s]\u001b[A\n",
      "batch 1, dev loss: 4.1038: : 2it [00:00,  3.43it/s]\u001b[A\n",
      "batch 2, dev loss: 3.7128: : 2it [00:00,  3.43it/s]\u001b[A\n",
      "batch 2, dev loss: 3.7128: : 3it [00:00,  4.15it/s]\u001b[A\n",
      "batch 3, dev loss: 3.8307: : 3it [00:00,  4.15it/s]\u001b[A\n",
      "batch 3, dev loss: 3.8307: : 4it [00:00,  4.53it/s]\u001b[A\n",
      "batch 4, dev loss: 3.8695: : 4it [00:01,  4.53it/s]\u001b[A\n",
      "batch 4, dev loss: 3.8695: : 5it [00:01,  4.75it/s]\u001b[A\n",
      "batch 5, dev loss: 3.8254: : 5it [00:01,  4.75it/s]\u001b[A\n",
      "batch 5, dev loss: 3.8254: : 6it [00:01,  4.97it/s]\u001b[A\n",
      "batch 6, dev loss: 3.9522: : 6it [00:01,  4.97it/s]\u001b[A\n",
      "batch 6, dev loss: 3.9522: : 7it [00:01,  5.24it/s]\u001b[A\n",
      "batch 7, dev loss: 3.723: : 7it [00:01,  5.24it/s] \u001b[A\n",
      "batch 7, dev loss: 3.723: : 8it [00:01,  5.34it/s]\u001b[A\n",
      "batch 8, dev loss: 3.9389: : 8it [00:01,  5.34it/s]\u001b[A\n",
      "batch 8, dev loss: 3.9389: : 9it [00:01,  5.12it/s]\u001b[A\n",
      "batch 9, dev loss: 3.8388: : 9it [00:02,  5.12it/s]\u001b[A\n",
      "batch 9, dev loss: 3.8388: : 10it [00:02,  4.88it/s]\u001b[A\n",
      "batch 10, dev loss: 3.8693: : 10it [00:02,  4.88it/s]\u001b[A\n",
      "batch 10, dev loss: 3.8693: : 11it [00:02,  5.00it/s]\u001b[A\n",
      "batch 11, dev loss: 3.9669: : 11it [00:02,  5.00it/s]\u001b[A\n",
      "batch 11, dev loss: 3.9669: : 12it [00:02,  4.88it/s]\u001b[A\n",
      "batch 12, dev loss: 3.8017: : 12it [00:02,  4.88it/s]\u001b[A\n",
      "batch 12, dev loss: 3.8017: : 13it [00:02,  4.63it/s]\u001b[A\n",
      "batch 13, dev loss: 3.937: : 13it [00:02,  4.63it/s] \u001b[A\n",
      "batch 13, dev loss: 3.937: : 14it [00:02,  4.77it/s]\u001b[A\n",
      "batch 14, dev loss: 4.0387: : 14it [00:03,  4.77it/s]\u001b[A\n",
      "batch 14, dev loss: 4.0387: : 15it [00:03,  4.64it/s]\u001b[A\n",
      "batch 15, dev loss: 3.8562: : 15it [00:03,  4.64it/s]\u001b[A\n",
      "batch 15, dev loss: 3.8562: : 16it [00:03,  5.04it/s]\u001b[A\n",
      "batch 16, dev loss: 4.2087: : 16it [00:03,  5.04it/s]\u001b[A\n",
      "batch 16, dev loss: 4.2087: : 17it [00:03,  4.57it/s]\u001b[A\n",
      "batch 17, dev loss: 3.9671: : 17it [00:03,  4.57it/s]\u001b[A\n",
      "batch 17, dev loss: 3.9671: : 18it [00:03,  4.26it/s]\u001b[A\n",
      "batch 18, dev loss: 3.8714: : 18it [00:04,  4.26it/s]\u001b[A\n",
      "batch 18, dev loss: 3.8714: : 19it [00:04,  4.09it/s]\u001b[A\n",
      "batch 19, dev loss: 4.0715: : 19it [00:04,  4.09it/s]\u001b[A\n",
      "batch 19, dev loss: 4.0715: : 20it [00:04,  3.98it/s]\u001b[A\n",
      "batch 20, dev loss: 3.8534: : 20it [00:04,  3.98it/s]\u001b[A\n",
      "batch 20, dev loss: 3.8534: : 21it [00:04,  4.00it/s]\u001b[A\n",
      "batch 21, dev loss: 3.7526: : 21it [00:04,  4.00it/s]\u001b[A\n",
      "batch 21, dev loss: 3.7526: : 22it [00:04,  3.94it/s]\u001b[A\n",
      "batch 22, dev loss: 3.9682: : 22it [00:05,  3.94it/s]\u001b[A\n",
      "batch 22, dev loss: 3.9682: : 23it [00:05,  4.02it/s]\u001b[A\n",
      "batch 23, dev loss: 4.0082: : 23it [00:05,  4.02it/s]\u001b[A\n",
      "batch 23, dev loss: 4.0082: : 24it [00:05,  4.63it/s]\u001b[A\n",
      "batch 24, dev loss: 3.9225: : 24it [00:05,  4.63it/s]\u001b[A\n",
      "batch 24, dev loss: 3.9225: : 25it [00:05,  4.05it/s]\u001b[A\n",
      "batch 25, dev loss: 3.9: : 25it [00:05,  4.05it/s]   \u001b[A\n",
      "batch 25, dev loss: 3.9: : 26it [00:05,  3.78it/s]\u001b[A\n",
      "batch 26, dev loss: 3.8537: : 26it [00:06,  3.78it/s]\u001b[A\n",
      "batch 26, dev loss: 3.8537: : 27it [00:06,  3.47it/s]\u001b[A\n",
      "batch 27, dev loss: 3.7837: : 27it [00:06,  3.47it/s]\u001b[A\n",
      "batch 27, dev loss: 3.7837: : 28it [00:06,  3.54it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9894: : 28it [00:06,  3.54it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9894: : 29it [00:06,  3.42it/s]\u001b[A\n",
      "batch 29, dev loss: 3.9347: : 29it [00:07,  3.42it/s]\u001b[A\n",
      "batch 29, dev loss: 3.9347: : 30it [00:07,  3.48it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1779: : 30it [00:07,  3.48it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1779: : 31it [00:07,  4.30it/s]\u001b[A\n",
      "batch 31, dev loss: 4.0172: : 31it [00:07,  4.30it/s]\u001b[A\n",
      "batch 31, dev loss: 4.0172: : 32it [00:07,  3.94it/s]\u001b[A\n",
      "batch 32, dev loss: 4.0296: : 32it [00:07,  3.94it/s]\u001b[A\n",
      "batch 32, dev loss: 4.0296: : 33it [00:07,  3.55it/s]\u001b[A\n",
      "batch 33, dev loss: 3.8151: : 33it [00:08,  3.55it/s]\u001b[A\n",
      "batch 33, dev loss: 3.8151: : 34it [00:08,  3.42it/s]\u001b[A\n",
      "batch 34, dev loss: 4.2467: : 34it [00:08,  3.42it/s]\u001b[A\n",
      "batch 34, dev loss: 4.2467: : 35it [00:08,  3.22it/s]\u001b[A\n",
      "batch 35, dev loss: 4.0138: : 35it [00:08,  3.22it/s]\u001b[A\n",
      "batch 35, dev loss: 4.0138: : 36it [00:08,  3.11it/s]\u001b[A\n",
      "batch 36, dev loss: 3.9976: : 36it [00:09,  3.11it/s]\u001b[A\n",
      "batch 36, dev loss: 3.9976: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 3.7528: : 37it [00:09,  3.39it/s]\u001b[A\n",
      "batch 37, dev loss: 3.7528: : 38it [00:09,  3.32it/s]\u001b[A\n",
      "batch 38, dev loss: 3.9985: : 38it [00:09,  3.32it/s]\u001b[A\n",
      "batch 38, dev loss: 3.9985: : 39it [00:09,  3.26it/s]\u001b[A\n",
      "batch 39, dev loss: 3.9929: : 39it [00:10,  3.26it/s]\u001b[A\n",
      "batch 39, dev loss: 3.9929: : 40it [00:10,  3.03it/s]\u001b[A\n",
      "batch 40, dev loss: 4.0434: : 40it [00:10,  3.03it/s]\u001b[A\n",
      "batch 40, dev loss: 4.0434: : 41it [00:10,  3.01it/s]\u001b[A\n",
      "batch 41, dev loss: 3.8136: : 41it [00:10,  3.01it/s]\u001b[A\n",
      "batch 41, dev loss: 3.8136: : 42it [00:10,  3.04it/s]\u001b[A\n",
      "batch 42, dev loss: 3.9779: : 42it [00:11,  3.04it/s]\u001b[A\n",
      "batch 42, dev loss: 3.9779: : 43it [00:11,  2.92it/s]\u001b[A\n",
      "batch 43, dev loss: 3.9871: : 43it [00:11,  2.92it/s]\u001b[A\n",
      "batch 43, dev loss: 3.9871: : 44it [00:11,  2.80it/s]\u001b[A\n",
      "batch 44, dev loss: 3.9144: : 44it [00:12,  2.80it/s]\u001b[A\n",
      "batch 44, dev loss: 3.9144: : 45it [00:12,  2.73it/s]\u001b[A\n",
      "batch 45, dev loss: 4.202: : 45it [00:12,  2.73it/s] \u001b[A\n",
      "batch 45, dev loss: 4.202: : 46it [00:12,  2.72it/s]\u001b[A\n",
      "batch 46, dev loss: 3.7398: : 46it [00:12,  2.72it/s]\u001b[A\n",
      "batch 46, dev loss: 3.7398: : 47it [00:12,  2.64it/s]\u001b[A\n",
      "batch 47, dev loss: 3.9146: : 47it [00:13,  2.64it/s]\u001b[A\n",
      "batch 47, dev loss: 3.9146: : 48it [00:13,  2.61it/s]\u001b[A\n",
      "batch 48, dev loss: 3.7441: : 48it [00:13,  2.61it/s]\u001b[A\n",
      "batch 48, dev loss: 3.7441: : 49it [00:13,  2.55it/s]\u001b[A\n",
      "batch 49, dev loss: 3.9252: : 49it [00:13,  2.55it/s]\u001b[A\n",
      "batch 49, dev loss: 3.9252: : 50it [00:13,  2.84it/s]\u001b[A\n",
      "batch 50, dev loss: 3.9137: : 50it [00:14,  2.84it/s]\u001b[A\n",
      "batch 50, dev loss: 3.9137: : 51it [00:14,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 3.9111: : 51it [00:14,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 3.9111: : 52it [00:14,  2.55it/s]\u001b[A\n",
      "batch 52, dev loss: 3.6851: : 52it [00:15,  2.55it/s]\u001b[A\n",
      "batch 52, dev loss: 3.6851: : 53it [00:15,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.9126: : 53it [00:15,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.9126: : 54it [00:15,  2.56it/s]\u001b[A\n",
      "batch 54, dev loss: 3.6763: : 54it [00:16,  2.56it/s]\u001b[A\n",
      "batch 54, dev loss: 3.6763: : 55it [00:16,  2.35it/s]\u001b[A\n",
      "batch 55, dev loss: 3.8385: : 55it [00:16,  2.35it/s]\u001b[A\n",
      "batch 55, dev loss: 3.8385: : 56it [00:16,  2.17it/s]\u001b[A\n",
      "batch 56, dev loss: 3.7219: : 56it [00:16,  2.17it/s]\u001b[A\n",
      "batch 56, dev loss: 3.7219: : 57it [00:16,  2.35it/s]\u001b[A\n",
      "batch 57, dev loss: 3.6349: : 57it [00:17,  2.35it/s]\u001b[A\n",
      "batch 57, dev loss: 3.6349: : 58it [00:17,  2.19it/s]\u001b[A\n",
      "batch 58, dev loss: 3.9915: : 58it [00:17,  2.19it/s]\u001b[A\n",
      "batch 58, dev loss: 3.9915: : 59it [00:17,  2.26it/s]\u001b[A\n",
      "batch 59, dev loss: 3.9099: : 59it [00:18,  2.26it/s]\u001b[A\n",
      "batch 59, dev loss: 3.9099: : 60it [00:18,  2.21it/s]\u001b[A\n",
      "batch 60, dev loss: 3.5726: : 60it [00:18,  2.21it/s]\u001b[A\n",
      "batch 60, dev loss: 3.5726: : 61it [00:18,  2.31it/s]\u001b[A\n",
      "batch 61, dev loss: 3.6009: : 61it [00:19,  2.31it/s]\u001b[A\n",
      "batch 61, dev loss: 3.6009: : 62it [00:19,  2.39it/s]\u001b[A\n",
      "batch 62, dev loss: 3.478: : 62it [00:19,  2.39it/s] \u001b[A\n",
      "batch 62, dev loss: 3.478: : 63it [00:19,  2.56it/s]\u001b[A\n",
      "batch 63, dev loss: 4.0703: : 63it [00:19,  2.56it/s]\u001b[A\n",
      "batch 63, dev loss: 4.0703: : 64it [00:19,  2.60it/s]\u001b[A\n",
      "batch 64, dev loss: 3.7363: : 64it [00:20,  2.60it/s]\u001b[A\n",
      "batch 64, dev loss: 3.7363: : 65it [00:20,  2.81it/s]\u001b[A\n",
      "batch 65, dev loss: 3.6379: : 65it [00:20,  2.81it/s]\u001b[A\n",
      "batch 65, dev loss: 3.6379: : 66it [00:20,  2.85it/s]\u001b[A\n",
      "batch 66, dev loss: 3.7098: : 66it [00:20,  2.85it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 66, dev loss: 3.7098: : 67it [00:20,  2.90it/s]\u001b[A\n",
      "batch 67, dev loss: 2.9727: : 67it [00:21,  2.90it/s]\u001b[A\n",
      "batch 67, dev loss: 2.9727: : 68it [00:21,  2.87it/s]\u001b[A\n",
      "batch 68, dev loss: 2.901: : 68it [00:21,  2.87it/s] \u001b[A\n",
      "batch 68, dev loss: 2.901: : 69it [00:21,  3.02it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2337: : 69it [00:21,  3.02it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2337: : 70it [00:21,  2.94it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0252: : 70it [00:22,  2.94it/s]\u001b[A\n",
      "batch 70, dev loss: 4.0252: : 71it [00:22,  3.04it/s]\u001b[A\n",
      "batch 71, dev loss: 3.5313: : 71it [00:22,  3.04it/s]\u001b[A\n",
      "batch 71, dev loss: 3.5313: : 72it [00:22,  2.92it/s]\u001b[A\n",
      "batch 72, dev loss: 4.2044: : 72it [00:22,  2.92it/s]\u001b[A\n",
      "batch 72, dev loss: 4.2044: : 73it [00:22,  2.98it/s]\u001b[A\n",
      "batch 72, dev loss: 4.2044: : 75it [00:22,  4.39it/s]\u001b[A\n",
      "batch 72, dev loss: 4.2044: : 76it [00:23,  3.29it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:00,  1.10it/s]\u001b[A\n",
      "2it [00:01,  1.12it/s]\u001b[A\n",
      "3it [00:03,  1.12s/it]\u001b[A\n",
      "4it [00:04,  1.27s/it]\u001b[A\n",
      "5it [00:06,  1.30s/it]\u001b[A\n",
      "6it [00:07,  1.32s/it]\u001b[A\n",
      "7it [00:08,  1.30s/it]\u001b[A\n",
      "8it [00:09,  1.25s/it]\u001b[A\n",
      "9it [00:10,  1.22s/it]\u001b[A\n",
      "10it [00:12,  1.44s/it]\u001b[A\n",
      "11it [00:14,  1.55s/it]\u001b[A\n",
      "12it [00:16,  1.64s/it]\u001b[A\n",
      "13it [00:18,  1.60s/it]\u001b[A\n",
      "14it [00:19,  1.55s/it]\u001b[A\n",
      "15it [00:21,  1.63s/it]\u001b[A\n",
      "16it [00:22,  1.44s/it]\u001b[A\n",
      "17it [00:23,  1.48s/it]\u001b[A\n",
      "18it [00:25,  1.61s/it]\u001b[A\n",
      "19it [00:27,  1.79s/it]\u001b[A\n",
      "20it [00:30,  1.88s/it]\u001b[A\n",
      "21it [00:31,  1.88s/it]\u001b[A\n",
      "22it [00:33,  1.91s/it]\u001b[A\n",
      "23it [00:35,  1.93s/it]\u001b[A\n",
      "24it [00:36,  1.53s/it]\u001b[A\n",
      "25it [00:39,  1.85s/it]\u001b[A\n",
      "26it [00:41,  2.08s/it]\u001b[A\n",
      "27it [00:43,  2.13s/it]\u001b[A\n",
      "28it [00:45,  2.10s/it]\u001b[A\n",
      "29it [00:47,  2.03s/it]\u001b[A\n",
      "30it [00:49,  2.05s/it]\u001b[A\n",
      "31it [00:52,  2.25s/it]\u001b[A\n",
      "32it [00:55,  2.39s/it]\u001b[A\n",
      "33it [00:57,  2.41s/it]\u001b[A\n",
      "34it [01:00,  2.39s/it]\u001b[A\n",
      "35it [01:03,  2.54s/it]\u001b[A\n",
      "36it [01:03,  1.92s/it]\u001b[A\n",
      "37it [01:07,  2.41s/it]\u001b[A\n",
      "38it [01:10,  2.72s/it]\u001b[A\n",
      "39it [01:13,  2.76s/it]\u001b[A\n",
      "40it [01:15,  2.41s/it]\u001b[A\n",
      "41it [01:15,  1.88s/it]\u001b[A\n",
      "42it [01:19,  2.36s/it]\u001b[A\n",
      "43it [01:22,  2.66s/it]\u001b[A\n",
      "44it [01:25,  2.90s/it]\u001b[A\n",
      "45it [01:28,  2.80s/it]\u001b[A\n",
      "46it [01:31,  2.96s/it]\u001b[A\n",
      "47it [01:35,  3.22s/it]\u001b[A\n",
      "48it [01:39,  3.43s/it]\u001b[A\n",
      "49it [01:39,  2.46s/it]\u001b[A\n",
      "50it [01:43,  2.96s/it]\u001b[A\n",
      "51it [01:47,  3.16s/it]\u001b[A\n",
      "52it [01:49,  2.79s/it]\u001b[A\n",
      "53it [01:53,  3.27s/it]\u001b[A\n",
      "54it [01:57,  3.42s/it]\u001b[A\n",
      "55it [02:02,  3.87s/it]\u001b[A\n",
      "56it [02:04,  3.31s/it]\u001b[A\n",
      "57it [02:08,  3.38s/it]\u001b[A\n",
      "58it [02:12,  3.56s/it]\u001b[A\n",
      "59it [02:15,  3.39s/it]\u001b[A\n",
      "60it [02:17,  3.08s/it]\u001b[A\n",
      "61it [02:18,  2.56s/it]\u001b[A\n",
      "62it [02:20,  2.21s/it]\u001b[A\n",
      "63it [02:21,  1.85s/it]\u001b[A\n",
      "64it [02:22,  1.55s/it]\u001b[A\n",
      "65it [02:22,  1.25s/it]\u001b[A\n",
      "66it [02:22,  1.00it/s]\u001b[A\n",
      "67it [02:23,  1.19it/s]\u001b[A\n",
      "68it [02:23,  1.37it/s]\u001b[A\n",
      "69it [02:24,  1.49it/s]\u001b[A\n",
      "70it [02:25,  2.07s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|██▊                                   | 492/6740 [00:00<00:01, 4910.79it/s]\u001b[A\n",
      " 15%|█████▌                               | 1021/6740 [00:00<00:01, 5125.96it/s]\u001b[A\n",
      " 23%|████████▍                            | 1534/6740 [00:00<00:01, 4821.26it/s]\u001b[A\n",
      " 30%|███████████                          | 2019/6740 [00:00<00:00, 4735.05it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 2494/6740 [00:00<00:00, 4526.50it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 2949/6740 [00:00<00:00, 4407.83it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 3391/6740 [00:00<00:00, 4357.66it/s]\u001b[A\n",
      " 57%|█████████████████████                | 3828/6740 [00:00<00:00, 4329.35it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 4262/6740 [00:00<00:00, 4253.19it/s]\u001b[A\n",
      " 70%|█████████████████████████▋           | 4688/6740 [00:01<00:00, 4196.62it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 5108/6740 [00:01<00:00, 4122.23it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 5521/6740 [00:01<00:00, 4036.31it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 5932/6740 [00:01<00:00, 4055.62it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 4260.20it/s]\u001b[A\n",
      "Epoch: 6, tfr: 1.0, loss(train/dev): 3.8444/3.8552, ppl(dev/test): 47.2381/54.18\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.7732: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.7732: : 1it [00:01,  1.85s/it]\u001b[A\n",
      "batch 2, training loss: 3.6961: : 1it [00:02,  1.85s/it]\u001b[A\n",
      "batch 2, training loss: 3.6961: : 2it [00:02,  1.12s/it]\u001b[A\n",
      "batch 3, training loss: 3.8101: : 2it [00:03,  1.12s/it]\u001b[A\n",
      "batch 3, training loss: 3.8101: : 3it [00:03,  1.10it/s]\u001b[A\n",
      "batch 4, training loss: 3.7553: : 3it [00:03,  1.10it/s]\u001b[A\n",
      "batch 4, training loss: 3.7553: : 4it [00:03,  1.20it/s]\u001b[A\n",
      "batch 5, training loss: 3.5935: : 4it [00:04,  1.20it/s]\u001b[A\n",
      "batch 5, training loss: 3.5935: : 5it [00:04,  1.29it/s]\u001b[A\n",
      "batch 6, training loss: 3.8143: : 5it [00:05,  1.29it/s]\u001b[A\n",
      "batch 6, training loss: 3.8143: : 6it [00:05,  1.45it/s]\u001b[A\n",
      "batch 7, training loss: 3.7976: : 6it [00:05,  1.45it/s]\u001b[A\n",
      "batch 7, training loss: 3.7976: : 7it [00:05,  1.53it/s]\u001b[A\n",
      "batch 8, training loss: 3.8139: : 7it [00:06,  1.53it/s]\u001b[A\n",
      "batch 8, training loss: 3.8139: : 8it [00:06,  1.53it/s]\u001b[A\n",
      "batch 9, training loss: 3.6615: : 8it [00:06,  1.53it/s]\u001b[A\n",
      "batch 9, training loss: 3.6615: : 9it [00:06,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.6738: : 9it [00:07,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.6738: : 10it [00:07,  1.53it/s]\u001b[A\n",
      "batch 11, training loss: 3.7263: : 10it [00:08,  1.53it/s]\u001b[A\n",
      "batch 11, training loss: 3.7263: : 11it [00:08,  1.56it/s]\u001b[A\n",
      "batch 12, training loss: 3.7697: : 11it [00:08,  1.56it/s]\u001b[A\n",
      "batch 12, training loss: 3.7697: : 12it [00:08,  1.58it/s]\u001b[A\n",
      "batch 13, training loss: 3.6551: : 12it [00:09,  1.58it/s]\u001b[A\n",
      "batch 13, training loss: 3.6551: : 13it [00:09,  1.55it/s]\u001b[A\n",
      "batch 14, training loss: 3.9364: : 13it [00:10,  1.55it/s]\u001b[A\n",
      "batch 14, training loss: 3.9364: : 14it [00:10,  1.54it/s]\u001b[A\n",
      "batch 15, training loss: 3.7797: : 14it [00:10,  1.54it/s]\u001b[A\n",
      "batch 15, training loss: 3.7797: : 15it [00:10,  1.50it/s]\u001b[A\n",
      "batch 16, training loss: 3.7765: : 15it [00:11,  1.50it/s]\u001b[A\n",
      "batch 16, training loss: 3.7765: : 16it [00:11,  1.47it/s]\u001b[A\n",
      "batch 17, training loss: 3.9355: : 16it [00:12,  1.47it/s]\u001b[A\n",
      "batch 17, training loss: 3.9355: : 17it [00:12,  1.49it/s]\u001b[A\n",
      "batch 18, training loss: 3.7373: : 17it [00:12,  1.49it/s]\u001b[A\n",
      "batch 18, training loss: 3.7373: : 18it [00:12,  1.57it/s]\u001b[A\n",
      "batch 19, training loss: 3.5271: : 18it [00:13,  1.57it/s]\u001b[A\n",
      "batch 19, training loss: 3.5271: : 19it [00:13,  1.65it/s]\u001b[A\n",
      "batch 20, training loss: 3.688: : 19it [00:13,  1.65it/s] \u001b[A\n",
      "batch 20, training loss: 3.688: : 20it [00:13,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 3.8139: : 20it [00:14,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 3.8139: : 21it [00:14,  1.57it/s]\u001b[A\n",
      "batch 22, training loss: 3.5911: : 21it [00:15,  1.57it/s]\u001b[A\n",
      "batch 22, training loss: 3.5911: : 22it [00:15,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 3.7381: : 22it [00:15,  1.56it/s]\u001b[A\n",
      "batch 23, training loss: 3.7381: : 23it [00:15,  1.59it/s]\u001b[A\n",
      "batch 24, training loss: 3.6724: : 23it [00:16,  1.59it/s]\u001b[A\n",
      "batch 24, training loss: 3.6724: : 24it [00:16,  1.60it/s]\u001b[A\n",
      "batch 25, training loss: 3.7518: : 24it [00:17,  1.60it/s]\u001b[A\n",
      "batch 25, training loss: 3.7518: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.5765: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.5765: : 26it [00:17,  1.56it/s]\u001b[A\n",
      "batch 27, training loss: 3.6958: : 26it [00:18,  1.56it/s]\u001b[A\n",
      "batch 27, training loss: 3.6958: : 27it [00:18,  1.51it/s]\u001b[A\n",
      "batch 28, training loss: 3.5779: : 27it [00:19,  1.51it/s]\u001b[A\n",
      "batch 28, training loss: 3.5779: : 28it [00:19,  1.49it/s]\u001b[A\n",
      "batch 29, training loss: 3.7297: : 28it [00:19,  1.49it/s]\u001b[A\n",
      "batch 29, training loss: 3.7297: : 29it [00:19,  1.48it/s]\u001b[A\n",
      "batch 30, training loss: 3.8457: : 29it [00:20,  1.48it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, training loss: 3.8457: : 30it [00:20,  1.61it/s]\u001b[A\n",
      "batch 31, training loss: 3.589: : 30it [00:20,  1.61it/s] \u001b[A\n",
      "batch 31, training loss: 3.589: : 31it [00:20,  1.65it/s]\u001b[A\n",
      "batch 32, training loss: 3.7676: : 31it [00:21,  1.65it/s]\u001b[A\n",
      "batch 32, training loss: 3.7676: : 32it [00:21,  1.61it/s]\u001b[A\n",
      "batch 33, training loss: 3.6713: : 32it [00:22,  1.61it/s]\u001b[A\n",
      "batch 33, training loss: 3.6713: : 33it [00:22,  1.57it/s]\u001b[A\n",
      "batch 34, training loss: 3.664: : 33it [00:22,  1.57it/s] \u001b[A\n",
      "batch 34, training loss: 3.664: : 34it [00:22,  1.55it/s]\u001b[A\n",
      "batch 35, training loss: 3.7361: : 34it [00:23,  1.55it/s]\u001b[A\n",
      "batch 35, training loss: 3.7361: : 35it [00:23,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 3.8233: : 35it [00:24,  1.57it/s]\u001b[A\n",
      "batch 36, training loss: 3.8233: : 36it [00:24,  1.58it/s]\u001b[A\n",
      "batch 37, training loss: 3.6778: : 36it [00:24,  1.58it/s]\u001b[A\n",
      "batch 37, training loss: 3.6778: : 37it [00:24,  1.58it/s]\u001b[A\n",
      "batch 38, training loss: 3.4843: : 37it [00:25,  1.58it/s]\u001b[A\n",
      "batch 38, training loss: 3.4843: : 38it [00:25,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 3.6148: : 38it [00:26,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 3.6148: : 39it [00:26,  1.56it/s]\u001b[A\n",
      "batch 40, training loss: 3.7143: : 39it [00:26,  1.56it/s]\u001b[A\n",
      "batch 40, training loss: 3.7143: : 40it [00:26,  1.52it/s]\u001b[A\n",
      "batch 41, training loss: 3.8761: : 40it [00:27,  1.52it/s]\u001b[A\n",
      "batch 41, training loss: 3.8761: : 41it [00:27,  1.55it/s]\u001b[A\n",
      "batch 42, training loss: 3.7054: : 41it [00:28,  1.55it/s]\u001b[A\n",
      "batch 42, training loss: 3.7054: : 42it [00:28,  1.59it/s]\u001b[A\n",
      "batch 43, training loss: 3.5053: : 42it [00:28,  1.59it/s]\u001b[A\n",
      "batch 43, training loss: 3.5053: : 43it [00:28,  1.62it/s]\u001b[A\n",
      "batch 44, training loss: 3.4966: : 43it [00:29,  1.62it/s]\u001b[A\n",
      "batch 44, training loss: 3.4966: : 44it [00:29,  1.59it/s]\u001b[A\n",
      "batch 45, training loss: 3.5108: : 44it [00:29,  1.59it/s]\u001b[A\n",
      "batch 45, training loss: 3.5108: : 45it [00:29,  1.56it/s]\u001b[A\n",
      "batch 46, training loss: 3.661: : 45it [00:30,  1.56it/s] \u001b[A\n",
      "batch 46, training loss: 3.661: : 46it [00:30,  1.68it/s]\u001b[A\n",
      "batch 47, training loss: 3.6593: : 46it [00:31,  1.68it/s]\u001b[A\n",
      "batch 47, training loss: 3.6593: : 47it [00:31,  1.69it/s]\u001b[A\n",
      "batch 48, training loss: 3.5771: : 47it [00:31,  1.69it/s]\u001b[A\n",
      "batch 48, training loss: 3.5771: : 48it [00:31,  1.64it/s]\u001b[A\n",
      "batch 49, training loss: 3.823: : 48it [00:32,  1.64it/s] \u001b[A\n",
      "batch 49, training loss: 3.823: : 49it [00:32,  1.60it/s]\u001b[A\n",
      "batch 50, training loss: 3.5792: : 49it [00:32,  1.60it/s]\u001b[A\n",
      "batch 50, training loss: 3.5792: : 50it [00:32,  1.70it/s]\u001b[A\n",
      "batch 51, training loss: 3.67: : 50it [00:33,  1.70it/s]  \u001b[A\n",
      "batch 51, training loss: 3.67: : 51it [00:33,  1.70it/s]\u001b[A\n",
      "batch 52, training loss: 3.5355: : 51it [00:34,  1.70it/s]\u001b[A\n",
      "batch 52, training loss: 3.5355: : 52it [00:34,  1.67it/s]\u001b[A\n",
      "batch 53, training loss: 3.555: : 52it [00:34,  1.67it/s] \u001b[A\n",
      "batch 53, training loss: 3.555: : 53it [00:34,  1.62it/s]\u001b[A\n",
      "batch 54, training loss: 3.4918: : 53it [00:35,  1.62it/s]\u001b[A\n",
      "batch 54, training loss: 3.4918: : 54it [00:35,  1.60it/s]\u001b[A\n",
      "batch 55, training loss: 3.6348: : 54it [00:35,  1.60it/s]\u001b[A\n",
      "batch 55, training loss: 3.6348: : 55it [00:35,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.6658: : 55it [00:36,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.6658: : 56it [00:36,  1.72it/s]\u001b[A\n",
      "batch 57, training loss: 3.6273: : 56it [00:37,  1.72it/s]\u001b[A\n",
      "batch 57, training loss: 3.6273: : 57it [00:37,  1.66it/s]\u001b[A\n",
      "batch 58, training loss: 3.6857: : 57it [00:37,  1.66it/s]\u001b[A\n",
      "batch 58, training loss: 3.6857: : 58it [00:37,  1.59it/s]\u001b[A\n",
      "batch 59, training loss: 3.5395: : 58it [00:38,  1.59it/s]\u001b[A\n",
      "batch 59, training loss: 3.5395: : 59it [00:38,  1.54it/s]\u001b[A\n",
      "batch 60, training loss: 3.4991: : 59it [00:39,  1.54it/s]\u001b[A\n",
      "batch 60, training loss: 3.4991: : 60it [00:39,  1.57it/s]\u001b[A\n",
      "batch 61, training loss: 3.7516: : 60it [00:39,  1.57it/s]\u001b[A\n",
      "batch 61, training loss: 3.7516: : 61it [00:39,  1.59it/s]\u001b[A\n",
      "batch 62, training loss: 3.5698: : 61it [00:40,  1.59it/s]\u001b[A\n",
      "batch 62, training loss: 3.5698: : 62it [00:40,  1.61it/s]\u001b[A\n",
      "batch 63, training loss: 3.6594: : 62it [00:40,  1.61it/s]\u001b[A\n",
      "batch 63, training loss: 3.6594: : 63it [00:40,  1.57it/s]\u001b[A\n",
      "batch 64, training loss: 3.6259: : 63it [00:41,  1.57it/s]\u001b[A\n",
      "batch 64, training loss: 3.6259: : 64it [00:41,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.6966: : 64it [00:42,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.6966: : 65it [00:42,  1.50it/s]\u001b[A\n",
      "batch 66, training loss: 3.7033: : 65it [00:43,  1.50it/s]\u001b[A\n",
      "batch 66, training loss: 3.7033: : 66it [00:43,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.5822: : 66it [00:43,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.5822: : 67it [00:43,  1.57it/s]\u001b[A\n",
      "batch 68, training loss: 3.6851: : 67it [00:44,  1.57it/s]\u001b[A\n",
      "batch 68, training loss: 3.6851: : 68it [00:44,  1.66it/s]\u001b[A\n",
      "batch 69, training loss: 3.5702: : 68it [00:44,  1.66it/s]\u001b[A\n",
      "batch 69, training loss: 3.5702: : 69it [00:44,  1.62it/s]\u001b[A\n",
      "batch 70, training loss: 3.7539: : 69it [00:45,  1.62it/s]\u001b[A\n",
      "batch 70, training loss: 3.7539: : 70it [00:45,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.5847: : 70it [00:46,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.5847: : 71it [00:46,  1.55it/s]\u001b[A\n",
      "batch 72, training loss: 3.5778: : 71it [00:46,  1.55it/s]\u001b[A\n",
      "batch 72, training loss: 3.5778: : 72it [00:46,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.6109: : 72it [00:47,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.6109: : 73it [00:47,  1.58it/s]\u001b[A\n",
      "batch 74, training loss: 3.4938: : 73it [00:47,  1.58it/s]\u001b[A\n",
      "batch 74, training loss: 3.4938: : 74it [00:47,  1.59it/s]\u001b[A\n",
      "batch 75, training loss: 3.7339: : 74it [00:48,  1.59it/s]\u001b[A\n",
      "batch 75, training loss: 3.7339: : 75it [00:48,  1.56it/s]\u001b[A\n",
      "batch 76, training loss: 3.5487: : 75it [00:49,  1.56it/s]\u001b[A\n",
      "batch 76, training loss: 3.5487: : 76it [00:49,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 3.6869: : 76it [00:50,  1.51it/s]\u001b[A\n",
      "batch 77, training loss: 3.6869: : 77it [00:50,  1.50it/s]\u001b[A\n",
      "batch 78, training loss: 3.6748: : 77it [00:50,  1.50it/s]\u001b[A\n",
      "batch 78, training loss: 3.6748: : 78it [00:50,  1.61it/s]\u001b[A\n",
      "batch 79, training loss: 3.7064: : 78it [00:51,  1.61it/s]\u001b[A\n",
      "batch 79, training loss: 3.7064: : 79it [00:51,  1.64it/s]\u001b[A\n",
      "batch 80, training loss: 3.6177: : 79it [00:51,  1.64it/s]\u001b[A\n",
      "batch 80, training loss: 3.6177: : 80it [00:51,  1.62it/s]\u001b[A\n",
      "batch 81, training loss: 3.6253: : 80it [00:52,  1.62it/s]\u001b[A\n",
      "batch 81, training loss: 3.6253: : 81it [00:52,  1.59it/s]\u001b[A\n",
      "batch 82, training loss: 3.777: : 81it [00:53,  1.59it/s] \u001b[A\n",
      "batch 82, training loss: 3.777: : 82it [00:53,  1.56it/s]\u001b[A\n",
      "batch 83, training loss: 3.6192: : 82it [00:53,  1.56it/s]\u001b[A\n",
      "batch 83, training loss: 3.6192: : 83it [00:53,  1.59it/s]\u001b[A\n",
      "batch 84, training loss: 3.7601: : 83it [00:54,  1.59it/s]\u001b[A\n",
      "batch 84, training loss: 3.7601: : 84it [00:54,  1.60it/s]\u001b[A\n",
      "batch 85, training loss: 3.799: : 84it [00:54,  1.60it/s] \u001b[A\n",
      "batch 85, training loss: 3.799: : 85it [00:54,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.7228: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.7228: : 86it [00:55,  1.54it/s]\u001b[A\n",
      "batch 87, training loss: 3.7761: : 86it [00:56,  1.54it/s]\u001b[A\n",
      "batch 87, training loss: 3.7761: : 87it [00:56,  1.56it/s]\u001b[A\n",
      "batch 88, training loss: 3.8715: : 87it [00:57,  1.56it/s]\u001b[A\n",
      "batch 88, training loss: 3.8715: : 88it [00:57,  1.47it/s]\u001b[A\n",
      "batch 89, training loss: 3.9816: : 88it [00:57,  1.47it/s]\u001b[A\n",
      "batch 89, training loss: 3.9816: : 89it [00:57,  1.41it/s]\u001b[A\n",
      "batch 90, training loss: 3.8749: : 89it [00:58,  1.41it/s]\u001b[A\n",
      "batch 90, training loss: 3.8749: : 90it [00:58,  1.37it/s]\u001b[A\n",
      "batch 91, training loss: 4.0156: : 90it [00:59,  1.37it/s]\u001b[A\n",
      "batch 91, training loss: 4.0156: : 91it [00:59,  1.33it/s]\u001b[A\n",
      "batch 92, training loss: 3.8552: : 91it [01:00,  1.33it/s]\u001b[A\n",
      "batch 92, training loss: 3.8552: : 92it [01:00,  1.32it/s]\u001b[A\n",
      "batch 93, training loss: 3.813: : 92it [01:00,  1.32it/s] \u001b[A\n",
      "batch 93, training loss: 3.813: : 93it [01:00,  1.31it/s]\u001b[A\n",
      "batch 94, training loss: 3.9094: : 93it [01:01,  1.31it/s]\u001b[A\n",
      "batch 94, training loss: 3.9094: : 94it [01:01,  1.30it/s]\u001b[A\n",
      "batch 95, training loss: 3.882: : 94it [01:02,  1.30it/s] \u001b[A\n",
      "batch 95, training loss: 3.882: : 95it [01:02,  1.29it/s]\u001b[A\n",
      "batch 96, training loss: 3.8804: : 95it [01:03,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 96, training loss: 3.8804: : 96it [01:03,  1.27it/s]\u001b[A\n",
      "batch 97, training loss: 3.9503: : 96it [01:04,  1.27it/s]\u001b[A\n",
      "batch 97, training loss: 3.9503: : 97it [01:04,  1.28it/s]\u001b[A\n",
      "batch 98, training loss: 3.9414: : 97it [01:04,  1.28it/s]\u001b[A\n",
      "batch 98, training loss: 3.9414: : 98it [01:04,  1.26it/s]\u001b[A\n",
      "batch 99, training loss: 3.7482: : 98it [01:05,  1.26it/s]\u001b[A\n",
      "batch 99, training loss: 3.7482: : 99it [01:05,  1.28it/s]\u001b[A\n",
      "batch 100, training loss: 3.571: : 99it [01:06,  1.28it/s]\u001b[A\n",
      "batch 100, training loss: 3.571: : 100it [01:06,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.7728: : 100it [01:07,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.7728: : 101it [01:07,  1.29it/s]\u001b[A\n",
      "batch 102, training loss: 3.7441: : 101it [01:08,  1.29it/s]\u001b[A\n",
      "batch 102, training loss: 3.7441: : 102it [01:08,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 3.6962: : 102it [01:08,  1.29it/s]\u001b[A\n",
      "batch 103, training loss: 3.6962: : 103it [01:08,  1.29it/s]\u001b[A\n",
      "batch 104, training loss: 3.5372: : 103it [01:09,  1.29it/s]\u001b[A\n",
      "batch 104, training loss: 3.5372: : 104it [01:09,  1.29it/s]\u001b[A\n",
      "batch 105, training loss: 3.7138: : 104it [01:10,  1.29it/s]\u001b[A\n",
      "batch 105, training loss: 3.7138: : 105it [01:10,  1.28it/s]\u001b[A\n",
      "batch 106, training loss: 3.8211: : 105it [01:11,  1.28it/s]\u001b[A\n",
      "batch 106, training loss: 3.8211: : 106it [01:11,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.5287: : 106it [01:11,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.5287: : 107it [01:11,  1.28it/s]\u001b[A\n",
      "batch 108, training loss: 3.8043: : 107it [01:12,  1.28it/s]\u001b[A\n",
      "batch 108, training loss: 3.8043: : 108it [01:12,  1.28it/s]\u001b[A\n",
      "batch 109, training loss: 3.8612: : 108it [01:13,  1.28it/s]\u001b[A\n",
      "batch 109, training loss: 3.8612: : 109it [01:13,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 3.9678: : 109it [01:14,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 3.9678: : 110it [01:14,  1.28it/s]\u001b[A\n",
      "batch 111, training loss: 3.6764: : 110it [01:15,  1.28it/s]\u001b[A\n",
      "batch 111, training loss: 3.6764: : 111it [01:15,  1.28it/s]\u001b[A\n",
      "batch 112, training loss: 3.7393: : 111it [01:15,  1.28it/s]\u001b[A\n",
      "batch 112, training loss: 3.7393: : 112it [01:15,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.8286: : 112it [01:16,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.8286: : 113it [01:16,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 3.8176: : 113it [01:17,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 3.8176: : 114it [01:17,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.7111: : 114it [01:18,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.7111: : 115it [01:18,  1.28it/s]\u001b[A\n",
      "batch 116, training loss: 3.7056: : 115it [01:18,  1.28it/s]\u001b[A\n",
      "batch 116, training loss: 3.7056: : 116it [01:18,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.7593: : 116it [01:19,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.7593: : 117it [01:19,  1.29it/s]\u001b[A\n",
      "batch 118, training loss: 3.8497: : 117it [01:20,  1.29it/s]\u001b[A\n",
      "batch 118, training loss: 3.8497: : 118it [01:20,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.7353: : 118it [01:21,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.7353: : 119it [01:21,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 3.6951: : 119it [01:21,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 3.6951: : 120it [01:21,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 3.9342: : 120it [01:22,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 3.9342: : 121it [01:22,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.5974: : 121it [01:23,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.5974: : 122it [01:23,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 3.7812: : 122it [01:24,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 3.7812: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.7679: : 123it [01:25,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.7679: : 124it [01:25,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.7896: : 124it [01:25,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.7896: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.8413: : 125it [01:26,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.8413: : 126it [01:26,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 3.5559: : 126it [01:27,  1.31it/s]\u001b[A\n",
      "batch 127, training loss: 3.5559: : 127it [01:27,  1.31it/s]\u001b[A\n",
      "batch 128, training loss: 3.7556: : 127it [01:28,  1.31it/s]\u001b[A\n",
      "batch 128, training loss: 3.7556: : 128it [01:28,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.7133: : 128it [01:28,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.7133: : 129it [01:28,  1.32it/s]\u001b[A\n",
      "batch 130, training loss: 3.7509: : 129it [01:29,  1.32it/s]\u001b[A\n",
      "batch 130, training loss: 3.7509: : 130it [01:29,  1.48it/s]\u001b[A\n",
      "batch 131, training loss: 3.8881: : 130it [01:30,  1.48it/s]\u001b[A\n",
      "batch 131, training loss: 3.8881: : 131it [01:30,  1.46it/s]\u001b[A\n",
      "batch 132, training loss: 3.6905: : 131it [01:30,  1.46it/s]\u001b[A\n",
      "batch 132, training loss: 3.6905: : 132it [01:30,  1.41it/s]\u001b[A\n",
      "batch 133, training loss: 3.5903: : 132it [01:31,  1.41it/s]\u001b[A\n",
      "batch 133, training loss: 3.5903: : 133it [01:31,  1.39it/s]\u001b[A\n",
      "batch 134, training loss: 3.7332: : 133it [01:32,  1.39it/s]\u001b[A\n",
      "batch 134, training loss: 3.7332: : 134it [01:32,  1.52it/s]\u001b[A\n",
      "batch 135, training loss: 3.6627: : 134it [01:32,  1.52it/s]\u001b[A\n",
      "batch 135, training loss: 3.6627: : 135it [01:32,  1.63it/s]\u001b[A\n",
      "batch 136, training loss: 3.6232: : 135it [01:33,  1.63it/s]\u001b[A\n",
      "batch 136, training loss: 3.6232: : 136it [01:33,  1.56it/s]\u001b[A\n",
      "batch 137, training loss: 3.7856: : 136it [01:34,  1.56it/s]\u001b[A\n",
      "batch 137, training loss: 3.7856: : 137it [01:34,  1.49it/s]\u001b[A\n",
      "batch 138, training loss: 3.8018: : 137it [01:34,  1.49it/s]\u001b[A\n",
      "batch 138, training loss: 3.8018: : 138it [01:34,  1.42it/s]\u001b[A\n",
      "batch 139, training loss: 3.5813: : 138it [01:35,  1.42it/s]\u001b[A\n",
      "batch 139, training loss: 3.5813: : 139it [01:35,  1.39it/s]\u001b[A\n",
      "batch 140, training loss: 3.6762: : 139it [01:36,  1.39it/s]\u001b[A\n",
      "batch 140, training loss: 3.6762: : 140it [01:36,  1.35it/s]\u001b[A\n",
      "batch 141, training loss: 3.6782: : 140it [01:37,  1.35it/s]\u001b[A\n",
      "batch 141, training loss: 3.6782: : 141it [01:37,  1.33it/s]\u001b[A\n",
      "batch 142, training loss: 3.7125: : 141it [01:37,  1.33it/s]\u001b[A\n",
      "batch 142, training loss: 3.7125: : 142it [01:37,  1.33it/s]\u001b[A\n",
      "batch 143, training loss: 3.5403: : 142it [01:38,  1.33it/s]\u001b[A\n",
      "batch 143, training loss: 3.5403: : 143it [01:38,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 3.5966: : 143it [01:39,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 3.5966: : 144it [01:39,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 3.7095: : 144it [01:40,  1.31it/s]\u001b[A\n",
      "batch 145, training loss: 3.7095: : 145it [01:40,  1.31it/s]\u001b[A\n",
      "batch 146, training loss: 3.6897: : 145it [01:40,  1.31it/s]\u001b[A\n",
      "batch 146, training loss: 3.6897: : 146it [01:40,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.5845: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.5845: : 147it [01:41,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.6662: : 147it [01:42,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.6662: : 148it [01:42,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.775: : 148it [01:43,  1.30it/s] \u001b[A\n",
      "batch 149, training loss: 3.775: : 149it [01:43,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 3.7464: : 149it [01:44,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 3.7464: : 150it [01:44,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.6834: : 150it [01:44,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.6834: : 151it [01:44,  1.30it/s]\u001b[A\n",
      "batch 152, training loss: 3.619: : 151it [01:45,  1.30it/s] \u001b[A\n",
      "batch 152, training loss: 3.619: : 152it [01:45,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 3.6538: : 152it [01:46,  1.32it/s]\u001b[A\n",
      "batch 153, training loss: 3.6538: : 153it [01:46,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.7625: : 153it [01:47,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.7625: : 154it [01:47,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 3.8857: : 154it [01:47,  1.30it/s]\u001b[A\n",
      "batch 155, training loss: 3.8857: : 155it [01:47,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.5091: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.5091: : 156it [01:48,  1.33it/s]\u001b[A\n",
      "batch 157, training loss: 3.7181: : 156it [01:49,  1.33it/s]\u001b[A\n",
      "batch 157, training loss: 3.7181: : 157it [01:49,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 3.6719: : 157it [01:50,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 3.6719: : 158it [01:50,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 3.5938: : 158it [01:50,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 3.5938: : 159it [01:50,  1.31it/s]\u001b[A\n",
      "batch 160, training loss: 3.7705: : 159it [01:51,  1.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 160, training loss: 3.7705: : 160it [01:51,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.6312: : 160it [01:52,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.6312: : 161it [01:52,  1.30it/s]\u001b[A\n",
      "batch 162, training loss: 3.6317: : 161it [01:53,  1.30it/s]\u001b[A\n",
      "batch 162, training loss: 3.6317: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.8606: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.8606: : 163it [01:53,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.6408: : 163it [01:54,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.6408: : 164it [01:54,  1.31it/s]\u001b[A\n",
      "batch 165, training loss: 3.6523: : 164it [01:55,  1.31it/s]\u001b[A\n",
      "batch 165, training loss: 3.6523: : 165it [01:55,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 3.6339: : 165it [01:56,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 3.6339: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.7936: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.7936: : 167it [01:56,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.7381: : 167it [01:57,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.7381: : 168it [01:57,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 3.8009: : 168it [01:58,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 3.8009: : 169it [01:58,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 3.671: : 169it [01:59,  1.31it/s] \u001b[A\n",
      "batch 170, training loss: 3.671: : 170it [01:59,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 2.98: : 170it [01:59,  1.30it/s] \u001b[A\n",
      "batch 171, training loss: 2.98: : 171it [01:59,  1.58it/s]\u001b[A\n",
      "batch 172, training loss: 4.0554: : 171it [02:00,  1.58it/s]\u001b[A\n",
      "batch 172, training loss: 4.0554: : 172it [02:00,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 3.9243: : 172it [02:01,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 3.9243: : 173it [02:01,  1.38it/s]\u001b[A\n",
      "batch 174, training loss: 4.0783: : 173it [02:02,  1.38it/s]\u001b[A\n",
      "batch 174, training loss: 4.0783: : 174it [02:02,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 4.0121: : 174it [02:02,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 4.0121: : 175it [02:02,  1.30it/s]\u001b[A\n",
      "batch 176, training loss: 3.937: : 175it [02:03,  1.30it/s] \u001b[A\n",
      "batch 176, training loss: 3.937: : 176it [02:03,  1.28it/s]\u001b[A\n",
      "batch 177, training loss: 3.9416: : 176it [02:04,  1.28it/s]\u001b[A\n",
      "batch 177, training loss: 3.9416: : 177it [02:04,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 3.9609: : 177it [02:05,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 3.9609: : 178it [02:05,  1.26it/s]\u001b[A\n",
      "batch 179, training loss: 3.9831: : 178it [02:06,  1.26it/s]\u001b[A\n",
      "batch 179, training loss: 3.9831: : 179it [02:06,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 4.0344: : 179it [02:06,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 4.0344: : 180it [02:06,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.0589: : 180it [02:07,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 4.0589: : 181it [02:07,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.9545: : 181it [02:08,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.9545: : 182it [02:08,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 4.0554: : 182it [02:09,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 4.0554: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.8386: : 183it [02:10,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.8386: : 184it [02:10,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 3.8717: : 184it [02:10,  1.24it/s]\u001b[A\n",
      "batch 185, training loss: 3.8717: : 185it [02:10,  1.23it/s]\u001b[A\n",
      "batch 186, training loss: 4.0337: : 185it [02:11,  1.23it/s]\u001b[A\n",
      "batch 186, training loss: 4.0337: : 186it [02:11,  1.21it/s]\u001b[A\n",
      "batch 187, training loss: 3.9911: : 186it [02:12,  1.21it/s]\u001b[A\n",
      "batch 187, training loss: 3.9911: : 187it [02:12,  1.20it/s]\u001b[A\n",
      "batch 188, training loss: 4.0938: : 187it [02:13,  1.20it/s]\u001b[A\n",
      "batch 188, training loss: 4.0938: : 188it [02:13,  1.21it/s]\u001b[A\n",
      "batch 189, training loss: 4.1222: : 188it [02:14,  1.21it/s]\u001b[A\n",
      "batch 189, training loss: 4.1222: : 189it [02:14,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.7778: : 189it [02:15,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.7778: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.7833: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.7833: : 191it [02:15,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.8877: : 191it [02:16,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.8877: : 192it [02:16,  1.23it/s]\u001b[A\n",
      "batch 193, training loss: 3.768: : 192it [02:17,  1.23it/s] \u001b[A\n",
      "batch 193, training loss: 3.768: : 193it [02:17,  1.25it/s]\u001b[A\n",
      "batch 194, training loss: 3.8545: : 193it [02:18,  1.25it/s]\u001b[A\n",
      "batch 194, training loss: 3.8545: : 194it [02:18,  1.23it/s]\u001b[A\n",
      "batch 195, training loss: 3.7891: : 194it [02:19,  1.23it/s]\u001b[A\n",
      "batch 195, training loss: 3.7891: : 195it [02:19,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.7451: : 195it [02:19,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.7451: : 196it [02:19,  1.25it/s]\u001b[A\n",
      "batch 197, training loss: 3.8991: : 196it [02:20,  1.25it/s]\u001b[A\n",
      "batch 197, training loss: 3.8991: : 197it [02:20,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.8117: : 197it [02:21,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.8117: : 198it [02:21,  1.23it/s]\u001b[A\n",
      "batch 199, training loss: 3.7615: : 198it [02:22,  1.23it/s]\u001b[A\n",
      "batch 199, training loss: 3.7615: : 199it [02:22,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 3.9428: : 199it [02:23,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 3.9428: : 200it [02:23,  1.21it/s]\u001b[A\n",
      "batch 201, training loss: 3.7247: : 200it [02:24,  1.21it/s]\u001b[A\n",
      "batch 201, training loss: 3.7247: : 201it [02:24,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.6028: : 201it [02:24,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.6028: : 202it [02:24,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.8104: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.8104: : 203it [02:25,  1.22it/s]\u001b[A\n",
      "batch 204, training loss: 3.8423: : 203it [02:26,  1.22it/s]\u001b[A\n",
      "batch 204, training loss: 3.8423: : 204it [02:26,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 3.7804: : 204it [02:27,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 3.7804: : 205it [02:27,  1.22it/s]\u001b[A\n",
      "batch 206, training loss: 3.9326: : 205it [02:28,  1.22it/s]\u001b[A\n",
      "batch 206, training loss: 3.9326: : 206it [02:28,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 3.7341: : 206it [02:28,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 3.7341: : 207it [02:28,  1.23it/s]\u001b[A\n",
      "batch 208, training loss: 3.7951: : 207it [02:29,  1.23it/s]\u001b[A\n",
      "batch 208, training loss: 3.7951: : 208it [02:29,  1.23it/s]\u001b[A\n",
      "batch 209, training loss: 3.7303: : 208it [02:30,  1.23it/s]\u001b[A\n",
      "batch 209, training loss: 3.7303: : 209it [02:30,  1.21it/s]\u001b[A\n",
      "batch 210, training loss: 3.7727: : 209it [02:31,  1.21it/s]\u001b[A\n",
      "batch 210, training loss: 3.7727: : 210it [02:31,  1.22it/s]\u001b[A\n",
      "batch 211, training loss: 3.7554: : 210it [02:32,  1.22it/s]\u001b[A\n",
      "batch 211, training loss: 3.7554: : 211it [02:32,  1.21it/s]\u001b[A\n",
      "batch 212, training loss: 3.7427: : 211it [02:33,  1.21it/s]\u001b[A\n",
      "batch 212, training loss: 3.7427: : 212it [02:33,  1.21it/s]\u001b[A\n",
      "batch 213, training loss: 3.9475: : 212it [02:33,  1.21it/s]\u001b[A\n",
      "batch 213, training loss: 3.9475: : 213it [02:33,  1.23it/s]\u001b[A\n",
      "batch 214, training loss: 3.82: : 213it [02:34,  1.23it/s]  \u001b[A\n",
      "batch 214, training loss: 3.82: : 214it [02:34,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.6526: : 214it [02:35,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.6526: : 215it [02:35,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.8012: : 215it [02:36,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.8012: : 216it [02:36,  1.24it/s]\u001b[A\n",
      "batch 217, training loss: 3.8815: : 216it [02:37,  1.24it/s]\u001b[A\n",
      "batch 217, training loss: 3.8815: : 217it [02:37,  1.23it/s]\u001b[A\n",
      "batch 218, training loss: 3.826: : 217it [02:37,  1.23it/s] \u001b[A\n",
      "batch 218, training loss: 3.826: : 218it [02:37,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 4.0241: : 218it [02:38,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 4.0241: : 219it [02:38,  1.22it/s]\u001b[A\n",
      "batch 220, training loss: 3.9256: : 219it [02:39,  1.22it/s]\u001b[A\n",
      "batch 220, training loss: 3.9256: : 220it [02:39,  1.21it/s]\u001b[A\n",
      "batch 221, training loss: 3.8052: : 220it [02:40,  1.21it/s]\u001b[A\n",
      "batch 221, training loss: 3.8052: : 221it [02:40,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 3.8387: : 221it [02:41,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 3.8387: : 222it [02:41,  1.22it/s]\u001b[A\n",
      "batch 223, training loss: 3.842: : 222it [02:42,  1.22it/s] \u001b[A\n",
      "batch 223, training loss: 3.842: : 223it [02:42,  1.22it/s]\u001b[A\n",
      "batch 224, training loss: 3.7509: : 223it [02:42,  1.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 224, training loss: 3.7509: : 224it [02:42,  1.22it/s]\u001b[A\n",
      "batch 225, training loss: 3.8726: : 224it [02:43,  1.22it/s]\u001b[A\n",
      "batch 225, training loss: 3.8726: : 225it [02:43,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 3.8963: : 225it [02:44,  1.23it/s]\u001b[A\n",
      "batch 226, training loss: 3.8963: : 226it [02:44,  1.23it/s]\u001b[A\n",
      "batch 227, training loss: 3.7524: : 226it [02:45,  1.23it/s]\u001b[A\n",
      "batch 227, training loss: 3.7524: : 227it [02:45,  1.22it/s]\u001b[A\n",
      "batch 228, training loss: 3.8221: : 227it [02:46,  1.22it/s]\u001b[A\n",
      "batch 228, training loss: 3.8221: : 228it [02:46,  1.22it/s]\u001b[A\n",
      "batch 229, training loss: 3.7955: : 228it [02:46,  1.22it/s]\u001b[A\n",
      "batch 229, training loss: 3.7955: : 229it [02:46,  1.22it/s]\u001b[A\n",
      "batch 230, training loss: 3.9355: : 229it [02:47,  1.22it/s]\u001b[A\n",
      "batch 230, training loss: 3.9355: : 230it [02:47,  1.21it/s]\u001b[A\n",
      "batch 231, training loss: 3.9862: : 230it [02:48,  1.21it/s]\u001b[A\n",
      "batch 231, training loss: 3.9862: : 231it [02:48,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.7835: : 231it [02:49,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.7835: : 232it [02:49,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.8409: : 232it [02:50,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.8409: : 233it [02:50,  1.22it/s]\u001b[A\n",
      "batch 234, training loss: 3.9594: : 233it [02:51,  1.22it/s]\u001b[A\n",
      "batch 234, training loss: 3.9594: : 234it [02:51,  1.22it/s]\u001b[A\n",
      "batch 235, training loss: 3.808: : 234it [02:51,  1.22it/s] \u001b[A\n",
      "batch 235, training loss: 3.808: : 235it [02:51,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 3.8572: : 235it [02:52,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 3.8572: : 236it [02:52,  1.23it/s]\u001b[A\n",
      "batch 237, training loss: 3.7054: : 236it [02:53,  1.23it/s]\u001b[A\n",
      "batch 237, training loss: 3.7054: : 237it [02:53,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 3.8623: : 237it [02:54,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 3.8623: : 238it [02:54,  1.22it/s]\u001b[A\n",
      "batch 239, training loss: 3.7723: : 238it [02:55,  1.22it/s]\u001b[A\n",
      "batch 239, training loss: 3.7723: : 239it [02:55,  1.20it/s]\u001b[A\n",
      "batch 240, training loss: 3.8091: : 239it [02:55,  1.20it/s]\u001b[A\n",
      "batch 240, training loss: 3.8091: : 240it [02:55,  1.21it/s]\u001b[A\n",
      "batch 241, training loss: 3.828: : 240it [02:56,  1.21it/s] \u001b[A\n",
      "batch 241, training loss: 3.828: : 241it [02:56,  1.22it/s]\u001b[A\n",
      "batch 242, training loss: 3.6614: : 241it [02:57,  1.22it/s]\u001b[A\n",
      "batch 242, training loss: 3.6614: : 242it [02:57,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 3.8415: : 242it [02:58,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 3.8415: : 243it [02:58,  1.29it/s]\u001b[A\n",
      "batch 244, training loss: 3.7567: : 243it [02:58,  1.29it/s]\u001b[A\n",
      "batch 244, training loss: 3.7567: : 244it [02:58,  1.38it/s]\u001b[A\n",
      "batch 245, training loss: 3.8553: : 244it [02:59,  1.38it/s]\u001b[A\n",
      "batch 245, training loss: 3.8553: : 245it [02:59,  1.34it/s]\u001b[A\n",
      "batch 246, training loss: 3.7928: : 245it [03:00,  1.34it/s]\u001b[A\n",
      "batch 246, training loss: 3.7928: : 246it [03:00,  1.30it/s]\u001b[A\n",
      "batch 247, training loss: 3.7942: : 246it [03:01,  1.30it/s]\u001b[A\n",
      "batch 247, training loss: 3.7942: : 247it [03:01,  1.29it/s]\u001b[A\n",
      "batch 248, training loss: 3.7177: : 247it [03:02,  1.29it/s]\u001b[A\n",
      "batch 248, training loss: 3.7177: : 248it [03:02,  1.27it/s]\u001b[A\n",
      "batch 249, training loss: 3.6877: : 248it [03:02,  1.27it/s]\u001b[A\n",
      "batch 249, training loss: 3.6877: : 249it [03:02,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 3.8791: : 249it [03:03,  1.28it/s]\u001b[A\n",
      "batch 250, training loss: 3.8791: : 250it [03:03,  1.30it/s]\u001b[A\n",
      "batch 251, training loss: 3.8834: : 250it [03:04,  1.30it/s]\u001b[A\n",
      "batch 251, training loss: 3.8834: : 251it [03:04,  1.28it/s]\u001b[A\n",
      "batch 252, training loss: 3.4109: : 251it [03:04,  1.28it/s]\u001b[A\n",
      "batch 252, training loss: 3.4109: : 252it [03:04,  1.50it/s]\u001b[A\n",
      "batch 253, training loss: 3.9012: : 252it [03:05,  1.50it/s]\u001b[A\n",
      "batch 253, training loss: 3.9012: : 253it [03:05,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 4.0534: : 253it [03:06,  1.35it/s]\u001b[A\n",
      "batch 254, training loss: 4.0534: : 254it [03:06,  1.26it/s]\u001b[A\n",
      "batch 255, training loss: 3.8656: : 254it [03:07,  1.26it/s]\u001b[A\n",
      "batch 255, training loss: 3.8656: : 255it [03:07,  1.22it/s]\u001b[A\n",
      "batch 256, training loss: 3.8759: : 255it [03:08,  1.22it/s]\u001b[A\n",
      "batch 256, training loss: 3.8759: : 256it [03:08,  1.20it/s]\u001b[A\n",
      "batch 257, training loss: 3.9793: : 256it [03:09,  1.20it/s]\u001b[A\n",
      "batch 257, training loss: 3.9793: : 257it [03:09,  1.16it/s]\u001b[A\n",
      "batch 258, training loss: 4.0441: : 257it [03:10,  1.16it/s]\u001b[A\n",
      "batch 258, training loss: 4.0441: : 258it [03:10,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.9236: : 258it [03:11,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.9236: : 259it [03:11,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.9252: : 259it [03:12,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.9252: : 260it [03:12,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 4.013: : 260it [03:12,  1.12it/s] \u001b[A\n",
      "batch 261, training loss: 4.013: : 261it [03:12,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 3.8905: : 261it [03:13,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 3.8905: : 262it [03:13,  1.09it/s]\u001b[A\n",
      "batch 263, training loss: 3.8846: : 262it [03:14,  1.09it/s]\u001b[A\n",
      "batch 263, training loss: 3.8846: : 263it [03:14,  1.09it/s]\u001b[A\n",
      "batch 264, training loss: 3.9685: : 263it [03:15,  1.09it/s]\u001b[A\n",
      "batch 264, training loss: 3.9685: : 264it [03:15,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.7686: : 264it [03:16,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.7686: : 265it [03:16,  1.08it/s]\u001b[A\n",
      "batch 266, training loss: 3.9626: : 265it [03:17,  1.08it/s]\u001b[A\n",
      "batch 266, training loss: 3.9626: : 266it [03:17,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.9058: : 266it [03:18,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.9058: : 267it [03:18,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.8051: : 267it [03:19,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.8051: : 268it [03:19,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.8241: : 268it [03:20,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.8241: : 269it [03:20,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.8469: : 269it [03:21,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.8469: : 270it [03:21,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.8374: : 270it [03:22,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.8374: : 271it [03:22,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.8169: : 271it [03:23,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.8169: : 272it [03:23,  1.11it/s]\u001b[A\n",
      "batch 273, training loss: 3.9179: : 272it [03:23,  1.11it/s]\u001b[A\n",
      "batch 273, training loss: 3.9179: : 273it [03:23,  1.11it/s]\u001b[A\n",
      "batch 274, training loss: 3.8911: : 273it [03:24,  1.11it/s]\u001b[A\n",
      "batch 274, training loss: 3.8911: : 274it [03:24,  1.09it/s]\u001b[A\n",
      "batch 275, training loss: 3.7783: : 274it [03:25,  1.09it/s]\u001b[A\n",
      "batch 275, training loss: 3.7783: : 275it [03:25,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.6802: : 275it [03:26,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.6802: : 276it [03:26,  1.10it/s]\u001b[A\n",
      "batch 277, training loss: 3.8171: : 276it [03:27,  1.10it/s]\u001b[A\n",
      "batch 277, training loss: 3.8171: : 277it [03:27,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.7511: : 277it [03:28,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.7511: : 278it [03:28,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 3.7587: : 278it [03:29,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 3.7587: : 279it [03:29,  1.08it/s]\u001b[A\n",
      "batch 280, training loss: 3.6246: : 279it [03:30,  1.08it/s]\u001b[A\n",
      "batch 280, training loss: 3.6246: : 280it [03:30,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.7374: : 280it [03:31,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.7374: : 281it [03:31,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.63: : 281it [03:32,  1.10it/s]  \u001b[A\n",
      "batch 282, training loss: 3.63: : 282it [03:32,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.7454: : 282it [03:33,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.7454: : 283it [03:33,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.7133: : 283it [03:33,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.7133: : 284it [03:33,  1.12it/s]\u001b[A\n",
      "batch 285, training loss: 3.7668: : 284it [03:34,  1.12it/s]\u001b[A\n",
      "batch 285, training loss: 3.7668: : 285it [03:34,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 3.9247: : 285it [03:35,  1.11it/s]\u001b[A\n",
      "batch 286, training loss: 3.9247: : 286it [03:35,  1.13it/s]\u001b[A\n",
      "batch 287, training loss: 3.6427: : 286it [03:36,  1.13it/s]\u001b[A\n",
      "batch 287, training loss: 3.6427: : 287it [03:36,  1.17it/s]\u001b[A\n",
      "batch 288, training loss: 3.6566: : 287it [03:37,  1.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 288, training loss: 3.6566: : 288it [03:37,  1.15it/s]\u001b[A\n",
      "batch 289, training loss: 3.8353: : 288it [03:38,  1.15it/s]\u001b[A\n",
      "batch 289, training loss: 3.8353: : 289it [03:38,  1.13it/s]\u001b[A\n",
      "batch 290, training loss: 3.5976: : 289it [03:39,  1.13it/s]\u001b[A\n",
      "batch 290, training loss: 3.5976: : 290it [03:39,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 3.8675: : 290it [03:40,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 3.8675: : 291it [03:40,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.669: : 291it [03:41,  1.11it/s] \u001b[A\n",
      "batch 292, training loss: 3.669: : 292it [03:41,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.7575: : 292it [03:41,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.7575: : 293it [03:41,  1.11it/s]\u001b[A\n",
      "batch 294, training loss: 3.849: : 293it [03:42,  1.11it/s] \u001b[A\n",
      "batch 294, training loss: 3.849: : 294it [03:42,  1.09it/s]\u001b[A\n",
      "batch 295, training loss: 3.7851: : 294it [03:43,  1.09it/s]\u001b[A\n",
      "batch 295, training loss: 3.7851: : 295it [03:43,  1.11it/s]\u001b[A\n",
      "batch 296, training loss: 3.6515: : 295it [03:44,  1.11it/s]\u001b[A\n",
      "batch 296, training loss: 3.6515: : 296it [03:44,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 3.8054: : 296it [03:45,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 3.8054: : 297it [03:45,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.6888: : 297it [03:46,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.6888: : 298it [03:46,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.7914: : 298it [03:47,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.7914: : 299it [03:47,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 3.8526: : 299it [03:48,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 3.8526: : 300it [03:48,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.8167: : 300it [03:49,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.8167: : 301it [03:49,  1.11it/s]\u001b[A\n",
      "batch 302, training loss: 3.7876: : 301it [03:50,  1.11it/s]\u001b[A\n",
      "batch 302, training loss: 3.7876: : 302it [03:50,  1.10it/s]\u001b[A\n",
      "batch 303, training loss: 3.7273: : 302it [03:51,  1.10it/s]\u001b[A\n",
      "batch 303, training loss: 3.7273: : 303it [03:51,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 3.8014: : 303it [03:52,  1.10it/s]\u001b[A\n",
      "batch 304, training loss: 3.8014: : 304it [03:52,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 3.7248: : 304it [03:52,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 3.7248: : 305it [03:52,  1.10it/s]\u001b[A\n",
      "batch 306, training loss: 3.7902: : 305it [03:53,  1.10it/s]\u001b[A\n",
      "batch 306, training loss: 3.7902: : 306it [03:53,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 3.9229: : 306it [03:54,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 3.9229: : 307it [03:54,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.5429: : 307it [03:55,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.5429: : 308it [03:55,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.9065: : 308it [03:56,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.9065: : 309it [03:56,  1.11it/s]\u001b[A\n",
      "batch 310, training loss: 3.6734: : 309it [03:57,  1.11it/s]\u001b[A\n",
      "batch 310, training loss: 3.6734: : 310it [03:57,  1.10it/s]\u001b[A\n",
      "batch 311, training loss: 3.7931: : 310it [03:58,  1.10it/s]\u001b[A\n",
      "batch 311, training loss: 3.7931: : 311it [03:58,  1.11it/s]\u001b[A\n",
      "batch 312, training loss: 3.6068: : 311it [03:59,  1.11it/s]\u001b[A\n",
      "batch 312, training loss: 3.6068: : 312it [03:59,  1.10it/s]\u001b[A\n",
      "batch 313, training loss: 3.743: : 312it [04:00,  1.10it/s] \u001b[A\n",
      "batch 313, training loss: 3.743: : 313it [04:00,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.7668: : 313it [04:01,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.7668: : 314it [04:01,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.8218: : 314it [04:01,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.8218: : 315it [04:01,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.9726: : 315it [04:02,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.9726: : 316it [04:02,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.6356: : 316it [04:03,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.6356: : 317it [04:03,  1.15it/s]\u001b[A\n",
      "batch 318, training loss: 3.9445: : 317it [04:04,  1.15it/s]\u001b[A\n",
      "batch 318, training loss: 3.9445: : 318it [04:04,  1.09it/s]\u001b[A\n",
      "batch 319, training loss: 4.0195: : 318it [04:05,  1.09it/s]\u001b[A\n",
      "batch 319, training loss: 4.0195: : 319it [04:05,  1.10it/s]\u001b[A\n",
      "batch 320, training loss: 4.0251: : 319it [04:06,  1.10it/s]\u001b[A\n",
      "batch 320, training loss: 4.0251: : 320it [04:06,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.0585: : 320it [04:07,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 4.0585: : 321it [04:07,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.1251: : 321it [04:08,  1.02it/s]\u001b[A\n",
      "batch 322, training loss: 4.1251: : 322it [04:08,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.8849: : 322it [04:09,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.8849: : 323it [04:09,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 3.9544: : 323it [04:10,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 3.9544: : 324it [04:10,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.0143: : 324it [04:11,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 4.0143: : 325it [04:11,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 3.955: : 325it [04:12,  1.02it/s] \u001b[A\n",
      "batch 326, training loss: 3.955: : 326it [04:12,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 4.0597: : 326it [04:13,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 4.0597: : 327it [04:13,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.9222: : 327it [04:14,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.9222: : 328it [04:14,  1.02it/s]\u001b[A\n",
      "batch 329, training loss: 3.8401: : 328it [04:15,  1.02it/s]\u001b[A\n",
      "batch 329, training loss: 3.8401: : 329it [04:15,  1.04it/s]\u001b[A\n",
      "batch 330, training loss: 3.9201: : 329it [04:16,  1.04it/s]\u001b[A\n",
      "batch 330, training loss: 3.9201: : 330it [04:16,  1.03it/s]\u001b[A\n",
      "batch 331, training loss: 3.9586: : 330it [04:17,  1.03it/s]\u001b[A\n",
      "batch 331, training loss: 3.9586: : 331it [04:17,  1.02it/s]\u001b[A\n",
      "batch 332, training loss: 3.8454: : 331it [04:18,  1.02it/s]\u001b[A\n",
      "batch 332, training loss: 3.8454: : 332it [04:18,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.8192: : 332it [04:19,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.8192: : 333it [04:19,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 3.9055: : 333it [04:20,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 3.9055: : 334it [04:20,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.0351: : 334it [04:21,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 4.0351: : 335it [04:21,  1.02it/s]\u001b[A\n",
      "batch 336, training loss: 3.9179: : 335it [04:22,  1.02it/s]\u001b[A\n",
      "batch 336, training loss: 3.9179: : 336it [04:22,  1.05it/s]\u001b[A\n",
      "batch 337, training loss: 3.9618: : 336it [04:23,  1.05it/s]\u001b[A\n",
      "batch 337, training loss: 3.9618: : 337it [04:23,  1.03it/s]\u001b[A\n",
      "batch 338, training loss: 3.8371: : 337it [04:24,  1.03it/s]\u001b[A\n",
      "batch 338, training loss: 3.8371: : 338it [04:24,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 3.9121: : 338it [04:25,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 3.9121: : 339it [04:25,  1.01it/s]\u001b[A\n",
      "batch 340, training loss: 4.1423: : 339it [04:26,  1.01it/s]\u001b[A\n",
      "batch 340, training loss: 4.1423: : 340it [04:26,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.7996: : 340it [04:27,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.7996: : 341it [04:27,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 3.9035: : 341it [04:28,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 3.9035: : 342it [04:28,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.8815: : 342it [04:29,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.8815: : 343it [04:29,  1.01s/it]\u001b[A\n",
      "batch 344, training loss: 3.9905: : 343it [04:30,  1.01s/it]\u001b[A\n",
      "batch 344, training loss: 3.9905: : 344it [04:30,  1.09it/s]\u001b[A\n",
      "batch 345, training loss: 3.931: : 344it [04:31,  1.09it/s] \u001b[A\n",
      "batch 345, training loss: 3.931: : 345it [04:31,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 3.9027: : 345it [04:32,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 3.9027: : 346it [04:32,  1.09it/s]\u001b[A\n",
      "batch 347, training loss: 3.8021: : 346it [04:33,  1.09it/s]\u001b[A\n",
      "batch 347, training loss: 3.8021: : 347it [04:33,  1.05it/s]\u001b[A\n",
      "batch 348, training loss: 3.889: : 347it [04:34,  1.05it/s] \u001b[A\n",
      "batch 348, training loss: 3.889: : 348it [04:34,  1.02it/s]\u001b[A\n",
      "batch 349, training loss: 4.0244: : 348it [04:35,  1.02it/s]\u001b[A\n",
      "batch 349, training loss: 4.0244: : 349it [04:35,  1.01it/s]\u001b[A\n",
      "batch 350, training loss: 3.8779: : 349it [04:36,  1.01it/s]\u001b[A\n",
      "batch 350, training loss: 3.8779: : 350it [04:36,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.7634: : 350it [04:36,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.7634: : 351it [04:36,  1.05it/s]\u001b[A\n",
      "batch 352, training loss: 3.8518: : 351it [04:38,  1.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 352, training loss: 3.8518: : 352it [04:38,  1.02it/s]\u001b[A\n",
      "batch 353, training loss: 3.8937: : 352it [04:39,  1.02it/s]\u001b[A\n",
      "batch 353, training loss: 3.8937: : 353it [04:39,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 3.9436: : 353it [04:40,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 3.9436: : 354it [04:40,  1.01it/s]\u001b[A\n",
      "batch 355, training loss: 3.8967: : 354it [04:41,  1.01it/s]\u001b[A\n",
      "batch 355, training loss: 3.8967: : 355it [04:41,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.7195: : 355it [04:41,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.7195: : 356it [04:41,  1.03it/s]\u001b[A\n",
      "batch 357, training loss: 3.744: : 356it [04:42,  1.03it/s] \u001b[A\n",
      "batch 357, training loss: 3.744: : 357it [04:42,  1.02it/s]\u001b[A\n",
      "batch 358, training loss: 3.9415: : 357it [04:43,  1.02it/s]\u001b[A\n",
      "batch 358, training loss: 3.9415: : 358it [04:43,  1.01it/s]\u001b[A\n",
      "batch 359, training loss: 3.7596: : 358it [04:44,  1.01it/s]\u001b[A\n",
      "batch 359, training loss: 3.7596: : 359it [04:44,  1.00it/s]\u001b[A\n",
      "batch 360, training loss: 3.8883: : 359it [04:46,  1.00it/s]\u001b[A\n",
      "batch 360, training loss: 3.8883: : 360it [04:46,  1.01s/it]\u001b[A\n",
      "batch 361, training loss: 3.8504: : 360it [04:46,  1.01s/it]\u001b[A\n",
      "batch 361, training loss: 3.8504: : 361it [04:46,  1.03it/s]\u001b[A\n",
      "batch 362, training loss: 3.9595: : 361it [04:47,  1.03it/s]\u001b[A\n",
      "batch 362, training loss: 3.9595: : 362it [04:47,  1.02it/s]\u001b[A\n",
      "batch 363, training loss: 3.8636: : 362it [04:48,  1.02it/s]\u001b[A\n",
      "batch 363, training loss: 3.8636: : 363it [04:48,  1.01it/s]\u001b[A\n",
      "batch 364, training loss: 3.7576: : 363it [04:49,  1.01it/s]\u001b[A\n",
      "batch 364, training loss: 3.7576: : 364it [04:49,  1.00it/s]\u001b[A\n",
      "batch 365, training loss: 3.7433: : 364it [04:50,  1.00it/s]\u001b[A\n",
      "batch 365, training loss: 3.7433: : 365it [04:50,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.8862: : 365it [04:51,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.8862: : 366it [04:51,  1.00s/it]\u001b[A\n",
      "batch 367, training loss: 3.8075: : 366it [04:52,  1.00s/it]\u001b[A\n",
      "batch 367, training loss: 3.8075: : 367it [04:52,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.8815: : 367it [04:54,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.8815: : 368it [04:54,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.9179: : 368it [04:54,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.9179: : 369it [04:54,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.7949: : 369it [04:55,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.7949: : 370it [04:55,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.8499: : 370it [04:56,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.8499: : 371it [04:56,  1.03it/s]\u001b[A\n",
      "batch 372, training loss: 3.7901: : 371it [04:57,  1.03it/s]\u001b[A\n",
      "batch 372, training loss: 3.7901: : 372it [04:57,  1.02it/s]\u001b[A\n",
      "batch 373, training loss: 3.8206: : 372it [04:58,  1.02it/s]\u001b[A\n",
      "batch 373, training loss: 3.8206: : 373it [04:58,  1.00it/s]\u001b[A\n",
      "batch 374, training loss: 3.8053: : 373it [04:59,  1.00it/s]\u001b[A\n",
      "batch 374, training loss: 3.8053: : 374it [04:59,  1.00s/it]\u001b[A\n",
      "batch 375, training loss: 3.695: : 374it [05:00,  1.00s/it] \u001b[A\n",
      "batch 375, training loss: 3.695: : 375it [05:00,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.9125: : 375it [05:01,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.9125: : 376it [05:01,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 4.0275: : 376it [05:02,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 4.0275: : 377it [05:02,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.9154: : 377it [05:03,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.9154: : 378it [05:03,  1.05s/it]\u001b[A\n",
      "batch 379, training loss: 3.8912: : 378it [05:05,  1.05s/it]\u001b[A\n",
      "batch 379, training loss: 3.8912: : 379it [05:05,  1.08s/it]\u001b[A\n",
      "batch 380, training loss: 3.8975: : 379it [05:06,  1.08s/it]\u001b[A\n",
      "batch 380, training loss: 3.8975: : 380it [05:06,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.9731: : 380it [05:07,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.9731: : 381it [05:07,  1.07s/it]\u001b[A\n",
      "batch 382, training loss: 3.8428: : 381it [05:08,  1.07s/it]\u001b[A\n",
      "batch 382, training loss: 3.8428: : 382it [05:08,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.8936: : 382it [05:09,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.8936: : 383it [05:09,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 4.0001: : 383it [05:10,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 4.0001: : 384it [05:10,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 3.9135: : 384it [05:11,  1.08s/it]\u001b[A\n",
      "batch 385, training loss: 3.9135: : 385it [05:11,  1.07s/it]\u001b[A\n",
      "batch 386, training loss: 3.8484: : 385it [05:12,  1.07s/it]\u001b[A\n",
      "batch 386, training loss: 3.8484: : 386it [05:12,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.9305: : 386it [05:13,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.9305: : 387it [05:13,  1.08s/it]\u001b[A\n",
      "batch 388, training loss: 3.6816: : 387it [05:14,  1.08s/it]\u001b[A\n",
      "batch 388, training loss: 3.6816: : 388it [05:14,  1.09s/it]\u001b[A\n",
      "batch 389, training loss: 3.8743: : 388it [05:15,  1.09s/it]\u001b[A\n",
      "batch 389, training loss: 3.8743: : 389it [05:15,  1.09s/it]\u001b[A\n",
      "batch 390, training loss: 3.9713: : 389it [05:17,  1.09s/it]\u001b[A\n",
      "batch 390, training loss: 3.9713: : 390it [05:17,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 3.9684: : 390it [05:18,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 3.9684: : 391it [05:18,  1.12s/it]\u001b[A\n",
      "batch 392, training loss: 3.9173: : 391it [05:19,  1.12s/it]\u001b[A\n",
      "batch 392, training loss: 3.9173: : 392it [05:19,  1.12s/it]\u001b[A\n",
      "batch 393, training loss: 3.7454: : 392it [05:20,  1.12s/it]\u001b[A\n",
      "batch 393, training loss: 3.7454: : 393it [05:20,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 3.682: : 393it [05:21,  1.11s/it] \u001b[A\n",
      "batch 394, training loss: 3.682: : 394it [05:21,  1.07s/it]\u001b[A\n",
      "batch 395, training loss: 3.7844: : 394it [05:22,  1.07s/it]\u001b[A\n",
      "batch 395, training loss: 3.7844: : 395it [05:22,  1.10s/it]\u001b[A\n",
      "batch 396, training loss: 3.9664: : 395it [05:23,  1.10s/it]\u001b[A\n",
      "batch 396, training loss: 3.9664: : 396it [05:23,  1.11s/it]\u001b[A\n",
      "batch 397, training loss: 3.7486: : 396it [05:24,  1.11s/it]\u001b[A\n",
      "batch 397, training loss: 3.7486: : 397it [05:24,  1.09s/it]\u001b[A\n",
      "batch 398, training loss: 3.8476: : 397it [05:25,  1.09s/it]\u001b[A\n",
      "batch 398, training loss: 3.8476: : 398it [05:25,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 3.96: : 398it [05:27,  1.10s/it]  \u001b[A\n",
      "batch 399, training loss: 3.96: : 399it [05:27,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.7314: : 399it [05:28,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.7314: : 400it [05:28,  1.13s/it]\u001b[A\n",
      "batch 401, training loss: 3.6888: : 400it [05:29,  1.13s/it]\u001b[A\n",
      "batch 401, training loss: 3.6888: : 401it [05:29,  1.12s/it]\u001b[A\n",
      "batch 402, training loss: 3.7734: : 401it [05:30,  1.12s/it]\u001b[A\n",
      "batch 402, training loss: 3.7734: : 402it [05:30,  1.12s/it]\u001b[A\n",
      "batch 403, training loss: 3.9552: : 402it [05:31,  1.12s/it]\u001b[A\n",
      "batch 403, training loss: 3.9552: : 403it [05:31,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.5637: : 403it [05:32,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.5637: : 404it [05:32,  1.09s/it]\u001b[A\n",
      "batch 405, training loss: 3.8301: : 404it [05:33,  1.09s/it]\u001b[A\n",
      "batch 405, training loss: 3.8301: : 405it [05:33,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.739: : 405it [05:34,  1.11s/it] \u001b[A\n",
      "batch 406, training loss: 3.739: : 406it [05:34,  1.13s/it]\u001b[A\n",
      "batch 407, training loss: 3.8187: : 406it [05:35,  1.13s/it]\u001b[A\n",
      "batch 407, training loss: 3.8187: : 407it [05:35,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.6674: : 407it [05:37,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.6674: : 408it [05:37,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.8562: : 408it [05:38,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.8562: : 409it [05:38,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.6971: : 409it [05:39,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.6971: : 410it [05:39,  1.12s/it]\u001b[A\n",
      "batch 411, training loss: 3.8073: : 410it [05:40,  1.12s/it]\u001b[A\n",
      "batch 411, training loss: 3.8073: : 411it [05:40,  1.13s/it]\u001b[A\n",
      "batch 412, training loss: 3.8005: : 411it [05:41,  1.13s/it]\u001b[A\n",
      "batch 412, training loss: 3.8005: : 412it [05:41,  1.13s/it]\u001b[A\n",
      "batch 413, training loss: 3.7657: : 412it [05:42,  1.13s/it]\u001b[A\n",
      "batch 413, training loss: 3.7657: : 413it [05:42,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.6577: : 413it [05:43,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.6577: : 414it [05:43,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.5658: : 414it [05:44,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.5658: : 415it [05:44,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.616: : 415it [05:45,  1.11s/it] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 416, training loss: 3.616: : 416it [05:45,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.8902: : 416it [05:47,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.8902: : 417it [05:47,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.7702: : 417it [05:48,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.7702: : 418it [05:48,  1.08s/it]\u001b[A\n",
      "batch 419, training loss: 3.5914: : 418it [05:49,  1.08s/it]\u001b[A\n",
      "batch 419, training loss: 3.5914: : 419it [05:49,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.6966: : 419it [05:50,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.6966: : 420it [05:50,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.8001: : 420it [05:51,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.8001: : 421it [05:51,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.8228: : 421it [05:52,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.8228: : 422it [05:52,  1.08s/it]\u001b[A\n",
      "batch 423, training loss: 3.6178: : 422it [05:53,  1.08s/it]\u001b[A\n",
      "batch 423, training loss: 3.6178: : 423it [05:53,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.8825: : 423it [05:54,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.8825: : 424it [05:54,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.9564: : 424it [05:55,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.9564: : 425it [05:55,  1.11s/it]\u001b[A\n",
      "batch 426, training loss: 3.9079: : 425it [05:57,  1.11s/it]\u001b[A\n",
      "batch 426, training loss: 3.9079: : 426it [05:57,  1.13s/it]\u001b[A\n",
      "batch 427, training loss: 3.9196: : 426it [05:58,  1.13s/it]\u001b[A\n",
      "batch 427, training loss: 3.9196: : 427it [05:58,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.0939: : 427it [05:59,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 4.0939: : 428it [05:59,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 3.9454: : 428it [06:00,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 3.9454: : 429it [06:00,  1.20s/it]\u001b[A\n",
      "batch 430, training loss: 3.9694: : 429it [06:01,  1.20s/it]\u001b[A\n",
      "batch 430, training loss: 3.9694: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.9894: : 430it [06:02,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.9894: : 431it [06:02,  1.14s/it]\u001b[A\n",
      "batch 432, training loss: 3.8244: : 431it [06:03,  1.14s/it]\u001b[A\n",
      "batch 432, training loss: 3.8244: : 432it [06:03,  1.12s/it]\u001b[A\n",
      "batch 433, training loss: 3.874: : 432it [06:05,  1.12s/it] \u001b[A\n",
      "batch 433, training loss: 3.874: : 433it [06:05,  1.14s/it]\u001b[A\n",
      "batch 434, training loss: 3.8598: : 433it [06:06,  1.14s/it]\u001b[A\n",
      "batch 434, training loss: 3.8598: : 434it [06:06,  1.16s/it]\u001b[A\n",
      "batch 435, training loss: 3.8637: : 434it [06:07,  1.16s/it]\u001b[A\n",
      "batch 435, training loss: 3.8637: : 435it [06:07,  1.17s/it]\u001b[A\n",
      "batch 436, training loss: 3.8748: : 435it [06:08,  1.17s/it]\u001b[A\n",
      "batch 436, training loss: 3.8748: : 436it [06:08,  1.17s/it]\u001b[A\n",
      "batch 437, training loss: 3.8059: : 436it [06:09,  1.17s/it]\u001b[A\n",
      "batch 437, training loss: 3.8059: : 437it [06:09,  1.16s/it]\u001b[A\n",
      "batch 438, training loss: 3.8685: : 437it [06:11,  1.16s/it]\u001b[A\n",
      "batch 438, training loss: 3.8685: : 438it [06:11,  1.15s/it]\u001b[A\n",
      "batch 439, training loss: 3.958: : 438it [06:12,  1.15s/it] \u001b[A\n",
      "batch 439, training loss: 3.958: : 439it [06:12,  1.18s/it]\u001b[A\n",
      "batch 440, training loss: 3.7938: : 439it [06:13,  1.18s/it]\u001b[A\n",
      "batch 440, training loss: 3.7938: : 440it [06:13,  1.19s/it]\u001b[A\n",
      "batch 441, training loss: 4.0539: : 440it [06:14,  1.19s/it]\u001b[A\n",
      "batch 441, training loss: 4.0539: : 441it [06:14,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.7331: : 441it [06:15,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.7331: : 442it [06:15,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 3.8037: : 442it [06:17,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 3.8037: : 443it [06:17,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.7925: : 443it [06:18,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.7925: : 444it [06:18,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.8594: : 444it [06:19,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.8594: : 445it [06:19,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.8969: : 445it [06:20,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.8969: : 446it [06:20,  1.21s/it]\u001b[A\n",
      "batch 447, training loss: 3.8042: : 446it [06:22,  1.21s/it]\u001b[A\n",
      "batch 447, training loss: 3.8042: : 447it [06:22,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.7144: : 447it [06:23,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.7144: : 448it [06:23,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.8558: : 448it [06:24,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.8558: : 449it [06:24,  1.24s/it]\u001b[A\n",
      "batch 450, training loss: 3.8662: : 449it [06:25,  1.24s/it]\u001b[A\n",
      "batch 450, training loss: 3.8662: : 450it [06:25,  1.23s/it]\u001b[A\n",
      "batch 451, training loss: 3.872: : 450it [06:27,  1.23s/it] \u001b[A\n",
      "batch 451, training loss: 3.872: : 451it [06:27,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.8859: : 451it [06:28,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.8859: : 452it [06:28,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 3.956: : 452it [06:29,  1.23s/it] \u001b[A\n",
      "batch 453, training loss: 3.956: : 453it [06:29,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.8079: : 453it [06:30,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.8079: : 454it [06:30,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.6959: : 454it [06:31,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.6959: : 455it [06:31,  1.24s/it]\u001b[A\n",
      "batch 456, training loss: 3.7445: : 455it [06:33,  1.24s/it]\u001b[A\n",
      "batch 456, training loss: 3.7445: : 456it [06:33,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 3.869: : 456it [06:34,  1.23s/it] \u001b[A\n",
      "batch 457, training loss: 3.869: : 457it [06:34,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.7478: : 457it [06:35,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.7478: : 458it [06:35,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.775: : 458it [06:36,  1.23s/it] \u001b[A\n",
      "batch 459, training loss: 3.775: : 459it [06:36,  1.24s/it]\u001b[A\n",
      "batch 460, training loss: 3.7853: : 459it [06:38,  1.24s/it]\u001b[A\n",
      "batch 460, training loss: 3.7853: : 460it [06:38,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 3.8716: : 460it [06:39,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 3.8716: : 461it [06:39,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.8112: : 461it [06:40,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.8112: : 462it [06:40,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.6436: : 462it [06:41,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.6436: : 463it [06:41,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.8069: : 463it [06:42,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.8069: : 464it [06:42,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.7857: : 464it [06:44,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.7857: : 465it [06:44,  1.18s/it]\u001b[A\n",
      "batch 466, training loss: 3.7518: : 465it [06:45,  1.18s/it]\u001b[A\n",
      "batch 466, training loss: 3.7518: : 466it [06:45,  1.17s/it]\u001b[A\n",
      "batch 467, training loss: 3.6858: : 466it [06:46,  1.17s/it]\u001b[A\n",
      "batch 467, training loss: 3.6858: : 467it [06:46,  1.21s/it]\u001b[A\n",
      "batch 468, training loss: 3.8509: : 467it [06:47,  1.21s/it]\u001b[A\n",
      "batch 468, training loss: 3.8509: : 468it [06:47,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.7707: : 468it [06:49,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.7707: : 469it [06:49,  1.24s/it]\u001b[A\n",
      "batch 470, training loss: 3.8848: : 469it [06:50,  1.24s/it]\u001b[A\n",
      "batch 470, training loss: 3.8848: : 470it [06:50,  1.23s/it]\u001b[A\n",
      "batch 471, training loss: 3.8568: : 470it [06:51,  1.23s/it]\u001b[A\n",
      "batch 471, training loss: 3.8568: : 471it [06:51,  1.25s/it]\u001b[A\n",
      "batch 472, training loss: 3.8067: : 471it [06:52,  1.25s/it]\u001b[A\n",
      "batch 472, training loss: 3.8067: : 472it [06:52,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 3.8308: : 472it [06:54,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 3.8308: : 473it [06:54,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 3.852: : 473it [06:55,  1.26s/it] \u001b[A\n",
      "batch 474, training loss: 3.852: : 474it [06:55,  1.25s/it]\u001b[A\n",
      "batch 475, training loss: 3.7895: : 474it [06:56,  1.25s/it]\u001b[A\n",
      "batch 475, training loss: 3.7895: : 475it [06:56,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.8715: : 475it [06:57,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.8715: : 476it [06:57,  1.25s/it]\u001b[A\n",
      "batch 477, training loss: 3.6894: : 476it [06:59,  1.25s/it]\u001b[A\n",
      "batch 477, training loss: 3.6894: : 477it [06:59,  1.23s/it]\u001b[A\n",
      "batch 478, training loss: 3.7581: : 477it [07:00,  1.23s/it]\u001b[A\n",
      "batch 478, training loss: 3.7581: : 478it [07:00,  1.23s/it]\u001b[A\n",
      "batch 479, training loss: 3.7231: : 478it [07:01,  1.23s/it]\u001b[A\n",
      "batch 479, training loss: 3.7231: : 479it [07:01,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.7436: : 479it [07:02,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 480, training loss: 3.7436: : 480it [07:02,  1.24s/it]\u001b[A\n",
      "batch 481, training loss: 3.7724: : 480it [07:04,  1.24s/it]\u001b[A\n",
      "batch 481, training loss: 3.7724: : 481it [07:04,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.7677: : 481it [07:05,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.7677: : 482it [07:05,  1.25s/it]\u001b[A\n",
      "batch 483, training loss: 3.7036: : 482it [07:06,  1.25s/it]\u001b[A\n",
      "batch 483, training loss: 3.7036: : 483it [07:06,  1.24s/it]\u001b[A\n",
      "batch 484, training loss: 3.7757: : 483it [07:07,  1.24s/it]\u001b[A\n",
      "batch 484, training loss: 3.7757: : 484it [07:07,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.7034: : 484it [07:09,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.7034: : 485it [07:09,  1.25s/it]\u001b[A\n",
      "batch 486, training loss: 3.7835: : 485it [07:10,  1.25s/it]\u001b[A\n",
      "batch 486, training loss: 3.7835: : 486it [07:10,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.9067: : 486it [07:11,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.9067: : 487it [07:11,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.6766: : 487it [07:12,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.6766: : 488it [07:12,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.594: : 488it [07:14,  1.26s/it] \u001b[A\n",
      "batch 489, training loss: 3.594: : 489it [07:14,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.6842: : 489it [07:15,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.6842: : 490it [07:15,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.7236: : 490it [07:16,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.7236: : 491it [07:16,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.6347: : 491it [07:17,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.6347: : 492it [07:17,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.806: : 492it [07:19,  1.26s/it] \u001b[A\n",
      "batch 493, training loss: 3.806: : 493it [07:19,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.8185: : 493it [07:20,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.8185: : 494it [07:20,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.6503: : 494it [07:21,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.6503: : 495it [07:21,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 3.6333: : 495it [07:22,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 3.6333: : 496it [07:22,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.5701: : 496it [07:24,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.5701: : 497it [07:24,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.6529: : 497it [07:25,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.6529: : 498it [07:25,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.6785: : 498it [07:26,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.6785: : 499it [07:26,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 3.9298: : 499it [07:27,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 3.9298: : 500it [07:27,  1.20s/it]\u001b[A\n",
      "batch 501, training loss: 3.7676: : 500it [07:29,  1.20s/it]\u001b[A\n",
      "batch 501, training loss: 3.7676: : 501it [07:29,  1.25s/it]\u001b[A\n",
      "batch 502, training loss: 3.7634: : 501it [07:30,  1.25s/it]\u001b[A\n",
      "batch 502, training loss: 3.7634: : 502it [07:30,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 3.8456: : 502it [07:31,  1.30s/it]\u001b[A\n",
      "batch 503, training loss: 3.8456: : 503it [07:31,  1.30s/it]\u001b[A\n",
      "batch 504, training loss: 3.832: : 503it [07:33,  1.30s/it] \u001b[A\n",
      "batch 504, training loss: 3.832: : 504it [07:33,  1.30s/it]\u001b[A\n",
      "batch 505, training loss: 3.8953: : 504it [07:34,  1.30s/it]\u001b[A\n",
      "batch 505, training loss: 3.8953: : 505it [07:34,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.8243: : 505it [07:35,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.8243: : 506it [07:35,  1.33s/it]\u001b[A\n",
      "batch 507, training loss: 3.6953: : 506it [07:36,  1.33s/it]\u001b[A\n",
      "batch 507, training loss: 3.6953: : 507it [07:36,  1.28s/it]\u001b[A\n",
      "batch 508, training loss: 3.8556: : 507it [07:37,  1.28s/it]\u001b[A\n",
      "batch 508, training loss: 3.8556: : 508it [07:37,  1.19s/it]\u001b[A\n",
      "batch 509, training loss: 3.7291: : 508it [07:38,  1.19s/it]\u001b[A\n",
      "batch 509, training loss: 3.7291: : 509it [07:38,  1.09s/it]\u001b[A\n",
      "batch 510, training loss: 3.8047: : 509it [07:39,  1.09s/it]\u001b[A\n",
      "batch 510, training loss: 3.8047: : 510it [07:39,  1.06s/it]\u001b[A\n",
      "batch 511, training loss: 3.814: : 510it [07:40,  1.06s/it] \u001b[A\n",
      "batch 511, training loss: 3.814: : 511it [07:40,  1.02s/it]\u001b[A\n",
      "batch 512, training loss: 3.8011: : 511it [07:41,  1.02s/it]\u001b[A\n",
      "batch 512, training loss: 3.8011: : 512it [07:41,  1.05s/it]\u001b[A\n",
      "batch 513, training loss: 3.8379: : 512it [07:42,  1.05s/it]\u001b[A\n",
      "batch 513, training loss: 3.8379: : 513it [07:42,  1.00it/s]\u001b[A\n",
      "batch 514, training loss: 3.6766: : 513it [07:43,  1.00it/s]\u001b[A\n",
      "batch 514, training loss: 3.6766: : 514it [07:43,  1.04it/s]\u001b[A\n",
      "batch 515, training loss: 3.8849: : 514it [07:44,  1.04it/s]\u001b[A\n",
      "batch 515, training loss: 3.8849: : 515it [07:44,  1.01s/it]\u001b[A\n",
      "batch 516, training loss: 3.7577: : 515it [07:45,  1.01s/it]\u001b[A\n",
      "batch 516, training loss: 3.7577: : 516it [07:45,  1.01it/s]\u001b[A\n",
      "batch 517, training loss: 3.7493: : 516it [07:46,  1.01it/s]\u001b[A\n",
      "batch 517, training loss: 3.7493: : 517it [07:46,  1.03s/it]\u001b[A\n",
      "batch 518, training loss: 3.8145: : 517it [07:47,  1.03s/it]\u001b[A\n",
      "batch 518, training loss: 3.8145: : 518it [07:47,  1.00s/it]\u001b[A\n",
      "batch 519, training loss: 3.6197: : 518it [07:48,  1.00s/it]\u001b[A\n",
      "batch 519, training loss: 3.6197: : 519it [07:48,  1.03it/s]\u001b[A\n",
      "batch 520, training loss: 3.7975: : 519it [07:49,  1.03it/s]\u001b[A\n",
      "batch 520, training loss: 3.7975: : 520it [07:49,  1.06it/s]\u001b[A\n",
      "batch 521, training loss: 3.7929: : 520it [07:50,  1.06it/s]\u001b[A\n",
      "batch 521, training loss: 3.7929: : 521it [07:50,  1.03it/s]\u001b[A\n",
      "batch 522, training loss: 3.8398: : 521it [07:51,  1.03it/s]\u001b[A\n",
      "batch 522, training loss: 3.8398: : 522it [07:51,  1.06it/s]\u001b[A\n",
      "batch 523, training loss: 3.6845: : 522it [07:52,  1.06it/s]\u001b[A\n",
      "batch 523, training loss: 3.6845: : 523it [07:52,  1.07it/s]\u001b[A\n",
      "batch 524, training loss: 3.8982: : 523it [07:53,  1.07it/s]\u001b[A\n",
      "batch 524, training loss: 3.8982: : 524it [07:53,  1.03it/s]\u001b[A\n",
      "batch 525, training loss: 3.914: : 524it [07:54,  1.03it/s] \u001b[A\n",
      "batch 525, training loss: 3.914: : 525it [07:54,  1.04it/s]\u001b[A\n",
      "batch 526, training loss: 3.7372: : 525it [07:55,  1.04it/s]\u001b[A\n",
      "batch 526, training loss: 3.7372: : 526it [07:55,  1.01s/it]\u001b[A\n",
      "batch 527, training loss: 3.6882: : 526it [07:56,  1.01s/it]\u001b[A\n",
      "batch 527, training loss: 3.6882: : 527it [07:56,  1.06it/s]\u001b[A\n",
      "batch 528, training loss: 3.777: : 527it [07:57,  1.06it/s] \u001b[A\n",
      "batch 528, training loss: 3.777: : 528it [07:57,  1.05it/s]\u001b[A\n",
      "batch 529, training loss: 3.8021: : 528it [07:58,  1.05it/s]\u001b[A\n",
      "batch 529, training loss: 3.8021: : 529it [07:58,  1.01it/s]\u001b[A\n",
      "batch 530, training loss: 3.7823: : 529it [07:59,  1.01it/s]\u001b[A\n",
      "batch 530, training loss: 3.7823: : 530it [07:59,  1.00s/it]\u001b[A\n",
      "batch 531, training loss: 3.7284: : 530it [08:00,  1.00s/it]\u001b[A\n",
      "batch 531, training loss: 3.7284: : 531it [08:00,  1.05s/it]\u001b[A\n",
      "batch 532, training loss: 3.5424: : 531it [08:01,  1.05s/it]\u001b[A\n",
      "batch 532, training loss: 3.5424: : 532it [08:01,  1.04s/it]\u001b[A\n",
      "batch 533, training loss: 3.7152: : 532it [08:02,  1.04s/it]\u001b[A\n",
      "batch 533, training loss: 3.7152: : 533it [08:02,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.8624: : 533it [08:03,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.8624: : 534it [08:03,  1.06s/it]\u001b[A\n",
      "batch 535, training loss: 3.7299: : 534it [08:04,  1.06s/it]\u001b[A\n",
      "batch 535, training loss: 3.7299: : 535it [08:04,  1.08s/it]\u001b[A\n",
      "batch 536, training loss: 3.5647: : 535it [08:05,  1.08s/it]\u001b[A\n",
      "batch 536, training loss: 3.5647: : 536it [08:05,  1.06s/it]\u001b[A\n",
      "batch 537, training loss: 3.6151: : 536it [08:06,  1.06s/it]\u001b[A\n",
      "batch 537, training loss: 3.6151: : 537it [08:06,  1.05s/it]\u001b[A\n",
      "batch 538, training loss: 3.7578: : 537it [08:07,  1.05s/it]\u001b[A\n",
      "batch 538, training loss: 3.7578: : 538it [08:07,  1.08s/it]\u001b[A\n",
      "batch 539, training loss: 3.72: : 538it [08:08,  1.08s/it]  \u001b[A\n",
      "batch 539, training loss: 3.72: : 539it [08:08,  1.06s/it]\u001b[A\n",
      "batch 540, training loss: 3.6608: : 539it [08:10,  1.06s/it]\u001b[A\n",
      "batch 540, training loss: 3.6608: : 540it [08:10,  1.09s/it]\u001b[A\n",
      "batch 541, training loss: 3.7661: : 540it [08:11,  1.09s/it]\u001b[A\n",
      "batch 541, training loss: 3.7661: : 541it [08:11,  1.08s/it]\u001b[A\n",
      "batch 542, training loss: 3.7941: : 541it [08:12,  1.08s/it]\u001b[A\n",
      "batch 542, training loss: 3.7941: : 542it [08:12,  1.06s/it]\u001b[A\n",
      "batch 543, training loss: 3.6535: : 542it [08:13,  1.06s/it]\u001b[A\n",
      "batch 543, training loss: 3.6535: : 543it [08:13,  1.06s/it]\u001b[A\n",
      "batch 544, training loss: 3.7215: : 543it [08:14,  1.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 544, training loss: 3.7215: : 544it [08:14,  1.06s/it]\u001b[A\n",
      "batch 545, training loss: 3.7198: : 544it [08:15,  1.06s/it]\u001b[A\n",
      "batch 545, training loss: 3.7198: : 545it [08:15,  1.09s/it]\u001b[A\n",
      "batch 546, training loss: 3.6258: : 545it [08:16,  1.09s/it]\u001b[A\n",
      "batch 546, training loss: 3.6258: : 546it [08:16,  1.07s/it]\u001b[A\n",
      "batch 547, training loss: 3.5221: : 546it [08:17,  1.07s/it]\u001b[A\n",
      "batch 547, training loss: 3.5221: : 547it [08:17,  1.06s/it]\u001b[A\n",
      "batch 548, training loss: 3.5824: : 547it [08:18,  1.06s/it]\u001b[A\n",
      "batch 548, training loss: 3.5824: : 548it [08:18,  1.06it/s]\u001b[A\n",
      "batch 549, training loss: 3.6969: : 548it [08:19,  1.06it/s]\u001b[A\n",
      "batch 549, training loss: 3.6969: : 549it [08:19,  1.03it/s]\u001b[A\n",
      "batch 550, training loss: 3.6069: : 549it [08:20,  1.03it/s]\u001b[A\n",
      "batch 550, training loss: 3.6069: : 550it [08:20,  1.04s/it]\u001b[A\n",
      "batch 551, training loss: 3.6763: : 550it [08:21,  1.04s/it]\u001b[A\n",
      "batch 551, training loss: 3.6763: : 551it [08:21,  1.04s/it]\u001b[A\n",
      "batch 552, training loss: 3.699: : 551it [08:22,  1.04s/it] \u001b[A\n",
      "batch 552, training loss: 3.699: : 552it [08:22,  1.04s/it]\u001b[A\n",
      "batch 553, training loss: 3.6694: : 552it [08:23,  1.04s/it]\u001b[A\n",
      "batch 553, training loss: 3.6694: : 553it [08:23,  1.08s/it]\u001b[A\n",
      "batch 554, training loss: 3.4332: : 553it [08:24,  1.08s/it]\u001b[A\n",
      "batch 554, training loss: 3.4332: : 554it [08:24,  1.07s/it]\u001b[A\n",
      "batch 555, training loss: 3.6031: : 554it [08:25,  1.07s/it]\u001b[A\n",
      "batch 555, training loss: 3.6031: : 555it [08:25,  1.11s/it]\u001b[A\n",
      "batch 556, training loss: 3.7369: : 555it [08:26,  1.11s/it]\u001b[A\n",
      "batch 556, training loss: 3.7369: : 556it [08:26,  1.09s/it]\u001b[A\n",
      "batch 557, training loss: 3.5835: : 556it [08:28,  1.09s/it]\u001b[A\n",
      "batch 557, training loss: 3.5835: : 557it [08:28,  1.11s/it]\u001b[A\n",
      "batch 558, training loss: 3.503: : 557it [08:29,  1.11s/it] \u001b[A\n",
      "batch 558, training loss: 3.503: : 558it [08:29,  1.09s/it]\u001b[A\n",
      "batch 559, training loss: 3.6093: : 558it [08:30,  1.09s/it]\u001b[A\n",
      "batch 559, training loss: 3.6093: : 559it [08:30,  1.12s/it]\u001b[A\n",
      "batch 560, training loss: 3.6318: : 559it [08:31,  1.12s/it]\u001b[A\n",
      "batch 560, training loss: 3.6318: : 560it [08:31,  1.10s/it]\u001b[A\n",
      "batch 561, training loss: 3.6178: : 560it [08:32,  1.10s/it]\u001b[A\n",
      "batch 561, training loss: 3.6178: : 561it [08:32,  1.08s/it]\u001b[A\n",
      "batch 562, training loss: 3.4492: : 561it [08:33,  1.08s/it]\u001b[A\n",
      "batch 562, training loss: 3.4492: : 562it [08:33,  1.11s/it]\u001b[A\n",
      "batch 563, training loss: 3.6074: : 562it [08:34,  1.11s/it]\u001b[A\n",
      "batch 563, training loss: 3.6074: : 563it [08:34,  1.09s/it]\u001b[A\n",
      "batch 564, training loss: 3.5091: : 563it [08:35,  1.09s/it]\u001b[A\n",
      "batch 564, training loss: 3.5091: : 564it [08:35,  1.12s/it]\u001b[A\n",
      "batch 565, training loss: 3.6001: : 564it [08:36,  1.12s/it]\u001b[A\n",
      "batch 565, training loss: 3.6001: : 565it [08:36,  1.07s/it]\u001b[A\n",
      "batch 566, training loss: 3.7398: : 565it [08:38,  1.07s/it]\u001b[A\n",
      "batch 566, training loss: 3.7398: : 566it [08:38,  1.13s/it]\u001b[A\n",
      "batch 567, training loss: 3.6709: : 566it [08:39,  1.13s/it]\u001b[A\n",
      "batch 567, training loss: 3.6709: : 567it [08:39,  1.11s/it]\u001b[A\n",
      "batch 568, training loss: 3.7747: : 567it [08:40,  1.11s/it]\u001b[A\n",
      "batch 568, training loss: 3.7747: : 568it [08:40,  1.15s/it]\u001b[A\n",
      "batch 569, training loss: 3.7343: : 568it [08:41,  1.15s/it]\u001b[A\n",
      "batch 569, training loss: 3.7343: : 569it [08:41,  1.13s/it]\u001b[A\n",
      "batch 570, training loss: 3.9078: : 569it [08:42,  1.13s/it]\u001b[A\n",
      "batch 570, training loss: 3.9078: : 570it [08:42,  1.16s/it]\u001b[A\n",
      "batch 571, training loss: 3.7242: : 570it [08:43,  1.16s/it]\u001b[A\n",
      "batch 571, training loss: 3.7242: : 571it [08:43,  1.14s/it]\u001b[A\n",
      "batch 572, training loss: 3.7905: : 571it [08:45,  1.14s/it]\u001b[A\n",
      "batch 572, training loss: 3.7905: : 572it [08:45,  1.17s/it]\u001b[A\n",
      "batch 573, training loss: 3.7077: : 572it [08:46,  1.17s/it]\u001b[A\n",
      "batch 573, training loss: 3.7077: : 573it [08:46,  1.15s/it]\u001b[A\n",
      "batch 574, training loss: 3.7979: : 573it [08:47,  1.15s/it]\u001b[A\n",
      "batch 574, training loss: 3.7979: : 574it [08:47,  1.17s/it]\u001b[A\n",
      "batch 575, training loss: 3.5541: : 574it [08:47,  1.17s/it]\u001b[A\n",
      "batch 575, training loss: 3.5541: : 575it [08:47,  1.02it/s]\u001b[A\n",
      "batch 576, training loss: 3.7516: : 575it [08:49,  1.02it/s]\u001b[A\n",
      "batch 576, training loss: 3.7516: : 576it [08:49,  1.08s/it]\u001b[A\n",
      "batch 577, training loss: 3.6438: : 576it [08:50,  1.08s/it]\u001b[A\n",
      "batch 577, training loss: 3.6438: : 577it [08:50,  1.10s/it]\u001b[A\n",
      "batch 578, training loss: 3.6125: : 577it [08:51,  1.10s/it]\u001b[A\n",
      "batch 578, training loss: 3.6125: : 578it [08:51,  1.15s/it]\u001b[A\n",
      "batch 579, training loss: 3.6688: : 578it [08:52,  1.15s/it]\u001b[A\n",
      "batch 579, training loss: 3.6688: : 579it [08:52,  1.14s/it]\u001b[A\n",
      "batch 580, training loss: 3.5601: : 579it [08:53,  1.14s/it]\u001b[A\n",
      "batch 580, training loss: 3.5601: : 580it [08:53,  1.16s/it]\u001b[A\n",
      "batch 581, training loss: 3.5768: : 580it [08:55,  1.16s/it]\u001b[A\n",
      "batch 581, training loss: 3.5768: : 581it [08:55,  1.15s/it]\u001b[A\n",
      "batch 582, training loss: 3.5871: : 581it [08:56,  1.15s/it]\u001b[A\n",
      "batch 582, training loss: 3.5871: : 582it [08:56,  1.13s/it]\u001b[A\n",
      "batch 583, training loss: 3.2227: : 582it [08:56,  1.13s/it]\u001b[A\n",
      "batch 583, training loss: 3.2227: : 583it [08:56,  1.12it/s]\u001b[A\n",
      "batch 584, training loss: 3.8205: : 583it [08:57,  1.12it/s]\u001b[A\n",
      "batch 584, training loss: 3.8205: : 584it [08:57,  1.19it/s]\u001b[A\n",
      "batch 585, training loss: 3.776: : 584it [08:58,  1.19it/s] \u001b[A\n",
      "batch 585, training loss: 3.776: : 585it [08:58,  1.03it/s]\u001b[A\n",
      "batch 586, training loss: 3.8681: : 585it [08:59,  1.03it/s]\u001b[A\n",
      "batch 586, training loss: 3.8681: : 586it [08:59,  1.08s/it]\u001b[A\n",
      "batch 587, training loss: 3.7371: : 586it [09:01,  1.08s/it]\u001b[A\n",
      "batch 587, training loss: 3.7371: : 587it [09:01,  1.17s/it]\u001b[A\n",
      "batch 588, training loss: 3.6324: : 587it [09:02,  1.17s/it]\u001b[A\n",
      "batch 588, training loss: 3.6324: : 588it [09:02,  1.21s/it]\u001b[A\n",
      "batch 589, training loss: 3.6993: : 588it [09:03,  1.21s/it]\u001b[A\n",
      "batch 589, training loss: 3.6993: : 589it [09:03,  1.26s/it]\u001b[A\n",
      "batch 590, training loss: 3.7128: : 589it [09:05,  1.26s/it]\u001b[A\n",
      "batch 590, training loss: 3.7128: : 590it [09:05,  1.28s/it]\u001b[A\n",
      "batch 591, training loss: 3.5828: : 590it [09:06,  1.28s/it]\u001b[A\n",
      "batch 591, training loss: 3.5828: : 591it [09:06,  1.28s/it]\u001b[A\n",
      "batch 592, training loss: 3.6256: : 591it [09:07,  1.28s/it]\u001b[A\n",
      "batch 592, training loss: 3.6256: : 592it [09:07,  1.28s/it]\u001b[A\n",
      "batch 593, training loss: 3.536: : 592it [09:09,  1.28s/it] \u001b[A\n",
      "batch 593, training loss: 3.536: : 593it [09:09,  1.28s/it]\u001b[A\n",
      "batch 594, training loss: 3.711: : 593it [09:10,  1.28s/it]\u001b[A\n",
      "batch 594, training loss: 3.711: : 594it [09:10,  1.35s/it]\u001b[A\n",
      "batch 595, training loss: 3.8212: : 594it [09:11,  1.35s/it]\u001b[A\n",
      "batch 595, training loss: 3.8212: : 595it [09:11,  1.26s/it]\u001b[A\n",
      "batch 596, training loss: 3.6929: : 595it [09:13,  1.26s/it]\u001b[A\n",
      "batch 596, training loss: 3.6929: : 596it [09:13,  1.33s/it]\u001b[A\n",
      "batch 597, training loss: 3.517: : 596it [09:14,  1.33s/it] \u001b[A\n",
      "batch 597, training loss: 3.517: : 597it [09:14,  1.32s/it]\u001b[A\n",
      "batch 598, training loss: 3.7596: : 597it [09:15,  1.32s/it]\u001b[A\n",
      "batch 598, training loss: 3.7596: : 598it [09:15,  1.35s/it]\u001b[A\n",
      "batch 599, training loss: 3.7062: : 598it [09:16,  1.35s/it]\u001b[A\n",
      "batch 599, training loss: 3.7062: : 599it [09:16,  1.19s/it]\u001b[A\n",
      "batch 600, training loss: 3.5652: : 599it [09:18,  1.19s/it]\u001b[A\n",
      "batch 600, training loss: 3.5652: : 600it [09:18,  1.29s/it]\u001b[A\n",
      "batch 601, training loss: 2.8572: : 600it [09:18,  1.29s/it]\u001b[A\n",
      "batch 601, training loss: 2.8572: : 601it [09:18,  1.06s/it]\u001b[A\n",
      "batch 602, training loss: 3.6014: : 601it [09:19,  1.06s/it]\u001b[A\n",
      "batch 602, training loss: 3.6014: : 602it [09:19,  1.12s/it]\u001b[A\n",
      "batch 603, training loss: 3.6207: : 602it [09:20,  1.12s/it]\u001b[A\n",
      "batch 603, training loss: 3.6207: : 603it [09:20,  1.11s/it]\u001b[A\n",
      "batch 604, training loss: 3.6579: : 603it [09:22,  1.11s/it]\u001b[A\n",
      "batch 604, training loss: 3.6579: : 604it [09:22,  1.13s/it]\u001b[A\n",
      "batch 605, training loss: 3.7913: : 604it [09:23,  1.13s/it]\u001b[A\n",
      "batch 605, training loss: 3.7913: : 605it [09:23,  1.11s/it]\u001b[A\n",
      "batch 606, training loss: 3.6006: : 605it [09:24,  1.11s/it]\u001b[A\n",
      "batch 606, training loss: 3.6006: : 606it [09:24,  1.06s/it]\u001b[A\n",
      "batch 607, training loss: 3.4902: : 606it [09:25,  1.06s/it]\u001b[A\n",
      "batch 607, training loss: 3.4902: : 607it [09:25,  1.03s/it]\u001b[A\n",
      "batch 608, training loss: 3.7282: : 607it [09:25,  1.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 608, training loss: 3.7282: : 608it [09:25,  1.06it/s]\u001b[A\n",
      "batch 609, training loss: 3.69: : 608it [09:26,  1.06it/s]  \u001b[A\n",
      "batch 609, training loss: 3.69: : 609it [09:26,  1.09it/s]\u001b[A\n",
      "batch 610, training loss: 3.1857: : 609it [09:27,  1.09it/s]\u001b[A\n",
      "batch 610, training loss: 3.1857: : 610it [09:27,  1.15it/s]\u001b[A\n",
      "batch 611, training loss: 3.1071: : 610it [09:28,  1.15it/s]\u001b[A\n",
      "batch 611, training loss: 3.1071: : 611it [09:28,  1.18it/s]\u001b[A\n",
      "batch 612, training loss: 2.4536: : 611it [09:28,  1.18it/s]\u001b[A\n",
      "batch 612, training loss: 2.4536: : 612it [09:28,  1.25it/s]\u001b[A\n",
      "batch 613, training loss: 3.1325: : 612it [09:29,  1.25it/s]\u001b[A\n",
      "batch 613, training loss: 3.1325: : 616it [09:29,  1.08it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.8448: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.8448: : 1it [00:00,  4.27it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0459: : 1it [00:00,  4.27it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0459: : 2it [00:00,  5.12it/s]\u001b[A\n",
      "batch 2, dev loss: 3.6561: : 2it [00:00,  5.12it/s]\u001b[A\n",
      "batch 2, dev loss: 3.6561: : 3it [00:00,  5.54it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7839: : 3it [00:00,  5.54it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7839: : 4it [00:00,  5.76it/s]\u001b[A\n",
      "batch 4, dev loss: 3.8169: : 4it [00:00,  5.76it/s]\u001b[A\n",
      "batch 4, dev loss: 3.8169: : 5it [00:00,  5.83it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7807: : 5it [00:01,  5.83it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7807: : 6it [00:01,  5.93it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8873: : 6it [00:01,  5.93it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8873: : 7it [00:01,  5.90it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6572: : 7it [00:01,  5.90it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6572: : 8it [00:01,  6.07it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8682: : 8it [00:01,  6.07it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8682: : 9it [00:01,  5.82it/s]\u001b[A\n",
      "batch 9, dev loss: 3.7867: : 9it [00:01,  5.82it/s]\u001b[A\n",
      "batch 9, dev loss: 3.7867: : 10it [00:01,  5.71it/s]\u001b[A\n",
      "batch 10, dev loss: 3.8138: : 10it [00:01,  5.71it/s]\u001b[A\n",
      "batch 10, dev loss: 3.8138: : 11it [00:01,  5.47it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8954: : 11it [00:02,  5.47it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8954: : 12it [00:02,  5.73it/s]\u001b[A\n",
      "batch 12, dev loss: 3.7344: : 12it [00:02,  5.73it/s]\u001b[A\n",
      "batch 12, dev loss: 3.7344: : 13it [00:02,  5.95it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8691: : 13it [00:02,  5.95it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8691: : 14it [00:02,  5.86it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9826: : 14it [00:02,  5.86it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9826: : 15it [00:02,  5.94it/s]\u001b[A\n",
      "batch 15, dev loss: 3.7852: : 15it [00:02,  5.94it/s]\u001b[A\n",
      "batch 15, dev loss: 3.7852: : 16it [00:02,  6.69it/s]\u001b[A\n",
      "batch 16, dev loss: 4.1253: : 16it [00:02,  6.69it/s]\u001b[A\n",
      "batch 16, dev loss: 4.1253: : 17it [00:02,  6.16it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8981: : 17it [00:03,  6.16it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8981: : 18it [00:03,  5.70it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7954: : 18it [00:03,  5.70it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7954: : 19it [00:03,  5.67it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9936: : 19it [00:03,  5.67it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9936: : 20it [00:03,  5.98it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7914: : 20it [00:03,  5.98it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7914: : 21it [00:03,  6.26it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6863: : 21it [00:03,  6.26it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6863: : 22it [00:03,  6.41it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8931: : 22it [00:03,  6.41it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8931: : 23it [00:03,  6.61it/s]\u001b[A\n",
      "batch 23, dev loss: 3.9337: : 23it [00:03,  6.61it/s]\u001b[A\n",
      "batch 24, dev loss: 3.8482: : 23it [00:04,  6.61it/s]\u001b[A\n",
      "batch 24, dev loss: 3.8482: : 25it [00:04,  7.45it/s]\u001b[A\n",
      "batch 25, dev loss: 3.833: : 25it [00:04,  7.45it/s] \u001b[A\n",
      "batch 25, dev loss: 3.833: : 26it [00:04,  7.07it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7812: : 26it [00:04,  7.07it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7812: : 27it [00:04,  6.76it/s]\u001b[A\n",
      "batch 27, dev loss: 3.7129: : 27it [00:04,  6.76it/s]\u001b[A\n",
      "batch 27, dev loss: 3.7129: : 28it [00:04,  5.95it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9289: : 28it [00:04,  5.95it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9289: : 29it [00:04,  5.43it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8707: : 29it [00:05,  5.43it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8707: : 30it [00:05,  5.06it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1198: : 30it [00:05,  5.06it/s]\u001b[A\n",
      "batch 31, dev loss: 3.9279: : 30it [00:05,  5.06it/s]\u001b[A\n",
      "batch 31, dev loss: 3.9279: : 32it [00:05,  5.37it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9609: : 32it [00:05,  5.37it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9609: : 33it [00:05,  4.93it/s]\u001b[A\n",
      "batch 33, dev loss: 3.7413: : 33it [00:05,  4.93it/s]\u001b[A\n",
      "batch 33, dev loss: 3.7413: : 34it [00:05,  4.75it/s]\u001b[A\n",
      "batch 34, dev loss: 4.1567: : 34it [00:06,  4.75it/s]\u001b[A\n",
      "batch 34, dev loss: 4.1567: : 35it [00:06,  4.49it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9452: : 35it [00:06,  4.49it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9452: : 36it [00:06,  4.34it/s]\u001b[A\n",
      "batch 36, dev loss: 3.9147: : 36it [00:06,  4.34it/s]\u001b[A\n",
      "batch 36, dev loss: 3.9147: : 37it [00:06,  4.70it/s]\u001b[A\n",
      "batch 37, dev loss: 3.6794: : 37it [00:06,  4.70it/s]\u001b[A\n",
      "batch 37, dev loss: 3.6794: : 38it [00:06,  4.36it/s]\u001b[A\n",
      "batch 38, dev loss: 3.9197: : 38it [00:07,  4.36it/s]\u001b[A\n",
      "batch 38, dev loss: 3.9197: : 39it [00:07,  4.03it/s]\u001b[A\n",
      "batch 39, dev loss: 3.9196: : 39it [00:07,  4.03it/s]\u001b[A\n",
      "batch 39, dev loss: 3.9196: : 40it [00:07,  4.01it/s]\u001b[A\n",
      "batch 40, dev loss: 3.9614: : 40it [00:07,  4.01it/s]\u001b[A\n",
      "batch 40, dev loss: 3.9614: : 41it [00:07,  3.90it/s]\u001b[A\n",
      "batch 41, dev loss: 3.7441: : 41it [00:07,  3.90it/s]\u001b[A\n",
      "batch 41, dev loss: 3.7441: : 42it [00:07,  4.16it/s]\u001b[A\n",
      "batch 42, dev loss: 3.9003: : 42it [00:08,  4.16it/s]\u001b[A\n",
      "batch 42, dev loss: 3.9003: : 43it [00:08,  3.89it/s]\u001b[A\n",
      "batch 43, dev loss: 3.9095: : 43it [00:08,  3.89it/s]\u001b[A\n",
      "batch 43, dev loss: 3.9095: : 44it [00:08,  3.64it/s]\u001b[A\n",
      "batch 44, dev loss: 3.8307: : 44it [00:08,  3.64it/s]\u001b[A\n",
      "batch 44, dev loss: 3.8307: : 45it [00:08,  3.53it/s]\u001b[A\n",
      "batch 45, dev loss: 4.1223: : 45it [00:09,  3.53it/s]\u001b[A\n",
      "batch 45, dev loss: 4.1223: : 46it [00:09,  3.52it/s]\u001b[A\n",
      "batch 46, dev loss: 3.6725: : 46it [00:09,  3.52it/s]\u001b[A\n",
      "batch 46, dev loss: 3.6725: : 47it [00:09,  3.33it/s]\u001b[A\n",
      "batch 47, dev loss: 3.8356: : 47it [00:09,  3.33it/s]\u001b[A\n",
      "batch 47, dev loss: 3.8356: : 48it [00:09,  2.91it/s]\u001b[A\n",
      "batch 48, dev loss: 3.6653: : 48it [00:10,  2.91it/s]\u001b[A\n",
      "batch 48, dev loss: 3.6653: : 49it [00:10,  2.74it/s]\u001b[A\n",
      "batch 49, dev loss: 3.8423: : 49it [00:10,  2.74it/s]\u001b[A\n",
      "batch 49, dev loss: 3.8423: : 50it [00:10,  2.97it/s]\u001b[A\n",
      "batch 50, dev loss: 3.8316: : 50it [00:10,  2.97it/s]\u001b[A\n",
      "batch 50, dev loss: 3.8316: : 51it [00:10,  2.76it/s]\u001b[A\n",
      "batch 51, dev loss: 3.8319: : 51it [00:11,  2.76it/s]\u001b[A\n",
      "batch 51, dev loss: 3.8319: : 52it [00:11,  2.55it/s]\u001b[A\n",
      "batch 52, dev loss: 3.6221: : 52it [00:11,  2.55it/s]\u001b[A\n",
      "batch 52, dev loss: 3.6221: : 53it [00:11,  2.54it/s]\u001b[A\n",
      "batch 53, dev loss: 3.8284: : 53it [00:12,  2.54it/s]\u001b[A\n",
      "batch 53, dev loss: 3.8284: : 54it [00:12,  2.40it/s]\u001b[A\n",
      "batch 54, dev loss: 3.5997: : 54it [00:12,  2.40it/s]\u001b[A\n",
      "batch 54, dev loss: 3.5997: : 55it [00:12,  2.32it/s]\u001b[A\n",
      "batch 55, dev loss: 3.7593: : 55it [00:13,  2.32it/s]\u001b[A\n",
      "batch 55, dev loss: 3.7593: : 56it [00:13,  2.27it/s]\u001b[A\n",
      "batch 56, dev loss: 3.635: : 56it [00:13,  2.27it/s] \u001b[A\n",
      "batch 56, dev loss: 3.635: : 57it [00:13,  2.26it/s]\u001b[A\n",
      "batch 57, dev loss: 3.5576: : 57it [00:14,  2.26it/s]\u001b[A\n",
      "batch 57, dev loss: 3.5576: : 58it [00:14,  2.19it/s]\u001b[A\n",
      "batch 58, dev loss: 3.9187: : 58it [00:14,  2.19it/s]\u001b[A\n",
      "batch 58, dev loss: 3.9187: : 59it [00:14,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 3.8245: : 59it [00:15,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 3.8245: : 60it [00:15,  2.29it/s]\u001b[A\n",
      "batch 60, dev loss: 3.5064: : 60it [00:15,  2.29it/s]\u001b[A\n",
      "batch 60, dev loss: 3.5064: : 61it [00:15,  2.39it/s]\u001b[A\n",
      "batch 61, dev loss: 3.5166: : 61it [00:15,  2.39it/s]\u001b[A\n",
      "batch 61, dev loss: 3.5166: : 62it [00:15,  2.61it/s]\u001b[A\n",
      "batch 62, dev loss: 3.3912: : 62it [00:16,  2.61it/s]\u001b[A\n",
      "batch 62, dev loss: 3.3912: : 63it [00:16,  2.67it/s]\u001b[A\n",
      "batch 63, dev loss: 3.9739: : 63it [00:16,  2.67it/s]\u001b[A\n",
      "batch 63, dev loss: 3.9739: : 64it [00:16,  2.75it/s]\u001b[A\n",
      "batch 64, dev loss: 3.6826: : 64it [00:16,  2.75it/s]\u001b[A\n",
      "batch 64, dev loss: 3.6826: : 65it [00:16,  2.75it/s]\u001b[A\n",
      "batch 65, dev loss: 3.5612: : 65it [00:17,  2.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 65, dev loss: 3.5612: : 66it [00:17,  2.97it/s]\u001b[A\n",
      "batch 66, dev loss: 3.6459: : 66it [00:17,  2.97it/s]\u001b[A\n",
      "batch 66, dev loss: 3.6459: : 67it [00:17,  2.94it/s]\u001b[A\n",
      "batch 67, dev loss: 2.8341: : 67it [00:17,  2.94it/s]\u001b[A\n",
      "batch 67, dev loss: 2.8341: : 68it [00:17,  3.06it/s]\u001b[A\n",
      "batch 68, dev loss: 2.8066: : 68it [00:18,  3.06it/s]\u001b[A\n",
      "batch 68, dev loss: 2.8066: : 69it [00:18,  2.97it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2031: : 69it [00:18,  2.97it/s]\u001b[A\n",
      "batch 69, dev loss: 3.2031: : 70it [00:18,  3.04it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9904: : 70it [00:18,  3.04it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9904: : 71it [00:18,  3.07it/s]\u001b[A\n",
      "batch 71, dev loss: 3.4388: : 71it [00:18,  3.07it/s]\u001b[A\n",
      "batch 71, dev loss: 3.4388: : 72it [00:18,  3.09it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1717: : 72it [00:19,  3.09it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1717: : 73it [00:19,  3.07it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1717: : 74it [00:19,  3.81it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1717: : 76it [00:19,  3.86it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.45s/it]\u001b[A\n",
      "2it [00:02,  1.29s/it]\u001b[A\n",
      "3it [00:03,  1.27s/it]\u001b[A\n",
      "4it [00:05,  1.34s/it]\u001b[A\n",
      "5it [00:06,  1.21s/it]\u001b[A\n",
      "6it [00:07,  1.28s/it]\u001b[A\n",
      "7it [00:09,  1.33s/it]\u001b[A\n",
      "8it [00:09,  1.14s/it]\u001b[A\n",
      "9it [00:11,  1.37s/it]\u001b[A\n",
      "10it [00:13,  1.52s/it]\u001b[A\n",
      "11it [00:14,  1.44s/it]\u001b[A\n",
      "12it [00:16,  1.40s/it]\u001b[A\n",
      "13it [00:17,  1.46s/it]\u001b[A\n",
      "14it [00:19,  1.53s/it]\u001b[A\n",
      "15it [00:20,  1.39s/it]\u001b[A\n",
      "16it [00:21,  1.25s/it]\u001b[A\n",
      "17it [00:23,  1.52s/it]\u001b[A\n",
      "18it [00:25,  1.61s/it]\u001b[A\n",
      "19it [00:26,  1.60s/it]\u001b[A\n",
      "20it [00:28,  1.69s/it]\u001b[A\n",
      "21it [00:30,  1.77s/it]\u001b[A\n",
      "22it [00:33,  1.92s/it]\u001b[A\n",
      "23it [00:35,  1.93s/it]\u001b[A\n",
      "24it [00:35,  1.51s/it]\u001b[A\n",
      "25it [00:37,  1.73s/it]\u001b[A\n",
      "26it [00:40,  1.91s/it]\u001b[A\n",
      "27it [00:42,  2.01s/it]\u001b[A\n",
      "28it [00:44,  2.05s/it]\u001b[A\n",
      "29it [00:46,  2.00s/it]\u001b[A\n",
      "30it [00:47,  1.62s/it]\u001b[A\n",
      "31it [00:49,  1.83s/it]\u001b[A\n",
      "32it [00:52,  2.04s/it]\u001b[A\n",
      "33it [00:54,  2.18s/it]\u001b[A\n",
      "34it [00:57,  2.27s/it]\u001b[A\n",
      "35it [00:59,  2.48s/it]\u001b[A\n",
      "36it [01:00,  1.87s/it]\u001b[A\n",
      "37it [01:03,  2.15s/it]\u001b[A\n",
      "38it [01:05,  2.18s/it]\u001b[A\n",
      "39it [01:08,  2.32s/it]\u001b[A\n",
      "40it [01:10,  2.39s/it]\u001b[A\n",
      "41it [01:11,  1.93s/it]\u001b[A\n",
      "42it [01:15,  2.45s/it]\u001b[A\n",
      "43it [01:18,  2.66s/it]\u001b[A\n",
      "44it [01:21,  2.78s/it]\u001b[A\n",
      "45it [01:23,  2.59s/it]\u001b[A\n",
      "46it [01:26,  2.75s/it]\u001b[A\n",
      "47it [01:30,  3.17s/it]\u001b[A\n",
      "48it [01:34,  3.17s/it]\u001b[A\n",
      "49it [01:34,  2.29s/it]\u001b[A\n",
      "50it [01:37,  2.70s/it]\u001b[A\n",
      "51it [01:41,  3.07s/it]\u001b[A\n",
      "52it [01:44,  2.84s/it]\u001b[A\n",
      "53it [01:47,  3.09s/it]\u001b[A\n",
      "54it [01:51,  3.35s/it]\u001b[A\n",
      "55it [01:56,  3.86s/it]\u001b[A\n",
      "56it [01:58,  3.26s/it]\u001b[A\n",
      "57it [02:02,  3.46s/it]\u001b[A\n",
      "58it [02:05,  3.37s/it]\u001b[A\n",
      "59it [02:07,  3.01s/it]\u001b[A\n",
      "60it [02:10,  2.83s/it]\u001b[A\n",
      "61it [02:11,  2.29s/it]\u001b[A\n",
      "62it [02:12,  2.02s/it]\u001b[A\n",
      "63it [02:13,  1.62s/it]\u001b[A\n",
      "64it [02:14,  1.42s/it]\u001b[A\n",
      "65it [02:15,  1.22s/it]\u001b[A\n",
      "66it [02:15,  1.03it/s]\u001b[A\n",
      "67it [02:16,  1.22it/s]\u001b[A\n",
      "68it [02:16,  1.36it/s]\u001b[A\n",
      "69it [02:17,  1.50it/s]\u001b[A\n",
      "70it [02:17,  1.96s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███                                   | 533/6740 [00:00<00:01, 5324.66it/s]\u001b[A\n",
      " 16%|█████▊                               | 1066/6740 [00:00<00:01, 3027.06it/s]\u001b[A\n",
      " 24%|████████▋                            | 1589/6740 [00:00<00:01, 3738.11it/s]\u001b[A\n",
      " 32%|███████████▋                         | 2135/6740 [00:00<00:01, 4277.57it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 2649/6740 [00:00<00:00, 4543.91it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 3152/6740 [00:00<00:00, 4692.47it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 3668/6740 [00:00<00:00, 4833.92it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 4170/6740 [00:00<00:00, 4758.24it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 4662/6740 [00:01<00:00, 4805.50it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 5162/6740 [00:01<00:00, 4859.28it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 5682/6740 [00:01<00:00, 4956.43it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 4683.94it/s]\u001b[A\n",
      "Epoch: 7, tfr: 1.0, loss(train/dev): 3.7656/3.7836, ppl(dev/test): 43.9741/50.58\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.7119: : 0it [00:02, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.7119: : 1it [00:02,  2.09s/it]\u001b[A\n",
      "batch 2, training loss: 3.628: : 1it [00:02,  2.09s/it] \u001b[A\n",
      "batch 2, training loss: 3.628: : 2it [00:02,  1.22s/it]\u001b[A\n",
      "batch 3, training loss: 3.7417: : 2it [00:03,  1.22s/it]\u001b[A\n",
      "batch 3, training loss: 3.7417: : 3it [00:03,  1.06it/s]\u001b[A\n",
      "batch 4, training loss: 3.6732: : 3it [00:03,  1.06it/s]\u001b[A\n",
      "batch 4, training loss: 3.6732: : 4it [00:03,  1.22it/s]\u001b[A\n",
      "batch 5, training loss: 3.5293: : 4it [00:04,  1.22it/s]\u001b[A\n",
      "batch 5, training loss: 3.5293: : 5it [00:04,  1.31it/s]\u001b[A\n",
      "batch 6, training loss: 3.7539: : 5it [00:05,  1.31it/s]\u001b[A\n",
      "batch 6, training loss: 3.7539: : 6it [00:05,  1.34it/s]\u001b[A\n",
      "batch 7, training loss: 3.7405: : 6it [00:05,  1.34it/s]\u001b[A\n",
      "batch 7, training loss: 3.7405: : 7it [00:05,  1.38it/s]\u001b[A\n",
      "batch 8, training loss: 3.7202: : 7it [00:06,  1.38it/s]\u001b[A\n",
      "batch 8, training loss: 3.7202: : 8it [00:06,  1.53it/s]\u001b[A\n",
      "batch 9, training loss: 3.5829: : 8it [00:07,  1.53it/s]\u001b[A\n",
      "batch 9, training loss: 3.5829: : 9it [00:07,  1.58it/s]\u001b[A\n",
      "batch 10, training loss: 3.5868: : 9it [00:07,  1.58it/s]\u001b[A\n",
      "batch 10, training loss: 3.5868: : 10it [00:07,  1.57it/s]\u001b[A\n",
      "batch 11, training loss: 3.653: : 10it [00:08,  1.57it/s] \u001b[A\n",
      "batch 11, training loss: 3.653: : 11it [00:08,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.7068: : 11it [00:09,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.7068: : 12it [00:09,  1.54it/s]\u001b[A\n",
      "batch 13, training loss: 3.5895: : 12it [00:09,  1.54it/s]\u001b[A\n",
      "batch 13, training loss: 3.5895: : 13it [00:09,  1.55it/s]\u001b[A\n",
      "batch 14, training loss: 3.8574: : 13it [00:10,  1.55it/s]\u001b[A\n",
      "batch 14, training loss: 3.8574: : 14it [00:10,  1.57it/s]\u001b[A\n",
      "batch 15, training loss: 3.6884: : 14it [00:10,  1.57it/s]\u001b[A\n",
      "batch 15, training loss: 3.6884: : 15it [00:10,  1.58it/s]\u001b[A\n",
      "batch 16, training loss: 3.6812: : 15it [00:11,  1.58it/s]\u001b[A\n",
      "batch 16, training loss: 3.6812: : 16it [00:11,  1.56it/s]\u001b[A\n",
      "batch 17, training loss: 3.8483: : 16it [00:12,  1.56it/s]\u001b[A\n",
      "batch 17, training loss: 3.8483: : 17it [00:12,  1.53it/s]\u001b[A\n",
      "batch 18, training loss: 3.661: : 17it [00:12,  1.53it/s] \u001b[A\n",
      "batch 18, training loss: 3.661: : 18it [00:12,  1.50it/s]\u001b[A\n",
      "batch 19, training loss: 3.4564: : 18it [00:13,  1.50it/s]\u001b[A\n",
      "batch 19, training loss: 3.4564: : 19it [00:13,  1.51it/s]\u001b[A\n",
      "batch 20, training loss: 3.6167: : 19it [00:14,  1.51it/s]\u001b[A\n",
      "batch 20, training loss: 3.6167: : 20it [00:14,  1.57it/s]\u001b[A\n",
      "batch 21, training loss: 3.734: : 20it [00:14,  1.57it/s] \u001b[A\n",
      "batch 21, training loss: 3.734: : 21it [00:14,  1.64it/s]\u001b[A\n",
      "batch 22, training loss: 3.5041: : 21it [00:15,  1.64it/s]\u001b[A\n",
      "batch 22, training loss: 3.5041: : 22it [00:15,  1.60it/s]\u001b[A\n",
      "batch 23, training loss: 3.6607: : 22it [00:16,  1.60it/s]\u001b[A\n",
      "batch 23, training loss: 3.6607: : 23it [00:16,  1.57it/s]\u001b[A\n",
      "batch 24, training loss: 3.5913: : 23it [00:16,  1.57it/s]\u001b[A\n",
      "batch 24, training loss: 3.5913: : 24it [00:16,  1.69it/s]\u001b[A\n",
      "batch 25, training loss: 3.702: : 24it [00:17,  1.69it/s] \u001b[A\n",
      "batch 25, training loss: 3.702: : 25it [00:17,  1.70it/s]\u001b[A\n",
      "batch 26, training loss: 3.5189: : 25it [00:17,  1.70it/s]\u001b[A\n",
      "batch 26, training loss: 3.5189: : 26it [00:17,  1.65it/s]\u001b[A\n",
      "batch 27, training loss: 3.6179: : 26it [00:18,  1.65it/s]\u001b[A\n",
      "batch 27, training loss: 3.6179: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 3.4939: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 3.4939: : 28it [00:18,  1.71it/s]\u001b[A\n",
      "batch 29, training loss: 3.649: : 28it [00:19,  1.71it/s] \u001b[A\n",
      "batch 29, training loss: 3.649: : 29it [00:19,  1.72it/s]\u001b[A\n",
      "batch 30, training loss: 3.7725: : 29it [00:20,  1.72it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, training loss: 3.7725: : 30it [00:20,  1.66it/s]\u001b[A\n",
      "batch 31, training loss: 3.5114: : 30it [00:20,  1.66it/s]\u001b[A\n",
      "batch 31, training loss: 3.5114: : 31it [00:20,  1.60it/s]\u001b[A\n",
      "batch 32, training loss: 3.7054: : 31it [00:21,  1.60it/s]\u001b[A\n",
      "batch 32, training loss: 3.7054: : 32it [00:21,  1.71it/s]\u001b[A\n",
      "batch 33, training loss: 3.6001: : 32it [00:21,  1.71it/s]\u001b[A\n",
      "batch 33, training loss: 3.6001: : 33it [00:21,  1.72it/s]\u001b[A\n",
      "batch 34, training loss: 3.5973: : 33it [00:22,  1.72it/s]\u001b[A\n",
      "batch 34, training loss: 3.5973: : 34it [00:22,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 3.6672: : 34it [00:23,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 3.6672: : 35it [00:23,  1.60it/s]\u001b[A\n",
      "batch 36, training loss: 3.7494: : 35it [00:23,  1.60it/s]\u001b[A\n",
      "batch 36, training loss: 3.7494: : 36it [00:23,  1.59it/s]\u001b[A\n",
      "batch 37, training loss: 3.5982: : 36it [00:24,  1.59it/s]\u001b[A\n",
      "batch 37, training loss: 3.5982: : 37it [00:24,  1.63it/s]\u001b[A\n",
      "batch 38, training loss: 3.4133: : 37it [00:24,  1.63it/s]\u001b[A\n",
      "batch 38, training loss: 3.4133: : 38it [00:24,  1.71it/s]\u001b[A\n",
      "batch 39, training loss: 3.5527: : 38it [00:25,  1.71it/s]\u001b[A\n",
      "batch 39, training loss: 3.5527: : 39it [00:25,  1.63it/s]\u001b[A\n",
      "batch 40, training loss: 3.6252: : 39it [00:26,  1.63it/s]\u001b[A\n",
      "batch 40, training loss: 3.6252: : 40it [00:26,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 3.7748: : 40it [00:27,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 3.7748: : 41it [00:27,  1.51it/s]\u001b[A\n",
      "batch 42, training loss: 3.6279: : 41it [00:27,  1.51it/s]\u001b[A\n",
      "batch 42, training loss: 3.6279: : 42it [00:27,  1.51it/s]\u001b[A\n",
      "batch 43, training loss: 3.434: : 42it [00:28,  1.51it/s] \u001b[A\n",
      "batch 43, training loss: 3.434: : 43it [00:28,  1.59it/s]\u001b[A\n",
      "batch 44, training loss: 3.41: : 43it [00:28,  1.59it/s] \u001b[A\n",
      "batch 44, training loss: 3.41: : 44it [00:28,  1.65it/s]\u001b[A\n",
      "batch 45, training loss: 3.4315: : 44it [00:29,  1.65it/s]\u001b[A\n",
      "batch 45, training loss: 3.4315: : 45it [00:29,  1.61it/s]\u001b[A\n",
      "batch 46, training loss: 3.5751: : 45it [00:30,  1.61it/s]\u001b[A\n",
      "batch 46, training loss: 3.5751: : 46it [00:30,  1.55it/s]\u001b[A\n",
      "batch 47, training loss: 3.5746: : 46it [00:30,  1.55it/s]\u001b[A\n",
      "batch 47, training loss: 3.5746: : 47it [00:30,  1.52it/s]\u001b[A\n",
      "batch 48, training loss: 3.4968: : 47it [00:31,  1.52it/s]\u001b[A\n",
      "batch 48, training loss: 3.4968: : 48it [00:31,  1.50it/s]\u001b[A\n",
      "batch 49, training loss: 3.7583: : 48it [00:32,  1.50it/s]\u001b[A\n",
      "batch 49, training loss: 3.7583: : 49it [00:32,  1.57it/s]\u001b[A\n",
      "batch 50, training loss: 3.5154: : 49it [00:32,  1.57it/s]\u001b[A\n",
      "batch 50, training loss: 3.5154: : 50it [00:32,  1.68it/s]\u001b[A\n",
      "batch 51, training loss: 3.5994: : 50it [00:33,  1.68it/s]\u001b[A\n",
      "batch 51, training loss: 3.5994: : 51it [00:33,  1.63it/s]\u001b[A\n",
      "batch 52, training loss: 3.4557: : 51it [00:33,  1.63it/s]\u001b[A\n",
      "batch 52, training loss: 3.4557: : 52it [00:33,  1.60it/s]\u001b[A\n",
      "batch 53, training loss: 3.4785: : 52it [00:34,  1.60it/s]\u001b[A\n",
      "batch 53, training loss: 3.4785: : 53it [00:34,  1.71it/s]\u001b[A\n",
      "batch 54, training loss: 3.4384: : 53it [00:35,  1.71it/s]\u001b[A\n",
      "batch 54, training loss: 3.4384: : 54it [00:35,  1.71it/s]\u001b[A\n",
      "batch 55, training loss: 3.5585: : 54it [00:35,  1.71it/s]\u001b[A\n",
      "batch 55, training loss: 3.5585: : 55it [00:35,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.6051: : 55it [00:36,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.6051: : 56it [00:36,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 3.5436: : 56it [00:36,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 3.5436: : 57it [00:36,  1.71it/s]\u001b[A\n",
      "batch 58, training loss: 3.6177: : 57it [00:37,  1.71it/s]\u001b[A\n",
      "batch 58, training loss: 3.6177: : 58it [00:37,  1.72it/s]\u001b[A\n",
      "batch 59, training loss: 3.479: : 58it [00:38,  1.72it/s] \u001b[A\n",
      "batch 59, training loss: 3.479: : 59it [00:38,  1.66it/s]\u001b[A\n",
      "batch 60, training loss: 3.412: : 59it [00:38,  1.66it/s]\u001b[A\n",
      "batch 60, training loss: 3.412: : 60it [00:38,  1.60it/s]\u001b[A\n",
      "batch 61, training loss: 3.668: : 60it [00:39,  1.60it/s]\u001b[A\n",
      "batch 61, training loss: 3.668: : 61it [00:39,  1.57it/s]\u001b[A\n",
      "batch 62, training loss: 3.5062: : 61it [00:40,  1.57it/s]\u001b[A\n",
      "batch 62, training loss: 3.5062: : 62it [00:40,  1.59it/s]\u001b[A\n",
      "batch 63, training loss: 3.5858: : 62it [00:40,  1.59it/s]\u001b[A\n",
      "batch 63, training loss: 3.5858: : 63it [00:40,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.5417: : 63it [00:41,  1.58it/s]\u001b[A\n",
      "batch 64, training loss: 3.5417: : 64it [00:41,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 3.6261: : 64it [00:41,  1.59it/s]\u001b[A\n",
      "batch 65, training loss: 3.6261: : 65it [00:41,  1.56it/s]\u001b[A\n",
      "batch 66, training loss: 3.6472: : 65it [00:42,  1.56it/s]\u001b[A\n",
      "batch 66, training loss: 3.6472: : 66it [00:42,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.5025: : 66it [00:43,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.5025: : 67it [00:43,  1.48it/s]\u001b[A\n",
      "batch 68, training loss: 3.61: : 67it [00:44,  1.48it/s]  \u001b[A\n",
      "batch 68, training loss: 3.61: : 68it [00:44,  1.47it/s]\u001b[A\n",
      "batch 69, training loss: 3.4888: : 68it [00:44,  1.47it/s]\u001b[A\n",
      "batch 69, training loss: 3.4888: : 69it [00:44,  1.48it/s]\u001b[A\n",
      "batch 70, training loss: 3.6805: : 69it [00:45,  1.48it/s]\u001b[A\n",
      "batch 70, training loss: 3.6805: : 70it [00:45,  1.52it/s]\u001b[A\n",
      "batch 71, training loss: 3.5271: : 70it [00:45,  1.52it/s]\u001b[A\n",
      "batch 71, training loss: 3.5271: : 71it [00:45,  1.54it/s]\u001b[A\n",
      "batch 72, training loss: 3.4977: : 71it [00:46,  1.54it/s]\u001b[A\n",
      "batch 72, training loss: 3.4977: : 72it [00:46,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.5473: : 72it [00:47,  1.56it/s]\u001b[A\n",
      "batch 73, training loss: 3.5473: : 73it [00:47,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 3.4279: : 73it [00:47,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 3.4279: : 74it [00:47,  1.50it/s]\u001b[A\n",
      "batch 75, training loss: 3.6737: : 74it [00:48,  1.50it/s]\u001b[A\n",
      "batch 75, training loss: 3.6737: : 75it [00:48,  1.49it/s]\u001b[A\n",
      "batch 76, training loss: 3.4779: : 75it [00:49,  1.49it/s]\u001b[A\n",
      "batch 76, training loss: 3.4779: : 76it [00:49,  1.52it/s]\u001b[A\n",
      "batch 77, training loss: 3.6159: : 76it [00:49,  1.52it/s]\u001b[A\n",
      "batch 77, training loss: 3.6159: : 77it [00:49,  1.56it/s]\u001b[A\n",
      "batch 78, training loss: 3.6029: : 77it [00:50,  1.56it/s]\u001b[A\n",
      "batch 78, training loss: 3.6029: : 78it [00:50,  1.64it/s]\u001b[A\n",
      "batch 79, training loss: 3.6363: : 78it [00:51,  1.64it/s]\u001b[A\n",
      "batch 79, training loss: 3.6363: : 79it [00:51,  1.61it/s]\u001b[A\n",
      "batch 80, training loss: 3.5319: : 79it [00:51,  1.61it/s]\u001b[A\n",
      "batch 80, training loss: 3.5319: : 80it [00:51,  1.57it/s]\u001b[A\n",
      "batch 81, training loss: 3.5516: : 80it [00:52,  1.57it/s]\u001b[A\n",
      "batch 81, training loss: 3.5516: : 81it [00:52,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 3.6887: : 81it [00:53,  1.55it/s]\u001b[A\n",
      "batch 82, training loss: 3.6887: : 82it [00:53,  1.57it/s]\u001b[A\n",
      "batch 83, training loss: 3.5403: : 82it [00:53,  1.57it/s]\u001b[A\n",
      "batch 83, training loss: 3.5403: : 83it [00:53,  1.60it/s]\u001b[A\n",
      "batch 84, training loss: 3.6875: : 83it [00:54,  1.60it/s]\u001b[A\n",
      "batch 84, training loss: 3.6875: : 84it [00:54,  1.57it/s]\u001b[A\n",
      "batch 85, training loss: 3.7162: : 84it [00:54,  1.57it/s]\u001b[A\n",
      "batch 85, training loss: 3.7162: : 85it [00:54,  1.54it/s]\u001b[A\n",
      "batch 86, training loss: 3.6584: : 85it [00:55,  1.54it/s]\u001b[A\n",
      "batch 86, training loss: 3.6584: : 86it [00:55,  1.50it/s]\u001b[A\n",
      "batch 87, training loss: 3.6893: : 86it [00:56,  1.50it/s]\u001b[A\n",
      "batch 87, training loss: 3.6893: : 87it [00:56,  1.51it/s]\u001b[A\n",
      "batch 88, training loss: 3.7949: : 87it [00:57,  1.51it/s]\u001b[A\n",
      "batch 88, training loss: 3.7949: : 88it [00:57,  1.47it/s]\u001b[A\n",
      "batch 89, training loss: 3.8975: : 88it [00:57,  1.47it/s]\u001b[A\n",
      "batch 89, training loss: 3.8975: : 89it [00:57,  1.44it/s]\u001b[A\n",
      "batch 90, training loss: 3.7965: : 89it [00:58,  1.44it/s]\u001b[A\n",
      "batch 90, training loss: 3.7965: : 90it [00:58,  1.47it/s]\u001b[A\n",
      "batch 91, training loss: 3.9309: : 90it [00:59,  1.47it/s]\u001b[A\n",
      "batch 91, training loss: 3.9309: : 91it [00:59,  1.51it/s]\u001b[A\n",
      "batch 92, training loss: 3.7677: : 91it [00:59,  1.51it/s]\u001b[A\n",
      "batch 92, training loss: 3.7677: : 92it [00:59,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 3.7139: : 92it [01:00,  1.49it/s]\u001b[A\n",
      "batch 93, training loss: 3.7139: : 93it [01:00,  1.43it/s]\u001b[A\n",
      "batch 94, training loss: 3.824: : 93it [01:01,  1.43it/s] \u001b[A\n",
      "batch 94, training loss: 3.824: : 94it [01:01,  1.41it/s]\u001b[A\n",
      "batch 95, training loss: 3.8163: : 94it [01:02,  1.41it/s]\u001b[A\n",
      "batch 95, training loss: 3.8163: : 95it [01:02,  1.36it/s]\u001b[A\n",
      "batch 96, training loss: 3.7952: : 95it [01:02,  1.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 96, training loss: 3.7952: : 96it [01:02,  1.35it/s]\u001b[A\n",
      "batch 97, training loss: 3.8693: : 96it [01:03,  1.35it/s]\u001b[A\n",
      "batch 97, training loss: 3.8693: : 97it [01:03,  1.34it/s]\u001b[A\n",
      "batch 98, training loss: 3.8534: : 97it [01:04,  1.34it/s]\u001b[A\n",
      "batch 98, training loss: 3.8534: : 98it [01:04,  1.32it/s]\u001b[A\n",
      "batch 99, training loss: 3.6685: : 98it [01:05,  1.32it/s]\u001b[A\n",
      "batch 99, training loss: 3.6685: : 99it [01:05,  1.32it/s]\u001b[A\n",
      "batch 100, training loss: 3.4966: : 99it [01:05,  1.32it/s]\u001b[A\n",
      "batch 100, training loss: 3.4966: : 100it [01:05,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 3.6881: : 100it [01:06,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 3.6881: : 101it [01:06,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.6783: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.6783: : 102it [01:07,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.6152: : 102it [01:08,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.6152: : 103it [01:08,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 3.462: : 103it [01:08,  1.31it/s] \u001b[A\n",
      "batch 104, training loss: 3.462: : 104it [01:08,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 3.6256: : 104it [01:09,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 3.6256: : 105it [01:09,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.7354: : 105it [01:10,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.7354: : 106it [01:10,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 3.4446: : 106it [01:11,  1.31it/s]\u001b[A\n",
      "batch 107, training loss: 3.4446: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.7163: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.7163: : 108it [01:11,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 3.776: : 108it [01:12,  1.30it/s] \u001b[A\n",
      "batch 109, training loss: 3.776: : 109it [01:12,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.8913: : 109it [01:13,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.8913: : 110it [01:13,  1.31it/s]\u001b[A\n",
      "batch 111, training loss: 3.6089: : 110it [01:14,  1.31it/s]\u001b[A\n",
      "batch 111, training loss: 3.6089: : 111it [01:14,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.6646: : 111it [01:15,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.6646: : 112it [01:15,  1.32it/s]\u001b[A\n",
      "batch 113, training loss: 3.7463: : 112it [01:15,  1.32it/s]\u001b[A\n",
      "batch 113, training loss: 3.7463: : 113it [01:15,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.7338: : 113it [01:16,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.7338: : 114it [01:16,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 3.6301: : 114it [01:17,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 3.6301: : 115it [01:17,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 3.6375: : 115it [01:18,  1.31it/s]\u001b[A\n",
      "batch 116, training loss: 3.6375: : 116it [01:18,  1.32it/s]\u001b[A\n",
      "batch 117, training loss: 3.7057: : 116it [01:18,  1.32it/s]\u001b[A\n",
      "batch 117, training loss: 3.7057: : 117it [01:18,  1.30it/s]\u001b[A\n",
      "batch 118, training loss: 3.7811: : 117it [01:19,  1.30it/s]\u001b[A\n",
      "batch 118, training loss: 3.7811: : 118it [01:19,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 3.6663: : 118it [01:20,  1.31it/s]\u001b[A\n",
      "batch 119, training loss: 3.6663: : 119it [01:20,  1.33it/s]\u001b[A\n",
      "batch 120, training loss: 3.6112: : 119it [01:21,  1.33it/s]\u001b[A\n",
      "batch 120, training loss: 3.6112: : 120it [01:21,  1.35it/s]\u001b[A\n",
      "batch 121, training loss: 3.8587: : 120it [01:21,  1.35it/s]\u001b[A\n",
      "batch 121, training loss: 3.8587: : 121it [01:21,  1.36it/s]\u001b[A\n",
      "batch 122, training loss: 3.5263: : 121it [01:22,  1.36it/s]\u001b[A\n",
      "batch 122, training loss: 3.5263: : 122it [01:22,  1.33it/s]\u001b[A\n",
      "batch 123, training loss: 3.7045: : 122it [01:23,  1.33it/s]\u001b[A\n",
      "batch 123, training loss: 3.7045: : 123it [01:23,  1.33it/s]\u001b[A\n",
      "batch 124, training loss: 3.7057: : 123it [01:24,  1.33it/s]\u001b[A\n",
      "batch 124, training loss: 3.7057: : 124it [01:24,  1.32it/s]\u001b[A\n",
      "batch 125, training loss: 3.7176: : 124it [01:24,  1.32it/s]\u001b[A\n",
      "batch 125, training loss: 3.7176: : 125it [01:24,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.7689: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.7689: : 126it [01:25,  1.41it/s]\u001b[A\n",
      "batch 127, training loss: 3.4875: : 126it [01:26,  1.41it/s]\u001b[A\n",
      "batch 127, training loss: 3.4875: : 127it [01:26,  1.45it/s]\u001b[A\n",
      "batch 128, training loss: 3.6882: : 127it [01:26,  1.45it/s]\u001b[A\n",
      "batch 128, training loss: 3.6882: : 128it [01:26,  1.42it/s]\u001b[A\n",
      "batch 129, training loss: 3.6558: : 128it [01:27,  1.42it/s]\u001b[A\n",
      "batch 129, training loss: 3.6558: : 129it [01:27,  1.38it/s]\u001b[A\n",
      "batch 130, training loss: 3.6726: : 129it [01:28,  1.38it/s]\u001b[A\n",
      "batch 130, training loss: 3.6726: : 130it [01:28,  1.34it/s]\u001b[A\n",
      "batch 131, training loss: 3.822: : 130it [01:29,  1.34it/s] \u001b[A\n",
      "batch 131, training loss: 3.822: : 131it [01:29,  1.33it/s]\u001b[A\n",
      "batch 132, training loss: 3.6174: : 131it [01:29,  1.33it/s]\u001b[A\n",
      "batch 132, training loss: 3.6174: : 132it [01:29,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 3.5217: : 132it [01:30,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 3.5217: : 133it [01:30,  1.31it/s]\u001b[A\n",
      "batch 134, training loss: 3.6797: : 133it [01:31,  1.31it/s]\u001b[A\n",
      "batch 134, training loss: 3.6797: : 134it [01:31,  1.30it/s]\u001b[A\n",
      "batch 135, training loss: 3.599: : 134it [01:32,  1.30it/s] \u001b[A\n",
      "batch 135, training loss: 3.599: : 135it [01:32,  1.30it/s]\u001b[A\n",
      "batch 136, training loss: 3.5447: : 135it [01:33,  1.30it/s]\u001b[A\n",
      "batch 136, training loss: 3.5447: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.7039: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.7039: : 137it [01:33,  1.29it/s]\u001b[A\n",
      "batch 138, training loss: 3.7133: : 137it [01:34,  1.29it/s]\u001b[A\n",
      "batch 138, training loss: 3.7133: : 138it [01:34,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.4931: : 138it [01:35,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.4931: : 139it [01:35,  1.29it/s]\u001b[A\n",
      "batch 140, training loss: 3.6091: : 139it [01:36,  1.29it/s]\u001b[A\n",
      "batch 140, training loss: 3.6091: : 140it [01:36,  1.29it/s]\u001b[A\n",
      "batch 141, training loss: 3.5925: : 140it [01:36,  1.29it/s]\u001b[A\n",
      "batch 141, training loss: 3.5925: : 141it [01:36,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.6399: : 141it [01:37,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.6399: : 142it [01:37,  1.31it/s]\u001b[A\n",
      "batch 143, training loss: 3.4729: : 142it [01:38,  1.31it/s]\u001b[A\n",
      "batch 143, training loss: 3.4729: : 143it [01:38,  1.34it/s]\u001b[A\n",
      "batch 144, training loss: 3.5184: : 143it [01:39,  1.34it/s]\u001b[A\n",
      "batch 144, training loss: 3.5184: : 144it [01:39,  1.35it/s]\u001b[A\n",
      "batch 145, training loss: 3.6276: : 144it [01:39,  1.35it/s]\u001b[A\n",
      "batch 145, training loss: 3.6276: : 145it [01:39,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.6108: : 145it [01:40,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.6108: : 146it [01:40,  1.32it/s]\u001b[A\n",
      "batch 147, training loss: 3.5109: : 146it [01:41,  1.32it/s]\u001b[A\n",
      "batch 147, training loss: 3.5109: : 147it [01:41,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.5988: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.5988: : 148it [01:42,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.7031: : 148it [01:42,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.7031: : 149it [01:42,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 3.6825: : 149it [01:43,  1.31it/s]\u001b[A\n",
      "batch 150, training loss: 3.6825: : 150it [01:43,  1.31it/s]\u001b[A\n",
      "batch 151, training loss: 3.608: : 150it [01:44,  1.31it/s] \u001b[A\n",
      "batch 151, training loss: 3.608: : 151it [01:44,  1.33it/s]\u001b[A\n",
      "batch 152, training loss: 3.5256: : 151it [01:45,  1.33it/s]\u001b[A\n",
      "batch 152, training loss: 3.5256: : 152it [01:45,  1.33it/s]\u001b[A\n",
      "batch 153, training loss: 3.5837: : 152it [01:45,  1.33it/s]\u001b[A\n",
      "batch 153, training loss: 3.5837: : 153it [01:45,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.6938: : 153it [01:46,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.6938: : 154it [01:46,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 3.814: : 154it [01:47,  1.31it/s] \u001b[A\n",
      "batch 155, training loss: 3.814: : 155it [01:47,  1.32it/s]\u001b[A\n",
      "batch 156, training loss: 3.416: : 155it [01:48,  1.32it/s]\u001b[A\n",
      "batch 156, training loss: 3.416: : 156it [01:48,  1.32it/s]\u001b[A\n",
      "batch 157, training loss: 3.6359: : 156it [01:48,  1.32it/s]\u001b[A\n",
      "batch 157, training loss: 3.6359: : 157it [01:48,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 3.6059: : 157it [01:49,  1.31it/s]\u001b[A\n",
      "batch 158, training loss: 3.6059: : 158it [01:49,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 3.5249: : 158it [01:50,  1.31it/s]\u001b[A\n",
      "batch 159, training loss: 3.5249: : 159it [01:50,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 3.7003: : 159it [01:51,  1.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 160, training loss: 3.7003: : 160it [01:51,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.5657: : 160it [01:52,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.5657: : 161it [01:52,  1.30it/s]\u001b[A\n",
      "batch 162, training loss: 3.5472: : 161it [01:52,  1.30it/s]\u001b[A\n",
      "batch 162, training loss: 3.5472: : 162it [01:52,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 3.7919: : 162it [01:53,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 3.7919: : 163it [01:53,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.5787: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.5787: : 164it [01:54,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 3.5878: : 164it [01:55,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 3.5878: : 165it [01:55,  1.29it/s]\u001b[A\n",
      "batch 166, training loss: 3.5542: : 165it [01:55,  1.29it/s]\u001b[A\n",
      "batch 166, training loss: 3.5542: : 166it [01:55,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.7136: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.7136: : 167it [01:56,  1.29it/s]\u001b[A\n",
      "batch 168, training loss: 3.669: : 167it [01:57,  1.29it/s] \u001b[A\n",
      "batch 168, training loss: 3.669: : 168it [01:57,  1.30it/s]\u001b[A\n",
      "batch 169, training loss: 3.7364: : 168it [01:58,  1.30it/s]\u001b[A\n",
      "batch 169, training loss: 3.7364: : 169it [01:58,  1.30it/s]\u001b[A\n",
      "batch 170, training loss: 3.592: : 169it [01:59,  1.30it/s] \u001b[A\n",
      "batch 170, training loss: 3.592: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.8618: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.8618: : 171it [01:59,  1.58it/s]\u001b[A\n",
      "batch 172, training loss: 3.9922: : 171it [02:00,  1.58it/s]\u001b[A\n",
      "batch 172, training loss: 3.9922: : 172it [02:00,  1.48it/s]\u001b[A\n",
      "batch 173, training loss: 3.8442: : 172it [02:00,  1.48it/s]\u001b[A\n",
      "batch 173, training loss: 3.8442: : 173it [02:00,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 4.0028: : 173it [02:01,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 4.0028: : 174it [02:01,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 3.9373: : 174it [02:02,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 3.9373: : 175it [02:02,  1.30it/s]\u001b[A\n",
      "batch 176, training loss: 3.8677: : 175it [02:03,  1.30it/s]\u001b[A\n",
      "batch 176, training loss: 3.8677: : 176it [02:03,  1.26it/s]\u001b[A\n",
      "batch 177, training loss: 3.8662: : 176it [02:04,  1.26it/s]\u001b[A\n",
      "batch 177, training loss: 3.8662: : 177it [02:04,  1.26it/s]\u001b[A\n",
      "batch 178, training loss: 3.8706: : 177it [02:05,  1.26it/s]\u001b[A\n",
      "batch 178, training loss: 3.8706: : 178it [02:05,  1.24it/s]\u001b[A\n",
      "batch 179, training loss: 3.9089: : 178it [02:05,  1.24it/s]\u001b[A\n",
      "batch 179, training loss: 3.9089: : 179it [02:05,  1.24it/s]\u001b[A\n",
      "batch 180, training loss: 3.9494: : 179it [02:06,  1.24it/s]\u001b[A\n",
      "batch 180, training loss: 3.9494: : 180it [02:06,  1.23it/s]\u001b[A\n",
      "batch 181, training loss: 3.9893: : 180it [02:07,  1.23it/s]\u001b[A\n",
      "batch 181, training loss: 3.9893: : 181it [02:07,  1.21it/s]\u001b[A\n",
      "batch 182, training loss: 3.889: : 181it [02:08,  1.21it/s] \u001b[A\n",
      "batch 182, training loss: 3.889: : 182it [02:08,  1.21it/s]\u001b[A\n",
      "batch 183, training loss: 3.9944: : 182it [02:09,  1.21it/s]\u001b[A\n",
      "batch 183, training loss: 3.9944: : 183it [02:09,  1.21it/s]\u001b[A\n",
      "batch 184, training loss: 3.7598: : 183it [02:09,  1.21it/s]\u001b[A\n",
      "batch 184, training loss: 3.7598: : 184it [02:09,  1.26it/s]\u001b[A\n",
      "batch 185, training loss: 3.7851: : 184it [02:10,  1.26it/s]\u001b[A\n",
      "batch 185, training loss: 3.7851: : 185it [02:10,  1.27it/s]\u001b[A\n",
      "batch 186, training loss: 3.944: : 185it [02:11,  1.27it/s] \u001b[A\n",
      "batch 186, training loss: 3.944: : 186it [02:11,  1.29it/s]\u001b[A\n",
      "batch 187, training loss: 3.9156: : 186it [02:12,  1.29it/s]\u001b[A\n",
      "batch 187, training loss: 3.9156: : 187it [02:12,  1.26it/s]\u001b[A\n",
      "batch 188, training loss: 4.0211: : 187it [02:12,  1.26it/s]\u001b[A\n",
      "batch 188, training loss: 4.0211: : 188it [02:12,  1.28it/s]\u001b[A\n",
      "batch 189, training loss: 4.0456: : 188it [02:13,  1.28it/s]\u001b[A\n",
      "batch 189, training loss: 4.0456: : 189it [02:13,  1.27it/s]\u001b[A\n",
      "batch 190, training loss: 3.7134: : 189it [02:14,  1.27it/s]\u001b[A\n",
      "batch 190, training loss: 3.7134: : 190it [02:14,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.7125: : 190it [02:15,  1.24it/s]\u001b[A\n",
      "batch 191, training loss: 3.7125: : 191it [02:15,  1.25it/s]\u001b[A\n",
      "batch 192, training loss: 3.8109: : 191it [02:16,  1.25it/s]\u001b[A\n",
      "batch 192, training loss: 3.8109: : 192it [02:16,  1.25it/s]\u001b[A\n",
      "batch 193, training loss: 3.6942: : 192it [02:17,  1.25it/s]\u001b[A\n",
      "batch 193, training loss: 3.6942: : 193it [02:17,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 3.7834: : 193it [02:17,  1.24it/s]\u001b[A\n",
      "batch 194, training loss: 3.7834: : 194it [02:17,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 3.7171: : 194it [02:18,  1.24it/s]\u001b[A\n",
      "batch 195, training loss: 3.7171: : 195it [02:18,  1.24it/s]\u001b[A\n",
      "batch 196, training loss: 3.655: : 195it [02:19,  1.24it/s] \u001b[A\n",
      "batch 196, training loss: 3.655: : 196it [02:19,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 3.8282: : 196it [02:20,  1.24it/s]\u001b[A\n",
      "batch 197, training loss: 3.8282: : 197it [02:20,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 3.7493: : 197it [02:21,  1.24it/s]\u001b[A\n",
      "batch 198, training loss: 3.7493: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.6835: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.6835: : 199it [02:21,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.8569: : 199it [02:22,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.8569: : 200it [02:22,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.6468: : 200it [02:23,  1.24it/s]\u001b[A\n",
      "batch 201, training loss: 3.6468: : 201it [02:23,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.5355: : 201it [02:24,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.5355: : 202it [02:24,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.7442: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.7442: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.7725: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.7725: : 204it [02:25,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.7153: : 204it [02:26,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.7153: : 205it [02:26,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 3.8571: : 205it [02:27,  1.23it/s]\u001b[A\n",
      "batch 206, training loss: 3.8571: : 206it [02:27,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 3.6609: : 206it [02:28,  1.24it/s]\u001b[A\n",
      "batch 207, training loss: 3.6609: : 207it [02:28,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.7047: : 207it [02:29,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.7047: : 208it [02:29,  1.27it/s]\u001b[A\n",
      "batch 209, training loss: 3.6635: : 208it [02:29,  1.27it/s]\u001b[A\n",
      "batch 209, training loss: 3.6635: : 209it [02:29,  1.24it/s]\u001b[A\n",
      "batch 210, training loss: 3.706: : 209it [02:30,  1.24it/s] \u001b[A\n",
      "batch 210, training loss: 3.706: : 210it [02:30,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 3.6797: : 210it [02:31,  1.23it/s]\u001b[A\n",
      "batch 211, training loss: 3.6797: : 211it [02:31,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 3.6704: : 211it [02:32,  1.25it/s]\u001b[A\n",
      "batch 212, training loss: 3.6704: : 212it [02:32,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.8764: : 212it [02:33,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.8764: : 213it [02:33,  1.26it/s]\u001b[A\n",
      "batch 214, training loss: 3.7382: : 213it [02:33,  1.26it/s]\u001b[A\n",
      "batch 214, training loss: 3.7382: : 214it [02:33,  1.25it/s]\u001b[A\n",
      "batch 215, training loss: 3.5678: : 214it [02:34,  1.25it/s]\u001b[A\n",
      "batch 215, training loss: 3.5678: : 215it [02:34,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 3.7227: : 215it [02:35,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 3.7227: : 216it [02:35,  1.25it/s]\u001b[A\n",
      "batch 217, training loss: 3.8252: : 216it [02:36,  1.25it/s]\u001b[A\n",
      "batch 217, training loss: 3.8252: : 217it [02:36,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.7464: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.7464: : 218it [02:37,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.9539: : 218it [02:38,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.9539: : 219it [02:38,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.8591: : 219it [02:38,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.8591: : 220it [02:38,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.7293: : 220it [02:39,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.7293: : 221it [02:39,  1.27it/s]\u001b[A\n",
      "batch 222, training loss: 3.7648: : 221it [02:40,  1.27it/s]\u001b[A\n",
      "batch 222, training loss: 3.7648: : 222it [02:40,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.7704: : 222it [02:41,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.7704: : 223it [02:41,  1.26it/s]\u001b[A\n",
      "batch 224, training loss: 3.6673: : 223it [02:41,  1.26it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 224, training loss: 3.6673: : 224it [02:41,  1.26it/s]\u001b[A\n",
      "batch 225, training loss: 3.8122: : 224it [02:42,  1.26it/s]\u001b[A\n",
      "batch 225, training loss: 3.8122: : 225it [02:42,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 3.8234: : 225it [02:43,  1.25it/s]\u001b[A\n",
      "batch 226, training loss: 3.8234: : 226it [02:43,  1.25it/s]\u001b[A\n",
      "batch 227, training loss: 3.6844: : 226it [02:44,  1.25it/s]\u001b[A\n",
      "batch 227, training loss: 3.6844: : 227it [02:44,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.7452: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.7452: : 228it [02:45,  1.26it/s]\u001b[A\n",
      "batch 229, training loss: 3.7155: : 228it [02:46,  1.26it/s]\u001b[A\n",
      "batch 229, training loss: 3.7155: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 3.868: : 229it [02:46,  1.24it/s] \u001b[A\n",
      "batch 230, training loss: 3.868: : 230it [02:46,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.9021: : 230it [02:47,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.9021: : 231it [02:47,  1.26it/s]\u001b[A\n",
      "batch 232, training loss: 3.7044: : 231it [02:48,  1.26it/s]\u001b[A\n",
      "batch 232, training loss: 3.7044: : 232it [02:48,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.7745: : 232it [02:49,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.7745: : 233it [02:49,  1.26it/s]\u001b[A\n",
      "batch 234, training loss: 3.8931: : 233it [02:49,  1.26it/s]\u001b[A\n",
      "batch 234, training loss: 3.8931: : 234it [02:49,  1.26it/s]\u001b[A\n",
      "batch 235, training loss: 3.7249: : 234it [02:50,  1.26it/s]\u001b[A\n",
      "batch 235, training loss: 3.7249: : 235it [02:50,  1.26it/s]\u001b[A\n",
      "batch 236, training loss: 3.779: : 235it [02:51,  1.26it/s] \u001b[A\n",
      "batch 236, training loss: 3.779: : 236it [02:51,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.6259: : 236it [02:52,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.6259: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.7916: : 237it [02:53,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.7916: : 238it [02:53,  1.27it/s]\u001b[A\n",
      "batch 239, training loss: 3.7073: : 238it [02:54,  1.27it/s]\u001b[A\n",
      "batch 239, training loss: 3.7073: : 239it [02:54,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 3.7323: : 239it [02:54,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 3.7323: : 240it [02:54,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.7528: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.7528: : 241it [02:55,  1.26it/s]\u001b[A\n",
      "batch 242, training loss: 3.6037: : 241it [02:56,  1.26it/s]\u001b[A\n",
      "batch 242, training loss: 3.6037: : 242it [02:56,  1.32it/s]\u001b[A\n",
      "batch 243, training loss: 3.7655: : 242it [02:56,  1.32it/s]\u001b[A\n",
      "batch 243, training loss: 3.7655: : 243it [02:56,  1.35it/s]\u001b[A\n",
      "batch 244, training loss: 3.6695: : 243it [02:57,  1.35it/s]\u001b[A\n",
      "batch 244, training loss: 3.6695: : 244it [02:57,  1.34it/s]\u001b[A\n",
      "batch 245, training loss: 3.7983: : 244it [02:58,  1.34it/s]\u001b[A\n",
      "batch 245, training loss: 3.7983: : 245it [02:58,  1.33it/s]\u001b[A\n",
      "batch 246, training loss: 3.738: : 245it [02:59,  1.33it/s] \u001b[A\n",
      "batch 246, training loss: 3.738: : 246it [02:59,  1.31it/s]\u001b[A\n",
      "batch 247, training loss: 3.7195: : 246it [03:00,  1.31it/s]\u001b[A\n",
      "batch 247, training loss: 3.7195: : 247it [03:00,  1.28it/s]\u001b[A\n",
      "batch 248, training loss: 3.6422: : 247it [03:00,  1.28it/s]\u001b[A\n",
      "batch 248, training loss: 3.6422: : 248it [03:00,  1.30it/s]\u001b[A\n",
      "batch 249, training loss: 3.6233: : 248it [03:01,  1.30it/s]\u001b[A\n",
      "batch 249, training loss: 3.6233: : 249it [03:01,  1.29it/s]\u001b[A\n",
      "batch 250, training loss: 3.8106: : 249it [03:02,  1.29it/s]\u001b[A\n",
      "batch 250, training loss: 3.8106: : 250it [03:02,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 3.8047: : 250it [03:03,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 3.8047: : 251it [03:03,  1.27it/s]\u001b[A\n",
      "batch 252, training loss: 3.2456: : 251it [03:03,  1.27it/s]\u001b[A\n",
      "batch 252, training loss: 3.2456: : 252it [03:03,  1.49it/s]\u001b[A\n",
      "batch 253, training loss: 3.8295: : 252it [03:04,  1.49it/s]\u001b[A\n",
      "batch 253, training loss: 3.8295: : 253it [03:04,  1.41it/s]\u001b[A\n",
      "batch 254, training loss: 3.9821: : 253it [03:05,  1.41it/s]\u001b[A\n",
      "batch 254, training loss: 3.9821: : 254it [03:05,  1.34it/s]\u001b[A\n",
      "batch 255, training loss: 3.7912: : 254it [03:06,  1.34it/s]\u001b[A\n",
      "batch 255, training loss: 3.7912: : 255it [03:06,  1.27it/s]\u001b[A\n",
      "batch 256, training loss: 3.811: : 255it [03:07,  1.27it/s] \u001b[A\n",
      "batch 256, training loss: 3.811: : 256it [03:07,  1.21it/s]\u001b[A\n",
      "batch 257, training loss: 3.9081: : 256it [03:08,  1.21it/s]\u001b[A\n",
      "batch 257, training loss: 3.9081: : 257it [03:08,  1.16it/s]\u001b[A\n",
      "batch 258, training loss: 3.9944: : 257it [03:08,  1.16it/s]\u001b[A\n",
      "batch 258, training loss: 3.9944: : 258it [03:08,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.8455: : 258it [03:09,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.8455: : 259it [03:09,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 3.8401: : 259it [03:10,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 3.8401: : 260it [03:10,  1.13it/s]\u001b[A\n",
      "batch 261, training loss: 3.9317: : 260it [03:11,  1.13it/s]\u001b[A\n",
      "batch 261, training loss: 3.9317: : 261it [03:11,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.8328: : 261it [03:12,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.8328: : 262it [03:12,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 3.817: : 262it [03:13,  1.10it/s] \u001b[A\n",
      "batch 263, training loss: 3.817: : 263it [03:13,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 3.9096: : 263it [03:14,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 3.9096: : 264it [03:14,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.7125: : 264it [03:15,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.7125: : 265it [03:15,  1.09it/s]\u001b[A\n",
      "batch 266, training loss: 3.8848: : 265it [03:16,  1.09it/s]\u001b[A\n",
      "batch 266, training loss: 3.8848: : 266it [03:16,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.8428: : 266it [03:17,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.8428: : 267it [03:17,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.7235: : 267it [03:18,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.7235: : 268it [03:18,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.7611: : 268it [03:18,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.7611: : 269it [03:18,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.7793: : 269it [03:19,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.7793: : 270it [03:19,  1.11it/s]\u001b[A\n",
      "batch 271, training loss: 3.766: : 270it [03:20,  1.11it/s] \u001b[A\n",
      "batch 271, training loss: 3.766: : 271it [03:20,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.7448: : 271it [03:21,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.7448: : 272it [03:21,  1.12it/s]\u001b[A\n",
      "batch 273, training loss: 3.844: : 272it [03:22,  1.12it/s] \u001b[A\n",
      "batch 273, training loss: 3.844: : 273it [03:22,  1.12it/s]\u001b[A\n",
      "batch 274, training loss: 3.8217: : 273it [03:23,  1.12it/s]\u001b[A\n",
      "batch 274, training loss: 3.8217: : 274it [03:23,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.6962: : 274it [03:24,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.6962: : 275it [03:24,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.5987: : 275it [03:25,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.5987: : 276it [03:25,  1.09it/s]\u001b[A\n",
      "batch 277, training loss: 3.7366: : 276it [03:26,  1.09it/s]\u001b[A\n",
      "batch 277, training loss: 3.7366: : 277it [03:26,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.6758: : 277it [03:27,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.6758: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.6782: : 278it [03:28,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.6782: : 279it [03:28,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.5503: : 279it [03:28,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.5503: : 280it [03:28,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.6777: : 280it [03:29,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.6777: : 281it [03:29,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.5686: : 281it [03:30,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.5686: : 282it [03:30,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.667: : 282it [03:31,  1.10it/s] \u001b[A\n",
      "batch 283, training loss: 3.667: : 283it [03:31,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.6478: : 283it [03:32,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.6478: : 284it [03:32,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.6981: : 284it [03:33,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.6981: : 285it [03:33,  1.08it/s]\u001b[A\n",
      "batch 286, training loss: 3.8476: : 285it [03:34,  1.08it/s]\u001b[A\n",
      "batch 286, training loss: 3.8476: : 286it [03:34,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.5602: : 286it [03:35,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.5602: : 287it [03:35,  1.11it/s]\u001b[A\n",
      "batch 288, training loss: 3.5942: : 287it [03:36,  1.11it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 288, training loss: 3.5942: : 288it [03:36,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.7667: : 288it [03:37,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.7667: : 289it [03:37,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.5257: : 289it [03:38,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.5257: : 290it [03:38,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 3.7917: : 290it [03:38,  1.11it/s]\u001b[A\n",
      "batch 291, training loss: 3.7917: : 291it [03:38,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.5907: : 291it [03:39,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.5907: : 292it [03:39,  1.16it/s]\u001b[A\n",
      "batch 293, training loss: 3.6905: : 292it [03:40,  1.16it/s]\u001b[A\n",
      "batch 293, training loss: 3.6905: : 293it [03:40,  1.16it/s]\u001b[A\n",
      "batch 294, training loss: 3.7854: : 293it [03:41,  1.16it/s]\u001b[A\n",
      "batch 294, training loss: 3.7854: : 294it [03:41,  1.14it/s]\u001b[A\n",
      "batch 295, training loss: 3.7184: : 294it [03:42,  1.14it/s]\u001b[A\n",
      "batch 295, training loss: 3.7184: : 295it [03:42,  1.13it/s]\u001b[A\n",
      "batch 296, training loss: 3.5809: : 295it [03:43,  1.13it/s]\u001b[A\n",
      "batch 296, training loss: 3.5809: : 296it [03:43,  1.11it/s]\u001b[A\n",
      "batch 297, training loss: 3.7472: : 296it [03:44,  1.11it/s]\u001b[A\n",
      "batch 297, training loss: 3.7472: : 297it [03:44,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.6218: : 297it [03:45,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.6218: : 298it [03:45,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.7211: : 298it [03:46,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.7211: : 299it [03:46,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 3.7825: : 299it [03:47,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 3.7825: : 300it [03:47,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.7475: : 300it [03:47,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.7475: : 301it [03:47,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.7236: : 301it [03:48,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.7236: : 302it [03:48,  1.10it/s]\u001b[A\n",
      "batch 303, training loss: 3.6599: : 302it [03:49,  1.10it/s]\u001b[A\n",
      "batch 303, training loss: 3.6599: : 303it [03:49,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 3.7395: : 303it [03:50,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 3.7395: : 304it [03:50,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 3.6536: : 304it [03:51,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 3.6536: : 305it [03:51,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.7193: : 305it [03:52,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.7193: : 306it [03:52,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 3.844: : 306it [03:53,  1.09it/s] \u001b[A\n",
      "batch 307, training loss: 3.844: : 307it [03:53,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.4606: : 307it [03:54,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.4606: : 308it [03:54,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 3.8446: : 308it [03:55,  1.10it/s]\u001b[A\n",
      "batch 309, training loss: 3.8446: : 309it [03:55,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.6063: : 309it [03:56,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.6063: : 310it [03:56,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 3.7179: : 310it [03:57,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 3.7179: : 311it [03:57,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.5315: : 311it [03:57,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.5315: : 312it [03:57,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.6675: : 312it [03:58,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.6675: : 313it [03:58,  1.15it/s]\u001b[A\n",
      "batch 314, training loss: 3.6773: : 313it [03:59,  1.15it/s]\u001b[A\n",
      "batch 314, training loss: 3.6773: : 314it [03:59,  1.13it/s]\u001b[A\n",
      "batch 315, training loss: 3.7447: : 314it [04:00,  1.13it/s]\u001b[A\n",
      "batch 315, training loss: 3.7447: : 315it [04:00,  1.11it/s]\u001b[A\n",
      "batch 316, training loss: 3.8817: : 315it [04:01,  1.11it/s]\u001b[A\n",
      "batch 316, training loss: 3.8817: : 316it [04:01,  1.10it/s]\u001b[A\n",
      "batch 317, training loss: 3.5609: : 316it [04:02,  1.10it/s]\u001b[A\n",
      "batch 317, training loss: 3.5609: : 317it [04:02,  1.14it/s]\u001b[A\n",
      "batch 318, training loss: 3.8751: : 317it [04:03,  1.14it/s]\u001b[A\n",
      "batch 318, training loss: 3.8751: : 318it [04:03,  1.08it/s]\u001b[A\n",
      "batch 319, training loss: 3.9447: : 318it [04:04,  1.08it/s]\u001b[A\n",
      "batch 319, training loss: 3.9447: : 319it [04:04,  1.06it/s]\u001b[A\n",
      "batch 320, training loss: 3.9649: : 319it [04:05,  1.06it/s]\u001b[A\n",
      "batch 320, training loss: 3.9649: : 320it [04:05,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 3.9891: : 320it [04:06,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 3.9891: : 321it [04:06,  1.01it/s]\u001b[A\n",
      "batch 322, training loss: 4.0493: : 321it [04:07,  1.01it/s]\u001b[A\n",
      "batch 322, training loss: 4.0493: : 322it [04:07,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 3.8148: : 322it [04:08,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 3.8148: : 323it [04:08,  1.01it/s]\u001b[A\n",
      "batch 324, training loss: 3.8861: : 323it [04:09,  1.01it/s]\u001b[A\n",
      "batch 324, training loss: 3.8861: : 324it [04:09,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 3.9401: : 324it [04:10,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 3.9401: : 325it [04:10,  1.00it/s]\u001b[A\n",
      "batch 326, training loss: 3.8942: : 325it [04:11,  1.00it/s]\u001b[A\n",
      "batch 326, training loss: 3.8942: : 326it [04:11,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 3.9843: : 326it [04:12,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 3.9843: : 327it [04:12,  1.03it/s]\u001b[A\n",
      "batch 328, training loss: 3.8697: : 327it [04:13,  1.03it/s]\u001b[A\n",
      "batch 328, training loss: 3.8697: : 328it [04:13,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 3.7701: : 328it [04:14,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 3.7701: : 329it [04:14,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.8506: : 329it [04:15,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.8506: : 330it [04:15,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 3.8779: : 330it [04:16,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 3.8779: : 331it [04:16,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.7832: : 331it [04:17,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.7832: : 332it [04:17,  1.01s/it]\u001b[A\n",
      "batch 333, training loss: 3.7401: : 332it [04:18,  1.01s/it]\u001b[A\n",
      "batch 333, training loss: 3.7401: : 333it [04:18,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 3.8309: : 333it [04:19,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 3.8309: : 334it [04:19,  1.05it/s]\u001b[A\n",
      "batch 335, training loss: 3.9651: : 334it [04:20,  1.05it/s]\u001b[A\n",
      "batch 335, training loss: 3.9651: : 335it [04:20,  1.02it/s]\u001b[A\n",
      "batch 336, training loss: 3.8582: : 335it [04:21,  1.02it/s]\u001b[A\n",
      "batch 336, training loss: 3.8582: : 336it [04:21,  1.01it/s]\u001b[A\n",
      "batch 337, training loss: 3.8949: : 336it [04:22,  1.01it/s]\u001b[A\n",
      "batch 337, training loss: 3.8949: : 337it [04:22,  1.01it/s]\u001b[A\n",
      "batch 338, training loss: 3.7589: : 337it [04:23,  1.01it/s]\u001b[A\n",
      "batch 338, training loss: 3.7589: : 338it [04:23,  1.00it/s]\u001b[A\n",
      "batch 339, training loss: 3.84: : 338it [04:24,  1.00it/s]  \u001b[A\n",
      "batch 339, training loss: 3.84: : 339it [04:24,  1.04it/s]\u001b[A\n",
      "batch 340, training loss: 4.0654: : 339it [04:25,  1.04it/s]\u001b[A\n",
      "batch 340, training loss: 4.0654: : 340it [04:25,  1.02it/s]\u001b[A\n",
      "batch 341, training loss: 3.7187: : 340it [04:26,  1.02it/s]\u001b[A\n",
      "batch 341, training loss: 3.7187: : 341it [04:26,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 3.8234: : 341it [04:27,  1.00it/s]\u001b[A\n",
      "batch 342, training loss: 3.8234: : 342it [04:27,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 3.8074: : 342it [04:27,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 3.8074: : 343it [04:27,  1.07it/s]\u001b[A\n",
      "batch 344, training loss: 3.9142: : 343it [04:28,  1.07it/s]\u001b[A\n",
      "batch 344, training loss: 3.9142: : 344it [04:28,  1.08it/s]\u001b[A\n",
      "batch 345, training loss: 3.8656: : 344it [04:29,  1.08it/s]\u001b[A\n",
      "batch 345, training loss: 3.8656: : 345it [04:29,  1.04it/s]\u001b[A\n",
      "batch 346, training loss: 3.8185: : 345it [04:30,  1.04it/s]\u001b[A\n",
      "batch 346, training loss: 3.8185: : 346it [04:30,  1.05it/s]\u001b[A\n",
      "batch 347, training loss: 3.7216: : 346it [04:31,  1.05it/s]\u001b[A\n",
      "batch 347, training loss: 3.7216: : 347it [04:31,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 3.8186: : 347it [04:32,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 3.8186: : 348it [04:32,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 3.9464: : 348it [04:33,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 3.9464: : 349it [04:33,  1.00s/it]\u001b[A\n",
      "batch 350, training loss: 3.8013: : 349it [04:34,  1.00s/it]\u001b[A\n",
      "batch 350, training loss: 3.8013: : 350it [04:34,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.6929: : 350it [04:35,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.6929: : 351it [04:35,  1.04it/s]\u001b[A\n",
      "batch 352, training loss: 3.7907: : 351it [04:36,  1.04it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 352, training loss: 3.7907: : 352it [04:36,  1.02it/s]\u001b[A\n",
      "batch 353, training loss: 3.8129: : 352it [04:37,  1.02it/s]\u001b[A\n",
      "batch 353, training loss: 3.8129: : 353it [04:37,  1.03it/s]\u001b[A\n",
      "batch 354, training loss: 3.8677: : 353it [04:38,  1.03it/s]\u001b[A\n",
      "batch 354, training loss: 3.8677: : 354it [04:38,  1.02it/s]\u001b[A\n",
      "batch 355, training loss: 3.818: : 354it [04:39,  1.02it/s] \u001b[A\n",
      "batch 355, training loss: 3.818: : 355it [04:39,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.6678: : 355it [04:40,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.6678: : 356it [04:40,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.6675: : 356it [04:41,  1.01it/s]\u001b[A\n",
      "batch 357, training loss: 3.6675: : 357it [04:41,  1.01it/s]\u001b[A\n",
      "batch 358, training loss: 3.872: : 357it [04:42,  1.01it/s] \u001b[A\n",
      "batch 358, training loss: 3.872: : 358it [04:42,  1.04it/s]\u001b[A\n",
      "batch 359, training loss: 3.6795: : 358it [04:43,  1.04it/s]\u001b[A\n",
      "batch 359, training loss: 3.6795: : 359it [04:43,  1.02it/s]\u001b[A\n",
      "batch 360, training loss: 3.8154: : 359it [04:44,  1.02it/s]\u001b[A\n",
      "batch 360, training loss: 3.8154: : 360it [04:44,  1.00it/s]\u001b[A\n",
      "batch 361, training loss: 3.78: : 360it [04:45,  1.00it/s]  \u001b[A\n",
      "batch 361, training loss: 3.78: : 361it [04:45,  1.01it/s]\u001b[A\n",
      "batch 362, training loss: 3.8771: : 361it [04:46,  1.01it/s]\u001b[A\n",
      "batch 362, training loss: 3.8771: : 362it [04:46,  1.00s/it]\u001b[A\n",
      "batch 363, training loss: 3.783: : 362it [04:47,  1.00s/it] \u001b[A\n",
      "batch 363, training loss: 3.783: : 363it [04:47,  1.03it/s]\u001b[A\n",
      "batch 364, training loss: 3.6819: : 363it [04:48,  1.03it/s]\u001b[A\n",
      "batch 364, training loss: 3.6819: : 364it [04:48,  1.02it/s]\u001b[A\n",
      "batch 365, training loss: 3.6698: : 364it [04:49,  1.02it/s]\u001b[A\n",
      "batch 365, training loss: 3.6698: : 365it [04:49,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 3.8125: : 365it [04:50,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 3.8125: : 366it [04:50,  1.00it/s]\u001b[A\n",
      "batch 367, training loss: 3.7164: : 366it [04:51,  1.00it/s]\u001b[A\n",
      "batch 367, training loss: 3.7164: : 367it [04:51,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 3.8262: : 367it [04:52,  1.00s/it]\u001b[A\n",
      "batch 368, training loss: 3.8262: : 368it [04:52,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 3.8426: : 368it [04:53,  1.03it/s]\u001b[A\n",
      "batch 369, training loss: 3.8426: : 369it [04:53,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 3.7357: : 369it [04:54,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 3.7357: : 370it [04:54,  1.00s/it]\u001b[A\n",
      "batch 371, training loss: 3.7774: : 370it [04:55,  1.00s/it]\u001b[A\n",
      "batch 371, training loss: 3.7774: : 371it [04:55,  1.00it/s]\u001b[A\n",
      "batch 372, training loss: 3.7227: : 371it [04:56,  1.00it/s]\u001b[A\n",
      "batch 372, training loss: 3.7227: : 372it [04:56,  1.00s/it]\u001b[A\n",
      "batch 373, training loss: 3.7535: : 372it [04:57,  1.00s/it]\u001b[A\n",
      "batch 373, training loss: 3.7535: : 373it [04:57,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 3.7305: : 373it [04:58,  1.00s/it]\u001b[A\n",
      "batch 374, training loss: 3.7305: : 374it [04:58,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 3.5756: : 374it [04:59,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 3.5756: : 375it [04:59,  1.14it/s]\u001b[A\n",
      "batch 376, training loss: 3.8532: : 375it [05:00,  1.14it/s]\u001b[A\n",
      "batch 376, training loss: 3.8532: : 376it [05:00,  1.04it/s]\u001b[A\n",
      "batch 377, training loss: 3.9548: : 376it [05:01,  1.04it/s]\u001b[A\n",
      "batch 377, training loss: 3.9548: : 377it [05:01,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.8564: : 377it [05:02,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.8564: : 378it [05:02,  1.06s/it]\u001b[A\n",
      "batch 379, training loss: 3.8212: : 378it [05:03,  1.06s/it]\u001b[A\n",
      "batch 379, training loss: 3.8212: : 379it [05:03,  1.07s/it]\u001b[A\n",
      "batch 380, training loss: 3.8336: : 379it [05:04,  1.07s/it]\u001b[A\n",
      "batch 380, training loss: 3.8336: : 380it [05:04,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.9066: : 380it [05:05,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.9066: : 381it [05:05,  1.09s/it]\u001b[A\n",
      "batch 382, training loss: 3.7695: : 381it [05:07,  1.09s/it]\u001b[A\n",
      "batch 382, training loss: 3.7695: : 382it [05:07,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 3.8111: : 382it [05:08,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 3.8111: : 383it [05:08,  1.09s/it]\u001b[A\n",
      "batch 384, training loss: 3.9176: : 383it [05:09,  1.09s/it]\u001b[A\n",
      "batch 384, training loss: 3.9176: : 384it [05:09,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 3.8331: : 384it [05:10,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 3.8331: : 385it [05:10,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.7839: : 385it [05:11,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.7839: : 386it [05:11,  1.10s/it]\u001b[A\n",
      "batch 387, training loss: 3.8469: : 386it [05:12,  1.10s/it]\u001b[A\n",
      "batch 387, training loss: 3.8469: : 387it [05:12,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 3.6291: : 387it [05:13,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 3.6291: : 388it [05:13,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.7998: : 388it [05:14,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.7998: : 389it [05:14,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.9014: : 389it [05:15,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.9014: : 390it [05:15,  1.09s/it]\u001b[A\n",
      "batch 391, training loss: 3.9095: : 390it [05:16,  1.09s/it]\u001b[A\n",
      "batch 391, training loss: 3.9095: : 391it [05:16,  1.10s/it]\u001b[A\n",
      "batch 392, training loss: 3.8323: : 391it [05:18,  1.10s/it]\u001b[A\n",
      "batch 392, training loss: 3.8323: : 392it [05:18,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 3.6725: : 392it [05:19,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 3.6725: : 393it [05:19,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 3.6154: : 393it [05:20,  1.11s/it]\u001b[A\n",
      "batch 394, training loss: 3.6154: : 394it [05:20,  1.10s/it]\u001b[A\n",
      "batch 395, training loss: 3.7101: : 394it [05:21,  1.10s/it]\u001b[A\n",
      "batch 395, training loss: 3.7101: : 395it [05:21,  1.11s/it]\u001b[A\n",
      "batch 396, training loss: 3.8782: : 395it [05:22,  1.11s/it]\u001b[A\n",
      "batch 396, training loss: 3.8782: : 396it [05:22,  1.08s/it]\u001b[A\n",
      "batch 397, training loss: 3.6763: : 396it [05:23,  1.08s/it]\u001b[A\n",
      "batch 397, training loss: 3.6763: : 397it [05:23,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 3.792: : 397it [05:24,  1.11s/it] \u001b[A\n",
      "batch 398, training loss: 3.792: : 398it [05:24,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 3.8854: : 398it [05:25,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 3.8854: : 399it [05:25,  1.08s/it]\u001b[A\n",
      "batch 400, training loss: 3.6603: : 399it [05:26,  1.08s/it]\u001b[A\n",
      "batch 400, training loss: 3.6603: : 400it [05:26,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 3.6316: : 400it [05:27,  1.09s/it]\u001b[A\n",
      "batch 401, training loss: 3.6316: : 401it [05:27,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 3.6882: : 401it [05:29,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 3.6882: : 402it [05:29,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.8705: : 402it [05:30,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.8705: : 403it [05:30,  1.05s/it]\u001b[A\n",
      "batch 404, training loss: 3.484: : 403it [05:31,  1.05s/it] \u001b[A\n",
      "batch 404, training loss: 3.484: : 404it [05:31,  1.04s/it]\u001b[A\n",
      "batch 405, training loss: 3.756: : 404it [05:32,  1.04s/it]\u001b[A\n",
      "batch 405, training loss: 3.756: : 405it [05:32,  1.06s/it]\u001b[A\n",
      "batch 406, training loss: 3.6657: : 405it [05:33,  1.06s/it]\u001b[A\n",
      "batch 406, training loss: 3.6657: : 406it [05:33,  1.08s/it]\u001b[A\n",
      "batch 407, training loss: 3.7511: : 406it [05:34,  1.08s/it]\u001b[A\n",
      "batch 407, training loss: 3.7511: : 407it [05:34,  1.09s/it]\u001b[A\n",
      "batch 408, training loss: 3.5954: : 407it [05:35,  1.09s/it]\u001b[A\n",
      "batch 408, training loss: 3.5954: : 408it [05:35,  1.08s/it]\u001b[A\n",
      "batch 409, training loss: 3.7856: : 408it [05:36,  1.08s/it]\u001b[A\n",
      "batch 409, training loss: 3.7856: : 409it [05:36,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.6124: : 409it [05:37,  1.09s/it]\u001b[A\n",
      "batch 410, training loss: 3.6124: : 410it [05:37,  1.08s/it]\u001b[A\n",
      "batch 411, training loss: 3.7335: : 410it [05:38,  1.08s/it]\u001b[A\n",
      "batch 411, training loss: 3.7335: : 411it [05:38,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.7432: : 411it [05:39,  1.09s/it]\u001b[A\n",
      "batch 412, training loss: 3.7432: : 412it [05:39,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 3.6922: : 412it [05:40,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 3.6922: : 413it [05:40,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.5879: : 413it [05:42,  1.11s/it]\u001b[A\n",
      "batch 414, training loss: 3.5879: : 414it [05:42,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.4921: : 414it [05:43,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.4921: : 415it [05:43,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.5314: : 415it [05:44,  1.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 416, training loss: 3.5314: : 416it [05:44,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 3.8063: : 416it [05:45,  1.09s/it]\u001b[A\n",
      "batch 417, training loss: 3.8063: : 417it [05:45,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.7039: : 417it [05:46,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.7039: : 418it [05:46,  1.11s/it]\u001b[A\n",
      "batch 419, training loss: 3.5188: : 418it [05:47,  1.11s/it]\u001b[A\n",
      "batch 419, training loss: 3.5188: : 419it [05:47,  1.08s/it]\u001b[A\n",
      "batch 420, training loss: 3.6146: : 419it [05:48,  1.08s/it]\u001b[A\n",
      "batch 420, training loss: 3.6146: : 420it [05:48,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.7255: : 420it [05:49,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.7255: : 421it [05:49,  1.11s/it]\u001b[A\n",
      "batch 422, training loss: 3.7506: : 421it [05:50,  1.11s/it]\u001b[A\n",
      "batch 422, training loss: 3.7506: : 422it [05:50,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.5424: : 422it [05:51,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.5424: : 423it [05:51,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.8061: : 423it [05:53,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.8061: : 424it [05:53,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 3.8772: : 424it [05:54,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 3.8772: : 425it [05:54,  1.15s/it]\u001b[A\n",
      "batch 426, training loss: 3.838: : 425it [05:55,  1.15s/it] \u001b[A\n",
      "batch 426, training loss: 3.838: : 426it [05:55,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.8407: : 426it [05:56,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.8407: : 427it [05:56,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 4.0089: : 427it [05:57,  1.18s/it]\u001b[A\n",
      "batch 428, training loss: 4.0089: : 428it [05:57,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.8573: : 428it [05:59,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.8573: : 429it [05:59,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.9093: : 429it [06:00,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.9093: : 430it [06:00,  1.14s/it]\u001b[A\n",
      "batch 431, training loss: 3.9127: : 430it [06:01,  1.14s/it]\u001b[A\n",
      "batch 431, training loss: 3.9127: : 431it [06:01,  1.10s/it]\u001b[A\n",
      "batch 432, training loss: 3.7437: : 431it [06:02,  1.10s/it]\u001b[A\n",
      "batch 432, training loss: 3.7437: : 432it [06:02,  1.13s/it]\u001b[A\n",
      "batch 433, training loss: 3.796: : 432it [06:03,  1.13s/it] \u001b[A\n",
      "batch 433, training loss: 3.796: : 433it [06:03,  1.16s/it]\u001b[A\n",
      "batch 434, training loss: 3.7985: : 433it [06:04,  1.16s/it]\u001b[A\n",
      "batch 434, training loss: 3.7985: : 434it [06:04,  1.18s/it]\u001b[A\n",
      "batch 435, training loss: 3.7943: : 434it [06:06,  1.18s/it]\u001b[A\n",
      "batch 435, training loss: 3.7943: : 435it [06:06,  1.20s/it]\u001b[A\n",
      "batch 436, training loss: 3.8107: : 435it [06:07,  1.20s/it]\u001b[A\n",
      "batch 436, training loss: 3.8107: : 436it [06:07,  1.20s/it]\u001b[A\n",
      "batch 437, training loss: 3.7468: : 436it [06:08,  1.20s/it]\u001b[A\n",
      "batch 437, training loss: 3.7468: : 437it [06:08,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.7907: : 437it [06:09,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.7907: : 438it [06:09,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.8762: : 438it [06:10,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.8762: : 439it [06:10,  1.21s/it]\u001b[A\n",
      "batch 440, training loss: 3.716: : 439it [06:12,  1.21s/it] \u001b[A\n",
      "batch 440, training loss: 3.716: : 440it [06:12,  1.22s/it]\u001b[A\n",
      "batch 441, training loss: 3.9782: : 440it [06:13,  1.22s/it]\u001b[A\n",
      "batch 441, training loss: 3.9782: : 441it [06:13,  1.22s/it]\u001b[A\n",
      "batch 442, training loss: 3.6574: : 441it [06:14,  1.22s/it]\u001b[A\n",
      "batch 442, training loss: 3.6574: : 442it [06:14,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 3.7328: : 442it [06:15,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 3.7328: : 443it [06:15,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.7353: : 443it [06:17,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.7353: : 444it [06:17,  1.23s/it]\u001b[A\n",
      "batch 445, training loss: 3.791: : 444it [06:18,  1.23s/it] \u001b[A\n",
      "batch 445, training loss: 3.791: : 445it [06:18,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.8039: : 445it [06:19,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.8039: : 446it [06:19,  1.23s/it]\u001b[A\n",
      "batch 447, training loss: 3.7378: : 446it [06:20,  1.23s/it]\u001b[A\n",
      "batch 447, training loss: 3.7378: : 447it [06:20,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.6352: : 447it [06:21,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.6352: : 448it [06:21,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.772: : 448it [06:23,  1.23s/it] \u001b[A\n",
      "batch 449, training loss: 3.772: : 449it [06:23,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 3.8031: : 449it [06:24,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 3.8031: : 450it [06:24,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.8052: : 450it [06:25,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.8052: : 451it [06:25,  1.24s/it]\u001b[A\n",
      "batch 452, training loss: 3.8046: : 451it [06:26,  1.24s/it]\u001b[A\n",
      "batch 452, training loss: 3.8046: : 452it [06:26,  1.24s/it]\u001b[A\n",
      "batch 453, training loss: 3.8857: : 452it [06:28,  1.24s/it]\u001b[A\n",
      "batch 453, training loss: 3.8857: : 453it [06:28,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.7214: : 453it [06:29,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.7214: : 454it [06:29,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.6363: : 454it [06:30,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.6363: : 455it [06:30,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.6798: : 455it [06:31,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.6798: : 456it [06:31,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.8032: : 456it [06:33,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.8032: : 457it [06:33,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.6685: : 457it [06:34,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.6685: : 458it [06:34,  1.24s/it]\u001b[A\n",
      "batch 459, training loss: 3.713: : 458it [06:35,  1.24s/it] \u001b[A\n",
      "batch 459, training loss: 3.713: : 459it [06:35,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.7012: : 459it [06:36,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.7012: : 460it [06:36,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.8007: : 460it [06:38,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.8007: : 461it [06:38,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.7454: : 461it [06:39,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.7454: : 462it [06:39,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.5907: : 462it [06:40,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.5907: : 463it [06:40,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.7256: : 463it [06:41,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.7256: : 464it [06:41,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.7141: : 464it [06:42,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.7141: : 465it [06:42,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.6703: : 465it [06:44,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.6703: : 466it [06:44,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.6042: : 466it [06:45,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.6042: : 467it [06:45,  1.22s/it]\u001b[A\n",
      "batch 468, training loss: 3.7841: : 467it [06:46,  1.22s/it]\u001b[A\n",
      "batch 468, training loss: 3.7841: : 468it [06:46,  1.27s/it]\u001b[A\n",
      "batch 469, training loss: 3.7053: : 468it [06:47,  1.27s/it]\u001b[A\n",
      "batch 469, training loss: 3.7053: : 469it [06:47,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.8139: : 469it [06:49,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.8139: : 470it [06:49,  1.28s/it]\u001b[A\n",
      "batch 471, training loss: 3.7872: : 470it [06:50,  1.28s/it]\u001b[A\n",
      "batch 471, training loss: 3.7872: : 471it [06:50,  1.26s/it]\u001b[A\n",
      "batch 472, training loss: 3.7387: : 471it [06:51,  1.26s/it]\u001b[A\n",
      "batch 472, training loss: 3.7387: : 472it [06:51,  1.28s/it]\u001b[A\n",
      "batch 473, training loss: 3.7454: : 472it [06:53,  1.28s/it]\u001b[A\n",
      "batch 473, training loss: 3.7454: : 473it [06:53,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 3.7865: : 473it [06:54,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 3.7865: : 474it [06:54,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.7216: : 474it [06:55,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.7216: : 475it [06:55,  1.29s/it]\u001b[A\n",
      "batch 476, training loss: 3.7969: : 475it [06:56,  1.29s/it]\u001b[A\n",
      "batch 476, training loss: 3.7969: : 476it [06:56,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.6101: : 476it [06:58,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.6101: : 477it [06:58,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.6813: : 477it [06:59,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.6813: : 478it [06:59,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.6498: : 478it [07:00,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.6498: : 479it [07:00,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.6748: : 479it [07:01,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 480, training loss: 3.6748: : 480it [07:01,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.7122: : 480it [07:03,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.7122: : 481it [07:03,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.6901: : 481it [07:04,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.6901: : 482it [07:04,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.6256: : 482it [07:05,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.6256: : 483it [07:05,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.7057: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.7057: : 484it [07:06,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.6244: : 484it [07:08,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.6244: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.7163: : 485it [07:09,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.7163: : 486it [07:09,  1.20s/it]\u001b[A\n",
      "batch 487, training loss: 3.8324: : 486it [07:10,  1.20s/it]\u001b[A\n",
      "batch 487, training loss: 3.8324: : 487it [07:10,  1.11s/it]\u001b[A\n",
      "batch 488, training loss: 3.603: : 487it [07:11,  1.11s/it] \u001b[A\n",
      "batch 488, training loss: 3.603: : 488it [07:11,  1.10s/it]\u001b[A\n",
      "batch 489, training loss: 3.5375: : 488it [07:12,  1.10s/it]\u001b[A\n",
      "batch 489, training loss: 3.5375: : 489it [07:12,  1.17s/it]\u001b[A\n",
      "batch 490, training loss: 3.6238: : 489it [07:13,  1.17s/it]\u001b[A\n",
      "batch 490, training loss: 3.6238: : 490it [07:13,  1.16s/it]\u001b[A\n",
      "batch 491, training loss: 3.66: : 490it [07:14,  1.16s/it]  \u001b[A\n",
      "batch 491, training loss: 3.66: : 491it [07:14,  1.21s/it]\u001b[A\n",
      "batch 492, training loss: 3.5537: : 491it [07:16,  1.21s/it]\u001b[A\n",
      "batch 492, training loss: 3.5537: : 492it [07:16,  1.21s/it]\u001b[A\n",
      "batch 493, training loss: 3.7451: : 492it [07:17,  1.21s/it]\u001b[A\n",
      "batch 493, training loss: 3.7451: : 493it [07:17,  1.23s/it]\u001b[A\n",
      "batch 494, training loss: 3.7419: : 493it [07:18,  1.23s/it]\u001b[A\n",
      "batch 494, training loss: 3.7419: : 494it [07:18,  1.23s/it]\u001b[A\n",
      "batch 495, training loss: 3.5728: : 494it [07:19,  1.23s/it]\u001b[A\n",
      "batch 495, training loss: 3.5728: : 495it [07:19,  1.25s/it]\u001b[A\n",
      "batch 496, training loss: 3.5507: : 495it [07:21,  1.25s/it]\u001b[A\n",
      "batch 496, training loss: 3.5507: : 496it [07:21,  1.24s/it]\u001b[A\n",
      "batch 497, training loss: 3.4863: : 496it [07:22,  1.24s/it]\u001b[A\n",
      "batch 497, training loss: 3.4863: : 497it [07:22,  1.25s/it]\u001b[A\n",
      "batch 498, training loss: 3.585: : 497it [07:23,  1.25s/it] \u001b[A\n",
      "batch 498, training loss: 3.585: : 498it [07:23,  1.25s/it]\u001b[A\n",
      "batch 499, training loss: 3.5979: : 498it [07:24,  1.25s/it]\u001b[A\n",
      "batch 499, training loss: 3.5979: : 499it [07:24,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 3.8539: : 499it [07:25,  1.12s/it]\u001b[A\n",
      "batch 500, training loss: 3.8539: : 500it [07:25,  1.21s/it]\u001b[A\n",
      "batch 501, training loss: 3.702: : 500it [07:27,  1.21s/it] \u001b[A\n",
      "batch 501, training loss: 3.702: : 501it [07:27,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 3.6929: : 501it [07:28,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 3.6929: : 502it [07:28,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 3.7661: : 502it [07:30,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 3.7661: : 503it [07:30,  1.31s/it]\u001b[A\n",
      "batch 504, training loss: 3.7527: : 503it [07:31,  1.31s/it]\u001b[A\n",
      "batch 504, training loss: 3.7527: : 504it [07:31,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 3.8281: : 504it [07:32,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 3.8281: : 505it [07:32,  1.32s/it]\u001b[A\n",
      "batch 506, training loss: 3.7619: : 505it [07:33,  1.32s/it]\u001b[A\n",
      "batch 506, training loss: 3.7619: : 506it [07:33,  1.28s/it]\u001b[A\n",
      "batch 507, training loss: 3.6311: : 506it [07:35,  1.28s/it]\u001b[A\n",
      "batch 507, training loss: 3.6311: : 507it [07:35,  1.30s/it]\u001b[A\n",
      "batch 508, training loss: 3.7955: : 507it [07:36,  1.30s/it]\u001b[A\n",
      "batch 508, training loss: 3.7955: : 508it [07:36,  1.33s/it]\u001b[A\n",
      "batch 509, training loss: 3.6661: : 508it [07:38,  1.33s/it]\u001b[A\n",
      "batch 509, training loss: 3.6661: : 509it [07:38,  1.35s/it]\u001b[A\n",
      "batch 510, training loss: 3.7277: : 509it [07:39,  1.35s/it]\u001b[A\n",
      "batch 510, training loss: 3.7277: : 510it [07:39,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.7357: : 510it [07:40,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.7357: : 511it [07:40,  1.34s/it]\u001b[A\n",
      "batch 512, training loss: 3.7228: : 511it [07:42,  1.34s/it]\u001b[A\n",
      "batch 512, training loss: 3.7228: : 512it [07:42,  1.36s/it]\u001b[A\n",
      "batch 513, training loss: 3.7709: : 512it [07:43,  1.36s/it]\u001b[A\n",
      "batch 513, training loss: 3.7709: : 513it [07:43,  1.39s/it]\u001b[A\n",
      "batch 514, training loss: 3.6094: : 513it [07:44,  1.39s/it]\u001b[A\n",
      "batch 514, training loss: 3.6094: : 514it [07:44,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.8002: : 514it [07:46,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.8002: : 515it [07:46,  1.36s/it]\u001b[A\n",
      "batch 516, training loss: 3.6866: : 515it [07:47,  1.36s/it]\u001b[A\n",
      "batch 516, training loss: 3.6866: : 516it [07:47,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.6691: : 516it [07:49,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.6691: : 517it [07:49,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.7463: : 517it [07:50,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.7463: : 518it [07:50,  1.36s/it]\u001b[A\n",
      "batch 519, training loss: 3.5474: : 518it [07:51,  1.36s/it]\u001b[A\n",
      "batch 519, training loss: 3.5474: : 519it [07:51,  1.32s/it]\u001b[A\n",
      "batch 520, training loss: 3.718: : 519it [07:53,  1.32s/it] \u001b[A\n",
      "batch 520, training loss: 3.718: : 520it [07:53,  1.35s/it]\u001b[A\n",
      "batch 521, training loss: 3.7121: : 520it [07:54,  1.35s/it]\u001b[A\n",
      "batch 521, training loss: 3.7121: : 521it [07:54,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 3.7634: : 521it [07:55,  1.37s/it]\u001b[A\n",
      "batch 522, training loss: 3.7634: : 522it [07:55,  1.34s/it]\u001b[A\n",
      "batch 523, training loss: 3.608: : 522it [07:57,  1.34s/it] \u001b[A\n",
      "batch 523, training loss: 3.608: : 523it [07:57,  1.36s/it]\u001b[A\n",
      "batch 524, training loss: 3.8146: : 523it [07:58,  1.36s/it]\u001b[A\n",
      "batch 524, training loss: 3.8146: : 524it [07:58,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.8441: : 524it [07:59,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.8441: : 525it [07:59,  1.35s/it]\u001b[A\n",
      "batch 526, training loss: 3.641: : 525it [08:01,  1.35s/it] \u001b[A\n",
      "batch 526, training loss: 3.641: : 526it [08:01,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.6221: : 526it [08:02,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.6221: : 527it [08:02,  1.34s/it]\u001b[A\n",
      "batch 528, training loss: 3.6959: : 527it [08:03,  1.34s/it]\u001b[A\n",
      "batch 528, training loss: 3.6959: : 528it [08:03,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 3.7461: : 528it [08:05,  1.37s/it]\u001b[A\n",
      "batch 529, training loss: 3.7461: : 529it [08:05,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.7206: : 529it [08:06,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.7206: : 530it [08:06,  1.45s/it]\u001b[A\n",
      "batch 531, training loss: 3.662: : 530it [08:08,  1.45s/it] \u001b[A\n",
      "batch 531, training loss: 3.662: : 531it [08:08,  1.47s/it]\u001b[A\n",
      "batch 532, training loss: 3.4631: : 531it [08:09,  1.47s/it]\u001b[A\n",
      "batch 532, training loss: 3.4631: : 532it [08:09,  1.45s/it]\u001b[A\n",
      "batch 533, training loss: 3.6458: : 532it [08:11,  1.45s/it]\u001b[A\n",
      "batch 533, training loss: 3.6458: : 533it [08:11,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.7758: : 533it [08:12,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.7758: : 534it [08:12,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.6481: : 534it [08:14,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.6481: : 535it [08:14,  1.49s/it]\u001b[A\n",
      "batch 536, training loss: 3.4959: : 535it [08:15,  1.49s/it]\u001b[A\n",
      "batch 536, training loss: 3.4959: : 536it [08:15,  1.49s/it]\u001b[A\n",
      "batch 537, training loss: 3.5618: : 536it [08:17,  1.49s/it]\u001b[A\n",
      "batch 537, training loss: 3.5618: : 537it [08:17,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.6769: : 537it [08:18,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.6769: : 538it [08:18,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 3.6508: : 538it [08:20,  1.46s/it]\u001b[A\n",
      "batch 539, training loss: 3.6508: : 539it [08:20,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 3.5886: : 539it [08:21,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 3.5886: : 540it [08:21,  1.49s/it]\u001b[A\n",
      "batch 541, training loss: 3.6858: : 540it [08:23,  1.49s/it]\u001b[A\n",
      "batch 541, training loss: 3.6858: : 541it [08:23,  1.47s/it]\u001b[A\n",
      "batch 542, training loss: 3.7368: : 541it [08:24,  1.47s/it]\u001b[A\n",
      "batch 542, training loss: 3.7368: : 542it [08:24,  1.44s/it]\u001b[A\n",
      "batch 543, training loss: 3.5879: : 542it [08:26,  1.44s/it]\u001b[A\n",
      "batch 543, training loss: 3.5879: : 543it [08:26,  1.46s/it]\u001b[A\n",
      "batch 544, training loss: 3.6451: : 543it [08:27,  1.46s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 544, training loss: 3.6451: : 544it [08:27,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.6362: : 544it [08:29,  1.48s/it]\u001b[A\n",
      "batch 545, training loss: 3.6362: : 545it [08:29,  1.46s/it]\u001b[A\n",
      "batch 546, training loss: 3.5443: : 545it [08:30,  1.46s/it]\u001b[A\n",
      "batch 546, training loss: 3.5443: : 546it [08:30,  1.46s/it]\u001b[A\n",
      "batch 547, training loss: 3.4379: : 546it [08:32,  1.46s/it]\u001b[A\n",
      "batch 547, training loss: 3.4379: : 547it [08:32,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 3.4633: : 547it [08:32,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 3.4633: : 548it [08:32,  1.31s/it]\u001b[A\n",
      "batch 549, training loss: 3.6183: : 548it [08:34,  1.31s/it]\u001b[A\n",
      "batch 549, training loss: 3.6183: : 549it [08:34,  1.39s/it]\u001b[A\n",
      "batch 550, training loss: 3.5394: : 549it [08:36,  1.39s/it]\u001b[A\n",
      "batch 550, training loss: 3.5394: : 550it [08:36,  1.45s/it]\u001b[A\n",
      "batch 551, training loss: 3.5949: : 550it [08:37,  1.45s/it]\u001b[A\n",
      "batch 551, training loss: 3.5949: : 551it [08:37,  1.48s/it]\u001b[A\n",
      "batch 552, training loss: 3.6172: : 551it [08:39,  1.48s/it]\u001b[A\n",
      "batch 552, training loss: 3.6172: : 552it [08:39,  1.52s/it]\u001b[A\n",
      "batch 553, training loss: 3.6011: : 552it [08:40,  1.52s/it]\u001b[A\n",
      "batch 553, training loss: 3.6011: : 553it [08:40,  1.55s/it]\u001b[A\n",
      "batch 554, training loss: 3.371: : 553it [08:42,  1.55s/it] \u001b[A\n",
      "batch 554, training loss: 3.371: : 554it [08:42,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 3.5283: : 554it [08:44,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 3.5283: : 555it [08:44,  1.59s/it]\u001b[A\n",
      "batch 556, training loss: 3.6742: : 555it [08:45,  1.59s/it]\u001b[A\n",
      "batch 556, training loss: 3.6742: : 556it [08:45,  1.58s/it]\u001b[A\n",
      "batch 557, training loss: 3.5142: : 556it [08:47,  1.58s/it]\u001b[A\n",
      "batch 557, training loss: 3.5142: : 557it [08:47,  1.58s/it]\u001b[A\n",
      "batch 558, training loss: 3.4457: : 557it [08:48,  1.58s/it]\u001b[A\n",
      "batch 558, training loss: 3.4457: : 558it [08:48,  1.58s/it]\u001b[A\n",
      "batch 559, training loss: 3.5393: : 558it [08:50,  1.58s/it]\u001b[A\n",
      "batch 559, training loss: 3.5393: : 559it [08:50,  1.56s/it]\u001b[A\n",
      "batch 560, training loss: 3.5619: : 559it [08:51,  1.56s/it]\u001b[A\n",
      "batch 560, training loss: 3.5619: : 560it [08:51,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.5505: : 560it [08:53,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.5505: : 561it [08:53,  1.56s/it]\u001b[A\n",
      "batch 562, training loss: 3.3792: : 561it [08:55,  1.56s/it]\u001b[A\n",
      "batch 562, training loss: 3.3792: : 562it [08:55,  1.56s/it]\u001b[A\n",
      "batch 563, training loss: 3.5398: : 562it [08:56,  1.56s/it]\u001b[A\n",
      "batch 563, training loss: 3.5398: : 563it [08:56,  1.58s/it]\u001b[A\n",
      "batch 564, training loss: 3.4352: : 563it [08:58,  1.58s/it]\u001b[A\n",
      "batch 564, training loss: 3.4352: : 564it [08:58,  1.57s/it]\u001b[A\n",
      "batch 565, training loss: 3.5375: : 564it [08:59,  1.57s/it]\u001b[A\n",
      "batch 565, training loss: 3.5375: : 565it [08:59,  1.51s/it]\u001b[A\n",
      "batch 566, training loss: 3.6608: : 565it [09:01,  1.51s/it]\u001b[A\n",
      "batch 566, training loss: 3.6608: : 566it [09:01,  1.56s/it]\u001b[A\n",
      "batch 567, training loss: 3.6068: : 566it [09:02,  1.56s/it]\u001b[A\n",
      "batch 567, training loss: 3.6068: : 567it [09:02,  1.56s/it]\u001b[A\n",
      "batch 568, training loss: 3.6917: : 567it [09:04,  1.56s/it]\u001b[A\n",
      "batch 568, training loss: 3.6917: : 568it [09:04,  1.61s/it]\u001b[A\n",
      "batch 569, training loss: 3.6655: : 568it [09:06,  1.61s/it]\u001b[A\n",
      "batch 569, training loss: 3.6655: : 569it [09:06,  1.61s/it]\u001b[A\n",
      "batch 570, training loss: 3.8389: : 569it [09:07,  1.61s/it]\u001b[A\n",
      "batch 570, training loss: 3.8389: : 570it [09:07,  1.57s/it]\u001b[A\n",
      "batch 571, training loss: 3.6459: : 570it [09:09,  1.57s/it]\u001b[A\n",
      "batch 571, training loss: 3.6459: : 571it [09:09,  1.56s/it]\u001b[A\n",
      "batch 572, training loss: 3.7202: : 571it [09:10,  1.56s/it]\u001b[A\n",
      "batch 572, training loss: 3.7202: : 572it [09:10,  1.60s/it]\u001b[A\n",
      "batch 573, training loss: 3.6439: : 572it [09:12,  1.60s/it]\u001b[A\n",
      "batch 573, training loss: 3.6439: : 573it [09:12,  1.60s/it]\u001b[A\n",
      "batch 574, training loss: 3.7276: : 573it [09:14,  1.60s/it]\u001b[A\n",
      "batch 574, training loss: 3.7276: : 574it [09:14,  1.62s/it]\u001b[A\n",
      "batch 575, training loss: 3.4619: : 574it [09:14,  1.62s/it]\u001b[A\n",
      "batch 575, training loss: 3.4619: : 575it [09:14,  1.39s/it]\u001b[A\n",
      "batch 576, training loss: 3.6763: : 575it [09:16,  1.39s/it]\u001b[A\n",
      "batch 576, training loss: 3.6763: : 576it [09:16,  1.49s/it]\u001b[A\n",
      "batch 577, training loss: 3.572: : 576it [09:18,  1.49s/it] \u001b[A\n",
      "batch 577, training loss: 3.572: : 577it [09:18,  1.56s/it]\u001b[A\n",
      "batch 578, training loss: 3.5389: : 577it [09:20,  1.56s/it]\u001b[A\n",
      "batch 578, training loss: 3.5389: : 578it [09:20,  1.61s/it]\u001b[A\n",
      "batch 579, training loss: 3.6029: : 578it [09:21,  1.61s/it]\u001b[A\n",
      "batch 579, training loss: 3.6029: : 579it [09:21,  1.64s/it]\u001b[A\n",
      "batch 580, training loss: 3.492: : 579it [09:23,  1.64s/it] \u001b[A\n",
      "batch 580, training loss: 3.492: : 580it [09:23,  1.65s/it]\u001b[A\n",
      "batch 581, training loss: 3.5025: : 580it [09:25,  1.65s/it]\u001b[A\n",
      "batch 581, training loss: 3.5025: : 581it [09:25,  1.66s/it]\u001b[A\n",
      "batch 582, training loss: 3.5155: : 581it [09:26,  1.66s/it]\u001b[A\n",
      "batch 582, training loss: 3.5155: : 582it [09:26,  1.69s/it]\u001b[A\n",
      "batch 583, training loss: 3.0585: : 582it [09:27,  1.69s/it]\u001b[A\n",
      "batch 583, training loss: 3.0585: : 583it [09:27,  1.39s/it]\u001b[A\n",
      "batch 584, training loss: 3.7546: : 583it [09:29,  1.39s/it]\u001b[A\n",
      "batch 584, training loss: 3.7546: : 584it [09:29,  1.53s/it]\u001b[A\n",
      "batch 585, training loss: 3.6989: : 584it [09:31,  1.53s/it]\u001b[A\n",
      "batch 585, training loss: 3.6989: : 585it [09:31,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 3.7876: : 585it [09:33,  1.64s/it]\u001b[A\n",
      "batch 586, training loss: 3.7876: : 586it [09:33,  1.69s/it]\u001b[A\n",
      "batch 587, training loss: 3.6784: : 586it [09:35,  1.69s/it]\u001b[A\n",
      "batch 587, training loss: 3.6784: : 587it [09:35,  1.75s/it]\u001b[A\n",
      "batch 588, training loss: 3.5543: : 587it [09:37,  1.75s/it]\u001b[A\n",
      "batch 588, training loss: 3.5543: : 588it [09:37,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.6444: : 588it [09:39,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.6444: : 589it [09:39,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.6285: : 589it [09:40,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.6285: : 590it [09:40,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.5182: : 590it [09:42,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.5182: : 591it [09:42,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.5646: : 591it [09:44,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.5646: : 592it [09:44,  1.89s/it]\u001b[A\n",
      "batch 593, training loss: 3.4685: : 592it [09:46,  1.89s/it]\u001b[A\n",
      "batch 593, training loss: 3.4685: : 593it [09:46,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.6339: : 593it [09:48,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.6339: : 594it [09:48,  1.97s/it]\u001b[A\n",
      "batch 595, training loss: 3.7523: : 594it [09:50,  1.97s/it]\u001b[A\n",
      "batch 595, training loss: 3.7523: : 595it [09:50,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.6415: : 595it [09:52,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.6415: : 596it [09:52,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.4579: : 596it [09:54,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.4579: : 597it [09:54,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.6846: : 597it [09:56,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.6846: : 598it [09:56,  2.03s/it]\u001b[A\n",
      "batch 599, training loss: 3.5969: : 598it [09:58,  2.03s/it]\u001b[A\n",
      "batch 599, training loss: 3.5969: : 599it [09:58,  1.86s/it]\u001b[A\n",
      "batch 600, training loss: 3.4973: : 599it [10:00,  1.86s/it]\u001b[A\n",
      "batch 600, training loss: 3.4973: : 600it [10:00,  2.02s/it]\u001b[A\n",
      "batch 601, training loss: 2.7215: : 600it [10:01,  2.02s/it]\u001b[A\n",
      "batch 601, training loss: 2.7215: : 601it [10:01,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 3.5454: : 601it [10:03,  1.68s/it]\u001b[A\n",
      "batch 602, training loss: 3.5454: : 602it [10:03,  1.81s/it]\u001b[A\n",
      "batch 603, training loss: 3.5477: : 602it [10:05,  1.81s/it]\u001b[A\n",
      "batch 603, training loss: 3.5477: : 603it [10:05,  1.84s/it]\u001b[A\n",
      "batch 604, training loss: 3.5872: : 603it [10:07,  1.84s/it]\u001b[A\n",
      "batch 604, training loss: 3.5872: : 604it [10:07,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 3.7119: : 604it [10:09,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 3.7119: : 605it [10:09,  1.81s/it]\u001b[A\n",
      "batch 606, training loss: 3.5192: : 605it [10:10,  1.81s/it]\u001b[A\n",
      "batch 606, training loss: 3.5192: : 606it [10:10,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.4039: : 606it [10:12,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.4039: : 607it [10:12,  1.70s/it]\u001b[A\n",
      "batch 608, training loss: 3.6141: : 607it [10:13,  1.70s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 608, training loss: 3.6141: : 608it [10:13,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 3.6001: : 608it [10:14,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 3.6001: : 609it [10:14,  1.49s/it]\u001b[A\n",
      "batch 610, training loss: 3.0114: : 609it [10:16,  1.49s/it]\u001b[A\n",
      "batch 610, training loss: 3.0114: : 610it [10:16,  1.42s/it]\u001b[A\n",
      "batch 611, training loss: 3.0414: : 610it [10:17,  1.42s/it]\u001b[A\n",
      "batch 611, training loss: 3.0414: : 611it [10:17,  1.38s/it]\u001b[A\n",
      "batch 612, training loss: 2.3145: : 611it [10:18,  1.38s/it]\u001b[A\n",
      "batch 612, training loss: 2.3145: : 612it [10:18,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.0346: : 612it [10:19,  1.31s/it]\u001b[A\n",
      "batch 613, training loss: 3.0346: : 613it [10:19,  1.18s/it]\u001b[A\n",
      "batch 613, training loss: 3.0346: : 614it [10:19,  1.16it/s]\u001b[A\n",
      "batch 613, training loss: 3.0346: : 616it [10:20,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.8428: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.8428: : 1it [00:00,  4.26it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0416: : 1it [00:00,  4.26it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0416: : 2it [00:00,  4.44it/s]\u001b[A\n",
      "batch 2, dev loss: 3.6435: : 2it [00:00,  4.44it/s]\u001b[A\n",
      "batch 2, dev loss: 3.6435: : 3it [00:00,  4.55it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7614: : 3it [00:00,  4.55it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7614: : 4it [00:00,  4.68it/s]\u001b[A\n",
      "batch 4, dev loss: 3.7914: : 4it [00:01,  4.68it/s]\u001b[A\n",
      "batch 4, dev loss: 3.7914: : 5it [00:01,  4.67it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7688: : 5it [00:01,  4.67it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7688: : 6it [00:01,  4.81it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8639: : 6it [00:01,  4.81it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8639: : 7it [00:01,  4.78it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6319: : 7it [00:01,  4.78it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6319: : 8it [00:01,  5.15it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8536: : 8it [00:01,  5.15it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8536: : 9it [00:01,  4.76it/s]\u001b[A\n",
      "batch 9, dev loss: 3.773: : 9it [00:02,  4.76it/s] \u001b[A\n",
      "batch 9, dev loss: 3.773: : 10it [00:02,  4.68it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7991: : 10it [00:02,  4.68it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7991: : 11it [00:02,  4.66it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8748: : 11it [00:02,  4.66it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8748: : 12it [00:02,  4.69it/s]\u001b[A\n",
      "batch 12, dev loss: 3.704: : 12it [00:02,  4.69it/s] \u001b[A\n",
      "batch 12, dev loss: 3.704: : 13it [00:02,  4.50it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8457: : 13it [00:03,  4.50it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8457: : 14it [00:03,  4.29it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9672: : 14it [00:03,  4.29it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9672: : 15it [00:03,  4.07it/s]\u001b[A\n",
      "batch 15, dev loss: 3.754: : 15it [00:03,  4.07it/s] \u001b[A\n",
      "batch 15, dev loss: 3.754: : 16it [00:03,  4.46it/s]\u001b[A\n",
      "batch 16, dev loss: 4.091: : 16it [00:03,  4.46it/s]\u001b[A\n",
      "batch 16, dev loss: 4.091: : 17it [00:03,  4.28it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8747: : 17it [00:04,  4.28it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8747: : 18it [00:04,  4.21it/s]\u001b[A\n",
      "batch 18, dev loss: 3.77: : 18it [00:04,  4.21it/s]  \u001b[A\n",
      "batch 18, dev loss: 3.77: : 19it [00:04,  4.02it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9658: : 19it [00:04,  4.02it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9658: : 20it [00:04,  4.07it/s]\u001b[A\n",
      "batch 20, dev loss: 3.776: : 20it [00:04,  4.07it/s] \u001b[A\n",
      "batch 20, dev loss: 3.776: : 21it [00:04,  3.98it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6679: : 21it [00:05,  3.98it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6679: : 22it [00:05,  3.82it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8677: : 22it [00:05,  3.82it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8677: : 23it [00:05,  3.84it/s]\u001b[A\n",
      "batch 23, dev loss: 3.9074: : 23it [00:05,  3.84it/s]\u001b[A\n",
      "batch 23, dev loss: 3.9074: : 24it [00:05,  4.40it/s]\u001b[A\n",
      "batch 24, dev loss: 3.8257: : 24it [00:05,  4.40it/s]\u001b[A\n",
      "batch 24, dev loss: 3.8257: : 25it [00:05,  3.91it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7965: : 25it [00:06,  3.91it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7965: : 26it [00:06,  3.94it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7396: : 26it [00:06,  3.94it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7396: : 27it [00:06,  3.81it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6797: : 27it [00:06,  3.81it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6797: : 28it [00:06,  3.61it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9152: : 28it [00:06,  3.61it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9152: : 29it [00:06,  3.42it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8414: : 29it [00:07,  3.42it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8414: : 30it [00:07,  3.31it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1189: : 30it [00:07,  3.31it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1189: : 31it [00:07,  4.07it/s]\u001b[A\n",
      "batch 31, dev loss: 3.889: : 31it [00:07,  4.07it/s] \u001b[A\n",
      "batch 31, dev loss: 3.889: : 32it [00:07,  3.80it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9409: : 32it [00:08,  3.80it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9409: : 33it [00:08,  3.45it/s]\u001b[A\n",
      "batch 33, dev loss: 3.7011: : 33it [00:08,  3.45it/s]\u001b[A\n",
      "batch 33, dev loss: 3.7011: : 34it [00:08,  3.24it/s]\u001b[A\n",
      "batch 34, dev loss: 4.1255: : 34it [00:08,  3.24it/s]\u001b[A\n",
      "batch 34, dev loss: 4.1255: : 35it [00:08,  3.10it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9249: : 35it [00:09,  3.10it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9249: : 36it [00:09,  3.01it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8734: : 36it [00:09,  3.01it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8734: : 37it [00:09,  3.31it/s]\u001b[A\n",
      "batch 37, dev loss: 3.631: : 37it [00:09,  3.31it/s] \u001b[A\n",
      "batch 37, dev loss: 3.631: : 38it [00:09,  3.24it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8875: : 38it [00:10,  3.24it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8875: : 39it [00:10,  3.13it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8822: : 39it [00:10,  3.13it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8822: : 40it [00:10,  3.00it/s]\u001b[A\n",
      "batch 40, dev loss: 3.9156: : 40it [00:10,  3.00it/s]\u001b[A\n",
      "batch 40, dev loss: 3.9156: : 41it [00:10,  2.93it/s]\u001b[A\n",
      "batch 41, dev loss: 3.7144: : 41it [00:11,  2.93it/s]\u001b[A\n",
      "batch 41, dev loss: 3.7144: : 42it [00:11,  3.06it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8719: : 42it [00:11,  3.06it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8719: : 43it [00:11,  2.87it/s]\u001b[A\n",
      "batch 43, dev loss: 3.862: : 43it [00:11,  2.87it/s] \u001b[A\n",
      "batch 43, dev loss: 3.862: : 44it [00:11,  2.79it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7866: : 44it [00:12,  2.79it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7866: : 45it [00:12,  2.70it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0851: : 45it [00:12,  2.70it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0851: : 46it [00:12,  2.65it/s]\u001b[A\n",
      "batch 46, dev loss: 3.6219: : 46it [00:13,  2.65it/s]\u001b[A\n",
      "batch 46, dev loss: 3.6219: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 3.8028: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 3.8028: : 48it [00:13,  2.59it/s]\u001b[A\n",
      "batch 48, dev loss: 3.6262: : 48it [00:13,  2.59it/s]\u001b[A\n",
      "batch 48, dev loss: 3.6262: : 49it [00:13,  2.50it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7939: : 49it [00:14,  2.50it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7939: : 50it [00:14,  2.83it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7816: : 50it [00:14,  2.83it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7816: : 51it [00:14,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7892: : 51it [00:14,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7892: : 52it [00:14,  2.57it/s]\u001b[A\n",
      "batch 52, dev loss: 3.5718: : 52it [00:15,  2.57it/s]\u001b[A\n",
      "batch 52, dev loss: 3.5718: : 53it [00:15,  2.58it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7813: : 53it [00:15,  2.58it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7813: : 54it [00:15,  2.50it/s]\u001b[A\n",
      "batch 54, dev loss: 3.548: : 54it [00:16,  2.50it/s] \u001b[A\n",
      "batch 54, dev loss: 3.548: : 55it [00:16,  2.35it/s]\u001b[A\n",
      "batch 55, dev loss: 3.7129: : 55it [00:16,  2.35it/s]\u001b[A\n",
      "batch 55, dev loss: 3.7129: : 56it [00:16,  2.19it/s]\u001b[A\n",
      "batch 56, dev loss: 3.585: : 56it [00:17,  2.19it/s] \u001b[A\n",
      "batch 56, dev loss: 3.585: : 57it [00:17,  2.32it/s]\u001b[A\n",
      "batch 57, dev loss: 3.511: : 57it [00:17,  2.32it/s]\u001b[A\n",
      "batch 57, dev loss: 3.511: : 58it [00:17,  2.17it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8803: : 58it [00:18,  2.17it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8803: : 59it [00:18,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7723: : 59it [00:18,  2.20it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7723: : 60it [00:18,  2.23it/s]\u001b[A\n",
      "batch 60, dev loss: 3.481: : 60it [00:18,  2.23it/s] \u001b[A\n",
      "batch 60, dev loss: 3.481: : 61it [00:18,  2.31it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4829: : 61it [00:19,  2.31it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4829: : 62it [00:19,  2.58it/s]\u001b[A\n",
      "batch 62, dev loss: 3.3129: : 62it [00:19,  2.58it/s]\u001b[A\n",
      "batch 62, dev loss: 3.3129: : 63it [00:19,  2.97it/s]\u001b[A\n",
      "batch 63, dev loss: 3.9302: : 63it [00:19,  2.97it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 63, dev loss: 3.9302: : 64it [00:19,  3.18it/s]\u001b[A\n",
      "batch 64, dev loss: 3.6201: : 64it [00:20,  3.18it/s]\u001b[A\n",
      "batch 64, dev loss: 3.6201: : 65it [00:20,  3.20it/s]\u001b[A\n",
      "batch 65, dev loss: 3.5324: : 65it [00:20,  3.20it/s]\u001b[A\n",
      "batch 65, dev loss: 3.5324: : 66it [00:20,  2.98it/s]\u001b[A\n",
      "batch 66, dev loss: 3.6188: : 66it [00:20,  2.98it/s]\u001b[A\n",
      "batch 66, dev loss: 3.6188: : 67it [00:20,  3.00it/s]\u001b[A\n",
      "batch 67, dev loss: 2.7631: : 67it [00:21,  3.00it/s]\u001b[A\n",
      "batch 67, dev loss: 2.7631: : 68it [00:21,  2.98it/s]\u001b[A\n",
      "batch 68, dev loss: 2.7779: : 68it [00:21,  2.98it/s]\u001b[A\n",
      "batch 68, dev loss: 2.7779: : 69it [00:21,  3.09it/s]\u001b[A\n",
      "batch 69, dev loss: 3.1122: : 69it [00:21,  3.09it/s]\u001b[A\n",
      "batch 69, dev loss: 3.1122: : 70it [00:21,  3.02it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9373: : 70it [00:22,  3.02it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9373: : 71it [00:22,  3.08it/s]\u001b[A\n",
      "batch 71, dev loss: 3.3407: : 71it [00:22,  3.08it/s]\u001b[A\n",
      "batch 71, dev loss: 3.3407: : 72it [00:22,  3.08it/s]\u001b[A\n",
      "batch 72, dev loss: 4.102: : 72it [00:22,  3.08it/s] \u001b[A\n",
      "batch 72, dev loss: 4.102: : 73it [00:22,  3.04it/s]\u001b[A\n",
      "batch 72, dev loss: 4.102: : 76it [00:22,  3.33it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.44s/it]\u001b[A\n",
      "2it [00:02,  1.28s/it]\u001b[A\n",
      "3it [00:04,  1.38s/it]\u001b[A\n",
      "4it [00:05,  1.38s/it]\u001b[A\n",
      "5it [00:07,  1.43s/it]\u001b[A\n",
      "6it [00:08,  1.43s/it]\u001b[A\n",
      "7it [00:09,  1.42s/it]\u001b[A\n",
      "8it [00:11,  1.36s/it]\u001b[A\n",
      "9it [00:12,  1.52s/it]\u001b[A\n",
      "10it [00:14,  1.50s/it]\u001b[A\n",
      "11it [00:15,  1.42s/it]\u001b[A\n",
      "12it [00:17,  1.55s/it]\u001b[A\n",
      "13it [00:19,  1.61s/it]\u001b[A\n",
      "14it [00:20,  1.53s/it]\u001b[A\n",
      "15it [00:22,  1.58s/it]\u001b[A\n",
      "16it [00:23,  1.43s/it]\u001b[A\n",
      "17it [00:24,  1.49s/it]\u001b[A\n",
      "18it [00:26,  1.62s/it]\u001b[A\n",
      "19it [00:28,  1.72s/it]\u001b[A\n",
      "20it [00:31,  1.88s/it]\u001b[A\n",
      "21it [00:33,  1.94s/it]\u001b[A\n",
      "22it [00:35,  2.05s/it]\u001b[A\n",
      "23it [00:37,  2.07s/it]\u001b[A\n",
      "24it [00:38,  1.59s/it]\u001b[A\n",
      "25it [00:40,  1.73s/it]\u001b[A\n",
      "26it [00:42,  1.89s/it]\u001b[A\n",
      "27it [00:44,  1.96s/it]\u001b[A\n",
      "28it [00:47,  2.12s/it]\u001b[A\n",
      "29it [00:49,  2.08s/it]\u001b[A\n",
      "30it [00:51,  2.10s/it]\u001b[A\n",
      "31it [00:53,  2.27s/it]\u001b[A\n",
      "32it [00:56,  2.35s/it]\u001b[A\n",
      "33it [00:58,  2.36s/it]\u001b[A\n",
      "34it [01:01,  2.41s/it]\u001b[A\n",
      "35it [01:04,  2.62s/it]\u001b[A\n",
      "36it [01:04,  2.00s/it]\u001b[A\n",
      "37it [01:07,  2.28s/it]\u001b[A\n",
      "38it [01:10,  2.27s/it]\u001b[A\n",
      "39it [01:13,  2.52s/it]\u001b[A\n",
      "40it [01:16,  2.70s/it]\u001b[A\n",
      "41it [01:16,  2.02s/it]\u001b[A\n",
      "42it [01:20,  2.55s/it]\u001b[A\n",
      "43it [01:23,  2.79s/it]\u001b[A\n",
      "44it [01:27,  2.94s/it]\u001b[A\n",
      "45it [01:29,  2.77s/it]\u001b[A\n",
      "46it [01:32,  2.96s/it]\u001b[A\n",
      "47it [01:36,  3.16s/it]\u001b[A\n",
      "48it [01:39,  3.19s/it]\u001b[A\n",
      "49it [01:40,  2.30s/it]\u001b[A\n",
      "50it [01:44,  2.87s/it]\u001b[A\n",
      "51it [01:48,  3.28s/it]\u001b[A\n",
      "52it [01:50,  2.96s/it]\u001b[A\n",
      "53it [01:54,  3.22s/it]\u001b[A\n",
      "54it [01:58,  3.45s/it]\u001b[A\n",
      "55it [02:03,  3.84s/it]\u001b[A\n",
      "56it [02:05,  3.44s/it]\u001b[A\n",
      "57it [02:10,  3.68s/it]\u001b[A\n",
      "58it [02:13,  3.57s/it]\u001b[A\n",
      "59it [02:15,  3.11s/it]\u001b[A\n",
      "60it [02:18,  2.96s/it]\u001b[A\n",
      "61it [02:19,  2.38s/it]\u001b[A\n",
      "62it [02:20,  2.10s/it]\u001b[A\n",
      "63it [02:21,  1.78s/it]\u001b[A\n",
      "64it [02:22,  1.43s/it]\u001b[A\n",
      "65it [02:22,  1.23s/it]\u001b[A\n",
      "66it [02:23,  1.02it/s]\u001b[A\n",
      "67it [02:23,  1.23it/s]\u001b[A\n",
      "68it [02:24,  1.42it/s]\u001b[A\n",
      "69it [02:24,  1.75it/s]\u001b[A\n",
      "70it [02:24,  2.07s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███▋                                  | 656/6740 [00:00<00:00, 6554.49it/s]\u001b[A\n",
      " 19%|███████▏                             | 1312/6740 [00:00<00:00, 6428.82it/s]\u001b[A\n",
      " 29%|██████████▋                          | 1956/6740 [00:00<00:00, 6268.12it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 2584/6740 [00:00<00:00, 5967.64it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 3183/6740 [00:00<00:00, 5855.72it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 3770/6740 [00:00<00:00, 5765.73it/s]\u001b[A\n",
      " 65%|███████████████████████▊             | 4348/6740 [00:00<00:00, 5560.97it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 4906/6740 [00:00<00:00, 5541.79it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 5461/6740 [00:00<00:00, 5432.52it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 6028/6740 [00:01<00:00, 5502.11it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5688.03it/s]\u001b[A\n",
      "Epoch: 8, tfr: 1.0, loss(train/dev): 3.6914/3.7484, ppl(dev/test): 42.4531/49.04\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.6815: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.6815: : 1it [00:01,  1.80s/it]\u001b[A\n",
      "batch 2, training loss: 3.5777: : 1it [00:02,  1.80s/it]\u001b[A\n",
      "batch 2, training loss: 3.5777: : 2it [00:02,  1.07s/it]\u001b[A\n",
      "batch 3, training loss: 3.6434: : 2it [00:03,  1.07s/it]\u001b[A\n",
      "batch 3, training loss: 3.6434: : 3it [00:03,  1.14it/s]\u001b[A\n",
      "batch 4, training loss: 3.5866: : 3it [00:03,  1.14it/s]\u001b[A\n",
      "batch 4, training loss: 3.5866: : 4it [00:03,  1.24it/s]\u001b[A\n",
      "batch 5, training loss: 3.4479: : 4it [00:04,  1.24it/s]\u001b[A\n",
      "batch 5, training loss: 3.4479: : 5it [00:04,  1.33it/s]\u001b[A\n",
      "batch 6, training loss: 3.7043: : 5it [00:04,  1.33it/s]\u001b[A\n",
      "batch 6, training loss: 3.7043: : 6it [00:04,  1.43it/s]\u001b[A\n",
      "batch 7, training loss: 3.6946: : 6it [00:05,  1.43it/s]\u001b[A\n",
      "batch 7, training loss: 3.6946: : 7it [00:05,  1.48it/s]\u001b[A\n",
      "batch 8, training loss: 3.6711: : 7it [00:06,  1.48it/s]\u001b[A\n",
      "batch 8, training loss: 3.6711: : 8it [00:06,  1.51it/s]\u001b[A\n",
      "batch 9, training loss: 3.5311: : 8it [00:06,  1.51it/s]\u001b[A\n",
      "batch 9, training loss: 3.5311: : 9it [00:06,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.5327: : 9it [00:07,  1.52it/s]\u001b[A\n",
      "batch 10, training loss: 3.5327: : 10it [00:07,  1.48it/s]\u001b[A\n",
      "batch 11, training loss: 3.5815: : 10it [00:08,  1.48it/s]\u001b[A\n",
      "batch 11, training loss: 3.5815: : 11it [00:08,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 3.6244: : 11it [00:08,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 3.6244: : 12it [00:08,  1.51it/s]\u001b[A\n",
      "batch 13, training loss: 3.5111: : 12it [00:09,  1.51it/s]\u001b[A\n",
      "batch 13, training loss: 3.5111: : 13it [00:09,  1.56it/s]\u001b[A\n",
      "batch 14, training loss: 3.7873: : 13it [00:10,  1.56it/s]\u001b[A\n",
      "batch 14, training loss: 3.7873: : 14it [00:10,  1.64it/s]\u001b[A\n",
      "batch 15, training loss: 3.6451: : 14it [00:10,  1.64it/s]\u001b[A\n",
      "batch 15, training loss: 3.6451: : 15it [00:10,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.6269: : 15it [00:11,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.6269: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.7783: : 16it [00:12,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.7783: : 17it [00:12,  1.51it/s]\u001b[A\n",
      "batch 18, training loss: 3.5931: : 17it [00:12,  1.51it/s]\u001b[A\n",
      "batch 18, training loss: 3.5931: : 18it [00:12,  1.55it/s]\u001b[A\n",
      "batch 19, training loss: 3.3777: : 18it [00:13,  1.55it/s]\u001b[A\n",
      "batch 19, training loss: 3.3777: : 19it [00:13,  1.57it/s]\u001b[A\n",
      "batch 20, training loss: 3.5615: : 19it [00:13,  1.57it/s]\u001b[A\n",
      "batch 20, training loss: 3.5615: : 20it [00:13,  1.60it/s]\u001b[A\n",
      "batch 21, training loss: 3.6801: : 20it [00:14,  1.60it/s]\u001b[A\n",
      "batch 21, training loss: 3.6801: : 21it [00:14,  1.58it/s]\u001b[A\n",
      "batch 22, training loss: 3.4537: : 21it [00:15,  1.58it/s]\u001b[A\n",
      "batch 22, training loss: 3.4537: : 22it [00:15,  1.52it/s]\u001b[A\n",
      "batch 23, training loss: 3.6026: : 22it [00:15,  1.52it/s]\u001b[A\n",
      "batch 23, training loss: 3.6026: : 23it [00:15,  1.49it/s]\u001b[A\n",
      "batch 24, training loss: 3.5132: : 23it [00:16,  1.49it/s]\u001b[A\n",
      "batch 24, training loss: 3.5132: : 24it [00:16,  1.48it/s]\u001b[A\n",
      "batch 25, training loss: 3.6038: : 24it [00:17,  1.48it/s]\u001b[A\n",
      "batch 25, training loss: 3.6038: : 25it [00:17,  1.48it/s]\u001b[A\n",
      "batch 26, training loss: 3.4386: : 25it [00:17,  1.48it/s]\u001b[A\n",
      "batch 26, training loss: 3.4386: : 26it [00:17,  1.52it/s]\u001b[A\n",
      "batch 27, training loss: 3.5532: : 26it [00:18,  1.52it/s]\u001b[A\n",
      "batch 27, training loss: 3.5532: : 27it [00:18,  1.54it/s]\u001b[A\n",
      "batch 28, training loss: 3.4271: : 27it [00:19,  1.54it/s]\u001b[A\n",
      "batch 28, training loss: 3.4271: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 3.5833: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 3.5833: : 29it [00:19,  1.55it/s]\u001b[A\n",
      "batch 30, training loss: 3.7044: : 29it [00:20,  1.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, training loss: 3.7044: : 30it [00:20,  1.51it/s]\u001b[A\n",
      "batch 31, training loss: 3.4535: : 30it [00:21,  1.51it/s]\u001b[A\n",
      "batch 31, training loss: 3.4535: : 31it [00:21,  1.48it/s]\u001b[A\n",
      "batch 32, training loss: 3.6358: : 31it [00:21,  1.48it/s]\u001b[A\n",
      "batch 32, training loss: 3.6358: : 32it [00:21,  1.49it/s]\u001b[A\n",
      "batch 33, training loss: 3.5306: : 32it [00:22,  1.49it/s]\u001b[A\n",
      "batch 33, training loss: 3.5306: : 33it [00:22,  1.55it/s]\u001b[A\n",
      "batch 34, training loss: 3.5378: : 33it [00:23,  1.55it/s]\u001b[A\n",
      "batch 34, training loss: 3.5378: : 34it [00:23,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 3.5943: : 34it [00:23,  1.66it/s]\u001b[A\n",
      "batch 35, training loss: 3.5943: : 35it [00:23,  1.63it/s]\u001b[A\n",
      "batch 36, training loss: 3.6775: : 35it [00:24,  1.63it/s]\u001b[A\n",
      "batch 36, training loss: 3.6775: : 36it [00:24,  1.58it/s]\u001b[A\n",
      "batch 37, training loss: 3.5282: : 36it [00:24,  1.58it/s]\u001b[A\n",
      "batch 37, training loss: 3.5282: : 37it [00:24,  1.69it/s]\u001b[A\n",
      "batch 38, training loss: 3.3202: : 37it [00:25,  1.69it/s]\u001b[A\n",
      "batch 38, training loss: 3.3202: : 38it [00:25,  1.71it/s]\u001b[A\n",
      "batch 39, training loss: 3.4854: : 38it [00:26,  1.71it/s]\u001b[A\n",
      "batch 39, training loss: 3.4854: : 39it [00:26,  1.65it/s]\u001b[A\n",
      "batch 40, training loss: 3.556: : 39it [00:26,  1.65it/s] \u001b[A\n",
      "batch 40, training loss: 3.556: : 40it [00:26,  1.59it/s]\u001b[A\n",
      "batch 41, training loss: 3.7159: : 40it [00:27,  1.59it/s]\u001b[A\n",
      "batch 41, training loss: 3.7159: : 41it [00:27,  1.56it/s]\u001b[A\n",
      "batch 42, training loss: 3.5436: : 41it [00:27,  1.56it/s]\u001b[A\n",
      "batch 42, training loss: 3.5436: : 42it [00:27,  1.68it/s]\u001b[A\n",
      "batch 43, training loss: 3.3537: : 42it [00:28,  1.68it/s]\u001b[A\n",
      "batch 43, training loss: 3.3537: : 43it [00:28,  1.70it/s]\u001b[A\n",
      "batch 44, training loss: 3.3263: : 43it [00:29,  1.70it/s]\u001b[A\n",
      "batch 44, training loss: 3.3263: : 44it [00:29,  1.65it/s]\u001b[A\n",
      "batch 45, training loss: 3.3695: : 44it [00:29,  1.65it/s]\u001b[A\n",
      "batch 45, training loss: 3.3695: : 45it [00:29,  1.60it/s]\u001b[A\n",
      "batch 46, training loss: 3.4989: : 45it [00:30,  1.60it/s]\u001b[A\n",
      "batch 46, training loss: 3.4989: : 46it [00:30,  1.59it/s]\u001b[A\n",
      "batch 47, training loss: 3.5085: : 46it [00:30,  1.59it/s]\u001b[A\n",
      "batch 47, training loss: 3.5085: : 47it [00:30,  1.63it/s]\u001b[A\n",
      "batch 48, training loss: 3.4227: : 47it [00:31,  1.63it/s]\u001b[A\n",
      "batch 48, training loss: 3.4227: : 48it [00:31,  1.69it/s]\u001b[A\n",
      "batch 49, training loss: 3.6808: : 48it [00:32,  1.69it/s]\u001b[A\n",
      "batch 49, training loss: 3.6808: : 49it [00:32,  1.64it/s]\u001b[A\n",
      "batch 50, training loss: 3.4379: : 49it [00:32,  1.64it/s]\u001b[A\n",
      "batch 50, training loss: 3.4379: : 50it [00:32,  1.57it/s]\u001b[A\n",
      "batch 51, training loss: 3.5302: : 50it [00:33,  1.57it/s]\u001b[A\n",
      "batch 51, training loss: 3.5302: : 51it [00:33,  1.52it/s]\u001b[A\n",
      "batch 52, training loss: 3.3812: : 51it [00:34,  1.52it/s]\u001b[A\n",
      "batch 52, training loss: 3.3812: : 52it [00:34,  1.52it/s]\u001b[A\n",
      "batch 53, training loss: 3.4017: : 52it [00:34,  1.52it/s]\u001b[A\n",
      "batch 53, training loss: 3.4017: : 53it [00:34,  1.64it/s]\u001b[A\n",
      "batch 54, training loss: 3.3524: : 53it [00:35,  1.64it/s]\u001b[A\n",
      "batch 54, training loss: 3.3524: : 54it [00:35,  1.82it/s]\u001b[A\n",
      "batch 55, training loss: 3.4767: : 54it [00:35,  1.82it/s]\u001b[A\n",
      "batch 55, training loss: 3.4767: : 55it [00:35,  1.97it/s]\u001b[A\n",
      "batch 56, training loss: 3.5158: : 55it [00:36,  1.97it/s]\u001b[A\n",
      "batch 56, training loss: 3.5158: : 56it [00:36,  1.89it/s]\u001b[A\n",
      "batch 57, training loss: 3.4723: : 56it [00:36,  1.89it/s]\u001b[A\n",
      "batch 57, training loss: 3.4723: : 57it [00:36,  1.75it/s]\u001b[A\n",
      "batch 58, training loss: 3.5367: : 57it [00:37,  1.75it/s]\u001b[A\n",
      "batch 58, training loss: 3.5367: : 58it [00:37,  1.64it/s]\u001b[A\n",
      "batch 59, training loss: 3.3881: : 58it [00:38,  1.64it/s]\u001b[A\n",
      "batch 59, training loss: 3.3881: : 59it [00:38,  1.58it/s]\u001b[A\n",
      "batch 60, training loss: 3.3525: : 59it [00:38,  1.58it/s]\u001b[A\n",
      "batch 60, training loss: 3.3525: : 60it [00:38,  1.61it/s]\u001b[A\n",
      "batch 61, training loss: 3.5922: : 60it [00:39,  1.61it/s]\u001b[A\n",
      "batch 61, training loss: 3.5922: : 61it [00:39,  1.63it/s]\u001b[A\n",
      "batch 62, training loss: 3.4355: : 61it [00:39,  1.63it/s]\u001b[A\n",
      "batch 62, training loss: 3.4355: : 62it [00:39,  1.68it/s]\u001b[A\n",
      "batch 63, training loss: 3.5193: : 62it [00:40,  1.68it/s]\u001b[A\n",
      "batch 63, training loss: 3.5193: : 63it [00:40,  1.62it/s]\u001b[A\n",
      "batch 64, training loss: 3.4711: : 63it [00:41,  1.62it/s]\u001b[A\n",
      "batch 64, training loss: 3.4711: : 64it [00:41,  1.56it/s]\u001b[A\n",
      "batch 65, training loss: 3.5579: : 64it [00:42,  1.56it/s]\u001b[A\n",
      "batch 65, training loss: 3.5579: : 65it [00:42,  1.50it/s]\u001b[A\n",
      "batch 66, training loss: 3.5903: : 65it [00:42,  1.50it/s]\u001b[A\n",
      "batch 66, training loss: 3.5903: : 66it [00:42,  1.49it/s]\u001b[A\n",
      "batch 67, training loss: 3.4355: : 66it [00:43,  1.49it/s]\u001b[A\n",
      "batch 67, training loss: 3.4355: : 67it [00:43,  1.48it/s]\u001b[A\n",
      "batch 68, training loss: 3.5349: : 67it [00:44,  1.48it/s]\u001b[A\n",
      "batch 68, training loss: 3.5349: : 68it [00:44,  1.52it/s]\u001b[A\n",
      "batch 69, training loss: 3.4159: : 68it [00:44,  1.52it/s]\u001b[A\n",
      "batch 69, training loss: 3.4159: : 69it [00:44,  1.56it/s]\u001b[A\n",
      "batch 70, training loss: 3.6144: : 69it [00:45,  1.56it/s]\u001b[A\n",
      "batch 70, training loss: 3.6144: : 70it [00:45,  1.59it/s]\u001b[A\n",
      "batch 71, training loss: 3.4511: : 70it [00:45,  1.59it/s]\u001b[A\n",
      "batch 71, training loss: 3.4511: : 71it [00:45,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 3.4397: : 71it [00:46,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 3.4397: : 72it [00:46,  1.54it/s]\u001b[A\n",
      "batch 73, training loss: 3.4664: : 72it [00:47,  1.54it/s]\u001b[A\n",
      "batch 73, training loss: 3.4664: : 73it [00:47,  1.53it/s]\u001b[A\n",
      "batch 74, training loss: 3.3819: : 73it [00:47,  1.53it/s]\u001b[A\n",
      "batch 74, training loss: 3.3819: : 74it [00:47,  1.55it/s]\u001b[A\n",
      "batch 75, training loss: 3.5873: : 74it [00:48,  1.55it/s]\u001b[A\n",
      "batch 75, training loss: 3.5873: : 75it [00:48,  1.59it/s]\u001b[A\n",
      "batch 76, training loss: 3.404: : 75it [00:49,  1.59it/s] \u001b[A\n",
      "batch 76, training loss: 3.404: : 76it [00:49,  1.58it/s]\u001b[A\n",
      "batch 77, training loss: 3.5448: : 76it [00:49,  1.58it/s]\u001b[A\n",
      "batch 77, training loss: 3.5448: : 77it [00:49,  1.53it/s]\u001b[A\n",
      "batch 78, training loss: 3.5097: : 77it [00:50,  1.53it/s]\u001b[A\n",
      "batch 78, training loss: 3.5097: : 78it [00:50,  1.49it/s]\u001b[A\n",
      "batch 79, training loss: 3.557: : 78it [00:51,  1.49it/s] \u001b[A\n",
      "batch 79, training loss: 3.557: : 79it [00:51,  1.49it/s]\u001b[A\n",
      "batch 80, training loss: 3.4656: : 79it [00:51,  1.49it/s]\u001b[A\n",
      "batch 80, training loss: 3.4656: : 80it [00:51,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.4722: : 80it [00:52,  1.60it/s]\u001b[A\n",
      "batch 81, training loss: 3.4722: : 81it [00:52,  1.63it/s]\u001b[A\n",
      "batch 82, training loss: 3.6165: : 81it [00:52,  1.63it/s]\u001b[A\n",
      "batch 82, training loss: 3.6165: : 82it [00:52,  1.61it/s]\u001b[A\n",
      "batch 83, training loss: 3.4759: : 82it [00:53,  1.61it/s]\u001b[A\n",
      "batch 83, training loss: 3.4759: : 83it [00:53,  1.58it/s]\u001b[A\n",
      "batch 84, training loss: 3.6298: : 83it [00:54,  1.58it/s]\u001b[A\n",
      "batch 84, training loss: 3.6298: : 84it [00:54,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 3.6266: : 84it [00:54,  1.55it/s]\u001b[A\n",
      "batch 85, training loss: 3.6266: : 85it [00:54,  1.57it/s]\u001b[A\n",
      "batch 86, training loss: 3.5835: : 85it [00:55,  1.57it/s]\u001b[A\n",
      "batch 86, training loss: 3.5835: : 86it [00:55,  1.59it/s]\u001b[A\n",
      "batch 87, training loss: 3.6155: : 86it [00:56,  1.59it/s]\u001b[A\n",
      "batch 87, training loss: 3.6155: : 87it [00:56,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 3.7163: : 87it [00:56,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 3.7163: : 88it [00:56,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 3.8258: : 88it [00:57,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 3.8258: : 89it [00:57,  1.45it/s]\u001b[A\n",
      "batch 90, training loss: 3.7338: : 89it [00:58,  1.45it/s]\u001b[A\n",
      "batch 90, training loss: 3.7338: : 90it [00:58,  1.42it/s]\u001b[A\n",
      "batch 91, training loss: 3.8803: : 90it [00:59,  1.42it/s]\u001b[A\n",
      "batch 91, training loss: 3.8803: : 91it [00:59,  1.37it/s]\u001b[A\n",
      "batch 92, training loss: 3.7044: : 91it [00:59,  1.37it/s]\u001b[A\n",
      "batch 92, training loss: 3.7044: : 92it [00:59,  1.35it/s]\u001b[A\n",
      "batch 93, training loss: 3.6448: : 92it [01:00,  1.35it/s]\u001b[A\n",
      "batch 93, training loss: 3.6448: : 93it [01:00,  1.34it/s]\u001b[A\n",
      "batch 94, training loss: 3.7681: : 93it [01:01,  1.34it/s]\u001b[A\n",
      "batch 94, training loss: 3.7681: : 94it [01:01,  1.33it/s]\u001b[A\n",
      "batch 95, training loss: 3.7321: : 94it [01:02,  1.33it/s]\u001b[A\n",
      "batch 95, training loss: 3.7321: : 95it [01:02,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 3.7152: : 95it [01:02,  1.32it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 96, training loss: 3.7152: : 96it [01:02,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 3.7907: : 96it [01:03,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 3.7907: : 97it [01:03,  1.30it/s]\u001b[A\n",
      "batch 98, training loss: 3.7689: : 97it [01:04,  1.30it/s]\u001b[A\n",
      "batch 98, training loss: 3.7689: : 98it [01:04,  1.29it/s]\u001b[A\n",
      "batch 99, training loss: 3.586: : 98it [01:05,  1.29it/s] \u001b[A\n",
      "batch 99, training loss: 3.586: : 99it [01:05,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.4086: : 99it [01:06,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.4086: : 100it [01:06,  1.32it/s]\u001b[A\n",
      "batch 101, training loss: 3.6008: : 100it [01:06,  1.32it/s]\u001b[A\n",
      "batch 101, training loss: 3.6008: : 101it [01:06,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.5925: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.5925: : 102it [01:07,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.5435: : 102it [01:08,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.5435: : 103it [01:08,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.3733: : 103it [01:09,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.3733: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.5445: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.5445: : 105it [01:09,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.6656: : 105it [01:10,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.6656: : 106it [01:10,  1.30it/s]\u001b[A\n",
      "batch 107, training loss: 3.3804: : 106it [01:11,  1.30it/s]\u001b[A\n",
      "batch 107, training loss: 3.3804: : 107it [01:11,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.6456: : 107it [01:12,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.6456: : 108it [01:12,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 3.6977: : 108it [01:12,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 3.6977: : 109it [01:12,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.8122: : 109it [01:13,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.8122: : 110it [01:13,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.5352: : 110it [01:14,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.5352: : 111it [01:14,  1.29it/s]\u001b[A\n",
      "batch 112, training loss: 3.58: : 111it [01:15,  1.29it/s]  \u001b[A\n",
      "batch 112, training loss: 3.58: : 112it [01:15,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.6693: : 112it [01:16,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.6693: : 113it [01:16,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 3.6506: : 113it [01:16,  1.29it/s]\u001b[A\n",
      "batch 114, training loss: 3.6506: : 114it [01:16,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.5598: : 114it [01:17,  1.28it/s]\u001b[A\n",
      "batch 115, training loss: 3.5598: : 115it [01:17,  1.28it/s]\u001b[A\n",
      "batch 116, training loss: 3.5708: : 115it [01:18,  1.28it/s]\u001b[A\n",
      "batch 116, training loss: 3.5708: : 116it [01:18,  1.28it/s]\u001b[A\n",
      "batch 117, training loss: 3.6209: : 116it [01:19,  1.28it/s]\u001b[A\n",
      "batch 117, training loss: 3.6209: : 117it [01:19,  1.29it/s]\u001b[A\n",
      "batch 118, training loss: 3.7009: : 117it [01:19,  1.29it/s]\u001b[A\n",
      "batch 118, training loss: 3.7009: : 118it [01:19,  1.29it/s]\u001b[A\n",
      "batch 119, training loss: 3.6028: : 118it [01:20,  1.29it/s]\u001b[A\n",
      "batch 119, training loss: 3.6028: : 119it [01:20,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.5391: : 119it [01:21,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.5391: : 120it [01:21,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 3.7931: : 120it [01:22,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 3.7931: : 121it [01:22,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.4561: : 121it [01:22,  1.30it/s]\u001b[A\n",
      "batch 122, training loss: 3.4561: : 122it [01:22,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 3.6263: : 122it [01:23,  1.30it/s]\u001b[A\n",
      "batch 123, training loss: 3.6263: : 123it [01:23,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.6273: : 123it [01:24,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.6273: : 124it [01:24,  1.36it/s]\u001b[A\n",
      "batch 125, training loss: 3.6535: : 124it [01:25,  1.36it/s]\u001b[A\n",
      "batch 125, training loss: 3.6535: : 125it [01:25,  1.44it/s]\u001b[A\n",
      "batch 126, training loss: 3.6988: : 125it [01:25,  1.44it/s]\u001b[A\n",
      "batch 126, training loss: 3.6988: : 126it [01:25,  1.42it/s]\u001b[A\n",
      "batch 127, training loss: 3.4187: : 126it [01:26,  1.42it/s]\u001b[A\n",
      "batch 127, training loss: 3.4187: : 127it [01:26,  1.39it/s]\u001b[A\n",
      "batch 128, training loss: 3.6305: : 127it [01:27,  1.39it/s]\u001b[A\n",
      "batch 128, training loss: 3.6305: : 128it [01:27,  1.38it/s]\u001b[A\n",
      "batch 129, training loss: 3.5623: : 128it [01:28,  1.38it/s]\u001b[A\n",
      "batch 129, training loss: 3.5623: : 129it [01:28,  1.34it/s]\u001b[A\n",
      "batch 130, training loss: 3.585: : 129it [01:28,  1.34it/s] \u001b[A\n",
      "batch 130, training loss: 3.585: : 130it [01:28,  1.33it/s]\u001b[A\n",
      "batch 131, training loss: 3.7467: : 130it [01:29,  1.33it/s]\u001b[A\n",
      "batch 131, training loss: 3.7467: : 131it [01:29,  1.31it/s]\u001b[A\n",
      "batch 132, training loss: 3.5503: : 131it [01:30,  1.31it/s]\u001b[A\n",
      "batch 132, training loss: 3.5503: : 132it [01:30,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 3.4498: : 132it [01:31,  1.33it/s]\u001b[A\n",
      "batch 133, training loss: 3.4498: : 133it [01:31,  1.31it/s]\u001b[A\n",
      "batch 134, training loss: 3.622: : 133it [01:31,  1.31it/s] \u001b[A\n",
      "batch 134, training loss: 3.622: : 134it [01:31,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.5187: : 134it [01:32,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.5187: : 135it [01:32,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 3.4742: : 135it [01:33,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 3.4742: : 136it [01:33,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 3.6415: : 136it [01:34,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 3.6415: : 137it [01:34,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 3.649: : 137it [01:34,  1.30it/s] \u001b[A\n",
      "batch 138, training loss: 3.649: : 138it [01:34,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 3.4323: : 138it [01:35,  1.31it/s]\u001b[A\n",
      "batch 139, training loss: 3.4323: : 139it [01:35,  1.32it/s]\u001b[A\n",
      "batch 140, training loss: 3.5406: : 139it [01:36,  1.32it/s]\u001b[A\n",
      "batch 140, training loss: 3.5406: : 140it [01:36,  1.31it/s]\u001b[A\n",
      "batch 141, training loss: 3.5222: : 140it [01:37,  1.31it/s]\u001b[A\n",
      "batch 141, training loss: 3.5222: : 141it [01:37,  1.31it/s]\u001b[A\n",
      "batch 142, training loss: 3.5791: : 141it [01:37,  1.31it/s]\u001b[A\n",
      "batch 142, training loss: 3.5791: : 142it [01:37,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.3962: : 142it [01:38,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.3962: : 143it [01:38,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.4443: : 143it [01:39,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.4443: : 144it [01:39,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.5632: : 144it [01:40,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.5632: : 145it [01:40,  1.31it/s]\u001b[A\n",
      "batch 146, training loss: 3.5529: : 145it [01:41,  1.31it/s]\u001b[A\n",
      "batch 146, training loss: 3.5529: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.4387: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.4387: : 147it [01:41,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.5379: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.5379: : 148it [01:42,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.6255: : 148it [01:43,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.6255: : 149it [01:43,  1.30it/s]\u001b[A\n",
      "batch 150, training loss: 3.622: : 149it [01:44,  1.30it/s] \u001b[A\n",
      "batch 150, training loss: 3.622: : 150it [01:44,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.5471: : 150it [01:44,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.5471: : 151it [01:44,  1.29it/s]\u001b[A\n",
      "batch 152, training loss: 3.4773: : 151it [01:45,  1.29it/s]\u001b[A\n",
      "batch 152, training loss: 3.4773: : 152it [01:45,  1.30it/s]\u001b[A\n",
      "batch 153, training loss: 3.493: : 152it [01:46,  1.30it/s] \u001b[A\n",
      "batch 153, training loss: 3.493: : 153it [01:46,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.6127: : 153it [01:47,  1.30it/s]\u001b[A\n",
      "batch 154, training loss: 3.6127: : 154it [01:47,  1.28it/s]\u001b[A\n",
      "batch 155, training loss: 3.7511: : 154it [01:48,  1.28it/s]\u001b[A\n",
      "batch 155, training loss: 3.7511: : 155it [01:48,  1.28it/s]\u001b[A\n",
      "batch 156, training loss: 3.3632: : 155it [01:48,  1.28it/s]\u001b[A\n",
      "batch 156, training loss: 3.3632: : 156it [01:48,  1.29it/s]\u001b[A\n",
      "batch 157, training loss: 3.5708: : 156it [01:49,  1.29it/s]\u001b[A\n",
      "batch 157, training loss: 3.5708: : 157it [01:49,  1.28it/s]\u001b[A\n",
      "batch 158, training loss: 3.5429: : 157it [01:50,  1.28it/s]\u001b[A\n",
      "batch 158, training loss: 3.5429: : 158it [01:50,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.451: : 158it [01:51,  1.29it/s] \u001b[A\n",
      "batch 159, training loss: 3.451: : 159it [01:51,  1.29it/s]\u001b[A\n",
      "batch 160, training loss: 3.6405: : 159it [01:51,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 160, training loss: 3.6405: : 160it [01:51,  1.28it/s]\u001b[A\n",
      "batch 161, training loss: 3.4926: : 160it [01:52,  1.28it/s]\u001b[A\n",
      "batch 161, training loss: 3.4926: : 161it [01:52,  1.29it/s]\u001b[A\n",
      "batch 162, training loss: 3.4824: : 161it [01:53,  1.29it/s]\u001b[A\n",
      "batch 162, training loss: 3.4824: : 162it [01:53,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 3.7218: : 162it [01:54,  1.30it/s]\u001b[A\n",
      "batch 163, training loss: 3.7218: : 163it [01:54,  1.29it/s]\u001b[A\n",
      "batch 164, training loss: 3.513: : 163it [01:55,  1.29it/s] \u001b[A\n",
      "batch 164, training loss: 3.513: : 164it [01:55,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 3.5162: : 164it [01:55,  1.29it/s]\u001b[A\n",
      "batch 165, training loss: 3.5162: : 165it [01:55,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.4967: : 165it [01:56,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.4967: : 166it [01:56,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.6327: : 166it [01:57,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.6327: : 167it [01:57,  1.28it/s]\u001b[A\n",
      "batch 168, training loss: 3.5937: : 167it [01:58,  1.28it/s]\u001b[A\n",
      "batch 168, training loss: 3.5937: : 168it [01:58,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 3.6688: : 168it [01:58,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 3.6688: : 169it [01:58,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.5354: : 169it [01:59,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.5354: : 170it [01:59,  1.28it/s]\u001b[A\n",
      "batch 171, training loss: 2.7288: : 170it [01:59,  1.28it/s]\u001b[A\n",
      "batch 171, training loss: 2.7288: : 171it [01:59,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.9237: : 171it [02:00,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.9237: : 172it [02:00,  1.45it/s]\u001b[A\n",
      "batch 173, training loss: 3.7775: : 172it [02:01,  1.45it/s]\u001b[A\n",
      "batch 173, training loss: 3.7775: : 173it [02:01,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 3.9496: : 173it [02:02,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 3.9496: : 174it [02:02,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 3.8684: : 174it [02:03,  1.32it/s]\u001b[A\n",
      "batch 175, training loss: 3.8684: : 175it [02:03,  1.28it/s]\u001b[A\n",
      "batch 176, training loss: 3.8031: : 175it [02:04,  1.28it/s]\u001b[A\n",
      "batch 176, training loss: 3.8031: : 176it [02:04,  1.26it/s]\u001b[A\n",
      "batch 177, training loss: 3.7888: : 176it [02:04,  1.26it/s]\u001b[A\n",
      "batch 177, training loss: 3.7888: : 177it [02:04,  1.26it/s]\u001b[A\n",
      "batch 178, training loss: 3.793: : 177it [02:05,  1.26it/s] \u001b[A\n",
      "batch 178, training loss: 3.793: : 178it [02:05,  1.25it/s]\u001b[A\n",
      "batch 179, training loss: 3.827: : 178it [02:06,  1.25it/s]\u001b[A\n",
      "batch 179, training loss: 3.827: : 179it [02:06,  1.27it/s]\u001b[A\n",
      "batch 180, training loss: 3.8659: : 179it [02:07,  1.27it/s]\u001b[A\n",
      "batch 180, training loss: 3.8659: : 180it [02:07,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 3.9136: : 180it [02:08,  1.25it/s]\u001b[A\n",
      "batch 181, training loss: 3.9136: : 181it [02:08,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.8209: : 181it [02:08,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.8209: : 182it [02:08,  1.22it/s]\u001b[A\n",
      "batch 183, training loss: 3.9298: : 182it [02:09,  1.22it/s]\u001b[A\n",
      "batch 183, training loss: 3.9298: : 183it [02:09,  1.21it/s]\u001b[A\n",
      "batch 184, training loss: 3.6764: : 183it [02:10,  1.21it/s]\u001b[A\n",
      "batch 184, training loss: 3.6764: : 184it [02:10,  1.22it/s]\u001b[A\n",
      "batch 185, training loss: 3.7045: : 184it [02:11,  1.22it/s]\u001b[A\n",
      "batch 185, training loss: 3.7045: : 185it [02:11,  1.21it/s]\u001b[A\n",
      "batch 186, training loss: 3.8506: : 185it [02:12,  1.21it/s]\u001b[A\n",
      "batch 186, training loss: 3.8506: : 186it [02:12,  1.21it/s]\u001b[A\n",
      "batch 187, training loss: 3.8342: : 186it [02:13,  1.21it/s]\u001b[A\n",
      "batch 187, training loss: 3.8342: : 187it [02:13,  1.21it/s]\u001b[A\n",
      "batch 188, training loss: 3.9134: : 187it [02:13,  1.21it/s]\u001b[A\n",
      "batch 188, training loss: 3.9134: : 188it [02:13,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.9498: : 188it [02:14,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.9498: : 189it [02:14,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.6193: : 189it [02:15,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.6193: : 190it [02:15,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.6281: : 190it [02:16,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.6281: : 191it [02:16,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.7179: : 191it [02:17,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.7179: : 192it [02:17,  1.22it/s]\u001b[A\n",
      "batch 193, training loss: 3.5963: : 192it [02:17,  1.22it/s]\u001b[A\n",
      "batch 193, training loss: 3.5963: : 193it [02:17,  1.22it/s]\u001b[A\n",
      "batch 194, training loss: 3.6905: : 193it [02:18,  1.22it/s]\u001b[A\n",
      "batch 194, training loss: 3.6905: : 194it [02:18,  1.21it/s]\u001b[A\n",
      "batch 195, training loss: 3.6299: : 194it [02:19,  1.21it/s]\u001b[A\n",
      "batch 195, training loss: 3.6299: : 195it [02:19,  1.22it/s]\u001b[A\n",
      "batch 196, training loss: 3.5676: : 195it [02:20,  1.22it/s]\u001b[A\n",
      "batch 196, training loss: 3.5676: : 196it [02:20,  1.23it/s]\u001b[A\n",
      "batch 197, training loss: 3.746: : 196it [02:21,  1.23it/s] \u001b[A\n",
      "batch 197, training loss: 3.746: : 197it [02:21,  1.25it/s]\u001b[A\n",
      "batch 198, training loss: 3.6592: : 197it [02:21,  1.25it/s]\u001b[A\n",
      "batch 198, training loss: 3.6592: : 198it [02:21,  1.26it/s]\u001b[A\n",
      "batch 199, training loss: 3.5999: : 198it [02:22,  1.26it/s]\u001b[A\n",
      "batch 199, training loss: 3.5999: : 199it [02:22,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.7764: : 199it [02:23,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.7764: : 200it [02:23,  1.23it/s]\u001b[A\n",
      "batch 201, training loss: 3.5549: : 200it [02:24,  1.23it/s]\u001b[A\n",
      "batch 201, training loss: 3.5549: : 201it [02:24,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.4421: : 201it [02:25,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.4421: : 202it [02:25,  1.22it/s]\u001b[A\n",
      "batch 203, training loss: 3.6534: : 202it [02:26,  1.22it/s]\u001b[A\n",
      "batch 203, training loss: 3.6534: : 203it [02:26,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.694: : 203it [02:26,  1.24it/s] \u001b[A\n",
      "batch 204, training loss: 3.694: : 204it [02:26,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.6354: : 204it [02:27,  1.24it/s]\u001b[A\n",
      "batch 205, training loss: 3.6354: : 205it [02:27,  1.22it/s]\u001b[A\n",
      "batch 206, training loss: 3.7685: : 205it [02:28,  1.22it/s]\u001b[A\n",
      "batch 206, training loss: 3.7685: : 206it [02:28,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 3.5791: : 206it [02:29,  1.22it/s]\u001b[A\n",
      "batch 207, training loss: 3.5791: : 207it [02:29,  1.21it/s]\u001b[A\n",
      "batch 208, training loss: 3.6185: : 207it [02:30,  1.21it/s]\u001b[A\n",
      "batch 208, training loss: 3.6185: : 208it [02:30,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 3.5866: : 208it [02:30,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 3.5866: : 209it [02:30,  1.22it/s]\u001b[A\n",
      "batch 210, training loss: 3.6331: : 209it [02:31,  1.22it/s]\u001b[A\n",
      "batch 210, training loss: 3.6331: : 210it [02:31,  1.21it/s]\u001b[A\n",
      "batch 211, training loss: 3.5952: : 210it [02:32,  1.21it/s]\u001b[A\n",
      "batch 211, training loss: 3.5952: : 211it [02:32,  1.22it/s]\u001b[A\n",
      "batch 212, training loss: 3.5935: : 211it [02:33,  1.22it/s]\u001b[A\n",
      "batch 212, training loss: 3.5935: : 212it [02:33,  1.22it/s]\u001b[A\n",
      "batch 213, training loss: 3.7987: : 212it [02:34,  1.22it/s]\u001b[A\n",
      "batch 213, training loss: 3.7987: : 213it [02:34,  1.22it/s]\u001b[A\n",
      "batch 214, training loss: 3.6612: : 213it [02:35,  1.22it/s]\u001b[A\n",
      "batch 214, training loss: 3.6612: : 214it [02:35,  1.22it/s]\u001b[A\n",
      "batch 215, training loss: 3.4965: : 214it [02:35,  1.22it/s]\u001b[A\n",
      "batch 215, training loss: 3.4965: : 215it [02:35,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.6458: : 215it [02:36,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.6458: : 216it [02:36,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.7503: : 216it [02:37,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.7503: : 217it [02:37,  1.22it/s]\u001b[A\n",
      "batch 218, training loss: 3.6808: : 217it [02:38,  1.22it/s]\u001b[A\n",
      "batch 218, training loss: 3.6808: : 218it [02:38,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.8789: : 218it [02:39,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.8789: : 219it [02:39,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.788: : 219it [02:39,  1.24it/s] \u001b[A\n",
      "batch 220, training loss: 3.788: : 220it [02:39,  1.23it/s]\u001b[A\n",
      "batch 221, training loss: 3.6607: : 220it [02:40,  1.23it/s]\u001b[A\n",
      "batch 221, training loss: 3.6607: : 221it [02:40,  1.23it/s]\u001b[A\n",
      "batch 222, training loss: 3.6979: : 221it [02:41,  1.23it/s]\u001b[A\n",
      "batch 222, training loss: 3.6979: : 222it [02:41,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.7025: : 222it [02:42,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.7025: : 223it [02:42,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.5816: : 223it [02:43,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 224, training loss: 3.5816: : 224it [02:43,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.7291: : 224it [02:44,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.7291: : 225it [02:44,  1.22it/s]\u001b[A\n",
      "batch 226, training loss: 3.7401: : 225it [02:44,  1.22it/s]\u001b[A\n",
      "batch 226, training loss: 3.7401: : 226it [02:44,  1.21it/s]\u001b[A\n",
      "batch 227, training loss: 3.5993: : 226it [02:45,  1.21it/s]\u001b[A\n",
      "batch 227, training loss: 3.5993: : 227it [02:45,  1.21it/s]\u001b[A\n",
      "batch 228, training loss: 3.6726: : 227it [02:46,  1.21it/s]\u001b[A\n",
      "batch 228, training loss: 3.6726: : 228it [02:46,  1.22it/s]\u001b[A\n",
      "batch 229, training loss: 3.6491: : 228it [02:47,  1.22it/s]\u001b[A\n",
      "batch 229, training loss: 3.6491: : 229it [02:47,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 3.7768: : 229it [02:48,  1.23it/s]\u001b[A\n",
      "batch 230, training loss: 3.7768: : 230it [02:48,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.8193: : 230it [02:48,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.8193: : 231it [02:48,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.6374: : 231it [02:49,  1.23it/s]\u001b[A\n",
      "batch 232, training loss: 3.6374: : 232it [02:49,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.689: : 232it [02:50,  1.23it/s] \u001b[A\n",
      "batch 233, training loss: 3.689: : 233it [02:50,  1.22it/s]\u001b[A\n",
      "batch 234, training loss: 3.8203: : 233it [02:51,  1.22it/s]\u001b[A\n",
      "batch 234, training loss: 3.8203: : 234it [02:51,  1.22it/s]\u001b[A\n",
      "batch 235, training loss: 3.6489: : 234it [02:52,  1.22it/s]\u001b[A\n",
      "batch 235, training loss: 3.6489: : 235it [02:52,  1.21it/s]\u001b[A\n",
      "batch 236, training loss: 3.6971: : 235it [02:53,  1.21it/s]\u001b[A\n",
      "batch 236, training loss: 3.6971: : 236it [02:53,  1.21it/s]\u001b[A\n",
      "batch 237, training loss: 3.5658: : 236it [02:53,  1.21it/s]\u001b[A\n",
      "batch 237, training loss: 3.5658: : 237it [02:53,  1.28it/s]\u001b[A\n",
      "batch 238, training loss: 3.7322: : 237it [02:54,  1.28it/s]\u001b[A\n",
      "batch 238, training loss: 3.7322: : 238it [02:54,  1.37it/s]\u001b[A\n",
      "batch 239, training loss: 3.633: : 238it [02:55,  1.37it/s] \u001b[A\n",
      "batch 239, training loss: 3.633: : 239it [02:55,  1.35it/s]\u001b[A\n",
      "batch 240, training loss: 3.6711: : 239it [02:55,  1.35it/s]\u001b[A\n",
      "batch 240, training loss: 3.6711: : 240it [02:55,  1.31it/s]\u001b[A\n",
      "batch 241, training loss: 3.6809: : 240it [02:56,  1.31it/s]\u001b[A\n",
      "batch 241, training loss: 3.6809: : 241it [02:56,  1.29it/s]\u001b[A\n",
      "batch 242, training loss: 3.5258: : 241it [02:57,  1.29it/s]\u001b[A\n",
      "batch 242, training loss: 3.5258: : 242it [02:57,  1.28it/s]\u001b[A\n",
      "batch 243, training loss: 3.6947: : 242it [02:58,  1.28it/s]\u001b[A\n",
      "batch 243, training loss: 3.6947: : 243it [02:58,  1.25it/s]\u001b[A\n",
      "batch 244, training loss: 3.5932: : 243it [02:59,  1.25it/s]\u001b[A\n",
      "batch 244, training loss: 3.5932: : 244it [02:59,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 3.7282: : 244it [03:00,  1.24it/s]\u001b[A\n",
      "batch 245, training loss: 3.7282: : 245it [03:00,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.6594: : 245it [03:00,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.6594: : 246it [03:00,  1.26it/s]\u001b[A\n",
      "batch 247, training loss: 3.6523: : 246it [03:01,  1.26it/s]\u001b[A\n",
      "batch 247, training loss: 3.6523: : 247it [03:01,  1.25it/s]\u001b[A\n",
      "batch 248, training loss: 3.5897: : 247it [03:02,  1.25it/s]\u001b[A\n",
      "batch 248, training loss: 3.5897: : 248it [03:02,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.5456: : 248it [03:03,  1.24it/s]\u001b[A\n",
      "batch 249, training loss: 3.5456: : 249it [03:03,  1.25it/s]\u001b[A\n",
      "batch 250, training loss: 3.7266: : 249it [03:04,  1.25it/s]\u001b[A\n",
      "batch 250, training loss: 3.7266: : 250it [03:04,  1.24it/s]\u001b[A\n",
      "batch 251, training loss: 3.7396: : 250it [03:04,  1.24it/s]\u001b[A\n",
      "batch 251, training loss: 3.7396: : 251it [03:04,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 3.1067: : 251it [03:05,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 3.1067: : 252it [03:05,  1.44it/s]\u001b[A\n",
      "batch 253, training loss: 3.8424: : 252it [03:06,  1.44it/s]\u001b[A\n",
      "batch 253, training loss: 3.8424: : 253it [03:06,  1.34it/s]\u001b[A\n",
      "batch 254, training loss: 3.975: : 253it [03:07,  1.34it/s] \u001b[A\n",
      "batch 254, training loss: 3.975: : 254it [03:07,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.7692: : 254it [03:07,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.7692: : 255it [03:07,  1.20it/s]\u001b[A\n",
      "batch 256, training loss: 3.7745: : 255it [03:08,  1.20it/s]\u001b[A\n",
      "batch 256, training loss: 3.7745: : 256it [03:08,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 3.8608: : 256it [03:09,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 3.8608: : 257it [03:09,  1.14it/s]\u001b[A\n",
      "batch 258, training loss: 3.9173: : 257it [03:10,  1.14it/s]\u001b[A\n",
      "batch 258, training loss: 3.9173: : 258it [03:10,  1.12it/s]\u001b[A\n",
      "batch 259, training loss: 3.7728: : 258it [03:11,  1.12it/s]\u001b[A\n",
      "batch 259, training loss: 3.7728: : 259it [03:11,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 3.743: : 259it [03:12,  1.12it/s] \u001b[A\n",
      "batch 260, training loss: 3.743: : 260it [03:12,  1.10it/s]\u001b[A\n",
      "batch 261, training loss: 3.8107: : 260it [03:13,  1.10it/s]\u001b[A\n",
      "batch 261, training loss: 3.8107: : 261it [03:13,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.7249: : 261it [03:14,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.7249: : 262it [03:14,  1.11it/s]\u001b[A\n",
      "batch 263, training loss: 3.7104: : 262it [03:15,  1.11it/s]\u001b[A\n",
      "batch 263, training loss: 3.7104: : 263it [03:15,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 3.7925: : 263it [03:16,  1.10it/s]\u001b[A\n",
      "batch 264, training loss: 3.7925: : 264it [03:16,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.588: : 264it [03:17,  1.10it/s] \u001b[A\n",
      "batch 265, training loss: 3.588: : 265it [03:17,  1.12it/s]\u001b[A\n",
      "batch 266, training loss: 3.7669: : 265it [03:17,  1.12it/s]\u001b[A\n",
      "batch 266, training loss: 3.7669: : 266it [03:17,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 3.734: : 266it [03:18,  1.10it/s] \u001b[A\n",
      "batch 267, training loss: 3.734: : 267it [03:18,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 3.6097: : 267it [03:19,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 3.6097: : 268it [03:19,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.655: : 268it [03:20,  1.11it/s] \u001b[A\n",
      "batch 269, training loss: 3.655: : 269it [03:20,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.6747: : 269it [03:21,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.6747: : 270it [03:21,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.6512: : 270it [03:22,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.6512: : 271it [03:22,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.6388: : 271it [03:23,  1.09it/s]\u001b[A\n",
      "batch 272, training loss: 3.6388: : 272it [03:23,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 3.7406: : 272it [03:24,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 3.7406: : 273it [03:24,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 3.7084: : 273it [03:25,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 3.7084: : 274it [03:25,  1.09it/s]\u001b[A\n",
      "batch 275, training loss: 3.599: : 274it [03:26,  1.09it/s] \u001b[A\n",
      "batch 275, training loss: 3.599: : 275it [03:26,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.5021: : 275it [03:27,  1.09it/s]\u001b[A\n",
      "batch 276, training loss: 3.5021: : 276it [03:27,  1.08it/s]\u001b[A\n",
      "batch 277, training loss: 3.6422: : 276it [03:28,  1.08it/s]\u001b[A\n",
      "batch 277, training loss: 3.6422: : 277it [03:28,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.5827: : 277it [03:28,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.5827: : 278it [03:28,  1.10it/s]\u001b[A\n",
      "batch 279, training loss: 3.5851: : 278it [03:29,  1.10it/s]\u001b[A\n",
      "batch 279, training loss: 3.5851: : 279it [03:29,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.4704: : 279it [03:30,  1.10it/s]\u001b[A\n",
      "batch 280, training loss: 3.4704: : 280it [03:30,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.5783: : 280it [03:31,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.5783: : 281it [03:31,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 3.4703: : 281it [03:32,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 3.4703: : 282it [03:32,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.5814: : 282it [03:33,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.5814: : 283it [03:33,  1.13it/s]\u001b[A\n",
      "batch 284, training loss: 3.5754: : 283it [03:34,  1.13it/s]\u001b[A\n",
      "batch 284, training loss: 3.5754: : 284it [03:34,  1.17it/s]\u001b[A\n",
      "batch 285, training loss: 3.614: : 284it [03:35,  1.17it/s] \u001b[A\n",
      "batch 285, training loss: 3.614: : 285it [03:35,  1.14it/s]\u001b[A\n",
      "batch 286, training loss: 3.7855: : 285it [03:36,  1.14it/s]\u001b[A\n",
      "batch 286, training loss: 3.7855: : 286it [03:36,  1.12it/s]\u001b[A\n",
      "batch 287, training loss: 3.4826: : 286it [03:36,  1.12it/s]\u001b[A\n",
      "batch 287, training loss: 3.4826: : 287it [03:36,  1.11it/s]\u001b[A\n",
      "batch 288, training loss: 3.5165: : 287it [03:37,  1.11it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 288, training loss: 3.5165: : 288it [03:37,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 3.6818: : 288it [03:38,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 3.6818: : 289it [03:38,  1.10it/s]\u001b[A\n",
      "batch 290, training loss: 3.4466: : 289it [03:39,  1.10it/s]\u001b[A\n",
      "batch 290, training loss: 3.4466: : 290it [03:39,  1.12it/s]\u001b[A\n",
      "batch 291, training loss: 3.7266: : 290it [03:40,  1.12it/s]\u001b[A\n",
      "batch 291, training loss: 3.7266: : 291it [03:40,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 3.5257: : 291it [03:41,  1.10it/s]\u001b[A\n",
      "batch 292, training loss: 3.5257: : 292it [03:41,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.6181: : 292it [03:42,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.6181: : 293it [03:42,  1.10it/s]\u001b[A\n",
      "batch 294, training loss: 3.7003: : 293it [03:43,  1.10it/s]\u001b[A\n",
      "batch 294, training loss: 3.7003: : 294it [03:43,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.6357: : 294it [03:44,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.6357: : 295it [03:44,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.5184: : 295it [03:45,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.5184: : 296it [03:45,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 3.6807: : 296it [03:46,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 3.6807: : 297it [03:46,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.5633: : 297it [03:46,  1.09it/s]\u001b[A\n",
      "batch 298, training loss: 3.5633: : 298it [03:46,  1.11it/s]\u001b[A\n",
      "batch 299, training loss: 3.6397: : 298it [03:47,  1.11it/s]\u001b[A\n",
      "batch 299, training loss: 3.6397: : 299it [03:47,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 3.7288: : 299it [03:48,  1.10it/s]\u001b[A\n",
      "batch 300, training loss: 3.7288: : 300it [03:48,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.6591: : 300it [03:49,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.6591: : 301it [03:49,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.6597: : 301it [03:50,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.6597: : 302it [03:50,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.5824: : 302it [03:51,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.5824: : 303it [03:51,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.6688: : 303it [03:52,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.6688: : 304it [03:52,  1.08it/s]\u001b[A\n",
      "batch 305, training loss: 3.5833: : 304it [03:53,  1.08it/s]\u001b[A\n",
      "batch 305, training loss: 3.5833: : 305it [03:53,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.6554: : 305it [03:54,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.6554: : 306it [03:54,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 3.7799: : 306it [03:55,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 3.7799: : 307it [03:55,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.3994: : 307it [03:56,  1.11it/s]\u001b[A\n",
      "batch 308, training loss: 3.3994: : 308it [03:56,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.7845: : 308it [03:57,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.7845: : 309it [03:57,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.5571: : 309it [03:57,  1.09it/s]\u001b[A\n",
      "batch 310, training loss: 3.5571: : 310it [03:57,  1.09it/s]\u001b[A\n",
      "batch 311, training loss: 3.647: : 310it [03:58,  1.09it/s] \u001b[A\n",
      "batch 311, training loss: 3.647: : 311it [03:58,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.4799: : 311it [03:59,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.4799: : 312it [03:59,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.6021: : 312it [04:00,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.6021: : 313it [04:00,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 3.6075: : 313it [04:01,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 3.6075: : 314it [04:01,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.6809: : 314it [04:02,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.6809: : 315it [04:02,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.8248: : 315it [04:03,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.8248: : 316it [04:03,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.4861: : 316it [04:04,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.4861: : 317it [04:04,  1.20it/s]\u001b[A\n",
      "batch 318, training loss: 3.832: : 317it [04:05,  1.20it/s] \u001b[A\n",
      "batch 318, training loss: 3.832: : 318it [04:05,  1.13it/s]\u001b[A\n",
      "batch 319, training loss: 3.9101: : 318it [04:06,  1.13it/s]\u001b[A\n",
      "batch 319, training loss: 3.9101: : 319it [04:06,  1.07it/s]\u001b[A\n",
      "batch 320, training loss: 3.9093: : 319it [04:07,  1.07it/s]\u001b[A\n",
      "batch 320, training loss: 3.9093: : 320it [04:07,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 3.9477: : 320it [04:08,  1.04it/s]\u001b[A\n",
      "batch 321, training loss: 3.9477: : 321it [04:08,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 3.9977: : 321it [04:09,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 3.9977: : 322it [04:09,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.7726: : 322it [04:10,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.7726: : 323it [04:10,  1.01s/it]\u001b[A\n",
      "batch 324, training loss: 3.8296: : 323it [04:11,  1.01s/it]\u001b[A\n",
      "batch 324, training loss: 3.8296: : 324it [04:11,  1.01s/it]\u001b[A\n",
      "batch 325, training loss: 3.8947: : 324it [04:12,  1.01s/it]\u001b[A\n",
      "batch 325, training loss: 3.8947: : 325it [04:12,  1.02s/it]\u001b[A\n",
      "batch 326, training loss: 3.8053: : 325it [04:13,  1.02s/it]\u001b[A\n",
      "batch 326, training loss: 3.8053: : 326it [04:13,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 3.8961: : 326it [04:14,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 3.8961: : 327it [04:14,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.7858: : 327it [04:15,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.7858: : 328it [04:15,  1.01s/it]\u001b[A\n",
      "batch 329, training loss: 3.6987: : 328it [04:16,  1.01s/it]\u001b[A\n",
      "batch 329, training loss: 3.6987: : 329it [04:16,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 3.7854: : 329it [04:17,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 3.7854: : 330it [04:17,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 3.8092: : 330it [04:18,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 3.8092: : 331it [04:18,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.6908: : 331it [04:19,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.6908: : 332it [04:19,  1.02s/it]\u001b[A\n",
      "batch 333, training loss: 3.6693: : 332it [04:20,  1.02s/it]\u001b[A\n",
      "batch 333, training loss: 3.6693: : 333it [04:20,  1.01s/it]\u001b[A\n",
      "batch 334, training loss: 3.729: : 333it [04:21,  1.01s/it] \u001b[A\n",
      "batch 334, training loss: 3.729: : 334it [04:21,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 3.8816: : 334it [04:22,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 3.8816: : 335it [04:22,  1.00s/it]\u001b[A\n",
      "batch 336, training loss: 3.7727: : 335it [04:23,  1.00s/it]\u001b[A\n",
      "batch 336, training loss: 3.7727: : 336it [04:23,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 3.8121: : 336it [04:24,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 3.8121: : 337it [04:24,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 3.6887: : 337it [04:25,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 3.6887: : 338it [04:25,  1.07it/s]\u001b[A\n",
      "batch 339, training loss: 3.7496: : 338it [04:26,  1.07it/s]\u001b[A\n",
      "batch 339, training loss: 3.7496: : 339it [04:26,  1.09it/s]\u001b[A\n",
      "batch 340, training loss: 3.9712: : 339it [04:27,  1.09it/s]\u001b[A\n",
      "batch 340, training loss: 3.9712: : 340it [04:27,  1.04it/s]\u001b[A\n",
      "batch 341, training loss: 3.6286: : 340it [04:28,  1.04it/s]\u001b[A\n",
      "batch 341, training loss: 3.6286: : 341it [04:28,  1.01it/s]\u001b[A\n",
      "batch 342, training loss: 3.7317: : 341it [04:29,  1.01it/s]\u001b[A\n",
      "batch 342, training loss: 3.7317: : 342it [04:29,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 3.7196: : 342it [04:30,  1.00it/s]\u001b[A\n",
      "batch 343, training loss: 3.7196: : 343it [04:30,  1.00it/s]\u001b[A\n",
      "batch 344, training loss: 3.8242: : 343it [04:31,  1.00it/s]\u001b[A\n",
      "batch 344, training loss: 3.8242: : 344it [04:31,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.7892: : 344it [04:32,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.7892: : 345it [04:32,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 3.7453: : 345it [04:33,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 3.7453: : 346it [04:33,  1.02s/it]\u001b[A\n",
      "batch 347, training loss: 3.6506: : 346it [04:34,  1.02s/it]\u001b[A\n",
      "batch 347, training loss: 3.6506: : 347it [04:34,  1.02it/s]\u001b[A\n",
      "batch 348, training loss: 3.7264: : 347it [04:35,  1.02it/s]\u001b[A\n",
      "batch 348, training loss: 3.7264: : 348it [04:35,  1.01it/s]\u001b[A\n",
      "batch 349, training loss: 3.8888: : 348it [04:36,  1.01it/s]\u001b[A\n",
      "batch 349, training loss: 3.8888: : 349it [04:36,  1.00it/s]\u001b[A\n",
      "batch 350, training loss: 3.7166: : 349it [04:37,  1.00it/s]\u001b[A\n",
      "batch 350, training loss: 3.7166: : 350it [04:37,  1.00s/it]\u001b[A\n",
      "batch 351, training loss: 3.636: : 350it [04:38,  1.00s/it] \u001b[A\n",
      "batch 351, training loss: 3.636: : 351it [04:38,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 3.6927: : 351it [04:39,  1.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 352, training loss: 3.6927: : 352it [04:39,  1.03it/s]\u001b[A\n",
      "batch 353, training loss: 3.7443: : 352it [04:40,  1.03it/s]\u001b[A\n",
      "batch 353, training loss: 3.7443: : 353it [04:40,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 3.7992: : 353it [04:41,  1.01it/s]\u001b[A\n",
      "batch 354, training loss: 3.7992: : 354it [04:41,  1.00s/it]\u001b[A\n",
      "batch 355, training loss: 3.7407: : 354it [04:42,  1.00s/it]\u001b[A\n",
      "batch 355, training loss: 3.7407: : 355it [04:42,  1.00it/s]\u001b[A\n",
      "batch 356, training loss: 3.602: : 355it [04:43,  1.00it/s] \u001b[A\n",
      "batch 356, training loss: 3.602: : 356it [04:43,  1.01s/it]\u001b[A\n",
      "batch 357, training loss: 3.597: : 356it [04:44,  1.01s/it]\u001b[A\n",
      "batch 357, training loss: 3.597: : 357it [04:44,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.8001: : 357it [04:45,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.8001: : 358it [04:45,  1.02s/it]\u001b[A\n",
      "batch 359, training loss: 3.6102: : 358it [04:46,  1.02s/it]\u001b[A\n",
      "batch 359, training loss: 3.6102: : 359it [04:46,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 3.7457: : 359it [04:47,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 3.7457: : 360it [04:47,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.7139: : 360it [04:48,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.7139: : 361it [04:48,  1.00s/it]\u001b[A\n",
      "batch 362, training loss: 3.8096: : 361it [04:49,  1.00s/it]\u001b[A\n",
      "batch 362, training loss: 3.8096: : 362it [04:49,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 3.7214: : 362it [04:50,  1.01s/it]\u001b[A\n",
      "batch 363, training loss: 3.7214: : 363it [04:50,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 3.613: : 363it [04:51,  1.01s/it] \u001b[A\n",
      "batch 364, training loss: 3.613: : 364it [04:51,  1.01s/it]\u001b[A\n",
      "batch 365, training loss: 3.5917: : 364it [04:52,  1.01s/it]\u001b[A\n",
      "batch 365, training loss: 3.5917: : 365it [04:52,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.7367: : 365it [04:53,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.7367: : 366it [04:53,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.6606: : 366it [04:54,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.6606: : 367it [04:54,  1.02s/it]\u001b[A\n",
      "batch 368, training loss: 3.7491: : 367it [04:55,  1.02s/it]\u001b[A\n",
      "batch 368, training loss: 3.7491: : 368it [04:55,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.7668: : 368it [04:56,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.7668: : 369it [04:56,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.6689: : 369it [04:57,  1.02it/s]\u001b[A\n",
      "batch 370, training loss: 3.6689: : 370it [04:57,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.7017: : 370it [04:58,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.7017: : 371it [04:58,  1.04it/s]\u001b[A\n",
      "batch 372, training loss: 3.6447: : 371it [04:59,  1.04it/s]\u001b[A\n",
      "batch 372, training loss: 3.6447: : 372it [04:59,  1.02it/s]\u001b[A\n",
      "batch 373, training loss: 3.6703: : 372it [05:00,  1.02it/s]\u001b[A\n",
      "batch 373, training loss: 3.6703: : 373it [05:00,  1.01it/s]\u001b[A\n",
      "batch 374, training loss: 3.671: : 373it [05:01,  1.01it/s] \u001b[A\n",
      "batch 374, training loss: 3.671: : 374it [05:01,  1.00s/it]\u001b[A\n",
      "batch 375, training loss: 3.4851: : 374it [05:01,  1.00s/it]\u001b[A\n",
      "batch 375, training loss: 3.4851: : 375it [05:01,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.7938: : 375it [05:02,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.7938: : 376it [05:02,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 3.9161: : 376it [05:03,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 3.9161: : 377it [05:03,  1.00s/it]\u001b[A\n",
      "batch 378, training loss: 3.818: : 377it [05:05,  1.00s/it] \u001b[A\n",
      "batch 378, training loss: 3.818: : 378it [05:05,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.7706: : 378it [05:06,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.7706: : 379it [05:06,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 3.7652: : 379it [05:07,  1.05s/it]\u001b[A\n",
      "batch 380, training loss: 3.7652: : 380it [05:07,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 3.8478: : 380it [05:08,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 3.8478: : 381it [05:08,  1.10s/it]\u001b[A\n",
      "batch 382, training loss: 3.7096: : 381it [05:09,  1.10s/it]\u001b[A\n",
      "batch 382, training loss: 3.7096: : 382it [05:09,  1.07s/it]\u001b[A\n",
      "batch 383, training loss: 3.7542: : 382it [05:10,  1.07s/it]\u001b[A\n",
      "batch 383, training loss: 3.7542: : 383it [05:10,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 3.839: : 383it [05:11,  1.08s/it] \u001b[A\n",
      "batch 384, training loss: 3.839: : 384it [05:11,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 3.7618: : 384it [05:12,  1.10s/it]\u001b[A\n",
      "batch 385, training loss: 3.7618: : 385it [05:12,  1.12s/it]\u001b[A\n",
      "batch 386, training loss: 3.7059: : 385it [05:13,  1.12s/it]\u001b[A\n",
      "batch 386, training loss: 3.7059: : 386it [05:13,  1.12s/it]\u001b[A\n",
      "batch 387, training loss: 3.7527: : 386it [05:15,  1.12s/it]\u001b[A\n",
      "batch 387, training loss: 3.7527: : 387it [05:15,  1.12s/it]\u001b[A\n",
      "batch 388, training loss: 3.5297: : 387it [05:16,  1.12s/it]\u001b[A\n",
      "batch 388, training loss: 3.5297: : 388it [05:16,  1.11s/it]\u001b[A\n",
      "batch 389, training loss: 3.709: : 388it [05:17,  1.11s/it] \u001b[A\n",
      "batch 389, training loss: 3.709: : 389it [05:17,  1.10s/it]\u001b[A\n",
      "batch 390, training loss: 3.8081: : 389it [05:18,  1.10s/it]\u001b[A\n",
      "batch 390, training loss: 3.8081: : 390it [05:18,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 3.8231: : 390it [05:19,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 3.8231: : 391it [05:19,  1.11s/it]\u001b[A\n",
      "batch 392, training loss: 3.7563: : 391it [05:20,  1.11s/it]\u001b[A\n",
      "batch 392, training loss: 3.7563: : 392it [05:20,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 3.5698: : 392it [05:21,  1.09s/it]\u001b[A\n",
      "batch 393, training loss: 3.5698: : 393it [05:21,  1.09s/it]\u001b[A\n",
      "batch 394, training loss: 3.5337: : 393it [05:22,  1.09s/it]\u001b[A\n",
      "batch 394, training loss: 3.5337: : 394it [05:22,  1.08s/it]\u001b[A\n",
      "batch 395, training loss: 3.633: : 394it [05:23,  1.08s/it] \u001b[A\n",
      "batch 395, training loss: 3.633: : 395it [05:23,  1.07s/it]\u001b[A\n",
      "batch 396, training loss: 3.7838: : 395it [05:24,  1.07s/it]\u001b[A\n",
      "batch 396, training loss: 3.7838: : 396it [05:24,  1.08s/it]\u001b[A\n",
      "batch 397, training loss: 3.604: : 396it [05:26,  1.08s/it] \u001b[A\n",
      "batch 397, training loss: 3.604: : 397it [05:26,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 3.7195: : 397it [05:27,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 3.7195: : 398it [05:27,  1.11s/it]\u001b[A\n",
      "batch 399, training loss: 3.8147: : 398it [05:28,  1.11s/it]\u001b[A\n",
      "batch 399, training loss: 3.8147: : 399it [05:28,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.5748: : 399it [05:29,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.5748: : 400it [05:29,  1.10s/it]\u001b[A\n",
      "batch 401, training loss: 3.5454: : 400it [05:30,  1.10s/it]\u001b[A\n",
      "batch 401, training loss: 3.5454: : 401it [05:30,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.6077: : 401it [05:31,  1.09s/it]\u001b[A\n",
      "batch 402, training loss: 3.6077: : 402it [05:31,  1.10s/it]\u001b[A\n",
      "batch 403, training loss: 3.7928: : 402it [05:32,  1.10s/it]\u001b[A\n",
      "batch 403, training loss: 3.7928: : 403it [05:32,  1.05s/it]\u001b[A\n",
      "batch 404, training loss: 3.4079: : 403it [05:33,  1.05s/it]\u001b[A\n",
      "batch 404, training loss: 3.4079: : 404it [05:33,  1.08s/it]\u001b[A\n",
      "batch 405, training loss: 3.684: : 404it [05:34,  1.08s/it] \u001b[A\n",
      "batch 405, training loss: 3.684: : 405it [05:34,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 3.5969: : 405it [05:35,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 3.5969: : 406it [05:35,  1.12s/it]\u001b[A\n",
      "batch 407, training loss: 3.6686: : 406it [05:37,  1.12s/it]\u001b[A\n",
      "batch 407, training loss: 3.6686: : 407it [05:37,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.522: : 407it [05:38,  1.12s/it] \u001b[A\n",
      "batch 408, training loss: 3.522: : 408it [05:38,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.7043: : 408it [05:39,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.7043: : 409it [05:39,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.5388: : 409it [05:40,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.5388: : 410it [05:40,  1.09s/it]\u001b[A\n",
      "batch 411, training loss: 3.65: : 410it [05:41,  1.09s/it]  \u001b[A\n",
      "batch 411, training loss: 3.65: : 411it [05:41,  1.10s/it]\u001b[A\n",
      "batch 412, training loss: 3.6697: : 411it [05:42,  1.10s/it]\u001b[A\n",
      "batch 412, training loss: 3.6697: : 412it [05:42,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 3.6238: : 412it [05:43,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 3.6238: : 413it [05:43,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.5247: : 413it [05:44,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.5247: : 414it [05:44,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.4253: : 414it [05:45,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.4253: : 415it [05:45,  1.12s/it]\u001b[A\n",
      "batch 416, training loss: 3.4753: : 415it [05:47,  1.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 416, training loss: 3.4753: : 416it [05:47,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.7373: : 416it [05:48,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.7373: : 417it [05:48,  1.08s/it]\u001b[A\n",
      "batch 418, training loss: 3.6323: : 417it [05:49,  1.08s/it]\u001b[A\n",
      "batch 418, training loss: 3.6323: : 418it [05:49,  1.07s/it]\u001b[A\n",
      "batch 419, training loss: 3.4424: : 418it [05:50,  1.07s/it]\u001b[A\n",
      "batch 419, training loss: 3.4424: : 419it [05:50,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.5503: : 419it [05:51,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.5503: : 420it [05:51,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.6603: : 420it [05:52,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.6603: : 421it [05:52,  1.06s/it]\u001b[A\n",
      "batch 422, training loss: 3.6849: : 421it [05:53,  1.06s/it]\u001b[A\n",
      "batch 422, training loss: 3.6849: : 422it [05:53,  1.08s/it]\u001b[A\n",
      "batch 423, training loss: 3.4697: : 422it [05:54,  1.08s/it]\u001b[A\n",
      "batch 423, training loss: 3.4697: : 423it [05:54,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 3.7579: : 423it [05:55,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 3.7579: : 424it [05:55,  1.14s/it]\u001b[A\n",
      "batch 425, training loss: 3.812: : 424it [05:57,  1.14s/it] \u001b[A\n",
      "batch 425, training loss: 3.812: : 425it [05:57,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 3.7819: : 425it [05:58,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 3.7819: : 426it [05:58,  1.12s/it]\u001b[A\n",
      "batch 427, training loss: 3.7927: : 426it [05:59,  1.12s/it]\u001b[A\n",
      "batch 427, training loss: 3.7927: : 427it [05:59,  1.09s/it]\u001b[A\n",
      "batch 428, training loss: 3.961: : 427it [06:00,  1.09s/it] \u001b[A\n",
      "batch 428, training loss: 3.961: : 428it [06:00,  1.12s/it]\u001b[A\n",
      "batch 429, training loss: 3.7784: : 428it [06:01,  1.12s/it]\u001b[A\n",
      "batch 429, training loss: 3.7784: : 429it [06:01,  1.14s/it]\u001b[A\n",
      "batch 430, training loss: 3.8212: : 429it [06:02,  1.14s/it]\u001b[A\n",
      "batch 430, training loss: 3.8212: : 430it [06:02,  1.15s/it]\u001b[A\n",
      "batch 431, training loss: 3.8431: : 430it [06:03,  1.15s/it]\u001b[A\n",
      "batch 431, training loss: 3.8431: : 431it [06:03,  1.17s/it]\u001b[A\n",
      "batch 432, training loss: 3.6746: : 431it [06:05,  1.17s/it]\u001b[A\n",
      "batch 432, training loss: 3.6746: : 432it [06:05,  1.19s/it]\u001b[A\n",
      "batch 433, training loss: 3.7147: : 432it [06:06,  1.19s/it]\u001b[A\n",
      "batch 433, training loss: 3.7147: : 433it [06:06,  1.19s/it]\u001b[A\n",
      "batch 434, training loss: 3.7105: : 433it [06:07,  1.19s/it]\u001b[A\n",
      "batch 434, training loss: 3.7105: : 434it [06:07,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.6926: : 434it [06:08,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.6926: : 435it [06:08,  1.21s/it]\u001b[A\n",
      "batch 436, training loss: 3.7113: : 435it [06:09,  1.21s/it]\u001b[A\n",
      "batch 436, training loss: 3.7113: : 436it [06:09,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.6418: : 436it [06:11,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.6418: : 437it [06:11,  1.20s/it]\u001b[A\n",
      "batch 438, training loss: 3.6835: : 437it [06:12,  1.20s/it]\u001b[A\n",
      "batch 438, training loss: 3.6835: : 438it [06:12,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 3.7839: : 438it [06:13,  1.21s/it]\u001b[A\n",
      "batch 439, training loss: 3.7839: : 439it [06:13,  1.21s/it]\u001b[A\n",
      "batch 440, training loss: 3.6055: : 439it [06:14,  1.21s/it]\u001b[A\n",
      "batch 440, training loss: 3.6055: : 440it [06:14,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 3.8806: : 440it [06:15,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 3.8806: : 441it [06:15,  1.19s/it]\u001b[A\n",
      "batch 442, training loss: 3.5587: : 441it [06:17,  1.19s/it]\u001b[A\n",
      "batch 442, training loss: 3.5587: : 442it [06:17,  1.19s/it]\u001b[A\n",
      "batch 443, training loss: 3.6291: : 442it [06:18,  1.19s/it]\u001b[A\n",
      "batch 443, training loss: 3.6291: : 443it [06:18,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.6371: : 443it [06:19,  1.21s/it]\u001b[A\n",
      "batch 444, training loss: 3.6371: : 444it [06:19,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 3.6838: : 444it [06:20,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 3.6838: : 445it [06:20,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 3.7131: : 445it [06:21,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 3.7131: : 446it [06:21,  1.17s/it]\u001b[A\n",
      "batch 447, training loss: 3.6406: : 446it [06:23,  1.17s/it]\u001b[A\n",
      "batch 447, training loss: 3.6406: : 447it [06:23,  1.18s/it]\u001b[A\n",
      "batch 448, training loss: 3.5402: : 447it [06:24,  1.18s/it]\u001b[A\n",
      "batch 448, training loss: 3.5402: : 448it [06:24,  1.20s/it]\u001b[A\n",
      "batch 449, training loss: 3.6646: : 448it [06:25,  1.20s/it]\u001b[A\n",
      "batch 449, training loss: 3.6646: : 449it [06:25,  1.19s/it]\u001b[A\n",
      "batch 450, training loss: 3.7052: : 449it [06:26,  1.19s/it]\u001b[A\n",
      "batch 450, training loss: 3.7052: : 450it [06:26,  1.21s/it]\u001b[A\n",
      "batch 451, training loss: 3.7169: : 450it [06:27,  1.21s/it]\u001b[A\n",
      "batch 451, training loss: 3.7169: : 451it [06:27,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 3.7268: : 451it [06:29,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 3.7268: : 452it [06:29,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 3.7954: : 452it [06:30,  1.23s/it]\u001b[A\n",
      "batch 453, training loss: 3.7954: : 453it [06:30,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.6474: : 453it [06:31,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.6474: : 454it [06:31,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.5494: : 454it [06:32,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.5494: : 455it [06:32,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.5868: : 455it [06:34,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.5868: : 456it [06:34,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 3.7057: : 456it [06:35,  1.23s/it]\u001b[A\n",
      "batch 457, training loss: 3.7057: : 457it [06:35,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.5701: : 457it [06:36,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.5701: : 458it [06:36,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.6396: : 458it [06:37,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.6396: : 459it [06:37,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.6418: : 459it [06:39,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.6418: : 460it [06:39,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.7205: : 460it [06:40,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.7205: : 461it [06:40,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.6764: : 461it [06:41,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.6764: : 462it [06:41,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.5143: : 462it [06:42,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.5143: : 463it [06:42,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.6545: : 463it [06:43,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.6545: : 464it [06:43,  1.24s/it]\u001b[A\n",
      "batch 465, training loss: 3.6414: : 464it [06:44,  1.24s/it]\u001b[A\n",
      "batch 465, training loss: 3.6414: : 465it [06:44,  1.16s/it]\u001b[A\n",
      "batch 466, training loss: 3.6134: : 465it [06:46,  1.16s/it]\u001b[A\n",
      "batch 466, training loss: 3.6134: : 466it [06:46,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.5432: : 466it [06:47,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.5432: : 467it [06:47,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 3.7167: : 467it [06:48,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 3.7167: : 468it [06:48,  1.25s/it]\u001b[A\n",
      "batch 469, training loss: 3.611: : 468it [06:50,  1.25s/it] \u001b[A\n",
      "batch 469, training loss: 3.611: : 469it [06:50,  1.28s/it]\u001b[A\n",
      "batch 470, training loss: 3.7222: : 469it [06:51,  1.28s/it]\u001b[A\n",
      "batch 470, training loss: 3.7222: : 470it [06:51,  1.29s/it]\u001b[A\n",
      "batch 471, training loss: 3.6967: : 470it [06:52,  1.29s/it]\u001b[A\n",
      "batch 471, training loss: 3.6967: : 471it [06:52,  1.28s/it]\u001b[A\n",
      "batch 472, training loss: 3.6318: : 471it [06:54,  1.28s/it]\u001b[A\n",
      "batch 472, training loss: 3.6318: : 472it [06:54,  1.28s/it]\u001b[A\n",
      "batch 473, training loss: 3.6398: : 472it [06:55,  1.28s/it]\u001b[A\n",
      "batch 473, training loss: 3.6398: : 473it [06:55,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 3.6795: : 473it [06:56,  1.27s/it]\u001b[A\n",
      "batch 474, training loss: 3.6795: : 474it [06:56,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.6009: : 474it [06:57,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.6009: : 475it [06:57,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.6778: : 475it [06:59,  1.27s/it]\u001b[A\n",
      "batch 476, training loss: 3.6778: : 476it [06:59,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.5094: : 476it [07:00,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.5094: : 477it [07:00,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.5613: : 477it [07:01,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.5613: : 478it [07:01,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.5419: : 478it [07:02,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.5419: : 479it [07:02,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.5718: : 479it [07:04,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 480, training loss: 3.5718: : 480it [07:04,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.584: : 480it [07:05,  1.26s/it] \u001b[A\n",
      "batch 481, training loss: 3.584: : 481it [07:05,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.5829: : 481it [07:06,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.5829: : 482it [07:06,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.5115: : 482it [07:07,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.5115: : 483it [07:07,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.6021: : 483it [07:09,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.6021: : 484it [07:09,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.5207: : 484it [07:10,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.5207: : 485it [07:10,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.6245: : 485it [07:11,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.6245: : 486it [07:11,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.7315: : 486it [07:12,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.7315: : 487it [07:12,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.5077: : 487it [07:14,  1.27s/it]\u001b[A\n",
      "batch 488, training loss: 3.5077: : 488it [07:14,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.447: : 488it [07:15,  1.26s/it] \u001b[A\n",
      "batch 489, training loss: 3.447: : 489it [07:15,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.5313: : 489it [07:16,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.5313: : 490it [07:16,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.5674: : 490it [07:17,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.5674: : 491it [07:17,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.4749: : 491it [07:19,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.4749: : 492it [07:19,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.6486: : 492it [07:20,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.6486: : 493it [07:20,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.6608: : 493it [07:21,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.6608: : 494it [07:21,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.487: : 494it [07:23,  1.26s/it] \u001b[A\n",
      "batch 495, training loss: 3.487: : 495it [07:23,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.4729: : 495it [07:24,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.4729: : 496it [07:24,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.4027: : 496it [07:25,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.4027: : 497it [07:25,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.4933: : 497it [07:26,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.4933: : 498it [07:26,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.5067: : 498it [07:27,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.5067: : 499it [07:27,  1.13s/it]\u001b[A\n",
      "batch 500, training loss: 3.8037: : 499it [07:28,  1.13s/it]\u001b[A\n",
      "batch 500, training loss: 3.8037: : 500it [07:28,  1.21s/it]\u001b[A\n",
      "batch 501, training loss: 3.658: : 500it [07:30,  1.21s/it] \u001b[A\n",
      "batch 501, training loss: 3.658: : 501it [07:30,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 3.6139: : 501it [07:31,  1.26s/it]\u001b[A\n",
      "batch 502, training loss: 3.6139: : 502it [07:31,  1.23s/it]\u001b[A\n",
      "batch 503, training loss: 3.6865: : 502it [07:32,  1.23s/it]\u001b[A\n",
      "batch 503, training loss: 3.6865: : 503it [07:32,  1.27s/it]\u001b[A\n",
      "batch 504, training loss: 3.6682: : 503it [07:34,  1.27s/it]\u001b[A\n",
      "batch 504, training loss: 3.6682: : 504it [07:34,  1.29s/it]\u001b[A\n",
      "batch 505, training loss: 3.7386: : 504it [07:35,  1.29s/it]\u001b[A\n",
      "batch 505, training loss: 3.7386: : 505it [07:35,  1.29s/it]\u001b[A\n",
      "batch 506, training loss: 3.6174: : 505it [07:36,  1.29s/it]\u001b[A\n",
      "batch 506, training loss: 3.6174: : 506it [07:36,  1.33s/it]\u001b[A\n",
      "batch 507, training loss: 3.508: : 506it [07:38,  1.33s/it] \u001b[A\n",
      "batch 507, training loss: 3.508: : 507it [07:38,  1.34s/it]\u001b[A\n",
      "batch 508, training loss: 3.6595: : 507it [07:39,  1.34s/it]\u001b[A\n",
      "batch 508, training loss: 3.6595: : 508it [07:39,  1.35s/it]\u001b[A\n",
      "batch 509, training loss: 3.5415: : 508it [07:41,  1.35s/it]\u001b[A\n",
      "batch 509, training loss: 3.5415: : 509it [07:41,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.6036: : 509it [07:42,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.6036: : 510it [07:42,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 3.6134: : 510it [07:43,  1.36s/it]\u001b[A\n",
      "batch 511, training loss: 3.6134: : 511it [07:43,  1.35s/it]\u001b[A\n",
      "batch 512, training loss: 3.6021: : 511it [07:45,  1.35s/it]\u001b[A\n",
      "batch 512, training loss: 3.6021: : 512it [07:45,  1.37s/it]\u001b[A\n",
      "batch 513, training loss: 3.6486: : 512it [07:46,  1.37s/it]\u001b[A\n",
      "batch 513, training loss: 3.6486: : 513it [07:46,  1.34s/it]\u001b[A\n",
      "batch 514, training loss: 3.5074: : 513it [07:47,  1.34s/it]\u001b[A\n",
      "batch 514, training loss: 3.5074: : 514it [07:47,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.6947: : 514it [07:49,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.6947: : 515it [07:49,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.5552: : 515it [07:50,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.5552: : 516it [07:50,  1.35s/it]\u001b[A\n",
      "batch 517, training loss: 3.556: : 516it [07:51,  1.35s/it] \u001b[A\n",
      "batch 517, training loss: 3.556: : 517it [07:51,  1.32s/it]\u001b[A\n",
      "batch 518, training loss: 3.6409: : 517it [07:53,  1.32s/it]\u001b[A\n",
      "batch 518, training loss: 3.6409: : 518it [07:53,  1.35s/it]\u001b[A\n",
      "batch 519, training loss: 3.4381: : 518it [07:54,  1.35s/it]\u001b[A\n",
      "batch 519, training loss: 3.4381: : 519it [07:54,  1.37s/it]\u001b[A\n",
      "batch 520, training loss: 3.6107: : 519it [07:56,  1.37s/it]\u001b[A\n",
      "batch 520, training loss: 3.6107: : 520it [07:56,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.6153: : 520it [07:57,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.6153: : 521it [07:57,  1.39s/it]\u001b[A\n",
      "batch 522, training loss: 3.6591: : 521it [07:58,  1.39s/it]\u001b[A\n",
      "batch 522, training loss: 3.6591: : 522it [07:58,  1.39s/it]\u001b[A\n",
      "batch 523, training loss: 3.5163: : 522it [08:00,  1.39s/it]\u001b[A\n",
      "batch 523, training loss: 3.5163: : 523it [08:00,  1.39s/it]\u001b[A\n",
      "batch 524, training loss: 3.7169: : 523it [08:01,  1.39s/it]\u001b[A\n",
      "batch 524, training loss: 3.7169: : 524it [08:01,  1.39s/it]\u001b[A\n",
      "batch 525, training loss: 3.7576: : 524it [08:02,  1.39s/it]\u001b[A\n",
      "batch 525, training loss: 3.7576: : 525it [08:02,  1.36s/it]\u001b[A\n",
      "batch 526, training loss: 3.5554: : 525it [08:04,  1.36s/it]\u001b[A\n",
      "batch 526, training loss: 3.5554: : 526it [08:04,  1.34s/it]\u001b[A\n",
      "batch 527, training loss: 3.5425: : 526it [08:05,  1.34s/it]\u001b[A\n",
      "batch 527, training loss: 3.5425: : 527it [08:05,  1.30s/it]\u001b[A\n",
      "batch 528, training loss: 3.6154: : 527it [08:06,  1.30s/it]\u001b[A\n",
      "batch 528, training loss: 3.6154: : 528it [08:06,  1.32s/it]\u001b[A\n",
      "batch 529, training loss: 3.6555: : 528it [08:08,  1.32s/it]\u001b[A\n",
      "batch 529, training loss: 3.6555: : 529it [08:08,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 3.6224: : 529it [08:09,  1.38s/it]\u001b[A\n",
      "batch 530, training loss: 3.6224: : 530it [08:09,  1.36s/it]\u001b[A\n",
      "batch 531, training loss: 3.5537: : 530it [08:10,  1.36s/it]\u001b[A\n",
      "batch 531, training loss: 3.5537: : 531it [08:10,  1.23s/it]\u001b[A\n",
      "batch 532, training loss: 3.356: : 531it [08:11,  1.23s/it] \u001b[A\n",
      "batch 532, training loss: 3.356: : 532it [08:11,  1.13s/it]\u001b[A\n",
      "batch 533, training loss: 3.5295: : 532it [08:12,  1.13s/it]\u001b[A\n",
      "batch 533, training loss: 3.5295: : 533it [08:12,  1.20s/it]\u001b[A\n",
      "batch 534, training loss: 3.6528: : 533it [08:14,  1.20s/it]\u001b[A\n",
      "batch 534, training loss: 3.6528: : 534it [08:14,  1.29s/it]\u001b[A\n",
      "batch 535, training loss: 3.5355: : 534it [08:15,  1.29s/it]\u001b[A\n",
      "batch 535, training loss: 3.5355: : 535it [08:15,  1.30s/it]\u001b[A\n",
      "batch 536, training loss: 3.3694: : 535it [08:16,  1.30s/it]\u001b[A\n",
      "batch 536, training loss: 3.3694: : 536it [08:16,  1.33s/it]\u001b[A\n",
      "batch 537, training loss: 3.443: : 536it [08:18,  1.33s/it] \u001b[A\n",
      "batch 537, training loss: 3.443: : 537it [08:18,  1.38s/it]\u001b[A\n",
      "batch 538, training loss: 3.5753: : 537it [08:20,  1.38s/it]\u001b[A\n",
      "batch 538, training loss: 3.5753: : 538it [08:20,  1.43s/it]\u001b[A\n",
      "batch 539, training loss: 3.532: : 538it [08:21,  1.43s/it] \u001b[A\n",
      "batch 539, training loss: 3.532: : 539it [08:21,  1.45s/it]\u001b[A\n",
      "batch 540, training loss: 3.4796: : 539it [08:23,  1.45s/it]\u001b[A\n",
      "batch 540, training loss: 3.4796: : 540it [08:23,  1.46s/it]\u001b[A\n",
      "batch 541, training loss: 3.5722: : 540it [08:24,  1.46s/it]\u001b[A\n",
      "batch 541, training loss: 3.5722: : 541it [08:24,  1.45s/it]\u001b[A\n",
      "batch 542, training loss: 3.6313: : 541it [08:25,  1.45s/it]\u001b[A\n",
      "batch 542, training loss: 3.6313: : 542it [08:25,  1.46s/it]\u001b[A\n",
      "batch 543, training loss: 3.4711: : 542it [08:27,  1.46s/it]\u001b[A\n",
      "batch 543, training loss: 3.4711: : 543it [08:27,  1.46s/it]\u001b[A\n",
      "batch 544, training loss: 3.5504: : 543it [08:28,  1.46s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 544, training loss: 3.5504: : 544it [08:28,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 3.5469: : 544it [08:30,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 3.5469: : 545it [08:30,  1.46s/it]\u001b[A\n",
      "batch 546, training loss: 3.468: : 545it [08:31,  1.46s/it] \u001b[A\n",
      "batch 546, training loss: 3.468: : 546it [08:31,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 3.3493: : 546it [08:33,  1.48s/it]\u001b[A\n",
      "batch 547, training loss: 3.3493: : 547it [08:33,  1.49s/it]\u001b[A\n",
      "batch 548, training loss: 3.3754: : 547it [08:34,  1.49s/it]\u001b[A\n",
      "batch 548, training loss: 3.3754: : 548it [08:34,  1.31s/it]\u001b[A\n",
      "batch 549, training loss: 3.5254: : 548it [08:35,  1.31s/it]\u001b[A\n",
      "batch 549, training loss: 3.5254: : 549it [08:35,  1.40s/it]\u001b[A\n",
      "batch 550, training loss: 3.4557: : 549it [08:37,  1.40s/it]\u001b[A\n",
      "batch 550, training loss: 3.4557: : 550it [08:37,  1.48s/it]\u001b[A\n",
      "batch 551, training loss: 3.4961: : 550it [08:39,  1.48s/it]\u001b[A\n",
      "batch 551, training loss: 3.4961: : 551it [08:39,  1.53s/it]\u001b[A\n",
      "batch 552, training loss: 3.5066: : 551it [08:40,  1.53s/it]\u001b[A\n",
      "batch 552, training loss: 3.5066: : 552it [08:40,  1.55s/it]\u001b[A\n",
      "batch 553, training loss: 3.4984: : 552it [08:42,  1.55s/it]\u001b[A\n",
      "batch 553, training loss: 3.4984: : 553it [08:42,  1.58s/it]\u001b[A\n",
      "batch 554, training loss: 3.2606: : 553it [08:43,  1.58s/it]\u001b[A\n",
      "batch 554, training loss: 3.2606: : 554it [08:43,  1.49s/it]\u001b[A\n",
      "batch 555, training loss: 3.4019: : 554it [08:44,  1.49s/it]\u001b[A\n",
      "batch 555, training loss: 3.4019: : 555it [08:44,  1.36s/it]\u001b[A\n",
      "batch 556, training loss: 3.5621: : 555it [08:45,  1.36s/it]\u001b[A\n",
      "batch 556, training loss: 3.5621: : 556it [08:45,  1.32s/it]\u001b[A\n",
      "batch 557, training loss: 3.3972: : 556it [08:47,  1.32s/it]\u001b[A\n",
      "batch 557, training loss: 3.3972: : 557it [08:47,  1.39s/it]\u001b[A\n",
      "batch 558, training loss: 3.3394: : 557it [08:49,  1.39s/it]\u001b[A\n",
      "batch 558, training loss: 3.3394: : 558it [08:49,  1.46s/it]\u001b[A\n",
      "batch 559, training loss: 3.4195: : 558it [08:50,  1.46s/it]\u001b[A\n",
      "batch 559, training loss: 3.4195: : 559it [08:50,  1.51s/it]\u001b[A\n",
      "batch 560, training loss: 3.4392: : 559it [08:52,  1.51s/it]\u001b[A\n",
      "batch 560, training loss: 3.4392: : 560it [08:52,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.4287: : 560it [08:54,  1.55s/it]\u001b[A\n",
      "batch 561, training loss: 3.4287: : 561it [08:54,  1.57s/it]\u001b[A\n",
      "batch 562, training loss: 3.2869: : 561it [08:55,  1.57s/it]\u001b[A\n",
      "batch 562, training loss: 3.2869: : 562it [08:55,  1.58s/it]\u001b[A\n",
      "batch 563, training loss: 3.4304: : 562it [08:57,  1.58s/it]\u001b[A\n",
      "batch 563, training loss: 3.4304: : 563it [08:57,  1.58s/it]\u001b[A\n",
      "batch 564, training loss: 3.3167: : 563it [08:58,  1.58s/it]\u001b[A\n",
      "batch 564, training loss: 3.3167: : 564it [08:58,  1.60s/it]\u001b[A\n",
      "batch 565, training loss: 3.4497: : 564it [09:00,  1.60s/it]\u001b[A\n",
      "batch 565, training loss: 3.4497: : 565it [09:00,  1.55s/it]\u001b[A\n",
      "batch 566, training loss: 3.5596: : 565it [09:01,  1.55s/it]\u001b[A\n",
      "batch 566, training loss: 3.5596: : 566it [09:01,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 3.4971: : 566it [09:03,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 3.4971: : 567it [09:03,  1.59s/it]\u001b[A\n",
      "batch 568, training loss: 3.5864: : 567it [09:05,  1.59s/it]\u001b[A\n",
      "batch 568, training loss: 3.5864: : 568it [09:05,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 3.5694: : 568it [09:06,  1.62s/it]\u001b[A\n",
      "batch 569, training loss: 3.5694: : 569it [09:06,  1.64s/it]\u001b[A\n",
      "batch 570, training loss: 3.7272: : 569it [09:08,  1.64s/it]\u001b[A\n",
      "batch 570, training loss: 3.7272: : 570it [09:08,  1.63s/it]\u001b[A\n",
      "batch 571, training loss: 3.5466: : 570it [09:10,  1.63s/it]\u001b[A\n",
      "batch 571, training loss: 3.5466: : 571it [09:10,  1.60s/it]\u001b[A\n",
      "batch 572, training loss: 3.6175: : 571it [09:11,  1.60s/it]\u001b[A\n",
      "batch 572, training loss: 3.6175: : 572it [09:11,  1.57s/it]\u001b[A\n",
      "batch 573, training loss: 3.5404: : 572it [09:13,  1.57s/it]\u001b[A\n",
      "batch 573, training loss: 3.5404: : 573it [09:13,  1.60s/it]\u001b[A\n",
      "batch 574, training loss: 3.6161: : 573it [09:14,  1.60s/it]\u001b[A\n",
      "batch 574, training loss: 3.6161: : 574it [09:14,  1.62s/it]\u001b[A\n",
      "batch 575, training loss: 3.2798: : 574it [09:15,  1.62s/it]\u001b[A\n",
      "batch 575, training loss: 3.2798: : 575it [09:15,  1.40s/it]\u001b[A\n",
      "batch 576, training loss: 3.5951: : 575it [09:17,  1.40s/it]\u001b[A\n",
      "batch 576, training loss: 3.5951: : 576it [09:17,  1.52s/it]\u001b[A\n",
      "batch 577, training loss: 3.4652: : 576it [09:19,  1.52s/it]\u001b[A\n",
      "batch 577, training loss: 3.4652: : 577it [09:19,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.4488: : 577it [09:21,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.4488: : 578it [09:21,  1.63s/it]\u001b[A\n",
      "batch 579, training loss: 3.5029: : 578it [09:22,  1.63s/it]\u001b[A\n",
      "batch 579, training loss: 3.5029: : 579it [09:22,  1.68s/it]\u001b[A\n",
      "batch 580, training loss: 3.4011: : 579it [09:24,  1.68s/it]\u001b[A\n",
      "batch 580, training loss: 3.4011: : 580it [09:24,  1.69s/it]\u001b[A\n",
      "batch 581, training loss: 3.423: : 580it [09:26,  1.69s/it] \u001b[A\n",
      "batch 581, training loss: 3.423: : 581it [09:26,  1.72s/it]\u001b[A\n",
      "batch 582, training loss: 3.4346: : 581it [09:28,  1.72s/it]\u001b[A\n",
      "batch 582, training loss: 3.4346: : 582it [09:28,  1.70s/it]\u001b[A\n",
      "batch 583, training loss: 2.9531: : 582it [09:28,  1.70s/it]\u001b[A\n",
      "batch 583, training loss: 2.9531: : 583it [09:28,  1.39s/it]\u001b[A\n",
      "batch 584, training loss: 3.646: : 583it [09:29,  1.39s/it] \u001b[A\n",
      "batch 584, training loss: 3.646: : 584it [09:29,  1.36s/it]\u001b[A\n",
      "batch 585, training loss: 3.5806: : 584it [09:31,  1.36s/it]\u001b[A\n",
      "batch 585, training loss: 3.5806: : 585it [09:31,  1.40s/it]\u001b[A\n",
      "batch 586, training loss: 3.6743: : 585it [09:33,  1.40s/it]\u001b[A\n",
      "batch 586, training loss: 3.6743: : 586it [09:33,  1.46s/it]\u001b[A\n",
      "batch 587, training loss: 3.5583: : 586it [09:34,  1.46s/it]\u001b[A\n",
      "batch 587, training loss: 3.5583: : 587it [09:34,  1.56s/it]\u001b[A\n",
      "batch 588, training loss: 3.4491: : 587it [09:36,  1.56s/it]\u001b[A\n",
      "batch 588, training loss: 3.4491: : 588it [09:36,  1.66s/it]\u001b[A\n",
      "batch 589, training loss: 3.5358: : 588it [09:38,  1.66s/it]\u001b[A\n",
      "batch 589, training loss: 3.5358: : 589it [09:38,  1.75s/it]\u001b[A\n",
      "batch 590, training loss: 3.5491: : 589it [09:40,  1.75s/it]\u001b[A\n",
      "batch 590, training loss: 3.5491: : 590it [09:40,  1.79s/it]\u001b[A\n",
      "batch 591, training loss: 3.4122: : 590it [09:42,  1.79s/it]\u001b[A\n",
      "batch 591, training loss: 3.4122: : 591it [09:42,  1.84s/it]\u001b[A\n",
      "batch 592, training loss: 3.4494: : 591it [09:44,  1.84s/it]\u001b[A\n",
      "batch 592, training loss: 3.4494: : 592it [09:44,  1.82s/it]\u001b[A\n",
      "batch 593, training loss: 3.359: : 592it [09:46,  1.82s/it] \u001b[A\n",
      "batch 593, training loss: 3.359: : 593it [09:46,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 3.5293: : 593it [09:48,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 3.5293: : 594it [09:48,  1.95s/it]\u001b[A\n",
      "batch 595, training loss: 3.6197: : 594it [09:50,  1.95s/it]\u001b[A\n",
      "batch 595, training loss: 3.6197: : 595it [09:50,  1.84s/it]\u001b[A\n",
      "batch 596, training loss: 3.5463: : 595it [09:52,  1.84s/it]\u001b[A\n",
      "batch 596, training loss: 3.5463: : 596it [09:52,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.3557: : 596it [09:54,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.3557: : 597it [09:54,  1.94s/it]\u001b[A\n",
      "batch 598, training loss: 3.5968: : 597it [09:56,  1.94s/it]\u001b[A\n",
      "batch 598, training loss: 3.5968: : 598it [09:56,  2.04s/it]\u001b[A\n",
      "batch 599, training loss: 3.487: : 598it [09:57,  2.04s/it] \u001b[A\n",
      "batch 599, training loss: 3.487: : 599it [09:57,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.4084: : 599it [10:00,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.4084: : 600it [10:00,  1.99s/it]\u001b[A\n",
      "batch 601, training loss: 2.564: : 600it [10:00,  1.99s/it] \u001b[A\n",
      "batch 601, training loss: 2.564: : 601it [10:00,  1.63s/it]\u001b[A\n",
      "batch 602, training loss: 3.4475: : 601it [10:03,  1.63s/it]\u001b[A\n",
      "batch 602, training loss: 3.4475: : 602it [10:03,  1.77s/it]\u001b[A\n",
      "batch 603, training loss: 3.4364: : 602it [10:05,  1.77s/it]\u001b[A\n",
      "batch 603, training loss: 3.4364: : 603it [10:05,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 3.4758: : 603it [10:06,  1.81s/it]\u001b[A\n",
      "batch 604, training loss: 3.4758: : 604it [10:06,  1.82s/it]\u001b[A\n",
      "batch 605, training loss: 3.5999: : 604it [10:08,  1.82s/it]\u001b[A\n",
      "batch 605, training loss: 3.5999: : 605it [10:08,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 3.3612: : 605it [10:10,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 3.3612: : 606it [10:10,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 3.2666: : 606it [10:11,  1.72s/it]\u001b[A\n",
      "batch 607, training loss: 3.2666: : 607it [10:11,  1.69s/it]\u001b[A\n",
      "batch 608, training loss: 3.4722: : 607it [10:13,  1.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 608, training loss: 3.4722: : 608it [10:13,  1.58s/it]\u001b[A\n",
      "batch 609, training loss: 3.4438: : 608it [10:14,  1.58s/it]\u001b[A\n",
      "batch 609, training loss: 3.4438: : 609it [10:14,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 2.8934: : 609it [10:15,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 2.8934: : 610it [10:15,  1.41s/it]\u001b[A\n",
      "batch 611, training loss: 2.917: : 610it [10:16,  1.41s/it] \u001b[A\n",
      "batch 611, training loss: 2.917: : 611it [10:16,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.111: : 611it [10:17,  1.35s/it]\u001b[A\n",
      "batch 612, training loss: 2.111: : 612it [10:17,  1.28s/it]\u001b[A\n",
      "batch 613, training loss: 2.9218: : 612it [10:18,  1.28s/it]\u001b[A\n",
      "batch 613, training loss: 2.9218: : 616it [10:19,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.7981: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.7981: : 1it [00:00,  2.55it/s]\u001b[A\n",
      "batch 1, dev loss: 3.9897: : 1it [00:00,  2.55it/s]\u001b[A\n",
      "batch 1, dev loss: 3.9897: : 2it [00:00,  3.59it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5874: : 2it [00:00,  3.59it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5874: : 3it [00:00,  4.11it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7293: : 3it [00:00,  4.11it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7293: : 4it [00:00,  4.64it/s]\u001b[A\n",
      "batch 4, dev loss: 3.7425: : 4it [00:01,  4.64it/s]\u001b[A\n",
      "batch 4, dev loss: 3.7425: : 5it [00:01,  4.74it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7339: : 5it [00:01,  4.74it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7339: : 6it [00:01,  4.87it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8117: : 6it [00:01,  4.87it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8117: : 7it [00:01,  4.75it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5895: : 7it [00:01,  4.75it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5895: : 8it [00:01,  4.88it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7689: : 8it [00:02,  4.88it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7689: : 9it [00:02,  4.66it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6962: : 9it [00:02,  4.66it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6962: : 10it [00:02,  4.92it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7262: : 10it [00:02,  4.92it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7262: : 11it [00:02,  4.79it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7917: : 11it [00:02,  4.79it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7917: : 12it [00:02,  4.42it/s]\u001b[A\n",
      "batch 12, dev loss: 3.6196: : 12it [00:02,  4.42it/s]\u001b[A\n",
      "batch 12, dev loss: 3.6196: : 13it [00:02,  4.27it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7605: : 13it [00:03,  4.27it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7605: : 14it [00:03,  4.09it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9033: : 14it [00:03,  4.09it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9033: : 15it [00:03,  4.01it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6652: : 15it [00:03,  4.01it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6652: : 16it [00:03,  4.68it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0382: : 16it [00:03,  4.68it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0382: : 17it [00:03,  4.22it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8179: : 17it [00:04,  4.22it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8179: : 18it [00:04,  4.01it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7181: : 18it [00:04,  4.01it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7181: : 19it [00:04,  3.86it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9079: : 19it [00:04,  3.86it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9079: : 20it [00:04,  3.79it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7262: : 20it [00:04,  3.79it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7262: : 21it [00:04,  3.89it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6114: : 21it [00:05,  3.89it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6114: : 22it [00:05,  3.87it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8188: : 22it [00:05,  3.87it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8188: : 23it [00:05,  3.77it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8542: : 23it [00:05,  3.77it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8542: : 24it [00:05,  4.36it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7629: : 24it [00:05,  4.36it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7629: : 25it [00:05,  3.99it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7249: : 25it [00:06,  3.99it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7249: : 26it [00:06,  3.69it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6735: : 26it [00:06,  3.69it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6735: : 27it [00:06,  3.60it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6038: : 27it [00:06,  3.60it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6038: : 28it [00:06,  3.49it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8563: : 28it [00:07,  3.49it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8563: : 29it [00:07,  3.56it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7786: : 29it [00:07,  3.56it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7786: : 30it [00:07,  3.58it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0882: : 30it [00:07,  3.58it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0882: : 31it [00:07,  4.25it/s]\u001b[A\n",
      "batch 31, dev loss: 3.833: : 31it [00:07,  4.25it/s] \u001b[A\n",
      "batch 31, dev loss: 3.833: : 32it [00:07,  3.75it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8943: : 32it [00:08,  3.75it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8943: : 33it [00:08,  3.55it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6377: : 33it [00:08,  3.55it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6377: : 34it [00:08,  3.28it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0723: : 34it [00:08,  3.28it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0723: : 35it [00:08,  3.23it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8837: : 35it [00:09,  3.23it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8837: : 36it [00:09,  3.13it/s]\u001b[A\n",
      "batch 36, dev loss: 3.816: : 36it [00:09,  3.13it/s] \u001b[A\n",
      "batch 36, dev loss: 3.816: : 37it [00:09,  3.30it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5582: : 37it [00:09,  3.30it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5582: : 38it [00:09,  3.06it/s]\u001b[A\n",
      "batch 38, dev loss: 3.829: : 38it [00:10,  3.06it/s] \u001b[A\n",
      "batch 38, dev loss: 3.829: : 39it [00:10,  2.90it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8165: : 39it [00:10,  2.90it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8165: : 40it [00:10,  2.83it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8534: : 40it [00:11,  2.83it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8534: : 41it [00:11,  2.73it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6509: : 41it [00:11,  2.73it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6509: : 42it [00:11,  2.88it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8137: : 42it [00:11,  2.88it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8137: : 43it [00:11,  2.75it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8026: : 43it [00:12,  2.75it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8026: : 44it [00:12,  2.62it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7304: : 44it [00:12,  2.62it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7304: : 45it [00:12,  2.66it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0314: : 45it [00:12,  2.66it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0314: : 46it [00:12,  2.60it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5555: : 46it [00:13,  2.60it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5555: : 47it [00:13,  2.55it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7353: : 47it [00:13,  2.55it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7353: : 48it [00:13,  2.55it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5606: : 48it [00:14,  2.55it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5606: : 49it [00:14,  2.46it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7325: : 49it [00:14,  2.46it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7325: : 50it [00:14,  2.73it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7177: : 50it [00:14,  2.73it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7177: : 51it [00:14,  2.62it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7297: : 51it [00:15,  2.62it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7297: : 52it [00:15,  2.59it/s]\u001b[A\n",
      "batch 52, dev loss: 3.5065: : 52it [00:15,  2.59it/s]\u001b[A\n",
      "batch 52, dev loss: 3.5065: : 53it [00:15,  2.64it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7138: : 53it [00:16,  2.64it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7138: : 54it [00:16,  2.56it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4688: : 54it [00:16,  2.56it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4688: : 55it [00:16,  2.35it/s]\u001b[A\n",
      "batch 55, dev loss: 3.659: : 55it [00:17,  2.35it/s] \u001b[A\n",
      "batch 55, dev loss: 3.659: : 56it [00:17,  2.21it/s]\u001b[A\n",
      "batch 56, dev loss: 3.5116: : 56it [00:17,  2.21it/s]\u001b[A\n",
      "batch 56, dev loss: 3.5116: : 57it [00:17,  2.29it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4419: : 57it [00:17,  2.29it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4419: : 58it [00:17,  2.14it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8332: : 58it [00:18,  2.14it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8332: : 59it [00:18,  2.12it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7122: : 59it [00:18,  2.12it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7122: : 60it [00:18,  2.20it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4364: : 60it [00:19,  2.20it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4364: : 61it [00:19,  2.38it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4023: : 61it [00:19,  2.38it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4023: : 62it [00:19,  2.45it/s]\u001b[A\n",
      "batch 62, dev loss: 3.2281: : 62it [00:19,  2.45it/s]\u001b[A\n",
      "batch 62, dev loss: 3.2281: : 63it [00:19,  2.56it/s]\u001b[A\n",
      "batch 63, dev loss: 3.8719: : 63it [00:20,  2.56it/s]\u001b[A\n",
      "batch 63, dev loss: 3.8719: : 64it [00:20,  2.61it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5596: : 64it [00:20,  2.61it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 64, dev loss: 3.5596: : 65it [00:20,  2.79it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4834: : 65it [00:21,  2.79it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4834: : 66it [00:21,  2.72it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5974: : 66it [00:21,  2.72it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5974: : 67it [00:21,  2.89it/s]\u001b[A\n",
      "batch 67, dev loss: 2.6486: : 67it [00:21,  2.89it/s]\u001b[A\n",
      "batch 67, dev loss: 2.6486: : 68it [00:21,  3.39it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6843: : 68it [00:21,  3.39it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6843: : 69it [00:21,  3.75it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0129: : 69it [00:21,  3.75it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0129: : 70it [00:21,  3.82it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9142: : 70it [00:22,  3.82it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9142: : 71it [00:22,  3.50it/s]\u001b[A\n",
      "batch 71, dev loss: 3.2264: : 71it [00:22,  3.50it/s]\u001b[A\n",
      "batch 71, dev loss: 3.2264: : 72it [00:22,  3.22it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0512: : 72it [00:22,  3.22it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0512: : 73it [00:22,  3.19it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0512: : 74it [00:23,  3.98it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0512: : 76it [00:23,  3.26it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.40s/it]\u001b[A\n",
      "2it [00:02,  1.42s/it]\u001b[A\n",
      "3it [00:04,  1.43s/it]\u001b[A\n",
      "4it [00:05,  1.44s/it]\u001b[A\n",
      "5it [00:07,  1.39s/it]\u001b[A\n",
      "6it [00:08,  1.41s/it]\u001b[A\n",
      "7it [00:09,  1.26s/it]\u001b[A\n",
      "8it [00:10,  1.24s/it]\u001b[A\n",
      "9it [00:11,  1.27s/it]\u001b[A\n",
      "10it [00:13,  1.38s/it]\u001b[A\n",
      "11it [00:15,  1.51s/it]\u001b[A\n",
      "12it [00:16,  1.49s/it]\u001b[A\n",
      "13it [00:18,  1.55s/it]\u001b[A\n",
      "14it [00:20,  1.64s/it]\u001b[A\n",
      "15it [00:22,  1.68s/it]\u001b[A\n",
      "16it [00:22,  1.41s/it]\u001b[A\n",
      "17it [00:25,  1.65s/it]\u001b[A\n",
      "18it [00:27,  1.78s/it]\u001b[A\n",
      "19it [00:28,  1.75s/it]\u001b[A\n",
      "20it [00:30,  1.72s/it]\u001b[A\n",
      "21it [00:32,  1.77s/it]\u001b[A\n",
      "22it [00:34,  1.88s/it]\u001b[A\n",
      "23it [00:36,  1.92s/it]\u001b[A\n",
      "24it [00:37,  1.50s/it]\u001b[A\n",
      "25it [00:39,  1.78s/it]\u001b[A\n",
      "26it [00:42,  2.04s/it]\u001b[A\n",
      "27it [00:44,  2.02s/it]\u001b[A\n",
      "28it [00:46,  2.15s/it]\u001b[A\n",
      "29it [00:48,  2.19s/it]\u001b[A\n",
      "30it [00:51,  2.16s/it]\u001b[A\n",
      "31it [00:53,  2.37s/it]\u001b[A\n",
      "32it [00:56,  2.42s/it]\u001b[A\n",
      "33it [00:59,  2.48s/it]\u001b[A\n",
      "34it [01:01,  2.43s/it]\u001b[A\n",
      "35it [01:03,  2.45s/it]\u001b[A\n",
      "36it [01:04,  1.87s/it]\u001b[A\n",
      "37it [01:07,  2.13s/it]\u001b[A\n",
      "38it [01:10,  2.41s/it]\u001b[A\n",
      "39it [01:12,  2.29s/it]\u001b[A\n",
      "40it [01:15,  2.52s/it]\u001b[A\n",
      "41it [01:16,  2.02s/it]\u001b[A\n",
      "42it [01:19,  2.48s/it]\u001b[A\n",
      "43it [01:23,  2.78s/it]\u001b[A\n",
      "44it [01:26,  2.96s/it]\u001b[A\n",
      "45it [01:28,  2.82s/it]\u001b[A\n",
      "46it [01:32,  3.08s/it]\u001b[A\n",
      "47it [01:36,  3.16s/it]\u001b[A\n",
      "48it [01:39,  3.27s/it]\u001b[A\n",
      "49it [01:39,  2.34s/it]\u001b[A\n",
      "50it [01:43,  2.83s/it]\u001b[A\n",
      "51it [01:47,  3.22s/it]\u001b[A\n",
      "52it [01:50,  2.92s/it]\u001b[A\n",
      "53it [01:54,  3.40s/it]\u001b[A\n",
      "54it [01:57,  3.41s/it]\u001b[A\n",
      "55it [02:02,  3.80s/it]\u001b[A\n",
      "56it [02:04,  3.32s/it]\u001b[A\n",
      "57it [02:09,  3.64s/it]\u001b[A\n",
      "58it [02:13,  3.68s/it]\u001b[A\n",
      "59it [02:15,  3.24s/it]\u001b[A\n",
      "60it [02:17,  2.89s/it]\u001b[A\n",
      "61it [02:18,  2.49s/it]\u001b[A\n",
      "62it [02:20,  2.16s/it]\u001b[A\n",
      "63it [02:21,  1.88s/it]\u001b[A\n",
      "64it [02:22,  1.62s/it]\u001b[A\n",
      "65it [02:23,  1.34s/it]\u001b[A\n",
      "66it [02:23,  1.06s/it]\u001b[A\n",
      "67it [02:24,  1.14it/s]\u001b[A\n",
      "68it [02:24,  1.29it/s]\u001b[A\n",
      "69it [02:25,  1.40it/s]\u001b[A\n",
      "70it [02:25,  2.08s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|███▍                                  | 608/6740 [00:00<00:01, 6073.75it/s]\u001b[A\n",
      " 19%|██████▉                              | 1266/6740 [00:00<00:00, 6370.99it/s]\u001b[A\n",
      " 28%|██████████▍                          | 1904/6740 [00:00<00:00, 6259.27it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 2531/6740 [00:00<00:00, 5915.15it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 3126/6740 [00:00<00:00, 5635.20it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 3693/6740 [00:00<00:00, 5431.56it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 4239/6740 [00:00<00:00, 5228.27it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 4764/6740 [00:00<00:00, 5178.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 5283/6740 [00:00<00:00, 5084.95it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 5814/6740 [00:01<00:00, 5150.51it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5370.57it/s]\u001b[A\n",
      "Epoch: 9, tfr: 1.0, loss(train/dev): 3.609/3.6865, ppl(dev/test): 39.9049/46.353\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.6301: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.6301: : 1it [00:01,  1.99s/it]\u001b[A\n",
      "batch 2, training loss: 3.5105: : 1it [00:02,  1.99s/it]\u001b[A\n",
      "batch 2, training loss: 3.5105: : 2it [00:02,  1.13s/it]\u001b[A\n",
      "batch 3, training loss: 3.5777: : 2it [00:03,  1.13s/it]\u001b[A\n",
      "batch 3, training loss: 3.5777: : 3it [00:03,  1.15it/s]\u001b[A\n",
      "batch 4, training loss: 3.5241: : 3it [00:03,  1.15it/s]\u001b[A\n",
      "batch 4, training loss: 3.5241: : 4it [00:03,  1.27it/s]\u001b[A\n",
      "batch 5, training loss: 3.3607: : 4it [00:04,  1.27it/s]\u001b[A\n",
      "batch 5, training loss: 3.3607: : 5it [00:04,  1.33it/s]\u001b[A\n",
      "batch 6, training loss: 3.6394: : 5it [00:05,  1.33it/s]\u001b[A\n",
      "batch 6, training loss: 3.6394: : 6it [00:05,  1.36it/s]\u001b[A\n",
      "batch 7, training loss: 3.6291: : 6it [00:05,  1.36it/s]\u001b[A\n",
      "batch 7, training loss: 3.6291: : 7it [00:05,  1.41it/s]\u001b[A\n",
      "batch 8, training loss: 3.6047: : 7it [00:06,  1.41it/s]\u001b[A\n",
      "batch 8, training loss: 3.6047: : 8it [00:06,  1.50it/s]\u001b[A\n",
      "batch 9, training loss: 3.4716: : 8it [00:06,  1.50it/s]\u001b[A\n",
      "batch 9, training loss: 3.4716: : 9it [00:06,  1.62it/s]\u001b[A\n",
      "batch 10, training loss: 3.4644: : 9it [00:07,  1.62it/s]\u001b[A\n",
      "batch 10, training loss: 3.4644: : 10it [00:07,  1.59it/s]\u001b[A\n",
      "batch 11, training loss: 3.499: : 10it [00:08,  1.59it/s] \u001b[A\n",
      "batch 11, training loss: 3.499: : 11it [00:08,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.5429: : 11it [00:08,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.5429: : 12it [00:08,  1.67it/s]\u001b[A\n",
      "batch 13, training loss: 3.4483: : 12it [00:09,  1.67it/s]\u001b[A\n",
      "batch 13, training loss: 3.4483: : 13it [00:09,  1.69it/s]\u001b[A\n",
      "batch 14, training loss: 3.7322: : 13it [00:09,  1.69it/s]\u001b[A\n",
      "batch 14, training loss: 3.7322: : 14it [00:09,  1.65it/s]\u001b[A\n",
      "batch 15, training loss: 3.5714: : 14it [00:10,  1.65it/s]\u001b[A\n",
      "batch 15, training loss: 3.5714: : 15it [00:10,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.5637: : 15it [00:11,  1.60it/s]\u001b[A\n",
      "batch 16, training loss: 3.5637: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.7053: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.7053: : 17it [00:11,  1.51it/s]\u001b[A\n",
      "batch 18, training loss: 3.532: : 17it [00:12,  1.51it/s] \u001b[A\n",
      "batch 18, training loss: 3.532: : 18it [00:12,  1.54it/s]\u001b[A\n",
      "batch 19, training loss: 3.3068: : 18it [00:13,  1.54it/s]\u001b[A\n",
      "batch 19, training loss: 3.3068: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 3.4839: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 3.4839: : 20it [00:13,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 3.5997: : 20it [00:14,  1.61it/s]\u001b[A\n",
      "batch 21, training loss: 3.5997: : 21it [00:14,  1.56it/s]\u001b[A\n",
      "batch 22, training loss: 3.3744: : 21it [00:15,  1.56it/s]\u001b[A\n",
      "batch 22, training loss: 3.3744: : 22it [00:15,  1.51it/s]\u001b[A\n",
      "batch 23, training loss: 3.5252: : 22it [00:15,  1.51it/s]\u001b[A\n",
      "batch 23, training loss: 3.5252: : 23it [00:15,  1.50it/s]\u001b[A\n",
      "batch 24, training loss: 3.468: : 23it [00:16,  1.50it/s] \u001b[A\n",
      "batch 24, training loss: 3.468: : 24it [00:16,  1.62it/s]\u001b[A\n",
      "batch 25, training loss: 3.557: : 24it [00:16,  1.62it/s]\u001b[A\n",
      "batch 25, training loss: 3.557: : 25it [00:16,  1.64it/s]\u001b[A\n",
      "batch 26, training loss: 3.3696: : 25it [00:17,  1.64it/s]\u001b[A\n",
      "batch 26, training loss: 3.3696: : 26it [00:17,  1.62it/s]\u001b[A\n",
      "batch 27, training loss: 3.498: : 26it [00:18,  1.62it/s] \u001b[A\n",
      "batch 27, training loss: 3.498: : 27it [00:18,  1.59it/s]\u001b[A\n",
      "batch 28, training loss: 3.3575: : 27it [00:18,  1.59it/s]\u001b[A\n",
      "batch 28, training loss: 3.3575: : 28it [00:18,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 3.5201: : 28it [00:19,  1.57it/s]\u001b[A\n",
      "batch 29, training loss: 3.5201: : 29it [00:19,  1.61it/s]\u001b[A\n",
      "batch 30, training loss: 3.648: : 29it [00:20,  1.61it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, training loss: 3.648: : 30it [00:20,  1.71it/s]\u001b[A\n",
      "batch 31, training loss: 3.3957: : 30it [00:20,  1.71it/s]\u001b[A\n",
      "batch 31, training loss: 3.3957: : 31it [00:20,  1.65it/s]\u001b[A\n",
      "batch 32, training loss: 3.5646: : 31it [00:21,  1.65it/s]\u001b[A\n",
      "batch 32, training loss: 3.5646: : 32it [00:21,  1.58it/s]\u001b[A\n",
      "batch 33, training loss: 3.4642: : 32it [00:22,  1.58it/s]\u001b[A\n",
      "batch 33, training loss: 3.4642: : 33it [00:22,  1.52it/s]\u001b[A\n",
      "batch 34, training loss: 3.4673: : 33it [00:22,  1.52it/s]\u001b[A\n",
      "batch 34, training loss: 3.4673: : 34it [00:22,  1.48it/s]\u001b[A\n",
      "batch 35, training loss: 3.5262: : 34it [00:23,  1.48it/s]\u001b[A\n",
      "batch 35, training loss: 3.5262: : 35it [00:23,  1.49it/s]\u001b[A\n",
      "batch 36, training loss: 3.6077: : 35it [00:24,  1.49it/s]\u001b[A\n",
      "batch 36, training loss: 3.6077: : 36it [00:24,  1.48it/s]\u001b[A\n",
      "batch 37, training loss: 3.4695: : 36it [00:24,  1.48it/s]\u001b[A\n",
      "batch 37, training loss: 3.4695: : 37it [00:24,  1.53it/s]\u001b[A\n",
      "batch 38, training loss: 3.2558: : 37it [00:25,  1.53it/s]\u001b[A\n",
      "batch 38, training loss: 3.2558: : 38it [00:25,  1.57it/s]\u001b[A\n",
      "batch 39, training loss: 3.423: : 38it [00:25,  1.57it/s] \u001b[A\n",
      "batch 39, training loss: 3.423: : 39it [00:25,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 3.4926: : 39it [00:26,  1.59it/s]\u001b[A\n",
      "batch 40, training loss: 3.4926: : 40it [00:26,  1.56it/s]\u001b[A\n",
      "batch 41, training loss: 3.6487: : 40it [00:27,  1.56it/s]\u001b[A\n",
      "batch 41, training loss: 3.6487: : 41it [00:27,  1.53it/s]\u001b[A\n",
      "batch 42, training loss: 3.4662: : 41it [00:27,  1.53it/s]\u001b[A\n",
      "batch 42, training loss: 3.4662: : 42it [00:27,  1.52it/s]\u001b[A\n",
      "batch 43, training loss: 3.2861: : 42it [00:28,  1.52it/s]\u001b[A\n",
      "batch 43, training loss: 3.2861: : 43it [00:28,  1.56it/s]\u001b[A\n",
      "batch 44, training loss: 3.2651: : 43it [00:29,  1.56it/s]\u001b[A\n",
      "batch 44, training loss: 3.2651: : 44it [00:29,  1.58it/s]\u001b[A\n",
      "batch 45, training loss: 3.3061: : 44it [00:29,  1.58it/s]\u001b[A\n",
      "batch 45, training loss: 3.3061: : 45it [00:29,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 3.446: : 45it [00:30,  1.58it/s] \u001b[A\n",
      "batch 46, training loss: 3.446: : 46it [00:30,  1.54it/s]\u001b[A\n",
      "batch 47, training loss: 3.445: : 46it [00:31,  1.54it/s]\u001b[A\n",
      "batch 47, training loss: 3.445: : 47it [00:31,  1.51it/s]\u001b[A\n",
      "batch 48, training loss: 3.3427: : 47it [00:31,  1.51it/s]\u001b[A\n",
      "batch 48, training loss: 3.3427: : 48it [00:31,  1.49it/s]\u001b[A\n",
      "batch 49, training loss: 3.6159: : 48it [00:32,  1.49it/s]\u001b[A\n",
      "batch 49, training loss: 3.6159: : 49it [00:32,  1.61it/s]\u001b[A\n",
      "batch 50, training loss: 3.3655: : 49it [00:32,  1.61it/s]\u001b[A\n",
      "batch 50, training loss: 3.3655: : 50it [00:32,  1.63it/s]\u001b[A\n",
      "batch 51, training loss: 3.4622: : 50it [00:33,  1.63it/s]\u001b[A\n",
      "batch 51, training loss: 3.4622: : 51it [00:33,  1.62it/s]\u001b[A\n",
      "batch 52, training loss: 3.3168: : 51it [00:34,  1.62it/s]\u001b[A\n",
      "batch 52, training loss: 3.3168: : 52it [00:34,  1.60it/s]\u001b[A\n",
      "batch 53, training loss: 3.3412: : 52it [00:34,  1.60it/s]\u001b[A\n",
      "batch 53, training loss: 3.3412: : 53it [00:34,  1.57it/s]\u001b[A\n",
      "batch 54, training loss: 3.2888: : 53it [00:35,  1.57it/s]\u001b[A\n",
      "batch 54, training loss: 3.2888: : 54it [00:35,  1.63it/s]\u001b[A\n",
      "batch 55, training loss: 3.4127: : 54it [00:36,  1.63it/s]\u001b[A\n",
      "batch 55, training loss: 3.4127: : 55it [00:36,  1.71it/s]\u001b[A\n",
      "batch 56, training loss: 3.4408: : 55it [00:36,  1.71it/s]\u001b[A\n",
      "batch 56, training loss: 3.4408: : 56it [00:36,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.4228: : 56it [00:37,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.4228: : 57it [00:37,  1.58it/s]\u001b[A\n",
      "batch 58, training loss: 3.4779: : 57it [00:38,  1.58it/s]\u001b[A\n",
      "batch 58, training loss: 3.4779: : 58it [00:38,  1.53it/s]\u001b[A\n",
      "batch 59, training loss: 3.3384: : 58it [00:38,  1.53it/s]\u001b[A\n",
      "batch 59, training loss: 3.3384: : 59it [00:38,  1.55it/s]\u001b[A\n",
      "batch 60, training loss: 3.2954: : 59it [00:39,  1.55it/s]\u001b[A\n",
      "batch 60, training loss: 3.2954: : 60it [00:39,  1.59it/s]\u001b[A\n",
      "batch 61, training loss: 3.5279: : 60it [00:39,  1.59it/s]\u001b[A\n",
      "batch 61, training loss: 3.5279: : 61it [00:39,  1.61it/s]\u001b[A\n",
      "batch 62, training loss: 3.3691: : 61it [00:40,  1.61it/s]\u001b[A\n",
      "batch 62, training loss: 3.3691: : 62it [00:40,  1.59it/s]\u001b[A\n",
      "batch 63, training loss: 3.4495: : 62it [00:41,  1.59it/s]\u001b[A\n",
      "batch 63, training loss: 3.4495: : 63it [00:41,  1.55it/s]\u001b[A\n",
      "batch 64, training loss: 3.4163: : 63it [00:41,  1.55it/s]\u001b[A\n",
      "batch 64, training loss: 3.4163: : 64it [00:41,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.4755: : 64it [00:42,  1.51it/s]\u001b[A\n",
      "batch 65, training loss: 3.4755: : 65it [00:42,  1.49it/s]\u001b[A\n",
      "batch 66, training loss: 3.5183: : 65it [00:43,  1.49it/s]\u001b[A\n",
      "batch 66, training loss: 3.5183: : 66it [00:43,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.3701: : 66it [00:43,  1.51it/s]\u001b[A\n",
      "batch 67, training loss: 3.3701: : 67it [00:43,  1.57it/s]\u001b[A\n",
      "batch 68, training loss: 3.4828: : 67it [00:44,  1.57it/s]\u001b[A\n",
      "batch 68, training loss: 3.4828: : 68it [00:44,  1.65it/s]\u001b[A\n",
      "batch 69, training loss: 3.3578: : 68it [00:45,  1.65it/s]\u001b[A\n",
      "batch 69, training loss: 3.3578: : 69it [00:45,  1.61it/s]\u001b[A\n",
      "batch 70, training loss: 3.5507: : 69it [00:45,  1.61it/s]\u001b[A\n",
      "batch 70, training loss: 3.5507: : 70it [00:45,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.3834: : 70it [00:46,  1.57it/s]\u001b[A\n",
      "batch 71, training loss: 3.3834: : 71it [00:46,  1.69it/s]\u001b[A\n",
      "batch 72, training loss: 3.3577: : 71it [00:46,  1.69it/s]\u001b[A\n",
      "batch 72, training loss: 3.3577: : 72it [00:46,  1.71it/s]\u001b[A\n",
      "batch 73, training loss: 3.4047: : 72it [00:47,  1.71it/s]\u001b[A\n",
      "batch 73, training loss: 3.4047: : 73it [00:47,  1.64it/s]\u001b[A\n",
      "batch 74, training loss: 3.3: : 73it [00:47,  1.64it/s]   \u001b[A\n",
      "batch 74, training loss: 3.3: : 74it [00:47,  1.70it/s]\u001b[A\n",
      "batch 75, training loss: 3.5227: : 74it [00:48,  1.70it/s]\u001b[A\n",
      "batch 75, training loss: 3.5227: : 75it [00:48,  1.69it/s]\u001b[A\n",
      "batch 76, training loss: 3.3335: : 75it [00:49,  1.69it/s]\u001b[A\n",
      "batch 76, training loss: 3.3335: : 76it [00:49,  1.70it/s]\u001b[A\n",
      "batch 77, training loss: 3.4702: : 76it [00:49,  1.70it/s]\u001b[A\n",
      "batch 77, training loss: 3.4702: : 77it [00:49,  1.64it/s]\u001b[A\n",
      "batch 78, training loss: 3.4408: : 77it [00:50,  1.64it/s]\u001b[A\n",
      "batch 78, training loss: 3.4408: : 78it [00:50,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 3.4812: : 78it [00:51,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 3.4812: : 79it [00:51,  1.55it/s]\u001b[A\n",
      "batch 80, training loss: 3.3998: : 79it [00:51,  1.55it/s]\u001b[A\n",
      "batch 80, training loss: 3.3998: : 80it [00:51,  1.58it/s]\u001b[A\n",
      "batch 81, training loss: 3.4018: : 80it [00:52,  1.58it/s]\u001b[A\n",
      "batch 81, training loss: 3.4018: : 81it [00:52,  1.61it/s]\u001b[A\n",
      "batch 82, training loss: 3.5419: : 81it [00:52,  1.61it/s]\u001b[A\n",
      "batch 82, training loss: 3.5419: : 82it [00:52,  1.59it/s]\u001b[A\n",
      "batch 83, training loss: 3.4177: : 82it [00:53,  1.59it/s]\u001b[A\n",
      "batch 83, training loss: 3.4177: : 83it [00:53,  1.55it/s]\u001b[A\n",
      "batch 84, training loss: 3.5548: : 83it [00:54,  1.55it/s]\u001b[A\n",
      "batch 84, training loss: 3.5548: : 84it [00:54,  1.50it/s]\u001b[A\n",
      "batch 85, training loss: 3.5687: : 84it [00:55,  1.50it/s]\u001b[A\n",
      "batch 85, training loss: 3.5687: : 85it [00:55,  1.49it/s]\u001b[A\n",
      "batch 86, training loss: 3.5271: : 85it [00:55,  1.49it/s]\u001b[A\n",
      "batch 86, training loss: 3.5271: : 86it [00:55,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 3.5333: : 86it [00:56,  1.61it/s]\u001b[A\n",
      "batch 87, training loss: 3.5333: : 87it [00:56,  1.70it/s]\u001b[A\n",
      "batch 88, training loss: 3.5409: : 87it [00:56,  1.70it/s]\u001b[A\n",
      "batch 88, training loss: 3.5409: : 88it [00:56,  1.59it/s]\u001b[A\n",
      "batch 89, training loss: 3.6699: : 88it [00:57,  1.59it/s]\u001b[A\n",
      "batch 89, training loss: 3.6699: : 89it [00:57,  1.54it/s]\u001b[A\n",
      "batch 90, training loss: 3.5653: : 89it [00:58,  1.54it/s]\u001b[A\n",
      "batch 90, training loss: 3.5653: : 90it [00:58,  1.45it/s]\u001b[A\n",
      "batch 91, training loss: 3.7157: : 90it [00:59,  1.45it/s]\u001b[A\n",
      "batch 91, training loss: 3.7157: : 91it [00:59,  1.40it/s]\u001b[A\n",
      "batch 92, training loss: 3.5283: : 91it [00:59,  1.40it/s]\u001b[A\n",
      "batch 92, training loss: 3.5283: : 92it [00:59,  1.37it/s]\u001b[A\n",
      "batch 93, training loss: 3.4905: : 92it [01:00,  1.37it/s]\u001b[A\n",
      "batch 93, training loss: 3.4905: : 93it [01:00,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 3.6072: : 93it [01:01,  1.35it/s]\u001b[A\n",
      "batch 94, training loss: 3.6072: : 94it [01:01,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.5671: : 94it [01:02,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.5671: : 95it [01:02,  1.32it/s]\u001b[A\n",
      "batch 96, training loss: 3.577: : 95it [01:02,  1.32it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 96, training loss: 3.577: : 96it [01:02,  1.33it/s]\u001b[A\n",
      "batch 97, training loss: 3.6659: : 96it [01:03,  1.33it/s]\u001b[A\n",
      "batch 97, training loss: 3.6659: : 97it [01:03,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 3.6416: : 97it [01:04,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 3.6416: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.4499: : 98it [01:05,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.4499: : 99it [01:05,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.303: : 99it [01:05,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.303: : 100it [01:05,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 3.4738: : 100it [01:06,  1.31it/s]\u001b[A\n",
      "batch 101, training loss: 3.4738: : 101it [01:06,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.4898: : 101it [01:07,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.4898: : 102it [01:07,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.4248: : 102it [01:08,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.4248: : 103it [01:08,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 3.2715: : 103it [01:09,  1.31it/s]\u001b[A\n",
      "batch 104, training loss: 3.2715: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.4287: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.4287: : 105it [01:09,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.5875: : 105it [01:10,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.5875: : 106it [01:10,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.2865: : 106it [01:11,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.2865: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.5546: : 107it [01:12,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.5546: : 108it [01:12,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 3.6042: : 108it [01:12,  1.29it/s]\u001b[A\n",
      "batch 109, training loss: 3.6042: : 109it [01:12,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.7279: : 109it [01:13,  1.30it/s]\u001b[A\n",
      "batch 110, training loss: 3.7279: : 110it [01:13,  1.32it/s]\u001b[A\n",
      "batch 111, training loss: 3.4407: : 110it [01:14,  1.32it/s]\u001b[A\n",
      "batch 111, training loss: 3.4407: : 111it [01:14,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.5221: : 111it [01:15,  1.31it/s]\u001b[A\n",
      "batch 112, training loss: 3.5221: : 112it [01:15,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.5936: : 112it [01:15,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.5936: : 113it [01:15,  1.31it/s]\u001b[A\n",
      "batch 114, training loss: 3.5857: : 113it [01:16,  1.31it/s]\u001b[A\n",
      "batch 114, training loss: 3.5857: : 114it [01:16,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 3.4869: : 114it [01:17,  1.31it/s]\u001b[A\n",
      "batch 115, training loss: 3.4869: : 115it [01:17,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.4962: : 115it [01:18,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.4962: : 116it [01:18,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.5428: : 116it [01:19,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.5428: : 117it [01:19,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 3.609: : 117it [01:19,  1.31it/s] \u001b[A\n",
      "batch 118, training loss: 3.609: : 118it [01:19,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.5336: : 118it [01:20,  1.30it/s]\u001b[A\n",
      "batch 119, training loss: 3.5336: : 119it [01:20,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.4643: : 119it [01:21,  1.30it/s]\u001b[A\n",
      "batch 120, training loss: 3.4643: : 120it [01:21,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 3.7169: : 120it [01:22,  1.29it/s]\u001b[A\n",
      "batch 121, training loss: 3.7169: : 121it [01:22,  1.29it/s]\u001b[A\n",
      "batch 122, training loss: 3.389: : 121it [01:22,  1.29it/s] \u001b[A\n",
      "batch 122, training loss: 3.389: : 122it [01:22,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.5519: : 122it [01:23,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.5519: : 123it [01:23,  1.29it/s]\u001b[A\n",
      "batch 124, training loss: 3.555: : 123it [01:24,  1.29it/s] \u001b[A\n",
      "batch 124, training loss: 3.555: : 124it [01:24,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 3.5752: : 124it [01:25,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 3.5752: : 125it [01:25,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 3.6454: : 125it [01:25,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 3.6454: : 126it [01:25,  1.30it/s]\u001b[A\n",
      "batch 127, training loss: 3.345: : 126it [01:26,  1.30it/s] \u001b[A\n",
      "batch 127, training loss: 3.345: : 127it [01:26,  1.46it/s]\u001b[A\n",
      "batch 128, training loss: 3.569: : 127it [01:27,  1.46it/s]\u001b[A\n",
      "batch 128, training loss: 3.569: : 128it [01:27,  1.45it/s]\u001b[A\n",
      "batch 129, training loss: 3.5034: : 128it [01:27,  1.45it/s]\u001b[A\n",
      "batch 129, training loss: 3.5034: : 129it [01:27,  1.41it/s]\u001b[A\n",
      "batch 130, training loss: 3.5073: : 129it [01:28,  1.41it/s]\u001b[A\n",
      "batch 130, training loss: 3.5073: : 130it [01:28,  1.37it/s]\u001b[A\n",
      "batch 131, training loss: 3.6857: : 130it [01:29,  1.37it/s]\u001b[A\n",
      "batch 131, training loss: 3.6857: : 131it [01:29,  1.35it/s]\u001b[A\n",
      "batch 132, training loss: 3.4882: : 131it [01:30,  1.35it/s]\u001b[A\n",
      "batch 132, training loss: 3.4882: : 132it [01:30,  1.35it/s]\u001b[A\n",
      "batch 133, training loss: 3.3919: : 132it [01:30,  1.35it/s]\u001b[A\n",
      "batch 133, training loss: 3.3919: : 133it [01:30,  1.33it/s]\u001b[A\n",
      "batch 134, training loss: 3.5517: : 133it [01:31,  1.33it/s]\u001b[A\n",
      "batch 134, training loss: 3.5517: : 134it [01:31,  1.32it/s]\u001b[A\n",
      "batch 135, training loss: 3.4672: : 134it [01:32,  1.32it/s]\u001b[A\n",
      "batch 135, training loss: 3.4672: : 135it [01:32,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 3.4137: : 135it [01:33,  1.32it/s]\u001b[A\n",
      "batch 136, training loss: 3.4137: : 136it [01:33,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 3.5674: : 136it [01:34,  1.32it/s]\u001b[A\n",
      "batch 137, training loss: 3.5674: : 137it [01:34,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 3.5769: : 137it [01:34,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 3.5769: : 138it [01:34,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.3675: : 138it [01:35,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.3675: : 139it [01:35,  1.31it/s]\u001b[A\n",
      "batch 140, training loss: 3.4723: : 139it [01:36,  1.31it/s]\u001b[A\n",
      "batch 140, training loss: 3.4723: : 140it [01:36,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.4791: : 140it [01:37,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.4791: : 141it [01:37,  1.30it/s]\u001b[A\n",
      "batch 142, training loss: 3.4988: : 141it [01:37,  1.30it/s]\u001b[A\n",
      "batch 142, training loss: 3.4988: : 142it [01:37,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.3443: : 142it [01:38,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.3443: : 143it [01:38,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 3.3994: : 143it [01:39,  1.30it/s]\u001b[A\n",
      "batch 144, training loss: 3.3994: : 144it [01:39,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.4975: : 144it [01:40,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.4975: : 145it [01:40,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 3.4878: : 145it [01:40,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 3.4878: : 146it [01:40,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.3903: : 146it [01:41,  1.31it/s]\u001b[A\n",
      "batch 147, training loss: 3.3903: : 147it [01:41,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.4759: : 147it [01:42,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.4759: : 148it [01:42,  1.30it/s]\u001b[A\n",
      "batch 149, training loss: 3.564: : 148it [01:43,  1.30it/s] \u001b[A\n",
      "batch 149, training loss: 3.564: : 149it [01:43,  1.30it/s]\u001b[A\n",
      "batch 150, training loss: 3.5525: : 149it [01:44,  1.30it/s]\u001b[A\n",
      "batch 150, training loss: 3.5525: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 3.4782: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 3.4782: : 151it [01:44,  1.30it/s]\u001b[A\n",
      "batch 152, training loss: 3.409: : 151it [01:45,  1.30it/s] \u001b[A\n",
      "batch 152, training loss: 3.409: : 152it [01:45,  1.31it/s]\u001b[A\n",
      "batch 153, training loss: 3.4374: : 152it [01:46,  1.31it/s]\u001b[A\n",
      "batch 153, training loss: 3.4374: : 153it [01:46,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.5442: : 153it [01:47,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.5442: : 154it [01:47,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 3.6801: : 154it [01:47,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 3.6801: : 155it [01:47,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.2961: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.2961: : 156it [01:48,  1.29it/s]\u001b[A\n",
      "batch 157, training loss: 3.4998: : 156it [01:49,  1.29it/s]\u001b[A\n",
      "batch 157, training loss: 3.4998: : 157it [01:49,  1.30it/s]\u001b[A\n",
      "batch 158, training loss: 3.4812: : 157it [01:50,  1.30it/s]\u001b[A\n",
      "batch 158, training loss: 3.4812: : 158it [01:50,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.3861: : 158it [01:50,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.3861: : 159it [01:50,  1.30it/s]\u001b[A\n",
      "batch 160, training loss: 3.5707: : 159it [01:51,  1.30it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 160, training loss: 3.5707: : 160it [01:51,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.4401: : 160it [01:52,  1.32it/s]\u001b[A\n",
      "batch 161, training loss: 3.4401: : 161it [01:52,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 3.4131: : 161it [01:53,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 3.4131: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.6543: : 162it [01:54,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.6543: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.4722: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.4722: : 164it [01:54,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.4469: : 164it [01:55,  1.33it/s]\u001b[A\n",
      "batch 165, training loss: 3.4469: : 165it [01:55,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.4317: : 165it [01:56,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.4317: : 166it [01:56,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.5821: : 166it [01:57,  1.31it/s]\u001b[A\n",
      "batch 167, training loss: 3.5821: : 167it [01:57,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 3.5421: : 167it [01:57,  1.30it/s]\u001b[A\n",
      "batch 168, training loss: 3.5421: : 168it [01:57,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 3.6019: : 168it [01:58,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 3.6019: : 169it [01:58,  1.30it/s]\u001b[A\n",
      "batch 170, training loss: 3.4864: : 169it [01:59,  1.30it/s]\u001b[A\n",
      "batch 170, training loss: 3.4864: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.6002: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.6002: : 171it [01:59,  1.55it/s]\u001b[A\n",
      "batch 172, training loss: 3.678: : 171it [02:00,  1.55it/s] \u001b[A\n",
      "batch 172, training loss: 3.678: : 172it [02:00,  1.44it/s]\u001b[A\n",
      "batch 173, training loss: 3.521: : 172it [02:01,  1.44it/s]\u001b[A\n",
      "batch 173, training loss: 3.521: : 173it [02:01,  1.42it/s]\u001b[A\n",
      "batch 174, training loss: 3.6986: : 173it [02:02,  1.42it/s]\u001b[A\n",
      "batch 174, training loss: 3.6986: : 174it [02:02,  1.35it/s]\u001b[A\n",
      "batch 175, training loss: 3.6182: : 174it [02:02,  1.35it/s]\u001b[A\n",
      "batch 175, training loss: 3.6182: : 175it [02:02,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 3.5678: : 175it [02:03,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 3.5678: : 176it [02:03,  1.29it/s]\u001b[A\n",
      "batch 177, training loss: 3.5426: : 176it [02:04,  1.29it/s]\u001b[A\n",
      "batch 177, training loss: 3.5426: : 177it [02:04,  1.27it/s]\u001b[A\n",
      "batch 178, training loss: 3.5586: : 177it [02:05,  1.27it/s]\u001b[A\n",
      "batch 178, training loss: 3.5586: : 178it [02:05,  1.26it/s]\u001b[A\n",
      "batch 179, training loss: 3.6234: : 178it [02:06,  1.26it/s]\u001b[A\n",
      "batch 179, training loss: 3.6234: : 179it [02:06,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 3.6482: : 179it [02:07,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 3.6482: : 180it [02:07,  1.24it/s]\u001b[A\n",
      "batch 181, training loss: 3.7146: : 180it [02:07,  1.24it/s]\u001b[A\n",
      "batch 181, training loss: 3.7146: : 181it [02:07,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.6231: : 181it [02:08,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.6231: : 182it [02:08,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 3.7463: : 182it [02:09,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 3.7463: : 183it [02:09,  1.24it/s]\u001b[A\n",
      "batch 184, training loss: 3.4746: : 183it [02:10,  1.24it/s]\u001b[A\n",
      "batch 184, training loss: 3.4746: : 184it [02:10,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 3.52: : 184it [02:11,  1.25it/s]  \u001b[A\n",
      "batch 185, training loss: 3.52: : 185it [02:11,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 3.6857: : 185it [02:11,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 3.6857: : 186it [02:11,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 3.6904: : 186it [02:12,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 3.6904: : 187it [02:12,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 3.7756: : 187it [02:13,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 3.7756: : 188it [02:13,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.8163: : 188it [02:14,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.8163: : 189it [02:14,  1.22it/s]\u001b[A\n",
      "batch 190, training loss: 3.4948: : 189it [02:15,  1.22it/s]\u001b[A\n",
      "batch 190, training loss: 3.4948: : 190it [02:15,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.5181: : 190it [02:15,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.5181: : 191it [02:15,  1.21it/s]\u001b[A\n",
      "batch 192, training loss: 3.6003: : 191it [02:16,  1.21it/s]\u001b[A\n",
      "batch 192, training loss: 3.6003: : 192it [02:16,  1.21it/s]\u001b[A\n",
      "batch 193, training loss: 3.4846: : 192it [02:17,  1.21it/s]\u001b[A\n",
      "batch 193, training loss: 3.4846: : 193it [02:17,  1.21it/s]\u001b[A\n",
      "batch 194, training loss: 3.5954: : 193it [02:18,  1.21it/s]\u001b[A\n",
      "batch 194, training loss: 3.5954: : 194it [02:18,  1.20it/s]\u001b[A\n",
      "batch 195, training loss: 3.5309: : 194it [02:19,  1.20it/s]\u001b[A\n",
      "batch 195, training loss: 3.5309: : 195it [02:19,  1.21it/s]\u001b[A\n",
      "batch 196, training loss: 3.4646: : 195it [02:20,  1.21it/s]\u001b[A\n",
      "batch 196, training loss: 3.4646: : 196it [02:20,  1.22it/s]\u001b[A\n",
      "batch 197, training loss: 3.6523: : 196it [02:20,  1.22it/s]\u001b[A\n",
      "batch 197, training loss: 3.6523: : 197it [02:20,  1.22it/s]\u001b[A\n",
      "batch 198, training loss: 3.5681: : 197it [02:21,  1.22it/s]\u001b[A\n",
      "batch 198, training loss: 3.5681: : 198it [02:21,  1.22it/s]\u001b[A\n",
      "batch 199, training loss: 3.5134: : 198it [02:22,  1.22it/s]\u001b[A\n",
      "batch 199, training loss: 3.5134: : 199it [02:22,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.6804: : 199it [02:23,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.6804: : 200it [02:23,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 3.4627: : 200it [02:24,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 3.4627: : 201it [02:24,  1.21it/s]\u001b[A\n",
      "batch 202, training loss: 3.3818: : 201it [02:25,  1.21it/s]\u001b[A\n",
      "batch 202, training loss: 3.3818: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.5828: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.5828: : 203it [02:25,  1.22it/s]\u001b[A\n",
      "batch 204, training loss: 3.6189: : 203it [02:26,  1.22it/s]\u001b[A\n",
      "batch 204, training loss: 3.6189: : 204it [02:26,  1.21it/s]\u001b[A\n",
      "batch 205, training loss: 3.582: : 204it [02:27,  1.21it/s] \u001b[A\n",
      "batch 205, training loss: 3.582: : 205it [02:27,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 3.6989: : 205it [02:28,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 3.6989: : 206it [02:28,  1.21it/s]\u001b[A\n",
      "batch 207, training loss: 3.4948: : 206it [02:29,  1.21it/s]\u001b[A\n",
      "batch 207, training loss: 3.4948: : 207it [02:29,  1.21it/s]\u001b[A\n",
      "batch 208, training loss: 3.5472: : 207it [02:29,  1.21it/s]\u001b[A\n",
      "batch 208, training loss: 3.5472: : 208it [02:29,  1.31it/s]\u001b[A\n",
      "batch 209, training loss: 3.5349: : 208it [02:30,  1.31it/s]\u001b[A\n",
      "batch 209, training loss: 3.5349: : 209it [02:30,  1.32it/s]\u001b[A\n",
      "batch 210, training loss: 3.5688: : 209it [02:31,  1.32it/s]\u001b[A\n",
      "batch 210, training loss: 3.5688: : 210it [02:31,  1.27it/s]\u001b[A\n",
      "batch 211, training loss: 3.529: : 210it [02:32,  1.27it/s] \u001b[A\n",
      "batch 211, training loss: 3.529: : 211it [02:32,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 3.5268: : 211it [02:33,  1.23it/s]\u001b[A\n",
      "batch 212, training loss: 3.5268: : 212it [02:33,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.7312: : 212it [02:33,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.7312: : 213it [02:33,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.5885: : 213it [02:34,  1.24it/s]\u001b[A\n",
      "batch 214, training loss: 3.5885: : 214it [02:34,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.4315: : 214it [02:35,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.4315: : 215it [02:35,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.5713: : 215it [02:36,  1.23it/s]\u001b[A\n",
      "batch 216, training loss: 3.5713: : 216it [02:36,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.6771: : 216it [02:37,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.6771: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.6199: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.6199: : 218it [02:37,  1.27it/s]\u001b[A\n",
      "batch 219, training loss: 3.8158: : 218it [02:38,  1.27it/s]\u001b[A\n",
      "batch 219, training loss: 3.8158: : 219it [02:38,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.7214: : 219it [02:39,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.7214: : 220it [02:39,  1.25it/s]\u001b[A\n",
      "batch 221, training loss: 3.5959: : 220it [02:40,  1.25it/s]\u001b[A\n",
      "batch 221, training loss: 3.5959: : 221it [02:40,  1.25it/s]\u001b[A\n",
      "batch 222, training loss: 3.6373: : 221it [02:41,  1.25it/s]\u001b[A\n",
      "batch 222, training loss: 3.6373: : 222it [02:41,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.6268: : 222it [02:41,  1.24it/s]\u001b[A\n",
      "batch 223, training loss: 3.6268: : 223it [02:41,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.5191: : 223it [02:42,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 224, training loss: 3.5191: : 224it [02:42,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 3.6735: : 224it [02:43,  1.24it/s]\u001b[A\n",
      "batch 225, training loss: 3.6735: : 225it [02:43,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.6552: : 225it [02:44,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.6552: : 226it [02:44,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 3.5316: : 226it [02:45,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 3.5316: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.609: : 227it [02:45,  1.24it/s] \u001b[A\n",
      "batch 228, training loss: 3.609: : 228it [02:45,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.5748: : 228it [02:46,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.5748: : 229it [02:46,  1.25it/s]\u001b[A\n",
      "batch 230, training loss: 3.716: : 229it [02:47,  1.25it/s] \u001b[A\n",
      "batch 230, training loss: 3.716: : 230it [02:47,  1.25it/s]\u001b[A\n",
      "batch 231, training loss: 3.7587: : 230it [02:48,  1.25it/s]\u001b[A\n",
      "batch 231, training loss: 3.7587: : 231it [02:48,  1.25it/s]\u001b[A\n",
      "batch 232, training loss: 3.5694: : 231it [02:49,  1.25it/s]\u001b[A\n",
      "batch 232, training loss: 3.5694: : 232it [02:49,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.6194: : 232it [02:49,  1.24it/s]\u001b[A\n",
      "batch 233, training loss: 3.6194: : 233it [02:49,  1.25it/s]\u001b[A\n",
      "batch 234, training loss: 3.753: : 233it [02:50,  1.25it/s] \u001b[A\n",
      "batch 234, training loss: 3.753: : 234it [02:50,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 3.5853: : 234it [02:51,  1.24it/s]\u001b[A\n",
      "batch 235, training loss: 3.5853: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 3.6297: : 235it [02:52,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 3.6297: : 236it [02:52,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.5019: : 236it [02:53,  1.24it/s]\u001b[A\n",
      "batch 237, training loss: 3.5019: : 237it [02:53,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.6789: : 237it [02:53,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.6789: : 238it [02:53,  1.27it/s]\u001b[A\n",
      "batch 239, training loss: 3.5579: : 238it [02:54,  1.27it/s]\u001b[A\n",
      "batch 239, training loss: 3.5579: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.5911: : 239it [02:55,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.5911: : 240it [02:55,  1.26it/s]\u001b[A\n",
      "batch 241, training loss: 3.6141: : 240it [02:56,  1.26it/s]\u001b[A\n",
      "batch 241, training loss: 3.6141: : 241it [02:56,  1.40it/s]\u001b[A\n",
      "batch 242, training loss: 3.4708: : 241it [02:56,  1.40it/s]\u001b[A\n",
      "batch 242, training loss: 3.4708: : 242it [02:56,  1.38it/s]\u001b[A\n",
      "batch 243, training loss: 3.6309: : 242it [02:57,  1.38it/s]\u001b[A\n",
      "batch 243, training loss: 3.6309: : 243it [02:57,  1.33it/s]\u001b[A\n",
      "batch 244, training loss: 3.5237: : 243it [02:58,  1.33it/s]\u001b[A\n",
      "batch 244, training loss: 3.5237: : 244it [02:58,  1.31it/s]\u001b[A\n",
      "batch 245, training loss: 3.6594: : 244it [02:59,  1.31it/s]\u001b[A\n",
      "batch 245, training loss: 3.6594: : 245it [02:59,  1.29it/s]\u001b[A\n",
      "batch 246, training loss: 3.5833: : 245it [03:00,  1.29it/s]\u001b[A\n",
      "batch 246, training loss: 3.5833: : 246it [03:00,  1.27it/s]\u001b[A\n",
      "batch 247, training loss: 3.5842: : 246it [03:00,  1.27it/s]\u001b[A\n",
      "batch 247, training loss: 3.5842: : 247it [03:00,  1.27it/s]\u001b[A\n",
      "batch 248, training loss: 3.5149: : 247it [03:01,  1.27it/s]\u001b[A\n",
      "batch 248, training loss: 3.5149: : 248it [03:01,  1.26it/s]\u001b[A\n",
      "batch 249, training loss: 3.4862: : 248it [03:02,  1.26it/s]\u001b[A\n",
      "batch 249, training loss: 3.4862: : 249it [03:02,  1.26it/s]\u001b[A\n",
      "batch 250, training loss: 3.6659: : 249it [03:03,  1.26it/s]\u001b[A\n",
      "batch 250, training loss: 3.6659: : 250it [03:03,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 3.668: : 250it [03:04,  1.25it/s] \u001b[A\n",
      "batch 251, training loss: 3.668: : 251it [03:04,  1.24it/s]\u001b[A\n",
      "batch 252, training loss: 2.9413: : 251it [03:04,  1.24it/s]\u001b[A\n",
      "batch 252, training loss: 2.9413: : 252it [03:04,  1.46it/s]\u001b[A\n",
      "batch 253, training loss: 3.5191: : 252it [03:05,  1.46it/s]\u001b[A\n",
      "batch 253, training loss: 3.5191: : 253it [03:05,  1.34it/s]\u001b[A\n",
      "batch 254, training loss: 3.6515: : 253it [03:06,  1.34it/s]\u001b[A\n",
      "batch 254, training loss: 3.6515: : 254it [03:06,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.4544: : 254it [03:07,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.4544: : 255it [03:07,  1.21it/s]\u001b[A\n",
      "batch 256, training loss: 3.506: : 255it [03:08,  1.21it/s] \u001b[A\n",
      "batch 256, training loss: 3.506: : 256it [03:08,  1.17it/s]\u001b[A\n",
      "batch 257, training loss: 3.6059: : 256it [03:08,  1.17it/s]\u001b[A\n",
      "batch 257, training loss: 3.6059: : 257it [03:08,  1.17it/s]\u001b[A\n",
      "batch 258, training loss: 3.6978: : 257it [03:09,  1.17it/s]\u001b[A\n",
      "batch 258, training loss: 3.6978: : 258it [03:09,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.5665: : 258it [03:10,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.5665: : 259it [03:10,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 3.5653: : 259it [03:11,  1.12it/s]\u001b[A\n",
      "batch 260, training loss: 3.5653: : 260it [03:11,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 3.6287: : 260it [03:12,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 3.6287: : 261it [03:12,  1.14it/s]\u001b[A\n",
      "batch 262, training loss: 3.5548: : 261it [03:13,  1.14it/s]\u001b[A\n",
      "batch 262, training loss: 3.5548: : 262it [03:13,  1.13it/s]\u001b[A\n",
      "batch 263, training loss: 3.5418: : 262it [03:14,  1.13it/s]\u001b[A\n",
      "batch 263, training loss: 3.5418: : 263it [03:14,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 3.6533: : 263it [03:15,  1.11it/s]\u001b[A\n",
      "batch 264, training loss: 3.6533: : 264it [03:15,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.4518: : 264it [03:16,  1.10it/s]\u001b[A\n",
      "batch 265, training loss: 3.4518: : 265it [03:16,  1.10it/s]\u001b[A\n",
      "batch 266, training loss: 3.641: : 265it [03:17,  1.10it/s] \u001b[A\n",
      "batch 266, training loss: 3.641: : 266it [03:17,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.615: : 266it [03:17,  1.09it/s]\u001b[A\n",
      "batch 267, training loss: 3.615: : 267it [03:17,  1.11it/s]\u001b[A\n",
      "batch 268, training loss: 3.4997: : 267it [03:18,  1.11it/s]\u001b[A\n",
      "batch 268, training loss: 3.4997: : 268it [03:18,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.5341: : 268it [03:19,  1.11it/s]\u001b[A\n",
      "batch 269, training loss: 3.5341: : 269it [03:19,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.5688: : 269it [03:20,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.5688: : 270it [03:20,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.5464: : 270it [03:21,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.5464: : 271it [03:21,  1.13it/s]\u001b[A\n",
      "batch 272, training loss: 3.544: : 271it [03:22,  1.13it/s] \u001b[A\n",
      "batch 272, training loss: 3.544: : 272it [03:22,  1.11it/s]\u001b[A\n",
      "batch 273, training loss: 3.6439: : 272it [03:23,  1.11it/s]\u001b[A\n",
      "batch 273, training loss: 3.6439: : 273it [03:23,  1.16it/s]\u001b[A\n",
      "batch 274, training loss: 3.6319: : 273it [03:24,  1.16it/s]\u001b[A\n",
      "batch 274, training loss: 3.6319: : 274it [03:24,  1.17it/s]\u001b[A\n",
      "batch 275, training loss: 3.5164: : 274it [03:25,  1.17it/s]\u001b[A\n",
      "batch 275, training loss: 3.5164: : 275it [03:25,  1.14it/s]\u001b[A\n",
      "batch 276, training loss: 3.4099: : 275it [03:25,  1.14it/s]\u001b[A\n",
      "batch 276, training loss: 3.4099: : 276it [03:25,  1.17it/s]\u001b[A\n",
      "batch 277, training loss: 3.5636: : 276it [03:26,  1.17it/s]\u001b[A\n",
      "batch 277, training loss: 3.5636: : 277it [03:26,  1.19it/s]\u001b[A\n",
      "batch 278, training loss: 3.5034: : 277it [03:27,  1.19it/s]\u001b[A\n",
      "batch 278, training loss: 3.5034: : 278it [03:27,  1.15it/s]\u001b[A\n",
      "batch 279, training loss: 3.4924: : 278it [03:28,  1.15it/s]\u001b[A\n",
      "batch 279, training loss: 3.4924: : 279it [03:28,  1.13it/s]\u001b[A\n",
      "batch 280, training loss: 3.4043: : 279it [03:29,  1.13it/s]\u001b[A\n",
      "batch 280, training loss: 3.4043: : 280it [03:29,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.5034: : 280it [03:30,  1.11it/s]\u001b[A\n",
      "batch 281, training loss: 3.5034: : 281it [03:30,  1.11it/s]\u001b[A\n",
      "batch 282, training loss: 3.402: : 281it [03:31,  1.11it/s] \u001b[A\n",
      "batch 282, training loss: 3.402: : 282it [03:31,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.5083: : 282it [03:32,  1.11it/s]\u001b[A\n",
      "batch 283, training loss: 3.5083: : 283it [03:32,  1.12it/s]\u001b[A\n",
      "batch 284, training loss: 3.504: : 283it [03:33,  1.12it/s] \u001b[A\n",
      "batch 284, training loss: 3.504: : 284it [03:33,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 3.5397: : 284it [03:33,  1.11it/s]\u001b[A\n",
      "batch 285, training loss: 3.5397: : 285it [03:33,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 3.708: : 285it [03:34,  1.10it/s] \u001b[A\n",
      "batch 286, training loss: 3.708: : 286it [03:34,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.4173: : 286it [03:35,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.4173: : 287it [03:35,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.4492: : 287it [03:36,  1.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 288, training loss: 3.4492: : 288it [03:36,  1.09it/s]\u001b[A\n",
      "batch 289, training loss: 3.6217: : 288it [03:37,  1.09it/s]\u001b[A\n",
      "batch 289, training loss: 3.6217: : 289it [03:37,  1.08it/s]\u001b[A\n",
      "batch 290, training loss: 3.3813: : 289it [03:38,  1.08it/s]\u001b[A\n",
      "batch 290, training loss: 3.3813: : 290it [03:38,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.6581: : 290it [03:39,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.6581: : 291it [03:39,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.4712: : 291it [03:40,  1.11it/s]\u001b[A\n",
      "batch 292, training loss: 3.4712: : 292it [03:40,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.5547: : 292it [03:41,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.5547: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.6361: : 293it [03:42,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.6361: : 294it [03:42,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 3.5714: : 294it [03:43,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 3.5714: : 295it [03:43,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.4442: : 295it [03:44,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.4442: : 296it [03:44,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 3.6043: : 296it [03:44,  1.08it/s]\u001b[A\n",
      "batch 297, training loss: 3.6043: : 297it [03:44,  1.08it/s]\u001b[A\n",
      "batch 298, training loss: 3.4954: : 297it [03:45,  1.08it/s]\u001b[A\n",
      "batch 298, training loss: 3.4954: : 298it [03:45,  1.08it/s]\u001b[A\n",
      "batch 299, training loss: 3.5799: : 298it [03:46,  1.08it/s]\u001b[A\n",
      "batch 299, training loss: 3.5799: : 299it [03:46,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 3.6406: : 299it [03:47,  1.11it/s]\u001b[A\n",
      "batch 300, training loss: 3.6406: : 300it [03:47,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.5875: : 300it [03:48,  1.10it/s]\u001b[A\n",
      "batch 301, training loss: 3.5875: : 301it [03:48,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.598: : 301it [03:49,  1.09it/s] \u001b[A\n",
      "batch 302, training loss: 3.598: : 302it [03:49,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.5026: : 302it [03:50,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.5026: : 303it [03:50,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.5904: : 303it [03:51,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.5904: : 304it [03:51,  1.12it/s]\u001b[A\n",
      "batch 305, training loss: 3.5155: : 304it [03:51,  1.12it/s]\u001b[A\n",
      "batch 305, training loss: 3.5155: : 305it [03:51,  1.24it/s]\u001b[A\n",
      "batch 306, training loss: 3.5826: : 305it [03:52,  1.24it/s]\u001b[A\n",
      "batch 306, training loss: 3.5826: : 306it [03:52,  1.33it/s]\u001b[A\n",
      "batch 307, training loss: 3.7214: : 306it [03:53,  1.33it/s]\u001b[A\n",
      "batch 307, training loss: 3.7214: : 307it [03:53,  1.27it/s]\u001b[A\n",
      "batch 308, training loss: 3.3462: : 307it [03:54,  1.27it/s]\u001b[A\n",
      "batch 308, training loss: 3.3462: : 308it [03:54,  1.25it/s]\u001b[A\n",
      "batch 309, training loss: 3.7286: : 308it [03:55,  1.25it/s]\u001b[A\n",
      "batch 309, training loss: 3.7286: : 309it [03:55,  1.24it/s]\u001b[A\n",
      "batch 310, training loss: 3.4825: : 309it [03:55,  1.24it/s]\u001b[A\n",
      "batch 310, training loss: 3.4825: : 310it [03:55,  1.18it/s]\u001b[A\n",
      "batch 311, training loss: 3.5933: : 310it [03:56,  1.18it/s]\u001b[A\n",
      "batch 311, training loss: 3.5933: : 311it [03:56,  1.15it/s]\u001b[A\n",
      "batch 312, training loss: 3.4232: : 311it [03:57,  1.15it/s]\u001b[A\n",
      "batch 312, training loss: 3.4232: : 312it [03:57,  1.13it/s]\u001b[A\n",
      "batch 313, training loss: 3.5313: : 312it [03:58,  1.13it/s]\u001b[A\n",
      "batch 313, training loss: 3.5313: : 313it [03:58,  1.13it/s]\u001b[A\n",
      "batch 314, training loss: 3.5503: : 313it [03:59,  1.13it/s]\u001b[A\n",
      "batch 314, training loss: 3.5503: : 314it [03:59,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.6172: : 314it [04:00,  1.11it/s]\u001b[A\n",
      "batch 315, training loss: 3.6172: : 315it [04:00,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.7464: : 315it [04:01,  1.10it/s]\u001b[A\n",
      "batch 316, training loss: 3.7464: : 316it [04:01,  1.11it/s]\u001b[A\n",
      "batch 317, training loss: 3.4119: : 316it [04:02,  1.11it/s]\u001b[A\n",
      "batch 317, training loss: 3.4119: : 317it [04:02,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 3.6083: : 317it [04:03,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 3.6083: : 318it [04:03,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 3.6735: : 318it [04:04,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 3.6735: : 319it [04:04,  1.11it/s]\u001b[A\n",
      "batch 320, training loss: 3.6678: : 319it [04:05,  1.11it/s]\u001b[A\n",
      "batch 320, training loss: 3.6678: : 320it [04:05,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 3.7284: : 320it [04:06,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 3.7284: : 321it [04:06,  1.08it/s]\u001b[A\n",
      "batch 322, training loss: 3.7685: : 321it [04:07,  1.08it/s]\u001b[A\n",
      "batch 322, training loss: 3.7685: : 322it [04:07,  1.05it/s]\u001b[A\n",
      "batch 323, training loss: 3.5455: : 322it [04:08,  1.05it/s]\u001b[A\n",
      "batch 323, training loss: 3.5455: : 323it [04:08,  1.03it/s]\u001b[A\n",
      "batch 324, training loss: 3.6196: : 323it [04:09,  1.03it/s]\u001b[A\n",
      "batch 324, training loss: 3.6196: : 324it [04:09,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 3.6882: : 324it [04:10,  1.02it/s]\u001b[A\n",
      "batch 325, training loss: 3.6882: : 325it [04:10,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 3.618: : 325it [04:10,  1.01it/s] \u001b[A\n",
      "batch 326, training loss: 3.618: : 326it [04:10,  1.04it/s]\u001b[A\n",
      "batch 327, training loss: 3.7258: : 326it [04:12,  1.04it/s]\u001b[A\n",
      "batch 327, training loss: 3.7258: : 327it [04:12,  1.02it/s]\u001b[A\n",
      "batch 328, training loss: 3.6055: : 327it [04:13,  1.02it/s]\u001b[A\n",
      "batch 328, training loss: 3.6055: : 328it [04:13,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 3.5209: : 328it [04:14,  1.01it/s]\u001b[A\n",
      "batch 329, training loss: 3.5209: : 329it [04:14,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.6251: : 329it [04:15,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.6251: : 330it [04:15,  1.00s/it]\u001b[A\n",
      "batch 331, training loss: 3.65: : 330it [04:15,  1.00s/it]  \u001b[A\n",
      "batch 331, training loss: 3.65: : 331it [04:15,  1.03it/s]\u001b[A\n",
      "batch 332, training loss: 3.5568: : 331it [04:16,  1.03it/s]\u001b[A\n",
      "batch 332, training loss: 3.5568: : 332it [04:16,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.5235: : 332it [04:18,  1.01it/s]\u001b[A\n",
      "batch 333, training loss: 3.5235: : 333it [04:18,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 3.5934: : 333it [04:18,  1.00s/it]\u001b[A\n",
      "batch 334, training loss: 3.5934: : 334it [04:18,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 3.7564: : 334it [04:20,  1.01it/s]\u001b[A\n",
      "batch 335, training loss: 3.7564: : 335it [04:20,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 3.6551: : 335it [04:20,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 3.6551: : 336it [04:20,  1.03it/s]\u001b[A\n",
      "batch 337, training loss: 3.7071: : 336it [04:21,  1.03it/s]\u001b[A\n",
      "batch 337, training loss: 3.7071: : 337it [04:21,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 3.5574: : 337it [04:22,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 3.5574: : 338it [04:22,  1.00it/s]\u001b[A\n",
      "batch 339, training loss: 3.6463: : 338it [04:23,  1.00it/s]\u001b[A\n",
      "batch 339, training loss: 3.6463: : 339it [04:23,  1.01it/s]\u001b[A\n",
      "batch 340, training loss: 3.8861: : 339it [04:24,  1.01it/s]\u001b[A\n",
      "batch 340, training loss: 3.8861: : 340it [04:24,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.5403: : 340it [04:26,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.5403: : 341it [04:26,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.6404: : 341it [04:26,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.6404: : 342it [04:26,  1.14it/s]\u001b[A\n",
      "batch 343, training loss: 3.628: : 342it [04:27,  1.14it/s] \u001b[A\n",
      "batch 343, training loss: 3.628: : 343it [04:27,  1.27it/s]\u001b[A\n",
      "batch 344, training loss: 3.7479: : 343it [04:27,  1.27it/s]\u001b[A\n",
      "batch 344, training loss: 3.7479: : 344it [04:27,  1.28it/s]\u001b[A\n",
      "batch 345, training loss: 3.7122: : 344it [04:28,  1.28it/s]\u001b[A\n",
      "batch 345, training loss: 3.7122: : 345it [04:28,  1.16it/s]\u001b[A\n",
      "batch 346, training loss: 3.6606: : 345it [04:29,  1.16it/s]\u001b[A\n",
      "batch 346, training loss: 3.6606: : 346it [04:29,  1.10it/s]\u001b[A\n",
      "batch 347, training loss: 3.5709: : 346it [04:30,  1.10it/s]\u001b[A\n",
      "batch 347, training loss: 3.5709: : 347it [04:30,  1.08it/s]\u001b[A\n",
      "batch 348, training loss: 3.653: : 347it [04:32,  1.08it/s] \u001b[A\n",
      "batch 348, training loss: 3.653: : 348it [04:32,  1.03it/s]\u001b[A\n",
      "batch 349, training loss: 3.8061: : 348it [04:32,  1.03it/s]\u001b[A\n",
      "batch 349, training loss: 3.8061: : 349it [04:32,  1.03it/s]\u001b[A\n",
      "batch 350, training loss: 3.639: : 349it [04:34,  1.03it/s] \u001b[A\n",
      "batch 350, training loss: 3.639: : 350it [04:34,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.565: : 350it [04:35,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.565: : 351it [04:35,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 3.6261: : 351it [04:36,  1.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 352, training loss: 3.6261: : 352it [04:36,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.6669: : 352it [04:36,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.6669: : 353it [04:36,  1.03it/s]\u001b[A\n",
      "batch 354, training loss: 3.7095: : 353it [04:37,  1.03it/s]\u001b[A\n",
      "batch 354, training loss: 3.7095: : 354it [04:37,  1.02it/s]\u001b[A\n",
      "batch 355, training loss: 3.6692: : 354it [04:38,  1.02it/s]\u001b[A\n",
      "batch 355, training loss: 3.6692: : 355it [04:38,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 3.5184: : 355it [04:39,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 3.5184: : 356it [04:39,  1.00it/s]\u001b[A\n",
      "batch 357, training loss: 3.5414: : 356it [04:41,  1.00it/s]\u001b[A\n",
      "batch 357, training loss: 3.5414: : 357it [04:41,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.7243: : 357it [04:42,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.7243: : 358it [04:42,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 3.5412: : 358it [04:43,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 3.5412: : 359it [04:43,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 3.6598: : 359it [04:43,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 3.6598: : 360it [04:43,  1.03it/s]\u001b[A\n",
      "batch 361, training loss: 3.6287: : 360it [04:44,  1.03it/s]\u001b[A\n",
      "batch 361, training loss: 3.6287: : 361it [04:44,  1.02it/s]\u001b[A\n",
      "batch 362, training loss: 3.7226: : 361it [04:45,  1.02it/s]\u001b[A\n",
      "batch 362, training loss: 3.7226: : 362it [04:45,  1.00it/s]\u001b[A\n",
      "batch 363, training loss: 3.6507: : 362it [04:46,  1.00it/s]\u001b[A\n",
      "batch 363, training loss: 3.6507: : 363it [04:46,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 3.5516: : 363it [04:47,  1.00it/s]\u001b[A\n",
      "batch 364, training loss: 3.5516: : 364it [04:47,  1.00s/it]\u001b[A\n",
      "batch 365, training loss: 3.5259: : 364it [04:48,  1.00s/it]\u001b[A\n",
      "batch 365, training loss: 3.5259: : 365it [04:48,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.6647: : 365it [04:50,  1.01s/it]\u001b[A\n",
      "batch 366, training loss: 3.6647: : 366it [04:50,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.5838: : 366it [04:51,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.5838: : 367it [04:51,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.6724: : 367it [04:51,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.6724: : 368it [04:51,  1.02it/s]\u001b[A\n",
      "batch 369, training loss: 3.6888: : 368it [04:52,  1.02it/s]\u001b[A\n",
      "batch 369, training loss: 3.6888: : 369it [04:52,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 3.5989: : 369it [04:53,  1.01it/s]\u001b[A\n",
      "batch 370, training loss: 3.5989: : 370it [04:53,  1.00it/s]\u001b[A\n",
      "batch 371, training loss: 3.6313: : 370it [04:54,  1.00it/s]\u001b[A\n",
      "batch 371, training loss: 3.6313: : 371it [04:54,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.5735: : 371it [04:55,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.5735: : 372it [04:55,  1.00s/it]\u001b[A\n",
      "batch 373, training loss: 3.608: : 372it [04:56,  1.00s/it] \u001b[A\n",
      "batch 373, training loss: 3.608: : 373it [04:56,  1.03it/s]\u001b[A\n",
      "batch 374, training loss: 3.6073: : 373it [04:57,  1.03it/s]\u001b[A\n",
      "batch 374, training loss: 3.6073: : 374it [04:57,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 3.3709: : 374it [04:58,  1.01it/s]\u001b[A\n",
      "batch 375, training loss: 3.3709: : 375it [04:58,  1.16it/s]\u001b[A\n",
      "batch 376, training loss: 3.5982: : 375it [04:59,  1.16it/s]\u001b[A\n",
      "batch 376, training loss: 3.5982: : 376it [04:59,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 3.6985: : 376it [05:00,  1.06it/s]\u001b[A\n",
      "batch 377, training loss: 3.6985: : 377it [05:00,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 3.6109: : 377it [05:01,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 3.6109: : 378it [05:01,  1.02s/it]\u001b[A\n",
      "batch 379, training loss: 3.556: : 378it [05:02,  1.02s/it] \u001b[A\n",
      "batch 379, training loss: 3.556: : 379it [05:02,  1.03s/it]\u001b[A\n",
      "batch 380, training loss: 3.5605: : 379it [05:03,  1.03s/it]\u001b[A\n",
      "batch 380, training loss: 3.5605: : 380it [05:03,  1.06s/it]\u001b[A\n",
      "batch 381, training loss: 3.6588: : 380it [05:05,  1.06s/it]\u001b[A\n",
      "batch 381, training loss: 3.6588: : 381it [05:05,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 3.5142: : 381it [05:06,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 3.5142: : 382it [05:06,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 3.5793: : 382it [05:07,  1.06s/it]\u001b[A\n",
      "batch 383, training loss: 3.5793: : 383it [05:07,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 3.6592: : 383it [05:08,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 3.6592: : 384it [05:08,  1.09s/it]\u001b[A\n",
      "batch 385, training loss: 3.5822: : 384it [05:09,  1.09s/it]\u001b[A\n",
      "batch 385, training loss: 3.5822: : 385it [05:09,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 3.5509: : 385it [05:10,  1.08s/it]\u001b[A\n",
      "batch 386, training loss: 3.5509: : 386it [05:10,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.6062: : 386it [05:11,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.6062: : 387it [05:11,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 3.4115: : 387it [05:12,  1.11s/it]\u001b[A\n",
      "batch 388, training loss: 3.4115: : 388it [05:12,  1.13s/it]\u001b[A\n",
      "batch 389, training loss: 3.5761: : 388it [05:13,  1.13s/it]\u001b[A\n",
      "batch 389, training loss: 3.5761: : 389it [05:13,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.6752: : 389it [05:15,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.6752: : 390it [05:15,  1.11s/it]\u001b[A\n",
      "batch 391, training loss: 3.684: : 390it [05:16,  1.11s/it] \u001b[A\n",
      "batch 391, training loss: 3.684: : 391it [05:16,  1.12s/it]\u001b[A\n",
      "batch 392, training loss: 3.6374: : 391it [05:17,  1.12s/it]\u001b[A\n",
      "batch 392, training loss: 3.6374: : 392it [05:17,  1.14s/it]\u001b[A\n",
      "batch 393, training loss: 3.4586: : 392it [05:18,  1.14s/it]\u001b[A\n",
      "batch 393, training loss: 3.4586: : 393it [05:18,  1.13s/it]\u001b[A\n",
      "batch 394, training loss: 3.4246: : 393it [05:19,  1.13s/it]\u001b[A\n",
      "batch 394, training loss: 3.4246: : 394it [05:19,  1.11s/it]\u001b[A\n",
      "batch 395, training loss: 3.5196: : 394it [05:20,  1.11s/it]\u001b[A\n",
      "batch 395, training loss: 3.5196: : 395it [05:20,  1.11s/it]\u001b[A\n",
      "batch 396, training loss: 3.6883: : 395it [05:21,  1.11s/it]\u001b[A\n",
      "batch 396, training loss: 3.6883: : 396it [05:21,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.4956: : 396it [05:22,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.4956: : 397it [05:22,  1.14s/it]\u001b[A\n",
      "batch 398, training loss: 3.6041: : 397it [05:24,  1.14s/it]\u001b[A\n",
      "batch 398, training loss: 3.6041: : 398it [05:24,  1.14s/it]\u001b[A\n",
      "batch 399, training loss: 3.7078: : 398it [05:25,  1.14s/it]\u001b[A\n",
      "batch 399, training loss: 3.7078: : 399it [05:25,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.4903: : 399it [05:26,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.4903: : 400it [05:26,  1.12s/it]\u001b[A\n",
      "batch 401, training loss: 3.4614: : 400it [05:27,  1.12s/it]\u001b[A\n",
      "batch 401, training loss: 3.4614: : 401it [05:27,  1.12s/it]\u001b[A\n",
      "batch 402, training loss: 3.5274: : 401it [05:28,  1.12s/it]\u001b[A\n",
      "batch 402, training loss: 3.5274: : 402it [05:28,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.6888: : 402it [05:29,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.6888: : 403it [05:29,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.3315: : 403it [05:30,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.3315: : 404it [05:30,  1.12s/it]\u001b[A\n",
      "batch 405, training loss: 3.6073: : 404it [05:31,  1.12s/it]\u001b[A\n",
      "batch 405, training loss: 3.6073: : 405it [05:31,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.5143: : 405it [05:32,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.5143: : 406it [05:32,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 3.6123: : 406it [05:34,  1.09s/it]\u001b[A\n",
      "batch 407, training loss: 3.6123: : 407it [05:34,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.4446: : 407it [05:35,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.4446: : 408it [05:35,  1.12s/it]\u001b[A\n",
      "batch 409, training loss: 3.6204: : 408it [05:36,  1.12s/it]\u001b[A\n",
      "batch 409, training loss: 3.6204: : 409it [05:36,  1.13s/it]\u001b[A\n",
      "batch 410, training loss: 3.4759: : 409it [05:37,  1.13s/it]\u001b[A\n",
      "batch 410, training loss: 3.4759: : 410it [05:37,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.5844: : 410it [05:38,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.5844: : 411it [05:38,  1.08s/it]\u001b[A\n",
      "batch 412, training loss: 3.6092: : 411it [05:39,  1.08s/it]\u001b[A\n",
      "batch 412, training loss: 3.6092: : 412it [05:39,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 3.5457: : 412it [05:40,  1.10s/it]\u001b[A\n",
      "batch 413, training loss: 3.5457: : 413it [05:40,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 3.4506: : 413it [05:41,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 3.4506: : 414it [05:41,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.3558: : 414it [05:42,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.3558: : 415it [05:42,  1.08s/it]\u001b[A\n",
      "batch 416, training loss: 3.3968: : 415it [05:43,  1.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 416, training loss: 3.3968: : 416it [05:43,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.6768: : 416it [05:45,  1.10s/it]\u001b[A\n",
      "batch 417, training loss: 3.6768: : 417it [05:45,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.5643: : 417it [05:46,  1.11s/it]\u001b[A\n",
      "batch 418, training loss: 3.5643: : 418it [05:46,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 3.3805: : 418it [05:47,  1.09s/it]\u001b[A\n",
      "batch 419, training loss: 3.3805: : 419it [05:47,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.4771: : 419it [05:48,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.4771: : 420it [05:48,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.5825: : 420it [05:49,  1.10s/it]\u001b[A\n",
      "batch 421, training loss: 3.5825: : 421it [05:49,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.6131: : 421it [05:50,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.6131: : 422it [05:50,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.4063: : 422it [05:51,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.4063: : 423it [05:51,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 3.5709: : 423it [05:52,  1.09s/it]\u001b[A\n",
      "batch 424, training loss: 3.5709: : 424it [05:52,  1.10s/it]\u001b[A\n",
      "batch 425, training loss: 3.6175: : 424it [05:53,  1.10s/it]\u001b[A\n",
      "batch 425, training loss: 3.6175: : 425it [05:53,  1.14s/it]\u001b[A\n",
      "batch 426, training loss: 3.5461: : 425it [05:55,  1.14s/it]\u001b[A\n",
      "batch 426, training loss: 3.5461: : 426it [05:55,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.6055: : 426it [05:56,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.6055: : 427it [05:56,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 3.7734: : 427it [05:57,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 3.7734: : 428it [05:57,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.5906: : 428it [05:58,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.5906: : 429it [05:58,  1.11s/it]\u001b[A\n",
      "batch 430, training loss: 3.6443: : 429it [05:59,  1.11s/it]\u001b[A\n",
      "batch 430, training loss: 3.6443: : 430it [05:59,  1.08s/it]\u001b[A\n",
      "batch 431, training loss: 3.6748: : 430it [06:00,  1.08s/it]\u001b[A\n",
      "batch 431, training loss: 3.6748: : 431it [06:00,  1.12s/it]\u001b[A\n",
      "batch 432, training loss: 3.5133: : 431it [06:01,  1.12s/it]\u001b[A\n",
      "batch 432, training loss: 3.5133: : 432it [06:01,  1.15s/it]\u001b[A\n",
      "batch 433, training loss: 3.5709: : 432it [06:03,  1.15s/it]\u001b[A\n",
      "batch 433, training loss: 3.5709: : 433it [06:03,  1.15s/it]\u001b[A\n",
      "batch 434, training loss: 3.5625: : 433it [06:04,  1.15s/it]\u001b[A\n",
      "batch 434, training loss: 3.5625: : 434it [06:04,  1.17s/it]\u001b[A\n",
      "batch 435, training loss: 3.5607: : 434it [06:05,  1.17s/it]\u001b[A\n",
      "batch 435, training loss: 3.5607: : 435it [06:05,  1.19s/it]\u001b[A\n",
      "batch 436, training loss: 3.584: : 435it [06:06,  1.19s/it] \u001b[A\n",
      "batch 436, training loss: 3.584: : 436it [06:06,  1.20s/it]\u001b[A\n",
      "batch 437, training loss: 3.5339: : 436it [06:08,  1.20s/it]\u001b[A\n",
      "batch 437, training loss: 3.5339: : 437it [06:08,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.5748: : 437it [06:09,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.5748: : 438it [06:09,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.6924: : 438it [06:10,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.6924: : 439it [06:10,  1.22s/it]\u001b[A\n",
      "batch 440, training loss: 3.5221: : 439it [06:11,  1.22s/it]\u001b[A\n",
      "batch 440, training loss: 3.5221: : 440it [06:11,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 3.7892: : 440it [06:12,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 3.7892: : 441it [06:12,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 3.465: : 441it [06:14,  1.23s/it] \u001b[A\n",
      "batch 442, training loss: 3.465: : 442it [06:14,  1.23s/it]\u001b[A\n",
      "batch 443, training loss: 3.5538: : 442it [06:15,  1.23s/it]\u001b[A\n",
      "batch 443, training loss: 3.5538: : 443it [06:15,  1.23s/it]\u001b[A\n",
      "batch 444, training loss: 3.5464: : 443it [06:16,  1.23s/it]\u001b[A\n",
      "batch 444, training loss: 3.5464: : 444it [06:16,  1.24s/it]\u001b[A\n",
      "batch 445, training loss: 3.6165: : 444it [06:17,  1.24s/it]\u001b[A\n",
      "batch 445, training loss: 3.6165: : 445it [06:17,  1.23s/it]\u001b[A\n",
      "batch 446, training loss: 3.6234: : 445it [06:19,  1.23s/it]\u001b[A\n",
      "batch 446, training loss: 3.6234: : 446it [06:19,  1.24s/it]\u001b[A\n",
      "batch 447, training loss: 3.5631: : 446it [06:20,  1.24s/it]\u001b[A\n",
      "batch 447, training loss: 3.5631: : 447it [06:20,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.4615: : 447it [06:21,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.4615: : 448it [06:21,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.5832: : 448it [06:22,  1.23s/it]\u001b[A\n",
      "batch 449, training loss: 3.5832: : 449it [06:22,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.6441: : 449it [06:24,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.6441: : 450it [06:24,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.6499: : 450it [06:25,  1.24s/it]\u001b[A\n",
      "batch 451, training loss: 3.6499: : 451it [06:25,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.6471: : 451it [06:26,  1.23s/it]\u001b[A\n",
      "batch 452, training loss: 3.6471: : 452it [06:26,  1.24s/it]\u001b[A\n",
      "batch 453, training loss: 3.7245: : 452it [06:27,  1.24s/it]\u001b[A\n",
      "batch 453, training loss: 3.7245: : 453it [06:27,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.5871: : 453it [06:28,  1.23s/it]\u001b[A\n",
      "batch 454, training loss: 3.5871: : 454it [06:28,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.4875: : 454it [06:30,  1.23s/it]\u001b[A\n",
      "batch 455, training loss: 3.4875: : 455it [06:30,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.5155: : 455it [06:31,  1.23s/it]\u001b[A\n",
      "batch 456, training loss: 3.5155: : 456it [06:31,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.6416: : 456it [06:32,  1.24s/it]\u001b[A\n",
      "batch 457, training loss: 3.6416: : 457it [06:32,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.5121: : 457it [06:33,  1.23s/it]\u001b[A\n",
      "batch 458, training loss: 3.5121: : 458it [06:33,  1.24s/it]\u001b[A\n",
      "batch 459, training loss: 3.5817: : 458it [06:35,  1.24s/it]\u001b[A\n",
      "batch 459, training loss: 3.5817: : 459it [06:35,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.5591: : 459it [06:36,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.5591: : 460it [06:36,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.6563: : 460it [06:37,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.6563: : 461it [06:37,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.618: : 461it [06:38,  1.23s/it] \u001b[A\n",
      "batch 462, training loss: 3.618: : 462it [06:38,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.453: : 462it [06:40,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.453: : 463it [06:40,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.598: : 463it [06:41,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.598: : 464it [06:41,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.5442: : 464it [06:42,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.5442: : 465it [06:42,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 3.4144: : 465it [06:43,  1.15s/it]\u001b[A\n",
      "batch 466, training loss: 3.4144: : 466it [06:43,  1.18s/it]\u001b[A\n",
      "batch 467, training loss: 3.3676: : 466it [06:44,  1.18s/it]\u001b[A\n",
      "batch 467, training loss: 3.3676: : 467it [06:44,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 3.5598: : 467it [06:46,  1.19s/it]\u001b[A\n",
      "batch 468, training loss: 3.5598: : 468it [06:46,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.4642: : 468it [06:47,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.4642: : 469it [06:47,  1.23s/it]\u001b[A\n",
      "batch 470, training loss: 3.5846: : 469it [06:48,  1.23s/it]\u001b[A\n",
      "batch 470, training loss: 3.5846: : 470it [06:48,  1.25s/it]\u001b[A\n",
      "batch 471, training loss: 3.5556: : 470it [06:49,  1.25s/it]\u001b[A\n",
      "batch 471, training loss: 3.5556: : 471it [06:49,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 3.5076: : 471it [06:51,  1.24s/it]\u001b[A\n",
      "batch 472, training loss: 3.5076: : 472it [06:51,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 3.5189: : 472it [06:52,  1.26s/it]\u001b[A\n",
      "batch 473, training loss: 3.5189: : 473it [06:52,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 3.5593: : 473it [06:53,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 3.5593: : 474it [06:53,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.4896: : 474it [06:54,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.4896: : 475it [06:54,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 3.5857: : 475it [06:56,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 3.5857: : 476it [06:56,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.397: : 476it [06:57,  1.26s/it] \u001b[A\n",
      "batch 477, training loss: 3.397: : 477it [06:57,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.4631: : 477it [06:58,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.4631: : 478it [06:58,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.436: : 478it [06:59,  1.26s/it] \u001b[A\n",
      "batch 479, training loss: 3.436: : 479it [06:59,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 3.4673: : 479it [07:01,  1.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 480, training loss: 3.4673: : 480it [07:01,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.5052: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.5052: : 481it [07:02,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.4985: : 481it [07:03,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.4985: : 482it [07:03,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 3.4243: : 482it [07:04,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 3.4243: : 483it [07:04,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.5093: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.5093: : 484it [07:06,  1.24s/it]\u001b[A\n",
      "batch 485, training loss: 3.442: : 484it [07:07,  1.24s/it] \u001b[A\n",
      "batch 485, training loss: 3.442: : 485it [07:07,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.5362: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.5362: : 486it [07:08,  1.24s/it]\u001b[A\n",
      "batch 487, training loss: 3.6521: : 486it [07:09,  1.24s/it]\u001b[A\n",
      "batch 487, training loss: 3.6521: : 487it [07:09,  1.25s/it]\u001b[A\n",
      "batch 488, training loss: 3.4333: : 487it [07:11,  1.25s/it]\u001b[A\n",
      "batch 488, training loss: 3.4333: : 488it [07:11,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.3727: : 488it [07:12,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.3727: : 489it [07:12,  1.25s/it]\u001b[A\n",
      "batch 490, training loss: 3.457: : 489it [07:13,  1.25s/it] \u001b[A\n",
      "batch 490, training loss: 3.457: : 490it [07:13,  1.27s/it]\u001b[A\n",
      "batch 491, training loss: 3.4917: : 490it [07:14,  1.27s/it]\u001b[A\n",
      "batch 491, training loss: 3.4917: : 491it [07:14,  1.25s/it]\u001b[A\n",
      "batch 492, training loss: 3.3811: : 491it [07:16,  1.25s/it]\u001b[A\n",
      "batch 492, training loss: 3.3811: : 492it [07:16,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.5838: : 492it [07:17,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.5838: : 493it [07:17,  1.25s/it]\u001b[A\n",
      "batch 494, training loss: 3.5879: : 493it [07:18,  1.25s/it]\u001b[A\n",
      "batch 494, training loss: 3.5879: : 494it [07:18,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.4154: : 494it [07:19,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.4154: : 495it [07:19,  1.25s/it]\u001b[A\n",
      "batch 496, training loss: 3.4024: : 495it [07:21,  1.25s/it]\u001b[A\n",
      "batch 496, training loss: 3.4024: : 496it [07:21,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.3202: : 496it [07:22,  1.26s/it]\u001b[A\n",
      "batch 497, training loss: 3.3202: : 497it [07:22,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.4372: : 497it [07:23,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.4372: : 498it [07:23,  1.28s/it]\u001b[A\n",
      "batch 499, training loss: 3.4045: : 498it [07:24,  1.28s/it]\u001b[A\n",
      "batch 499, training loss: 3.4045: : 499it [07:24,  1.15s/it]\u001b[A\n",
      "batch 500, training loss: 3.6604: : 499it [07:26,  1.15s/it]\u001b[A\n",
      "batch 500, training loss: 3.6604: : 500it [07:26,  1.22s/it]\u001b[A\n",
      "batch 501, training loss: 3.5116: : 500it [07:27,  1.22s/it]\u001b[A\n",
      "batch 501, training loss: 3.5116: : 501it [07:27,  1.28s/it]\u001b[A\n",
      "batch 502, training loss: 3.449: : 501it [07:28,  1.28s/it] \u001b[A\n",
      "batch 502, training loss: 3.449: : 502it [07:28,  1.33s/it]\u001b[A\n",
      "batch 503, training loss: 3.5515: : 502it [07:30,  1.33s/it]\u001b[A\n",
      "batch 503, training loss: 3.5515: : 503it [07:30,  1.34s/it]\u001b[A\n",
      "batch 504, training loss: 3.5369: : 503it [07:31,  1.34s/it]\u001b[A\n",
      "batch 504, training loss: 3.5369: : 504it [07:31,  1.36s/it]\u001b[A\n",
      "batch 505, training loss: 3.6212: : 504it [07:32,  1.36s/it]\u001b[A\n",
      "batch 505, training loss: 3.6212: : 505it [07:32,  1.30s/it]\u001b[A\n",
      "batch 506, training loss: 3.5347: : 505it [07:34,  1.30s/it]\u001b[A\n",
      "batch 506, training loss: 3.5347: : 506it [07:34,  1.30s/it]\u001b[A\n",
      "batch 507, training loss: 3.4156: : 506it [07:35,  1.30s/it]\u001b[A\n",
      "batch 507, training loss: 3.4156: : 507it [07:35,  1.31s/it]\u001b[A\n",
      "batch 508, training loss: 3.5792: : 507it [07:36,  1.31s/it]\u001b[A\n",
      "batch 508, training loss: 3.5792: : 508it [07:36,  1.29s/it]\u001b[A\n",
      "batch 509, training loss: 3.4611: : 508it [07:38,  1.29s/it]\u001b[A\n",
      "batch 509, training loss: 3.4611: : 509it [07:38,  1.30s/it]\u001b[A\n",
      "batch 510, training loss: 3.5234: : 509it [07:39,  1.30s/it]\u001b[A\n",
      "batch 510, training loss: 3.5234: : 510it [07:39,  1.32s/it]\u001b[A\n",
      "batch 511, training loss: 3.5328: : 510it [07:40,  1.32s/it]\u001b[A\n",
      "batch 511, training loss: 3.5328: : 511it [07:40,  1.33s/it]\u001b[A\n",
      "batch 512, training loss: 3.5115: : 511it [07:42,  1.33s/it]\u001b[A\n",
      "batch 512, training loss: 3.5115: : 512it [07:42,  1.36s/it]\u001b[A\n",
      "batch 513, training loss: 3.5716: : 512it [07:43,  1.36s/it]\u001b[A\n",
      "batch 513, training loss: 3.5716: : 513it [07:43,  1.38s/it]\u001b[A\n",
      "batch 514, training loss: 3.4256: : 513it [07:44,  1.38s/it]\u001b[A\n",
      "batch 514, training loss: 3.4256: : 514it [07:44,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.6226: : 514it [07:46,  1.36s/it]\u001b[A\n",
      "batch 515, training loss: 3.6226: : 515it [07:46,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.4874: : 515it [07:47,  1.38s/it]\u001b[A\n",
      "batch 516, training loss: 3.4874: : 516it [07:47,  1.39s/it]\u001b[A\n",
      "batch 517, training loss: 3.4796: : 516it [07:49,  1.39s/it]\u001b[A\n",
      "batch 517, training loss: 3.4796: : 517it [07:49,  1.38s/it]\u001b[A\n",
      "batch 518, training loss: 3.5686: : 517it [07:50,  1.38s/it]\u001b[A\n",
      "batch 518, training loss: 3.5686: : 518it [07:50,  1.38s/it]\u001b[A\n",
      "batch 519, training loss: 3.3783: : 518it [07:51,  1.38s/it]\u001b[A\n",
      "batch 519, training loss: 3.3783: : 519it [07:51,  1.38s/it]\u001b[A\n",
      "batch 520, training loss: 3.5451: : 519it [07:53,  1.38s/it]\u001b[A\n",
      "batch 520, training loss: 3.5451: : 520it [07:53,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 3.5385: : 520it [07:54,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 3.5385: : 521it [07:54,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 3.5783: : 521it [07:56,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 3.5783: : 522it [07:56,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 3.4494: : 522it [07:57,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 3.4494: : 523it [07:57,  1.37s/it]\u001b[A\n",
      "batch 524, training loss: 3.6463: : 523it [07:58,  1.37s/it]\u001b[A\n",
      "batch 524, training loss: 3.6463: : 524it [07:58,  1.38s/it]\u001b[A\n",
      "batch 525, training loss: 3.697: : 524it [08:00,  1.38s/it] \u001b[A\n",
      "batch 525, training loss: 3.697: : 525it [08:00,  1.38s/it]\u001b[A\n",
      "batch 526, training loss: 3.4969: : 525it [08:01,  1.38s/it]\u001b[A\n",
      "batch 526, training loss: 3.4969: : 526it [08:01,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.4817: : 526it [08:02,  1.38s/it]\u001b[A\n",
      "batch 527, training loss: 3.4817: : 527it [08:02,  1.35s/it]\u001b[A\n",
      "batch 528, training loss: 3.5104: : 527it [08:04,  1.35s/it]\u001b[A\n",
      "batch 528, training loss: 3.5104: : 528it [08:04,  1.40s/it]\u001b[A\n",
      "batch 529, training loss: 3.5587: : 528it [08:05,  1.40s/it]\u001b[A\n",
      "batch 529, training loss: 3.5587: : 529it [08:05,  1.42s/it]\u001b[A\n",
      "batch 530, training loss: 3.5291: : 529it [08:07,  1.42s/it]\u001b[A\n",
      "batch 530, training loss: 3.5291: : 530it [08:07,  1.41s/it]\u001b[A\n",
      "batch 531, training loss: 3.4764: : 530it [08:08,  1.41s/it]\u001b[A\n",
      "batch 531, training loss: 3.4764: : 531it [08:08,  1.45s/it]\u001b[A\n",
      "batch 532, training loss: 3.2611: : 531it [08:10,  1.45s/it]\u001b[A\n",
      "batch 532, training loss: 3.2611: : 532it [08:10,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 3.4487: : 532it [08:11,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 3.4487: : 533it [08:11,  1.44s/it]\u001b[A\n",
      "batch 534, training loss: 3.5754: : 533it [08:13,  1.44s/it]\u001b[A\n",
      "batch 534, training loss: 3.5754: : 534it [08:13,  1.44s/it]\u001b[A\n",
      "batch 535, training loss: 3.4484: : 534it [08:14,  1.44s/it]\u001b[A\n",
      "batch 535, training loss: 3.4484: : 535it [08:14,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.2893: : 535it [08:16,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.2893: : 536it [08:16,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.3571: : 536it [08:17,  1.48s/it]\u001b[A\n",
      "batch 537, training loss: 3.3571: : 537it [08:17,  1.50s/it]\u001b[A\n",
      "batch 538, training loss: 3.4772: : 537it [08:19,  1.50s/it]\u001b[A\n",
      "batch 538, training loss: 3.4772: : 538it [08:19,  1.49s/it]\u001b[A\n",
      "batch 539, training loss: 3.4682: : 538it [08:20,  1.49s/it]\u001b[A\n",
      "batch 539, training loss: 3.4682: : 539it [08:20,  1.46s/it]\u001b[A\n",
      "batch 540, training loss: 3.4083: : 539it [08:22,  1.46s/it]\u001b[A\n",
      "batch 540, training loss: 3.4083: : 540it [08:22,  1.48s/it]\u001b[A\n",
      "batch 541, training loss: 3.5107: : 540it [08:23,  1.48s/it]\u001b[A\n",
      "batch 541, training loss: 3.5107: : 541it [08:23,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 3.5643: : 541it [08:25,  1.50s/it]\u001b[A\n",
      "batch 542, training loss: 3.5643: : 542it [08:25,  1.51s/it]\u001b[A\n",
      "batch 543, training loss: 3.3976: : 542it [08:26,  1.51s/it]\u001b[A\n",
      "batch 543, training loss: 3.3976: : 543it [08:26,  1.49s/it]\u001b[A\n",
      "batch 544, training loss: 3.4779: : 543it [08:27,  1.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 544, training loss: 3.4779: : 544it [08:27,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 3.4805: : 544it [08:29,  1.47s/it]\u001b[A\n",
      "batch 545, training loss: 3.4805: : 545it [08:29,  1.49s/it]\u001b[A\n",
      "batch 546, training loss: 3.3861: : 545it [08:31,  1.49s/it]\u001b[A\n",
      "batch 546, training loss: 3.3861: : 546it [08:31,  1.50s/it]\u001b[A\n",
      "batch 547, training loss: 3.2837: : 546it [08:32,  1.50s/it]\u001b[A\n",
      "batch 547, training loss: 3.2837: : 547it [08:32,  1.50s/it]\u001b[A\n",
      "batch 548, training loss: 3.2781: : 547it [08:33,  1.50s/it]\u001b[A\n",
      "batch 548, training loss: 3.2781: : 548it [08:33,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.4595: : 548it [08:34,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.4595: : 549it [08:34,  1.36s/it]\u001b[A\n",
      "batch 550, training loss: 3.3806: : 549it [08:36,  1.36s/it]\u001b[A\n",
      "batch 550, training loss: 3.3806: : 550it [08:36,  1.39s/it]\u001b[A\n",
      "batch 551, training loss: 3.4235: : 550it [08:37,  1.39s/it]\u001b[A\n",
      "batch 551, training loss: 3.4235: : 551it [08:37,  1.46s/it]\u001b[A\n",
      "batch 552, training loss: 3.4394: : 551it [08:39,  1.46s/it]\u001b[A\n",
      "batch 552, training loss: 3.4394: : 552it [08:39,  1.49s/it]\u001b[A\n",
      "batch 553, training loss: 3.4228: : 552it [08:41,  1.49s/it]\u001b[A\n",
      "batch 553, training loss: 3.4228: : 553it [08:41,  1.54s/it]\u001b[A\n",
      "batch 554, training loss: 3.1918: : 553it [08:42,  1.54s/it]\u001b[A\n",
      "batch 554, training loss: 3.1918: : 554it [08:42,  1.55s/it]\u001b[A\n",
      "batch 555, training loss: 3.337: : 554it [08:44,  1.55s/it] \u001b[A\n",
      "batch 555, training loss: 3.337: : 555it [08:44,  1.58s/it]\u001b[A\n",
      "batch 556, training loss: 3.4977: : 555it [08:46,  1.58s/it]\u001b[A\n",
      "batch 556, training loss: 3.4977: : 556it [08:46,  1.59s/it]\u001b[A\n",
      "batch 557, training loss: 3.3301: : 556it [08:47,  1.59s/it]\u001b[A\n",
      "batch 557, training loss: 3.3301: : 557it [08:47,  1.60s/it]\u001b[A\n",
      "batch 558, training loss: 3.28: : 557it [08:49,  1.60s/it]  \u001b[A\n",
      "batch 558, training loss: 3.28: : 558it [08:49,  1.61s/it]\u001b[A\n",
      "batch 559, training loss: 3.3759: : 558it [08:50,  1.61s/it]\u001b[A\n",
      "batch 559, training loss: 3.3759: : 559it [08:50,  1.61s/it]\u001b[A\n",
      "batch 560, training loss: 3.3849: : 559it [08:52,  1.61s/it]\u001b[A\n",
      "batch 560, training loss: 3.3849: : 560it [08:52,  1.61s/it]\u001b[A\n",
      "batch 561, training loss: 3.3644: : 560it [08:54,  1.61s/it]\u001b[A\n",
      "batch 561, training loss: 3.3644: : 561it [08:54,  1.60s/it]\u001b[A\n",
      "batch 562, training loss: 3.2244: : 561it [08:55,  1.60s/it]\u001b[A\n",
      "batch 562, training loss: 3.2244: : 562it [08:55,  1.61s/it]\u001b[A\n",
      "batch 563, training loss: 3.3698: : 562it [08:57,  1.61s/it]\u001b[A\n",
      "batch 563, training loss: 3.3698: : 563it [08:57,  1.61s/it]\u001b[A\n",
      "batch 564, training loss: 3.2491: : 563it [08:58,  1.61s/it]\u001b[A\n",
      "batch 564, training loss: 3.2491: : 564it [08:58,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 3.3757: : 564it [09:00,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 3.3757: : 565it [09:00,  1.53s/it]\u001b[A\n",
      "batch 566, training loss: 3.4867: : 565it [09:01,  1.53s/it]\u001b[A\n",
      "batch 566, training loss: 3.4867: : 566it [09:01,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 3.4372: : 566it [09:03,  1.57s/it]\u001b[A\n",
      "batch 567, training loss: 3.4372: : 567it [09:03,  1.56s/it]\u001b[A\n",
      "batch 568, training loss: 3.522: : 567it [09:05,  1.56s/it] \u001b[A\n",
      "batch 568, training loss: 3.522: : 568it [09:05,  1.59s/it]\u001b[A\n",
      "batch 569, training loss: 3.4953: : 568it [09:06,  1.59s/it]\u001b[A\n",
      "batch 569, training loss: 3.4953: : 569it [09:06,  1.55s/it]\u001b[A\n",
      "batch 570, training loss: 3.6657: : 569it [09:08,  1.55s/it]\u001b[A\n",
      "batch 570, training loss: 3.6657: : 570it [09:08,  1.58s/it]\u001b[A\n",
      "batch 571, training loss: 3.4573: : 570it [09:09,  1.58s/it]\u001b[A\n",
      "batch 571, training loss: 3.4573: : 571it [09:09,  1.61s/it]\u001b[A\n",
      "batch 572, training loss: 3.5522: : 571it [09:11,  1.61s/it]\u001b[A\n",
      "batch 572, training loss: 3.5522: : 572it [09:11,  1.62s/it]\u001b[A\n",
      "batch 573, training loss: 3.4741: : 572it [09:13,  1.62s/it]\u001b[A\n",
      "batch 573, training loss: 3.4741: : 573it [09:13,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 3.5501: : 573it [09:14,  1.64s/it]\u001b[A\n",
      "batch 574, training loss: 3.5501: : 574it [09:14,  1.65s/it]\u001b[A\n",
      "batch 575, training loss: 3.1559: : 574it [09:15,  1.65s/it]\u001b[A\n",
      "batch 575, training loss: 3.1559: : 575it [09:15,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.5334: : 575it [09:17,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.5334: : 576it [09:17,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.4065: : 576it [09:19,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.4065: : 577it [09:19,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.38: : 577it [09:21,  1.59s/it]  \u001b[A\n",
      "batch 578, training loss: 3.38: : 578it [09:21,  1.63s/it]\u001b[A\n",
      "batch 579, training loss: 3.4317: : 578it [09:22,  1.63s/it]\u001b[A\n",
      "batch 579, training loss: 3.4317: : 579it [09:22,  1.68s/it]\u001b[A\n",
      "batch 580, training loss: 3.3503: : 579it [09:24,  1.68s/it]\u001b[A\n",
      "batch 580, training loss: 3.3503: : 580it [09:24,  1.69s/it]\u001b[A\n",
      "batch 581, training loss: 3.3492: : 580it [09:26,  1.69s/it]\u001b[A\n",
      "batch 581, training loss: 3.3492: : 581it [09:26,  1.72s/it]\u001b[A\n",
      "batch 582, training loss: 3.385: : 581it [09:28,  1.72s/it] \u001b[A\n",
      "batch 582, training loss: 3.385: : 582it [09:28,  1.73s/it]\u001b[A\n",
      "batch 583, training loss: 2.8135: : 582it [09:28,  1.73s/it]\u001b[A\n",
      "batch 583, training loss: 2.8135: : 583it [09:28,  1.41s/it]\u001b[A\n",
      "batch 584, training loss: 3.5762: : 583it [09:30,  1.41s/it]\u001b[A\n",
      "batch 584, training loss: 3.5762: : 584it [09:30,  1.55s/it]\u001b[A\n",
      "batch 585, training loss: 3.5192: : 584it [09:32,  1.55s/it]\u001b[A\n",
      "batch 585, training loss: 3.5192: : 585it [09:32,  1.65s/it]\u001b[A\n",
      "batch 586, training loss: 3.6203: : 585it [09:34,  1.65s/it]\u001b[A\n",
      "batch 586, training loss: 3.6203: : 586it [09:34,  1.74s/it]\u001b[A\n",
      "batch 587, training loss: 3.4971: : 586it [09:36,  1.74s/it]\u001b[A\n",
      "batch 587, training loss: 3.4971: : 587it [09:36,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.3793: : 587it [09:38,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.3793: : 588it [09:38,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 3.4667: : 588it [09:40,  1.81s/it]\u001b[A\n",
      "batch 589, training loss: 3.4667: : 589it [09:40,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.4721: : 589it [09:42,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.4721: : 590it [09:42,  1.89s/it]\u001b[A\n",
      "batch 591, training loss: 3.3707: : 590it [09:44,  1.89s/it]\u001b[A\n",
      "batch 591, training loss: 3.3707: : 591it [09:44,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.4007: : 591it [09:45,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.4007: : 592it [09:45,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.3042: : 592it [09:47,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.3042: : 593it [09:47,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.4583: : 593it [09:49,  1.93s/it]\u001b[A\n",
      "batch 594, training loss: 3.4583: : 594it [09:49,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.5497: : 594it [09:51,  1.98s/it]\u001b[A\n",
      "batch 595, training loss: 3.5497: : 595it [09:51,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.4904: : 595it [09:53,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.4904: : 596it [09:53,  1.96s/it]\u001b[A\n",
      "batch 597, training loss: 3.3038: : 596it [09:55,  1.96s/it]\u001b[A\n",
      "batch 597, training loss: 3.3038: : 597it [09:55,  1.95s/it]\u001b[A\n",
      "batch 598, training loss: 3.5197: : 597it [09:57,  1.95s/it]\u001b[A\n",
      "batch 598, training loss: 3.5197: : 598it [09:57,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.4115: : 598it [09:59,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.4115: : 599it [09:59,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.3389: : 599it [10:01,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.3389: : 600it [10:01,  1.99s/it]\u001b[A\n",
      "batch 601, training loss: 2.4897: : 600it [10:02,  1.99s/it]\u001b[A\n",
      "batch 601, training loss: 2.4897: : 601it [10:02,  1.67s/it]\u001b[A\n",
      "batch 602, training loss: 3.3695: : 601it [10:04,  1.67s/it]\u001b[A\n",
      "batch 602, training loss: 3.3695: : 602it [10:04,  1.81s/it]\u001b[A\n",
      "batch 603, training loss: 3.3761: : 602it [10:06,  1.81s/it]\u001b[A\n",
      "batch 603, training loss: 3.3761: : 603it [10:06,  1.86s/it]\u001b[A\n",
      "batch 604, training loss: 3.4071: : 603it [10:08,  1.86s/it]\u001b[A\n",
      "batch 604, training loss: 3.4071: : 604it [10:08,  1.87s/it]\u001b[A\n",
      "batch 605, training loss: 3.5293: : 604it [10:10,  1.87s/it]\u001b[A\n",
      "batch 605, training loss: 3.5293: : 605it [10:10,  1.83s/it]\u001b[A\n",
      "batch 606, training loss: 3.2916: : 605it [10:11,  1.83s/it]\u001b[A\n",
      "batch 606, training loss: 3.2916: : 606it [10:11,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.1719: : 606it [10:13,  1.75s/it]\u001b[A\n",
      "batch 607, training loss: 3.1719: : 607it [10:13,  1.70s/it]\u001b[A\n",
      "batch 608, training loss: 3.3833: : 607it [10:14,  1.70s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 608, training loss: 3.3833: : 608it [10:14,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 3.3289: : 608it [10:16,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 3.3289: : 609it [10:16,  1.54s/it]\u001b[A\n",
      "batch 610, training loss: 2.7783: : 609it [10:17,  1.54s/it]\u001b[A\n",
      "batch 610, training loss: 2.7783: : 610it [10:17,  1.44s/it]\u001b[A\n",
      "batch 611, training loss: 2.7452: : 610it [10:18,  1.44s/it]\u001b[A\n",
      "batch 611, training loss: 2.7452: : 611it [10:18,  1.39s/it]\u001b[A\n",
      "batch 612, training loss: 1.9882: : 611it [10:19,  1.39s/it]\u001b[A\n",
      "batch 612, training loss: 1.9882: : 612it [10:19,  1.32s/it]\u001b[A\n",
      "batch 613, training loss: 2.6924: : 612it [10:20,  1.32s/it]\u001b[A\n",
      "batch 613, training loss: 2.6924: : 616it [10:21,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.8276: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.8276: : 1it [00:00,  3.13it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0219: : 1it [00:00,  3.13it/s]\u001b[A\n",
      "batch 1, dev loss: 4.0219: : 2it [00:00,  4.06it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5996: : 2it [00:00,  4.06it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5996: : 3it [00:00,  4.84it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7335: : 3it [00:00,  4.84it/s]\u001b[A\n",
      "batch 3, dev loss: 3.7335: : 4it [00:00,  4.77it/s]\u001b[A\n",
      "batch 4, dev loss: 3.756: : 4it [00:01,  4.77it/s] \u001b[A\n",
      "batch 4, dev loss: 3.756: : 5it [00:01,  4.84it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7571: : 5it [00:01,  4.84it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7571: : 6it [00:01,  4.95it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8225: : 6it [00:01,  4.95it/s]\u001b[A\n",
      "batch 6, dev loss: 3.8225: : 7it [00:01,  4.87it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6069: : 7it [00:01,  4.87it/s]\u001b[A\n",
      "batch 7, dev loss: 3.6069: : 8it [00:01,  5.01it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8083: : 8it [00:01,  5.01it/s]\u001b[A\n",
      "batch 8, dev loss: 3.8083: : 9it [00:01,  4.57it/s]\u001b[A\n",
      "batch 9, dev loss: 3.7403: : 9it [00:02,  4.57it/s]\u001b[A\n",
      "batch 9, dev loss: 3.7403: : 10it [00:02,  4.30it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7691: : 10it [00:02,  4.30it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7691: : 11it [00:02,  4.07it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8352: : 11it [00:02,  4.07it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8352: : 12it [00:02,  4.13it/s]\u001b[A\n",
      "batch 12, dev loss: 3.665: : 12it [00:02,  4.13it/s] \u001b[A\n",
      "batch 12, dev loss: 3.665: : 13it [00:02,  4.10it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8029: : 13it [00:03,  4.10it/s]\u001b[A\n",
      "batch 13, dev loss: 3.8029: : 14it [00:03,  4.08it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9496: : 14it [00:03,  4.08it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9496: : 15it [00:03,  4.27it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6873: : 15it [00:03,  4.27it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6873: : 16it [00:03,  5.03it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0518: : 16it [00:03,  5.03it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0518: : 17it [00:03,  4.74it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8317: : 17it [00:04,  4.74it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8317: : 18it [00:04,  4.27it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7361: : 18it [00:04,  4.27it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7361: : 19it [00:04,  4.26it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9138: : 19it [00:04,  4.26it/s]\u001b[A\n",
      "batch 19, dev loss: 3.9138: : 20it [00:04,  4.12it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7472: : 20it [00:04,  4.12it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7472: : 21it [00:04,  4.07it/s]\u001b[A\n",
      "batch 21, dev loss: 3.636: : 21it [00:05,  4.07it/s] \u001b[A\n",
      "batch 21, dev loss: 3.636: : 22it [00:05,  3.95it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8295: : 22it [00:05,  3.95it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8295: : 23it [00:05,  3.86it/s]\u001b[A\n",
      "batch 23, dev loss: 3.856: : 23it [00:05,  3.86it/s] \u001b[A\n",
      "batch 23, dev loss: 3.856: : 24it [00:05,  4.34it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7812: : 24it [00:05,  4.34it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7812: : 25it [00:05,  3.91it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7577: : 25it [00:06,  3.91it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7577: : 26it [00:06,  3.68it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6978: : 26it [00:06,  3.68it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6978: : 27it [00:06,  3.75it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6303: : 27it [00:06,  3.75it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6303: : 28it [00:06,  3.69it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8885: : 28it [00:07,  3.69it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8885: : 29it [00:07,  3.49it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8055: : 29it [00:07,  3.49it/s]\u001b[A\n",
      "batch 29, dev loss: 3.8055: : 30it [00:07,  3.49it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1484: : 30it [00:07,  3.49it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1484: : 31it [00:07,  4.18it/s]\u001b[A\n",
      "batch 31, dev loss: 3.8283: : 31it [00:07,  4.18it/s]\u001b[A\n",
      "batch 31, dev loss: 3.8283: : 32it [00:07,  3.74it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9043: : 32it [00:08,  3.74it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9043: : 33it [00:08,  3.46it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6371: : 33it [00:08,  3.46it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6371: : 34it [00:08,  3.29it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0794: : 34it [00:08,  3.29it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0794: : 35it [00:08,  3.13it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9037: : 35it [00:09,  3.13it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9037: : 36it [00:09,  2.99it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8015: : 36it [00:09,  2.99it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8015: : 37it [00:09,  3.24it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5769: : 37it [00:09,  3.24it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5769: : 38it [00:09,  3.24it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8354: : 38it [00:10,  3.24it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8354: : 39it [00:10,  3.09it/s]\u001b[A\n",
      "batch 39, dev loss: 3.827: : 39it [00:10,  3.09it/s] \u001b[A\n",
      "batch 39, dev loss: 3.827: : 40it [00:10,  3.01it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8573: : 40it [00:10,  3.01it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8573: : 41it [00:10,  2.87it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6683: : 41it [00:11,  2.87it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6683: : 42it [00:11,  3.02it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8199: : 42it [00:11,  3.02it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8199: : 43it [00:11,  2.91it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8062: : 43it [00:11,  2.91it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8062: : 44it [00:11,  2.72it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7212: : 44it [00:12,  2.72it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7212: : 45it [00:12,  2.66it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0297: : 45it [00:12,  2.66it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0297: : 46it [00:12,  2.67it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5602: : 46it [00:13,  2.67it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5602: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7385: : 47it [00:13,  2.57it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7385: : 48it [00:13,  2.53it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5642: : 48it [00:13,  2.53it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5642: : 49it [00:13,  2.55it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7415: : 49it [00:14,  2.55it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7415: : 50it [00:14,  2.89it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7065: : 50it [00:14,  2.89it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7065: : 51it [00:14,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7285: : 51it [00:15,  2.64it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7285: : 52it [00:15,  2.52it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4992: : 52it [00:15,  2.52it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4992: : 53it [00:15,  2.63it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7078: : 53it [00:15,  2.63it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7078: : 54it [00:15,  2.41it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4592: : 54it [00:16,  2.41it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4592: : 55it [00:16,  2.30it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6582: : 55it [00:16,  2.30it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6582: : 56it [00:16,  2.30it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4938: : 56it [00:17,  2.30it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4938: : 57it [00:17,  2.61it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4539: : 57it [00:17,  2.61it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4539: : 58it [00:17,  2.43it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8233: : 58it [00:17,  2.43it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8233: : 59it [00:17,  2.39it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7148: : 59it [00:18,  2.39it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7148: : 60it [00:18,  2.33it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4392: : 60it [00:18,  2.33it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4392: : 61it [00:18,  2.37it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4061: : 61it [00:19,  2.37it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4061: : 62it [00:19,  2.51it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1929: : 62it [00:19,  2.51it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1929: : 63it [00:19,  2.70it/s]\u001b[A\n",
      "batch 63, dev loss: 3.84: : 63it [00:19,  2.70it/s]  \u001b[A\n",
      "batch 63, dev loss: 3.84: : 64it [00:19,  2.66it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5248: : 64it [00:20,  2.66it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 64, dev loss: 3.5248: : 65it [00:20,  2.83it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4968: : 65it [00:20,  2.83it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4968: : 66it [00:20,  2.77it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5762: : 66it [00:20,  2.77it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5762: : 67it [00:20,  2.82it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5969: : 67it [00:21,  2.82it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5969: : 68it [00:21,  2.80it/s]\u001b[A\n",
      "batch 68, dev loss: 2.7056: : 68it [00:21,  2.80it/s]\u001b[A\n",
      "batch 68, dev loss: 2.7056: : 69it [00:21,  2.94it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0307: : 69it [00:21,  2.94it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0307: : 70it [00:21,  2.94it/s]\u001b[A\n",
      "batch 70, dev loss: 3.8989: : 70it [00:22,  2.94it/s]\u001b[A\n",
      "batch 70, dev loss: 3.8989: : 71it [00:22,  2.94it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1849: : 71it [00:22,  2.94it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1849: : 72it [00:22,  2.85it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0717: : 72it [00:22,  2.85it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0717: : 73it [00:22,  2.92it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0717: : 74it [00:23,  3.57it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0717: : 76it [00:23,  3.27it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.43s/it]\u001b[A\n",
      "2it [00:02,  1.36s/it]\u001b[A\n",
      "3it [00:04,  1.35s/it]\u001b[A\n",
      "4it [00:05,  1.33s/it]\u001b[A\n",
      "5it [00:06,  1.37s/it]\u001b[A\n",
      "6it [00:08,  1.30s/it]\u001b[A\n",
      "7it [00:09,  1.34s/it]\u001b[A\n",
      "8it [00:10,  1.15s/it]\u001b[A\n",
      "9it [00:11,  1.35s/it]\u001b[A\n",
      "10it [00:13,  1.51s/it]\u001b[A\n",
      "11it [00:15,  1.45s/it]\u001b[A\n",
      "12it [00:16,  1.52s/it]\u001b[A\n",
      "13it [00:18,  1.50s/it]\u001b[A\n",
      "14it [00:19,  1.49s/it]\u001b[A\n",
      "15it [00:21,  1.60s/it]\u001b[A\n",
      "16it [00:22,  1.41s/it]\u001b[A\n",
      "17it [00:24,  1.62s/it]\u001b[A\n",
      "18it [00:26,  1.82s/it]\u001b[A\n",
      "19it [00:29,  1.90s/it]\u001b[A\n",
      "20it [00:31,  2.01s/it]\u001b[A\n",
      "21it [00:33,  2.00s/it]\u001b[A\n",
      "22it [00:35,  2.01s/it]\u001b[A\n",
      "23it [00:36,  1.92s/it]\u001b[A\n",
      "24it [00:37,  1.50s/it]\u001b[A\n",
      "25it [00:39,  1.70s/it]\u001b[A\n",
      "26it [00:41,  1.78s/it]\u001b[A\n",
      "27it [00:44,  2.02s/it]\u001b[A\n",
      "28it [00:46,  2.20s/it]\u001b[A\n",
      "29it [00:49,  2.31s/it]\u001b[A\n",
      "30it [00:51,  2.25s/it]\u001b[A\n",
      "31it [00:54,  2.34s/it]\u001b[A\n",
      "32it [00:57,  2.56s/it]\u001b[A\n",
      "33it [00:59,  2.56s/it]\u001b[A\n",
      "34it [01:02,  2.60s/it]\u001b[A\n",
      "35it [01:05,  2.66s/it]\u001b[A\n",
      "36it [01:05,  1.99s/it]\u001b[A\n",
      "37it [01:07,  2.08s/it]\u001b[A\n",
      "38it [01:11,  2.39s/it]\u001b[A\n",
      "39it [01:14,  2.63s/it]\u001b[A\n",
      "40it [01:17,  2.88s/it]\u001b[A\n",
      "41it [01:18,  2.26s/it]\u001b[A\n",
      "42it [01:21,  2.61s/it]\u001b[A\n",
      "43it [01:25,  2.81s/it]\u001b[A\n",
      "44it [01:28,  3.00s/it]\u001b[A\n",
      "45it [01:31,  2.88s/it]\u001b[A\n",
      "46it [01:34,  3.13s/it]\u001b[A\n",
      "47it [01:38,  3.30s/it]\u001b[A\n",
      "48it [01:42,  3.34s/it]\u001b[A\n",
      "49it [01:42,  2.42s/it]\u001b[A\n",
      "50it [01:46,  2.92s/it]\u001b[A\n",
      "51it [01:50,  3.35s/it]\u001b[A\n",
      "52it [01:53,  3.10s/it]\u001b[A\n",
      "53it [01:58,  3.62s/it]\u001b[A\n",
      "54it [02:01,  3.66s/it]\u001b[A\n",
      "55it [02:05,  3.61s/it]\u001b[A\n",
      "56it [02:07,  3.17s/it]\u001b[A\n",
      "57it [02:11,  3.37s/it]\u001b[A\n",
      "58it [02:15,  3.50s/it]\u001b[A\n",
      "59it [02:17,  3.11s/it]\u001b[A\n",
      "60it [02:19,  2.78s/it]\u001b[A\n",
      "61it [02:20,  2.42s/it]\u001b[A\n",
      "62it [02:22,  2.03s/it]\u001b[A\n",
      "63it [02:23,  1.80s/it]\u001b[A\n",
      "64it [02:24,  1.52s/it]\u001b[A\n",
      "65it [02:24,  1.29s/it]\u001b[A\n",
      "66it [02:25,  1.03s/it]\u001b[A\n",
      "67it [02:25,  1.14it/s]\u001b[A\n",
      "68it [02:26,  1.34it/s]\u001b[A\n",
      "69it [02:26,  1.48it/s]\u001b[A\n",
      "70it [02:27,  2.11s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████                                  | 726/6740 [00:00<00:00, 7252.55it/s]\u001b[A\n",
      " 22%|███████▉                             | 1452/6740 [00:00<00:00, 7195.32it/s]\u001b[A\n",
      " 32%|███████████▉                         | 2172/6740 [00:00<00:00, 7067.31it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 2879/6740 [00:00<00:00, 6794.21it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 3560/6740 [00:00<00:00, 6706.87it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 4232/6740 [00:00<00:00, 6479.75it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 4882/6740 [00:00<00:00, 6435.06it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 5527/6740 [00:00<00:00, 6269.76it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 6511.31it/s]\u001b[A\n",
      "Epoch: 10, tfr: 1.0, loss(train/dev): 3.516/3.696, ppl(dev/test): 40.2858/46.917\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.6025: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.6025: : 1it [00:01,  1.92s/it]\u001b[A\n",
      "batch 2, training loss: 3.4882: : 1it [00:02,  1.92s/it]\u001b[A\n",
      "batch 2, training loss: 3.4882: : 2it [00:02,  1.18s/it]\u001b[A\n",
      "batch 3, training loss: 3.4931: : 2it [00:03,  1.18s/it]\u001b[A\n",
      "batch 3, training loss: 3.4931: : 3it [00:03,  1.13it/s]\u001b[A\n",
      "batch 4, training loss: 3.4427: : 3it [00:03,  1.13it/s]\u001b[A\n",
      "batch 4, training loss: 3.4427: : 4it [00:03,  1.31it/s]\u001b[A\n",
      "batch 5, training loss: 3.2802: : 4it [00:04,  1.31it/s]\u001b[A\n",
      "batch 5, training loss: 3.2802: : 5it [00:04,  1.38it/s]\u001b[A\n",
      "batch 6, training loss: 3.5693: : 5it [00:05,  1.38it/s]\u001b[A\n",
      "batch 6, training loss: 3.5693: : 6it [00:05,  1.40it/s]\u001b[A\n",
      "batch 7, training loss: 3.5537: : 6it [00:05,  1.40it/s]\u001b[A\n",
      "batch 7, training loss: 3.5537: : 7it [00:05,  1.42it/s]\u001b[A\n",
      "batch 8, training loss: 3.5601: : 7it [00:06,  1.42it/s]\u001b[A\n",
      "batch 8, training loss: 3.5601: : 8it [00:06,  1.54it/s]\u001b[A\n",
      "batch 9, training loss: 3.4097: : 8it [00:06,  1.54it/s]\u001b[A\n",
      "batch 9, training loss: 3.4097: : 9it [00:06,  1.59it/s]\u001b[A\n",
      "batch 10, training loss: 3.4349: : 9it [00:07,  1.59it/s]\u001b[A\n",
      "batch 10, training loss: 3.4349: : 10it [00:07,  1.57it/s]\u001b[A\n",
      "batch 11, training loss: 3.4653: : 10it [00:08,  1.57it/s]\u001b[A\n",
      "batch 11, training loss: 3.4653: : 11it [00:08,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.4822: : 11it [00:08,  1.55it/s]\u001b[A\n",
      "batch 12, training loss: 3.4822: : 12it [00:08,  1.59it/s]\u001b[A\n",
      "batch 13, training loss: 3.344: : 12it [00:09,  1.59it/s] \u001b[A\n",
      "batch 13, training loss: 3.344: : 13it [00:09,  1.61it/s]\u001b[A\n",
      "batch 14, training loss: 3.6569: : 13it [00:09,  1.61it/s]\u001b[A\n",
      "batch 14, training loss: 3.6569: : 14it [00:09,  1.68it/s]\u001b[A\n",
      "batch 15, training loss: 3.5118: : 14it [00:10,  1.68it/s]\u001b[A\n",
      "batch 15, training loss: 3.5118: : 15it [00:10,  1.62it/s]\u001b[A\n",
      "batch 16, training loss: 3.5294: : 15it [00:11,  1.62it/s]\u001b[A\n",
      "batch 16, training loss: 3.5294: : 16it [00:11,  1.59it/s]\u001b[A\n",
      "batch 17, training loss: 3.6493: : 16it [00:11,  1.59it/s]\u001b[A\n",
      "batch 17, training loss: 3.6493: : 17it [00:11,  1.60it/s]\u001b[A\n",
      "batch 18, training loss: 3.4846: : 17it [00:12,  1.60it/s]\u001b[A\n",
      "batch 18, training loss: 3.4846: : 18it [00:12,  1.59it/s]\u001b[A\n",
      "batch 19, training loss: 3.2594: : 18it [00:13,  1.59it/s]\u001b[A\n",
      "batch 19, training loss: 3.2594: : 19it [00:13,  1.60it/s]\u001b[A\n",
      "batch 20, training loss: 3.4205: : 19it [00:13,  1.60it/s]\u001b[A\n",
      "batch 20, training loss: 3.4205: : 20it [00:13,  1.56it/s]\u001b[A\n",
      "batch 21, training loss: 3.5424: : 20it [00:14,  1.56it/s]\u001b[A\n",
      "batch 21, training loss: 3.5424: : 21it [00:14,  1.51it/s]\u001b[A\n",
      "batch 22, training loss: 3.3105: : 21it [00:15,  1.51it/s]\u001b[A\n",
      "batch 22, training loss: 3.3105: : 22it [00:15,  1.50it/s]\u001b[A\n",
      "batch 23, training loss: 3.4604: : 22it [00:15,  1.50it/s]\u001b[A\n",
      "batch 23, training loss: 3.4604: : 23it [00:15,  1.61it/s]\u001b[A\n",
      "batch 24, training loss: 3.4: : 23it [00:16,  1.61it/s]   \u001b[A\n",
      "batch 24, training loss: 3.4: : 24it [00:16,  1.65it/s]\u001b[A\n",
      "batch 25, training loss: 3.506: : 24it [00:16,  1.65it/s]\u001b[A\n",
      "batch 25, training loss: 3.506: : 25it [00:16,  1.61it/s]\u001b[A\n",
      "batch 26, training loss: 3.304: : 25it [00:17,  1.61it/s]\u001b[A\n",
      "batch 26, training loss: 3.304: : 26it [00:17,  1.58it/s]\u001b[A\n",
      "batch 27, training loss: 3.4395: : 26it [00:18,  1.58it/s]\u001b[A\n",
      "batch 27, training loss: 3.4395: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 3.2991: : 27it [00:18,  1.60it/s]\u001b[A\n",
      "batch 28, training loss: 3.2991: : 28it [00:18,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 3.4589: : 28it [00:19,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 3.4589: : 29it [00:19,  1.69it/s]\u001b[A\n",
      "batch 30, training loss: 3.5771: : 29it [00:19,  1.69it/s]\u001b[A\n",
      "batch 30, training loss: 3.5771: : 30it [00:19,  1.64it/s]\u001b[A\n",
      "batch 31, training loss: 3.3315: : 30it [00:20,  1.64it/s]\u001b[A\n",
      "batch 31, training loss: 3.3315: : 31it [00:20,  1.60it/s]\u001b[A\n",
      "batch 32, training loss: 3.5157: : 31it [00:21,  1.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 32, training loss: 3.5157: : 32it [00:21,  1.70it/s]\u001b[A\n",
      "batch 33, training loss: 3.3939: : 32it [00:21,  1.70it/s]\u001b[A\n",
      "batch 33, training loss: 3.3939: : 33it [00:21,  1.71it/s]\u001b[A\n",
      "batch 34, training loss: 3.3985: : 33it [00:22,  1.71it/s]\u001b[A\n",
      "batch 34, training loss: 3.3985: : 34it [00:22,  1.65it/s]\u001b[A\n",
      "batch 35, training loss: 3.4585: : 34it [00:23,  1.65it/s]\u001b[A\n",
      "batch 35, training loss: 3.4585: : 35it [00:23,  1.60it/s]\u001b[A\n",
      "batch 36, training loss: 3.5497: : 35it [00:23,  1.60it/s]\u001b[A\n",
      "batch 36, training loss: 3.5497: : 36it [00:23,  1.60it/s]\u001b[A\n",
      "batch 37, training loss: 3.403: : 36it [00:24,  1.60it/s] \u001b[A\n",
      "batch 37, training loss: 3.403: : 37it [00:24,  1.62it/s]\u001b[A\n",
      "batch 38, training loss: 3.2048: : 37it [00:24,  1.62it/s]\u001b[A\n",
      "batch 38, training loss: 3.2048: : 38it [00:24,  1.64it/s]\u001b[A\n",
      "batch 39, training loss: 3.3794: : 38it [00:25,  1.64it/s]\u001b[A\n",
      "batch 39, training loss: 3.3794: : 39it [00:25,  1.60it/s]\u001b[A\n",
      "batch 40, training loss: 3.4403: : 39it [00:26,  1.60it/s]\u001b[A\n",
      "batch 40, training loss: 3.4403: : 40it [00:26,  1.54it/s]\u001b[A\n",
      "batch 41, training loss: 3.5887: : 40it [00:26,  1.54it/s]\u001b[A\n",
      "batch 41, training loss: 3.5887: : 41it [00:26,  1.52it/s]\u001b[A\n",
      "batch 42, training loss: 3.415: : 41it [00:27,  1.52it/s] \u001b[A\n",
      "batch 42, training loss: 3.415: : 42it [00:27,  1.48it/s]\u001b[A\n",
      "batch 43, training loss: 3.227: : 42it [00:28,  1.48it/s]\u001b[A\n",
      "batch 43, training loss: 3.227: : 43it [00:28,  1.53it/s]\u001b[A\n",
      "batch 44, training loss: 3.2012: : 43it [00:28,  1.53it/s]\u001b[A\n",
      "batch 44, training loss: 3.2012: : 44it [00:28,  1.58it/s]\u001b[A\n",
      "batch 45, training loss: 3.2465: : 44it [00:29,  1.58it/s]\u001b[A\n",
      "batch 45, training loss: 3.2465: : 45it [00:29,  1.62it/s]\u001b[A\n",
      "batch 46, training loss: 3.3858: : 45it [00:29,  1.62it/s]\u001b[A\n",
      "batch 46, training loss: 3.3858: : 46it [00:29,  1.59it/s]\u001b[A\n",
      "batch 47, training loss: 3.3825: : 46it [00:30,  1.59it/s]\u001b[A\n",
      "batch 47, training loss: 3.3825: : 47it [00:30,  1.55it/s]\u001b[A\n",
      "batch 48, training loss: 3.2882: : 47it [00:31,  1.55it/s]\u001b[A\n",
      "batch 48, training loss: 3.2882: : 48it [00:31,  1.51it/s]\u001b[A\n",
      "batch 49, training loss: 3.5517: : 48it [00:31,  1.51it/s]\u001b[A\n",
      "batch 49, training loss: 3.5517: : 49it [00:31,  1.55it/s]\u001b[A\n",
      "batch 50, training loss: 3.311: : 49it [00:32,  1.55it/s] \u001b[A\n",
      "batch 50, training loss: 3.311: : 50it [00:32,  1.58it/s]\u001b[A\n",
      "batch 51, training loss: 3.3914: : 50it [00:33,  1.58it/s]\u001b[A\n",
      "batch 51, training loss: 3.3914: : 51it [00:33,  1.60it/s]\u001b[A\n",
      "batch 52, training loss: 3.2657: : 51it [00:33,  1.60it/s]\u001b[A\n",
      "batch 52, training loss: 3.2657: : 52it [00:33,  1.57it/s]\u001b[A\n",
      "batch 53, training loss: 3.269: : 52it [00:34,  1.57it/s] \u001b[A\n",
      "batch 53, training loss: 3.269: : 53it [00:34,  1.52it/s]\u001b[A\n",
      "batch 54, training loss: 3.234: : 53it [00:35,  1.52it/s]\u001b[A\n",
      "batch 54, training loss: 3.234: : 54it [00:35,  1.53it/s]\u001b[A\n",
      "batch 55, training loss: 3.3531: : 54it [00:35,  1.53it/s]\u001b[A\n",
      "batch 55, training loss: 3.3531: : 55it [00:35,  1.58it/s]\u001b[A\n",
      "batch 56, training loss: 3.3903: : 55it [00:36,  1.58it/s]\u001b[A\n",
      "batch 56, training loss: 3.3903: : 56it [00:36,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.3486: : 56it [00:36,  1.65it/s]\u001b[A\n",
      "batch 57, training loss: 3.3486: : 57it [00:36,  1.61it/s]\u001b[A\n",
      "batch 58, training loss: 3.4214: : 57it [00:37,  1.61it/s]\u001b[A\n",
      "batch 58, training loss: 3.4214: : 58it [00:37,  1.57it/s]\u001b[A\n",
      "batch 59, training loss: 3.283: : 58it [00:38,  1.57it/s] \u001b[A\n",
      "batch 59, training loss: 3.283: : 59it [00:38,  1.68it/s]\u001b[A\n",
      "batch 60, training loss: 3.2296: : 59it [00:38,  1.68it/s]\u001b[A\n",
      "batch 60, training loss: 3.2296: : 60it [00:38,  1.70it/s]\u001b[A\n",
      "batch 61, training loss: 3.447: : 60it [00:39,  1.70it/s] \u001b[A\n",
      "batch 61, training loss: 3.447: : 61it [00:39,  1.65it/s]\u001b[A\n",
      "batch 62, training loss: 3.3144: : 61it [00:40,  1.65it/s]\u001b[A\n",
      "batch 62, training loss: 3.3144: : 62it [00:40,  1.60it/s]\u001b[A\n",
      "batch 63, training loss: 3.3837: : 62it [00:40,  1.60it/s]\u001b[A\n",
      "batch 63, training loss: 3.3837: : 63it [00:40,  1.59it/s]\u001b[A\n",
      "batch 64, training loss: 3.3603: : 63it [00:41,  1.59it/s]\u001b[A\n",
      "batch 64, training loss: 3.3603: : 64it [00:41,  1.62it/s]\u001b[A\n",
      "batch 65, training loss: 3.4173: : 64it [00:41,  1.62it/s]\u001b[A\n",
      "batch 65, training loss: 3.4173: : 65it [00:41,  1.65it/s]\u001b[A\n",
      "batch 66, training loss: 3.4518: : 65it [00:42,  1.65it/s]\u001b[A\n",
      "batch 66, training loss: 3.4518: : 66it [00:42,  1.60it/s]\u001b[A\n",
      "batch 67, training loss: 3.3013: : 66it [00:43,  1.60it/s]\u001b[A\n",
      "batch 67, training loss: 3.3013: : 67it [00:43,  1.54it/s]\u001b[A\n",
      "batch 68, training loss: 3.4083: : 67it [00:43,  1.54it/s]\u001b[A\n",
      "batch 68, training loss: 3.4083: : 68it [00:43,  1.51it/s]\u001b[A\n",
      "batch 69, training loss: 3.2757: : 68it [00:44,  1.51it/s]\u001b[A\n",
      "batch 69, training loss: 3.2757: : 69it [00:44,  1.48it/s]\u001b[A\n",
      "batch 70, training loss: 3.4867: : 69it [00:45,  1.48it/s]\u001b[A\n",
      "batch 70, training loss: 3.4867: : 70it [00:45,  1.47it/s]\u001b[A\n",
      "batch 71, training loss: 3.3205: : 70it [00:45,  1.47it/s]\u001b[A\n",
      "batch 71, training loss: 3.3205: : 71it [00:45,  1.58it/s]\u001b[A\n",
      "batch 72, training loss: 3.2928: : 71it [00:46,  1.58it/s]\u001b[A\n",
      "batch 72, training loss: 3.2928: : 72it [00:46,  1.62it/s]\u001b[A\n",
      "batch 73, training loss: 3.3459: : 72it [00:47,  1.62it/s]\u001b[A\n",
      "batch 73, training loss: 3.3459: : 73it [00:47,  1.59it/s]\u001b[A\n",
      "batch 74, training loss: 3.2473: : 73it [00:47,  1.59it/s]\u001b[A\n",
      "batch 74, training loss: 3.2473: : 74it [00:47,  1.56it/s]\u001b[A\n",
      "batch 75, training loss: 3.4638: : 74it [00:48,  1.56it/s]\u001b[A\n",
      "batch 75, training loss: 3.4638: : 75it [00:48,  1.68it/s]\u001b[A\n",
      "batch 76, training loss: 3.2696: : 75it [00:48,  1.68it/s]\u001b[A\n",
      "batch 76, training loss: 3.2696: : 76it [00:48,  1.70it/s]\u001b[A\n",
      "batch 77, training loss: 3.3942: : 76it [00:49,  1.70it/s]\u001b[A\n",
      "batch 77, training loss: 3.3942: : 77it [00:49,  1.65it/s]\u001b[A\n",
      "batch 78, training loss: 3.3864: : 77it [00:50,  1.65it/s]\u001b[A\n",
      "batch 78, training loss: 3.3864: : 78it [00:50,  1.61it/s]\u001b[A\n",
      "batch 79, training loss: 3.4481: : 78it [00:50,  1.61it/s]\u001b[A\n",
      "batch 79, training loss: 3.4481: : 79it [00:50,  1.58it/s]\u001b[A\n",
      "batch 80, training loss: 3.3484: : 79it [00:51,  1.58it/s]\u001b[A\n",
      "batch 80, training loss: 3.3484: : 80it [00:51,  1.63it/s]\u001b[A\n",
      "batch 81, training loss: 3.3365: : 80it [00:51,  1.63it/s]\u001b[A\n",
      "batch 81, training loss: 3.3365: : 81it [00:51,  1.70it/s]\u001b[A\n",
      "batch 82, training loss: 3.4799: : 81it [00:52,  1.70it/s]\u001b[A\n",
      "batch 82, training loss: 3.4799: : 82it [00:52,  1.63it/s]\u001b[A\n",
      "batch 83, training loss: 3.3541: : 82it [00:53,  1.63it/s]\u001b[A\n",
      "batch 83, training loss: 3.3541: : 83it [00:53,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 3.5024: : 83it [00:53,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 3.5024: : 84it [00:53,  1.51it/s]\u001b[A\n",
      "batch 85, training loss: 3.494: : 84it [00:54,  1.51it/s] \u001b[A\n",
      "batch 85, training loss: 3.494: : 85it [00:54,  1.52it/s]\u001b[A\n",
      "batch 86, training loss: 3.4628: : 85it [00:55,  1.52it/s]\u001b[A\n",
      "batch 86, training loss: 3.4628: : 86it [00:55,  1.60it/s]\u001b[A\n",
      "batch 87, training loss: 3.4649: : 86it [00:55,  1.60it/s]\u001b[A\n",
      "batch 87, training loss: 3.4649: : 87it [00:55,  1.65it/s]\u001b[A\n",
      "batch 88, training loss: 3.4622: : 87it [00:56,  1.65it/s]\u001b[A\n",
      "batch 88, training loss: 3.4622: : 88it [00:56,  1.57it/s]\u001b[A\n",
      "batch 89, training loss: 3.5757: : 88it [00:57,  1.57it/s]\u001b[A\n",
      "batch 89, training loss: 3.5757: : 89it [00:57,  1.48it/s]\u001b[A\n",
      "batch 90, training loss: 3.5094: : 89it [00:57,  1.48it/s]\u001b[A\n",
      "batch 90, training loss: 3.5094: : 90it [00:57,  1.43it/s]\u001b[A\n",
      "batch 91, training loss: 3.636: : 90it [00:58,  1.43it/s] \u001b[A\n",
      "batch 91, training loss: 3.636: : 91it [00:58,  1.38it/s]\u001b[A\n",
      "batch 92, training loss: 3.4628: : 91it [00:59,  1.38it/s]\u001b[A\n",
      "batch 92, training loss: 3.4628: : 92it [00:59,  1.36it/s]\u001b[A\n",
      "batch 93, training loss: 3.4459: : 92it [01:00,  1.36it/s]\u001b[A\n",
      "batch 93, training loss: 3.4459: : 93it [01:00,  1.36it/s]\u001b[A\n",
      "batch 94, training loss: 3.5345: : 93it [01:01,  1.36it/s]\u001b[A\n",
      "batch 94, training loss: 3.5345: : 94it [01:01,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 3.4918: : 94it [01:01,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 3.4918: : 95it [01:01,  1.33it/s]\u001b[A\n",
      "batch 96, training loss: 3.5037: : 95it [01:02,  1.33it/s]\u001b[A\n",
      "batch 96, training loss: 3.5037: : 96it [01:02,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 3.5906: : 96it [01:03,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 3.5906: : 97it [01:03,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 3.5802: : 97it [01:04,  1.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 98, training loss: 3.5802: : 98it [01:04,  1.30it/s]\u001b[A\n",
      "batch 99, training loss: 3.3862: : 98it [01:04,  1.30it/s]\u001b[A\n",
      "batch 99, training loss: 3.3862: : 99it [01:04,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.2512: : 99it [01:05,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.2512: : 100it [01:05,  1.32it/s]\u001b[A\n",
      "batch 101, training loss: 3.4128: : 100it [01:06,  1.32it/s]\u001b[A\n",
      "batch 101, training loss: 3.4128: : 101it [01:06,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.4282: : 101it [01:07,  1.31it/s]\u001b[A\n",
      "batch 102, training loss: 3.4282: : 102it [01:07,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 3.3784: : 102it [01:07,  1.31it/s]\u001b[A\n",
      "batch 103, training loss: 3.3784: : 103it [01:07,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.2132: : 103it [01:08,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 3.2132: : 104it [01:08,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.3684: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.3684: : 105it [01:09,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.5212: : 105it [01:10,  1.30it/s]\u001b[A\n",
      "batch 106, training loss: 3.5212: : 106it [01:10,  1.29it/s]\u001b[A\n",
      "batch 107, training loss: 3.231: : 106it [01:10,  1.29it/s] \u001b[A\n",
      "batch 107, training loss: 3.231: : 107it [01:10,  1.32it/s]\u001b[A\n",
      "batch 108, training loss: 3.4999: : 107it [01:11,  1.32it/s]\u001b[A\n",
      "batch 108, training loss: 3.4999: : 108it [01:11,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 3.5489: : 108it [01:12,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 3.5489: : 109it [01:12,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 3.6774: : 109it [01:13,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 3.6774: : 110it [01:13,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 3.3968: : 110it [01:14,  1.29it/s]\u001b[A\n",
      "batch 111, training loss: 3.3968: : 111it [01:14,  1.30it/s]\u001b[A\n",
      "batch 112, training loss: 3.455: : 111it [01:14,  1.30it/s] \u001b[A\n",
      "batch 112, training loss: 3.455: : 112it [01:14,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.5357: : 112it [01:15,  1.29it/s]\u001b[A\n",
      "batch 113, training loss: 3.5357: : 113it [01:15,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.5142: : 113it [01:16,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.5142: : 114it [01:16,  1.32it/s]\u001b[A\n",
      "batch 115, training loss: 3.4296: : 114it [01:17,  1.32it/s]\u001b[A\n",
      "batch 115, training loss: 3.4296: : 115it [01:17,  1.30it/s]\u001b[A\n",
      "batch 116, training loss: 3.4313: : 115it [01:17,  1.30it/s]\u001b[A\n",
      "batch 116, training loss: 3.4313: : 116it [01:17,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.4919: : 116it [01:18,  1.30it/s]\u001b[A\n",
      "batch 117, training loss: 3.4919: : 117it [01:18,  1.45it/s]\u001b[A\n",
      "batch 118, training loss: 3.5579: : 117it [01:19,  1.45it/s]\u001b[A\n",
      "batch 118, training loss: 3.5579: : 118it [01:19,  1.45it/s]\u001b[A\n",
      "batch 119, training loss: 3.4755: : 118it [01:19,  1.45it/s]\u001b[A\n",
      "batch 119, training loss: 3.4755: : 119it [01:19,  1.44it/s]\u001b[A\n",
      "batch 120, training loss: 3.4194: : 119it [01:20,  1.44it/s]\u001b[A\n",
      "batch 120, training loss: 3.4194: : 120it [01:20,  1.38it/s]\u001b[A\n",
      "batch 121, training loss: 3.6597: : 120it [01:21,  1.38it/s]\u001b[A\n",
      "batch 121, training loss: 3.6597: : 121it [01:21,  1.35it/s]\u001b[A\n",
      "batch 122, training loss: 3.3353: : 121it [01:22,  1.35it/s]\u001b[A\n",
      "batch 122, training loss: 3.3353: : 122it [01:22,  1.34it/s]\u001b[A\n",
      "batch 123, training loss: 3.5005: : 122it [01:22,  1.34it/s]\u001b[A\n",
      "batch 123, training loss: 3.5005: : 123it [01:22,  1.33it/s]\u001b[A\n",
      "batch 124, training loss: 3.5121: : 123it [01:23,  1.33it/s]\u001b[A\n",
      "batch 124, training loss: 3.5121: : 124it [01:23,  1.31it/s]\u001b[A\n",
      "batch 125, training loss: 3.5103: : 124it [01:24,  1.31it/s]\u001b[A\n",
      "batch 125, training loss: 3.5103: : 125it [01:24,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.5827: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.5827: : 126it [01:25,  1.33it/s]\u001b[A\n",
      "batch 127, training loss: 3.2838: : 126it [01:25,  1.33it/s]\u001b[A\n",
      "batch 127, training loss: 3.2838: : 127it [01:25,  1.31it/s]\u001b[A\n",
      "batch 128, training loss: 3.4973: : 127it [01:26,  1.31it/s]\u001b[A\n",
      "batch 128, training loss: 3.4973: : 128it [01:26,  1.31it/s]\u001b[A\n",
      "batch 129, training loss: 3.4488: : 128it [01:27,  1.31it/s]\u001b[A\n",
      "batch 129, training loss: 3.4488: : 129it [01:27,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 3.4579: : 129it [01:28,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 3.4579: : 130it [01:28,  1.29it/s]\u001b[A\n",
      "batch 131, training loss: 3.6338: : 130it [01:29,  1.29it/s]\u001b[A\n",
      "batch 131, training loss: 3.6338: : 131it [01:29,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.4317: : 131it [01:29,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.4317: : 132it [01:29,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.3305: : 132it [01:30,  1.29it/s]\u001b[A\n",
      "batch 133, training loss: 3.3305: : 133it [01:30,  1.29it/s]\u001b[A\n",
      "batch 134, training loss: 3.5111: : 133it [01:31,  1.29it/s]\u001b[A\n",
      "batch 134, training loss: 3.5111: : 134it [01:31,  1.29it/s]\u001b[A\n",
      "batch 135, training loss: 3.406: : 134it [01:32,  1.29it/s] \u001b[A\n",
      "batch 135, training loss: 3.406: : 135it [01:32,  1.29it/s]\u001b[A\n",
      "batch 136, training loss: 3.3447: : 135it [01:32,  1.29it/s]\u001b[A\n",
      "batch 136, training loss: 3.3447: : 136it [01:32,  1.29it/s]\u001b[A\n",
      "batch 137, training loss: 3.5187: : 136it [01:33,  1.29it/s]\u001b[A\n",
      "batch 137, training loss: 3.5187: : 137it [01:33,  1.28it/s]\u001b[A\n",
      "batch 138, training loss: 3.5233: : 137it [01:34,  1.28it/s]\u001b[A\n",
      "batch 138, training loss: 3.5233: : 138it [01:34,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.296: : 138it [01:35,  1.29it/s] \u001b[A\n",
      "batch 139, training loss: 3.296: : 139it [01:35,  1.28it/s]\u001b[A\n",
      "batch 140, training loss: 3.4115: : 139it [01:36,  1.28it/s]\u001b[A\n",
      "batch 140, training loss: 3.4115: : 140it [01:36,  1.28it/s]\u001b[A\n",
      "batch 141, training loss: 3.4043: : 140it [01:36,  1.28it/s]\u001b[A\n",
      "batch 141, training loss: 3.4043: : 141it [01:36,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.4547: : 141it [01:37,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.4547: : 142it [01:37,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.2838: : 142it [01:38,  1.30it/s]\u001b[A\n",
      "batch 143, training loss: 3.2838: : 143it [01:38,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 3.3346: : 143it [01:39,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 3.3346: : 144it [01:39,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.4392: : 144it [01:39,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.4392: : 145it [01:39,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 3.4357: : 145it [01:40,  1.30it/s]\u001b[A\n",
      "batch 146, training loss: 3.4357: : 146it [01:40,  1.29it/s]\u001b[A\n",
      "batch 147, training loss: 3.3492: : 146it [01:41,  1.29it/s]\u001b[A\n",
      "batch 147, training loss: 3.3492: : 147it [01:41,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.4166: : 147it [01:42,  1.30it/s]\u001b[A\n",
      "batch 148, training loss: 3.4166: : 148it [01:42,  1.29it/s]\u001b[A\n",
      "batch 149, training loss: 3.508: : 148it [01:43,  1.29it/s] \u001b[A\n",
      "batch 149, training loss: 3.508: : 149it [01:43,  1.30it/s]\u001b[A\n",
      "batch 150, training loss: 3.4775: : 149it [01:43,  1.30it/s]\u001b[A\n",
      "batch 150, training loss: 3.4775: : 150it [01:43,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.4108: : 150it [01:44,  1.29it/s]\u001b[A\n",
      "batch 151, training loss: 3.4108: : 151it [01:44,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 3.3505: : 151it [01:45,  1.31it/s]\u001b[A\n",
      "batch 152, training loss: 3.3505: : 152it [01:45,  1.31it/s]\u001b[A\n",
      "batch 153, training loss: 3.3753: : 152it [01:46,  1.31it/s]\u001b[A\n",
      "batch 153, training loss: 3.3753: : 153it [01:46,  1.29it/s]\u001b[A\n",
      "batch 154, training loss: 3.498: : 153it [01:46,  1.29it/s] \u001b[A\n",
      "batch 154, training loss: 3.498: : 154it [01:46,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 3.6192: : 154it [01:47,  1.29it/s]\u001b[A\n",
      "batch 155, training loss: 3.6192: : 155it [01:47,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.2201: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 3.2201: : 156it [01:48,  1.30it/s]\u001b[A\n",
      "batch 157, training loss: 3.4498: : 156it [01:49,  1.30it/s]\u001b[A\n",
      "batch 157, training loss: 3.4498: : 157it [01:49,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.4325: : 157it [01:49,  1.29it/s]\u001b[A\n",
      "batch 158, training loss: 3.4325: : 158it [01:49,  1.29it/s]\u001b[A\n",
      "batch 159, training loss: 3.326: : 158it [01:50,  1.29it/s] \u001b[A\n",
      "batch 159, training loss: 3.326: : 159it [01:50,  1.29it/s]\u001b[A\n",
      "batch 160, training loss: 3.5164: : 159it [01:51,  1.29it/s]\u001b[A\n",
      "batch 160, training loss: 3.5164: : 160it [01:51,  1.30it/s]\u001b[A\n",
      "batch 161, training loss: 3.3866: : 160it [01:52,  1.30it/s]\u001b[A\n",
      "batch 161, training loss: 3.3866: : 161it [01:52,  1.29it/s]\u001b[A\n",
      "batch 162, training loss: 3.3615: : 161it [01:53,  1.29it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 162, training loss: 3.3615: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.5992: : 162it [01:53,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.5992: : 163it [01:53,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.4011: : 163it [01:54,  1.30it/s]\u001b[A\n",
      "batch 164, training loss: 3.4011: : 164it [01:54,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 3.3776: : 164it [01:55,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 3.3776: : 165it [01:55,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.3675: : 165it [01:56,  1.31it/s]\u001b[A\n",
      "batch 166, training loss: 3.3675: : 166it [01:56,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.5059: : 166it [01:56,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.5059: : 167it [01:56,  1.29it/s]\u001b[A\n",
      "batch 168, training loss: 3.4813: : 167it [01:57,  1.29it/s]\u001b[A\n",
      "batch 168, training loss: 3.4813: : 168it [01:57,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 3.5337: : 168it [01:58,  1.29it/s]\u001b[A\n",
      "batch 169, training loss: 3.5337: : 169it [01:58,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.4321: : 169it [01:59,  1.29it/s]\u001b[A\n",
      "batch 170, training loss: 3.4321: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.4971: : 170it [01:59,  1.29it/s]\u001b[A\n",
      "batch 171, training loss: 2.4971: : 171it [01:59,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 3.6105: : 171it [02:00,  1.59it/s]\u001b[A\n",
      "batch 172, training loss: 3.6105: : 172it [02:00,  1.47it/s]\u001b[A\n",
      "batch 173, training loss: 3.4691: : 172it [02:01,  1.47it/s]\u001b[A\n",
      "batch 173, training loss: 3.4691: : 173it [02:01,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 3.6403: : 173it [02:01,  1.39it/s]\u001b[A\n",
      "batch 174, training loss: 3.6403: : 174it [02:01,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 3.542: : 174it [02:02,  1.33it/s] \u001b[A\n",
      "batch 175, training loss: 3.542: : 175it [02:02,  1.29it/s]\u001b[A\n",
      "batch 176, training loss: 3.5078: : 175it [02:03,  1.29it/s]\u001b[A\n",
      "batch 176, training loss: 3.5078: : 176it [02:03,  1.25it/s]\u001b[A\n",
      "batch 177, training loss: 3.4831: : 176it [02:04,  1.25it/s]\u001b[A\n",
      "batch 177, training loss: 3.4831: : 177it [02:04,  1.25it/s]\u001b[A\n",
      "batch 178, training loss: 3.4935: : 177it [02:05,  1.25it/s]\u001b[A\n",
      "batch 178, training loss: 3.4935: : 178it [02:05,  1.24it/s]\u001b[A\n",
      "batch 179, training loss: 3.5623: : 178it [02:06,  1.24it/s]\u001b[A\n",
      "batch 179, training loss: 3.5623: : 179it [02:06,  1.23it/s]\u001b[A\n",
      "batch 180, training loss: 3.5772: : 179it [02:06,  1.23it/s]\u001b[A\n",
      "batch 180, training loss: 3.5772: : 180it [02:06,  1.22it/s]\u001b[A\n",
      "batch 181, training loss: 3.646: : 180it [02:07,  1.22it/s] \u001b[A\n",
      "batch 181, training loss: 3.646: : 181it [02:07,  1.21it/s]\u001b[A\n",
      "batch 182, training loss: 3.5606: : 181it [02:08,  1.21it/s]\u001b[A\n",
      "batch 182, training loss: 3.5606: : 182it [02:08,  1.21it/s]\u001b[A\n",
      "batch 183, training loss: 3.6766: : 182it [02:09,  1.21it/s]\u001b[A\n",
      "batch 183, training loss: 3.6766: : 183it [02:09,  1.21it/s]\u001b[A\n",
      "batch 184, training loss: 3.415: : 183it [02:10,  1.21it/s] \u001b[A\n",
      "batch 184, training loss: 3.415: : 184it [02:10,  1.21it/s]\u001b[A\n",
      "batch 185, training loss: 3.4727: : 184it [02:11,  1.21it/s]\u001b[A\n",
      "batch 185, training loss: 3.4727: : 185it [02:11,  1.20it/s]\u001b[A\n",
      "batch 186, training loss: 3.6261: : 185it [02:11,  1.20it/s]\u001b[A\n",
      "batch 186, training loss: 3.6261: : 186it [02:11,  1.20it/s]\u001b[A\n",
      "batch 187, training loss: 3.6297: : 186it [02:12,  1.20it/s]\u001b[A\n",
      "batch 187, training loss: 3.6297: : 187it [02:12,  1.20it/s]\u001b[A\n",
      "batch 188, training loss: 3.7065: : 187it [02:13,  1.20it/s]\u001b[A\n",
      "batch 188, training loss: 3.7065: : 188it [02:13,  1.22it/s]\u001b[A\n",
      "batch 189, training loss: 3.7427: : 188it [02:14,  1.22it/s]\u001b[A\n",
      "batch 189, training loss: 3.7427: : 189it [02:14,  1.21it/s]\u001b[A\n",
      "batch 190, training loss: 3.4444: : 189it [02:15,  1.21it/s]\u001b[A\n",
      "batch 190, training loss: 3.4444: : 190it [02:15,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.4424: : 190it [02:16,  1.22it/s]\u001b[A\n",
      "batch 191, training loss: 3.4424: : 191it [02:16,  1.21it/s]\u001b[A\n",
      "batch 192, training loss: 3.5319: : 191it [02:16,  1.21it/s]\u001b[A\n",
      "batch 192, training loss: 3.5319: : 192it [02:16,  1.21it/s]\u001b[A\n",
      "batch 193, training loss: 3.4366: : 192it [02:17,  1.21it/s]\u001b[A\n",
      "batch 193, training loss: 3.4366: : 193it [02:17,  1.22it/s]\u001b[A\n",
      "batch 194, training loss: 3.5399: : 193it [02:18,  1.22it/s]\u001b[A\n",
      "batch 194, training loss: 3.5399: : 194it [02:18,  1.20it/s]\u001b[A\n",
      "batch 195, training loss: 3.4718: : 194it [02:19,  1.20it/s]\u001b[A\n",
      "batch 195, training loss: 3.4718: : 195it [02:19,  1.21it/s]\u001b[A\n",
      "batch 196, training loss: 3.3815: : 195it [02:20,  1.21it/s]\u001b[A\n",
      "batch 196, training loss: 3.3815: : 196it [02:20,  1.22it/s]\u001b[A\n",
      "batch 197, training loss: 3.5987: : 196it [02:21,  1.22it/s]\u001b[A\n",
      "batch 197, training loss: 3.5987: : 197it [02:21,  1.22it/s]\u001b[A\n",
      "batch 198, training loss: 3.4977: : 197it [02:21,  1.22it/s]\u001b[A\n",
      "batch 198, training loss: 3.4977: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.4647: : 198it [02:22,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.4647: : 199it [02:22,  1.23it/s]\u001b[A\n",
      "batch 200, training loss: 3.621: : 199it [02:23,  1.23it/s] \u001b[A\n",
      "batch 200, training loss: 3.621: : 200it [02:23,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 3.4227: : 200it [02:24,  1.22it/s]\u001b[A\n",
      "batch 201, training loss: 3.4227: : 201it [02:24,  1.23it/s]\u001b[A\n",
      "batch 202, training loss: 3.316: : 201it [02:25,  1.23it/s] \u001b[A\n",
      "batch 202, training loss: 3.316: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.5226: : 202it [02:25,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.5226: : 203it [02:25,  1.23it/s]\u001b[A\n",
      "batch 204, training loss: 3.558: : 203it [02:26,  1.23it/s] \u001b[A\n",
      "batch 204, training loss: 3.558: : 204it [02:26,  1.22it/s]\u001b[A\n",
      "batch 205, training loss: 3.5081: : 204it [02:27,  1.22it/s]\u001b[A\n",
      "batch 205, training loss: 3.5081: : 205it [02:27,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 3.6288: : 205it [02:28,  1.21it/s]\u001b[A\n",
      "batch 206, training loss: 3.6288: : 206it [02:28,  1.21it/s]\u001b[A\n",
      "batch 207, training loss: 3.4273: : 206it [02:29,  1.21it/s]\u001b[A\n",
      "batch 207, training loss: 3.4273: : 207it [02:29,  1.23it/s]\u001b[A\n",
      "batch 208, training loss: 3.486: : 207it [02:29,  1.23it/s] \u001b[A\n",
      "batch 208, training loss: 3.486: : 208it [02:29,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 3.4665: : 208it [02:30,  1.22it/s]\u001b[A\n",
      "batch 209, training loss: 3.4665: : 209it [02:30,  1.22it/s]\u001b[A\n",
      "batch 210, training loss: 3.5208: : 209it [02:31,  1.22it/s]\u001b[A\n",
      "batch 210, training loss: 3.5208: : 210it [02:31,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.4618: : 210it [02:32,  1.24it/s]\u001b[A\n",
      "batch 211, training loss: 3.4618: : 211it [02:32,  1.22it/s]\u001b[A\n",
      "batch 212, training loss: 3.467: : 211it [02:33,  1.22it/s] \u001b[A\n",
      "batch 212, training loss: 3.467: : 212it [02:33,  1.22it/s]\u001b[A\n",
      "batch 213, training loss: 3.6678: : 212it [02:34,  1.22it/s]\u001b[A\n",
      "batch 213, training loss: 3.6678: : 213it [02:34,  1.22it/s]\u001b[A\n",
      "batch 214, training loss: 3.5282: : 213it [02:34,  1.22it/s]\u001b[A\n",
      "batch 214, training loss: 3.5282: : 214it [02:34,  1.22it/s]\u001b[A\n",
      "batch 215, training loss: 3.3838: : 214it [02:35,  1.22it/s]\u001b[A\n",
      "batch 215, training loss: 3.3838: : 215it [02:35,  1.25it/s]\u001b[A\n",
      "batch 216, training loss: 3.515: : 215it [02:36,  1.25it/s] \u001b[A\n",
      "batch 216, training loss: 3.515: : 216it [02:36,  1.26it/s]\u001b[A\n",
      "batch 217, training loss: 3.6283: : 216it [02:37,  1.26it/s]\u001b[A\n",
      "batch 217, training loss: 3.6283: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.5678: : 217it [02:38,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.5678: : 218it [02:38,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 3.7423: : 218it [02:38,  1.25it/s]\u001b[A\n",
      "batch 219, training loss: 3.7423: : 219it [02:38,  1.27it/s]\u001b[A\n",
      "batch 220, training loss: 3.67: : 219it [02:39,  1.27it/s]  \u001b[A\n",
      "batch 220, training loss: 3.67: : 220it [02:39,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.5314: : 220it [02:40,  1.24it/s]\u001b[A\n",
      "batch 221, training loss: 3.5314: : 221it [02:40,  1.26it/s]\u001b[A\n",
      "batch 222, training loss: 3.5566: : 221it [02:41,  1.26it/s]\u001b[A\n",
      "batch 222, training loss: 3.5566: : 222it [02:41,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.5924: : 222it [02:42,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.5924: : 223it [02:42,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.4455: : 223it [02:42,  1.24it/s]\u001b[A\n",
      "batch 224, training loss: 3.4455: : 224it [02:42,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 3.6128: : 224it [02:43,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 3.6128: : 225it [02:43,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.5862: : 225it [02:44,  1.24it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 226, training loss: 3.5862: : 226it [02:44,  1.26it/s]\u001b[A\n",
      "batch 227, training loss: 3.4754: : 226it [02:45,  1.26it/s]\u001b[A\n",
      "batch 227, training loss: 3.4754: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.5361: : 227it [02:46,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.5361: : 228it [02:46,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.5083: : 228it [02:46,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.5083: : 229it [02:46,  1.26it/s]\u001b[A\n",
      "batch 230, training loss: 3.6556: : 229it [02:47,  1.26it/s]\u001b[A\n",
      "batch 230, training loss: 3.6556: : 230it [02:47,  1.31it/s]\u001b[A\n",
      "batch 231, training loss: 3.6975: : 230it [02:48,  1.31it/s]\u001b[A\n",
      "batch 231, training loss: 3.6975: : 231it [02:48,  1.35it/s]\u001b[A\n",
      "batch 232, training loss: 3.5087: : 231it [02:48,  1.35it/s]\u001b[A\n",
      "batch 232, training loss: 3.5087: : 232it [02:48,  1.31it/s]\u001b[A\n",
      "batch 233, training loss: 3.5509: : 232it [02:49,  1.31it/s]\u001b[A\n",
      "batch 233, training loss: 3.5509: : 233it [02:49,  1.28it/s]\u001b[A\n",
      "batch 234, training loss: 3.7011: : 233it [02:50,  1.28it/s]\u001b[A\n",
      "batch 234, training loss: 3.7011: : 234it [02:50,  1.26it/s]\u001b[A\n",
      "batch 235, training loss: 3.5307: : 234it [02:51,  1.26it/s]\u001b[A\n",
      "batch 235, training loss: 3.5307: : 235it [02:51,  1.28it/s]\u001b[A\n",
      "batch 236, training loss: 3.5634: : 235it [02:52,  1.28it/s]\u001b[A\n",
      "batch 236, training loss: 3.5634: : 236it [02:52,  1.26it/s]\u001b[A\n",
      "batch 237, training loss: 3.4412: : 236it [02:53,  1.26it/s]\u001b[A\n",
      "batch 237, training loss: 3.4412: : 237it [02:53,  1.26it/s]\u001b[A\n",
      "batch 238, training loss: 3.6163: : 237it [02:53,  1.26it/s]\u001b[A\n",
      "batch 238, training loss: 3.6163: : 238it [02:53,  1.26it/s]\u001b[A\n",
      "batch 239, training loss: 3.4981: : 238it [02:54,  1.26it/s]\u001b[A\n",
      "batch 239, training loss: 3.4981: : 239it [02:54,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 3.5294: : 239it [02:55,  1.23it/s]\u001b[A\n",
      "batch 240, training loss: 3.5294: : 240it [02:55,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.5687: : 240it [02:56,  1.24it/s]\u001b[A\n",
      "batch 241, training loss: 3.5687: : 241it [02:56,  1.25it/s]\u001b[A\n",
      "batch 242, training loss: 3.4116: : 241it [02:57,  1.25it/s]\u001b[A\n",
      "batch 242, training loss: 3.4116: : 242it [02:57,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 3.5649: : 242it [02:57,  1.23it/s]\u001b[A\n",
      "batch 243, training loss: 3.5649: : 243it [02:57,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 3.4699: : 243it [02:58,  1.24it/s]\u001b[A\n",
      "batch 244, training loss: 3.4699: : 244it [02:58,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 3.6161: : 244it [02:59,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 3.6161: : 245it [02:59,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.5442: : 245it [03:00,  1.23it/s]\u001b[A\n",
      "batch 246, training loss: 3.5442: : 246it [03:00,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 3.53: : 246it [03:01,  1.24it/s]  \u001b[A\n",
      "batch 247, training loss: 3.53: : 247it [03:01,  1.24it/s]\u001b[A\n",
      "batch 248, training loss: 3.4601: : 247it [03:01,  1.24it/s]\u001b[A\n",
      "batch 248, training loss: 3.4601: : 248it [03:01,  1.23it/s]\u001b[A\n",
      "batch 249, training loss: 3.4255: : 248it [03:02,  1.23it/s]\u001b[A\n",
      "batch 249, training loss: 3.4255: : 249it [03:02,  1.25it/s]\u001b[A\n",
      "batch 250, training loss: 3.6076: : 249it [03:03,  1.25it/s]\u001b[A\n",
      "batch 250, training loss: 3.6076: : 250it [03:03,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 3.6013: : 250it [03:04,  1.25it/s]\u001b[A\n",
      "batch 251, training loss: 3.6013: : 251it [03:04,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 2.7658: : 251it [03:04,  1.23it/s]\u001b[A\n",
      "batch 252, training loss: 2.7658: : 252it [03:04,  1.48it/s]\u001b[A\n",
      "batch 253, training loss: 3.4649: : 252it [03:05,  1.48it/s]\u001b[A\n",
      "batch 253, training loss: 3.4649: : 253it [03:05,  1.33it/s]\u001b[A\n",
      "batch 254, training loss: 3.6008: : 253it [03:06,  1.33it/s]\u001b[A\n",
      "batch 254, training loss: 3.6008: : 254it [03:06,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.4054: : 254it [03:07,  1.25it/s]\u001b[A\n",
      "batch 255, training loss: 3.4054: : 255it [03:07,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 3.4504: : 255it [03:08,  1.19it/s]\u001b[A\n",
      "batch 256, training loss: 3.4504: : 256it [03:08,  1.15it/s]\u001b[A\n",
      "batch 257, training loss: 3.5352: : 256it [03:09,  1.15it/s]\u001b[A\n",
      "batch 257, training loss: 3.5352: : 257it [03:09,  1.15it/s]\u001b[A\n",
      "batch 258, training loss: 3.6267: : 257it [03:10,  1.15it/s]\u001b[A\n",
      "batch 258, training loss: 3.6267: : 258it [03:10,  1.17it/s]\u001b[A\n",
      "batch 259, training loss: 3.5195: : 258it [03:11,  1.17it/s]\u001b[A\n",
      "batch 259, training loss: 3.5195: : 259it [03:11,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 3.4928: : 259it [03:11,  1.14it/s]\u001b[A\n",
      "batch 260, training loss: 3.4928: : 260it [03:11,  1.11it/s]\u001b[A\n",
      "batch 261, training loss: 3.5643: : 260it [03:12,  1.11it/s]\u001b[A\n",
      "batch 261, training loss: 3.5643: : 261it [03:12,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.5006: : 261it [03:13,  1.12it/s]\u001b[A\n",
      "batch 262, training loss: 3.5006: : 262it [03:13,  1.12it/s]\u001b[A\n",
      "batch 263, training loss: 3.4838: : 262it [03:14,  1.12it/s]\u001b[A\n",
      "batch 263, training loss: 3.4838: : 263it [03:14,  1.12it/s]\u001b[A\n",
      "batch 264, training loss: 3.5957: : 263it [03:15,  1.12it/s]\u001b[A\n",
      "batch 264, training loss: 3.5957: : 264it [03:15,  1.15it/s]\u001b[A\n",
      "batch 265, training loss: 3.3955: : 264it [03:16,  1.15it/s]\u001b[A\n",
      "batch 265, training loss: 3.3955: : 265it [03:16,  1.13it/s]\u001b[A\n",
      "batch 266, training loss: 3.5837: : 265it [03:17,  1.13it/s]\u001b[A\n",
      "batch 266, training loss: 3.5837: : 266it [03:17,  1.12it/s]\u001b[A\n",
      "batch 267, training loss: 3.5702: : 266it [03:18,  1.12it/s]\u001b[A\n",
      "batch 267, training loss: 3.5702: : 267it [03:18,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 3.4346: : 267it [03:19,  1.10it/s]\u001b[A\n",
      "batch 268, training loss: 3.4346: : 268it [03:19,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.4728: : 268it [03:20,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.4728: : 269it [03:20,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.5005: : 269it [03:20,  1.10it/s]\u001b[A\n",
      "batch 270, training loss: 3.5005: : 270it [03:20,  1.12it/s]\u001b[A\n",
      "batch 271, training loss: 3.4852: : 270it [03:21,  1.12it/s]\u001b[A\n",
      "batch 271, training loss: 3.4852: : 271it [03:21,  1.11it/s]\u001b[A\n",
      "batch 272, training loss: 3.4831: : 271it [03:22,  1.11it/s]\u001b[A\n",
      "batch 272, training loss: 3.4831: : 272it [03:22,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 3.5791: : 272it [03:23,  1.09it/s]\u001b[A\n",
      "batch 273, training loss: 3.5791: : 273it [03:23,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 3.5702: : 273it [03:24,  1.10it/s]\u001b[A\n",
      "batch 274, training loss: 3.5702: : 274it [03:24,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.4657: : 274it [03:25,  1.10it/s]\u001b[A\n",
      "batch 275, training loss: 3.4657: : 275it [03:25,  1.10it/s]\u001b[A\n",
      "batch 276, training loss: 3.3553: : 275it [03:26,  1.10it/s]\u001b[A\n",
      "batch 276, training loss: 3.3553: : 276it [03:26,  1.10it/s]\u001b[A\n",
      "batch 277, training loss: 3.4972: : 276it [03:27,  1.10it/s]\u001b[A\n",
      "batch 277, training loss: 3.4972: : 277it [03:27,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.4452: : 277it [03:28,  1.09it/s]\u001b[A\n",
      "batch 278, training loss: 3.4452: : 278it [03:28,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 3.4409: : 278it [03:29,  1.09it/s]\u001b[A\n",
      "batch 279, training loss: 3.4409: : 279it [03:29,  1.11it/s]\u001b[A\n",
      "batch 280, training loss: 3.3384: : 279it [03:29,  1.11it/s]\u001b[A\n",
      "batch 280, training loss: 3.3384: : 280it [03:29,  1.13it/s]\u001b[A\n",
      "batch 281, training loss: 3.4576: : 280it [03:30,  1.13it/s]\u001b[A\n",
      "batch 281, training loss: 3.4576: : 281it [03:30,  1.12it/s]\u001b[A\n",
      "batch 282, training loss: 3.339: : 281it [03:31,  1.12it/s] \u001b[A\n",
      "batch 282, training loss: 3.339: : 282it [03:31,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.4517: : 282it [03:32,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.4517: : 283it [03:32,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.4513: : 283it [03:33,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.4513: : 284it [03:33,  1.13it/s]\u001b[A\n",
      "batch 285, training loss: 3.478: : 284it [03:34,  1.13it/s] \u001b[A\n",
      "batch 285, training loss: 3.478: : 285it [03:34,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 3.6529: : 285it [03:35,  1.10it/s]\u001b[A\n",
      "batch 286, training loss: 3.6529: : 286it [03:35,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.3559: : 286it [03:36,  1.10it/s]\u001b[A\n",
      "batch 287, training loss: 3.3559: : 287it [03:36,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.3933: : 287it [03:37,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.3933: : 288it [03:37,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.5444: : 288it [03:38,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.5444: : 289it [03:38,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.3206: : 289it [03:39,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 290, training loss: 3.3206: : 290it [03:39,  1.12it/s]\u001b[A\n",
      "batch 291, training loss: 3.5883: : 290it [03:39,  1.12it/s]\u001b[A\n",
      "batch 291, training loss: 3.5883: : 291it [03:39,  1.12it/s]\u001b[A\n",
      "batch 292, training loss: 3.4021: : 291it [03:40,  1.12it/s]\u001b[A\n",
      "batch 292, training loss: 3.4021: : 292it [03:40,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.5002: : 292it [03:41,  1.11it/s]\u001b[A\n",
      "batch 293, training loss: 3.5002: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.5782: : 293it [03:42,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.5782: : 294it [03:42,  1.12it/s]\u001b[A\n",
      "batch 295, training loss: 3.5256: : 294it [03:43,  1.12it/s]\u001b[A\n",
      "batch 295, training loss: 3.5256: : 295it [03:43,  1.11it/s]\u001b[A\n",
      "batch 296, training loss: 3.3897: : 295it [03:44,  1.11it/s]\u001b[A\n",
      "batch 296, training loss: 3.3897: : 296it [03:44,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 3.5351: : 296it [03:45,  1.10it/s]\u001b[A\n",
      "batch 297, training loss: 3.5351: : 297it [03:45,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.4567: : 297it [03:46,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.4567: : 298it [03:46,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 3.5302: : 298it [03:47,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 3.5302: : 299it [03:47,  1.08it/s]\u001b[A\n",
      "batch 300, training loss: 3.5855: : 299it [03:48,  1.08it/s]\u001b[A\n",
      "batch 300, training loss: 3.5855: : 300it [03:48,  1.08it/s]\u001b[A\n",
      "batch 301, training loss: 3.5325: : 300it [03:49,  1.08it/s]\u001b[A\n",
      "batch 301, training loss: 3.5325: : 301it [03:49,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.5421: : 301it [03:50,  1.09it/s]\u001b[A\n",
      "batch 302, training loss: 3.5421: : 302it [03:50,  1.07it/s]\u001b[A\n",
      "batch 303, training loss: 3.4426: : 302it [03:50,  1.07it/s]\u001b[A\n",
      "batch 303, training loss: 3.4426: : 303it [03:50,  1.08it/s]\u001b[A\n",
      "batch 304, training loss: 3.5437: : 303it [03:51,  1.08it/s]\u001b[A\n",
      "batch 304, training loss: 3.5437: : 304it [03:51,  1.08it/s]\u001b[A\n",
      "batch 305, training loss: 3.4541: : 304it [03:52,  1.08it/s]\u001b[A\n",
      "batch 305, training loss: 3.4541: : 305it [03:52,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.5254: : 305it [03:53,  1.09it/s]\u001b[A\n",
      "batch 306, training loss: 3.5254: : 306it [03:53,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 3.6638: : 306it [03:54,  1.09it/s]\u001b[A\n",
      "batch 307, training loss: 3.6638: : 307it [03:54,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.272: : 307it [03:55,  1.09it/s] \u001b[A\n",
      "batch 308, training loss: 3.272: : 308it [03:55,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.6704: : 308it [03:56,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.6704: : 309it [03:56,  1.12it/s]\u001b[A\n",
      "batch 310, training loss: 3.4329: : 309it [03:57,  1.12it/s]\u001b[A\n",
      "batch 310, training loss: 3.4329: : 310it [03:57,  1.10it/s]\u001b[A\n",
      "batch 311, training loss: 3.5397: : 310it [03:58,  1.10it/s]\u001b[A\n",
      "batch 311, training loss: 3.5397: : 311it [03:58,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 3.3725: : 311it [03:59,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 3.3725: : 312it [03:59,  1.10it/s]\u001b[A\n",
      "batch 313, training loss: 3.4632: : 312it [04:00,  1.10it/s]\u001b[A\n",
      "batch 313, training loss: 3.4632: : 313it [04:00,  1.12it/s]\u001b[A\n",
      "batch 314, training loss: 3.4709: : 313it [04:00,  1.12it/s]\u001b[A\n",
      "batch 314, training loss: 3.4709: : 314it [04:00,  1.12it/s]\u001b[A\n",
      "batch 315, training loss: 3.5461: : 314it [04:01,  1.12it/s]\u001b[A\n",
      "batch 315, training loss: 3.5461: : 315it [04:01,  1.11it/s]\u001b[A\n",
      "batch 316, training loss: 3.686: : 315it [04:02,  1.11it/s] \u001b[A\n",
      "batch 316, training loss: 3.686: : 316it [04:02,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.3627: : 316it [04:03,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.3627: : 317it [04:03,  1.21it/s]\u001b[A\n",
      "batch 318, training loss: 3.5414: : 317it [04:04,  1.21it/s]\u001b[A\n",
      "batch 318, training loss: 3.5414: : 318it [04:04,  1.12it/s]\u001b[A\n",
      "batch 319, training loss: 3.6159: : 318it [04:05,  1.12it/s]\u001b[A\n",
      "batch 319, training loss: 3.6159: : 319it [04:05,  1.08it/s]\u001b[A\n",
      "batch 320, training loss: 3.6066: : 319it [04:06,  1.08it/s]\u001b[A\n",
      "batch 320, training loss: 3.6066: : 320it [04:06,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 3.6791: : 320it [04:07,  1.06it/s]\u001b[A\n",
      "batch 321, training loss: 3.6791: : 321it [04:07,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 3.7115: : 321it [04:08,  1.03it/s]\u001b[A\n",
      "batch 322, training loss: 3.7115: : 322it [04:08,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 3.4816: : 322it [04:09,  1.03it/s]\u001b[A\n",
      "batch 323, training loss: 3.4816: : 323it [04:09,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 3.5509: : 323it [04:10,  1.02it/s]\u001b[A\n",
      "batch 324, training loss: 3.5509: : 324it [04:10,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 3.6407: : 324it [04:11,  1.00it/s]\u001b[A\n",
      "batch 325, training loss: 3.6407: : 325it [04:11,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 3.5445: : 325it [04:12,  1.01it/s]\u001b[A\n",
      "batch 326, training loss: 3.5445: : 326it [04:12,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 3.6794: : 326it [04:13,  1.00s/it]\u001b[A\n",
      "batch 327, training loss: 3.6794: : 327it [04:13,  1.02s/it]\u001b[A\n",
      "batch 328, training loss: 3.5474: : 327it [04:14,  1.02s/it]\u001b[A\n",
      "batch 328, training loss: 3.5474: : 328it [04:14,  1.02s/it]\u001b[A\n",
      "batch 329, training loss: 3.4589: : 328it [04:15,  1.02s/it]\u001b[A\n",
      "batch 329, training loss: 3.4589: : 329it [04:15,  1.02s/it]\u001b[A\n",
      "batch 330, training loss: 3.5655: : 329it [04:16,  1.02s/it]\u001b[A\n",
      "batch 330, training loss: 3.5655: : 330it [04:16,  1.02s/it]\u001b[A\n",
      "batch 331, training loss: 3.5937: : 330it [04:17,  1.02s/it]\u001b[A\n",
      "batch 331, training loss: 3.5937: : 331it [04:17,  1.02it/s]\u001b[A\n",
      "batch 332, training loss: 3.4948: : 331it [04:18,  1.02it/s]\u001b[A\n",
      "batch 332, training loss: 3.4948: : 332it [04:18,  1.06it/s]\u001b[A\n",
      "batch 333, training loss: 3.4674: : 332it [04:19,  1.06it/s]\u001b[A\n",
      "batch 333, training loss: 3.4674: : 333it [04:19,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 3.5381: : 333it [04:20,  1.02it/s]\u001b[A\n",
      "batch 334, training loss: 3.5381: : 334it [04:20,  1.02it/s]\u001b[A\n",
      "batch 335, training loss: 3.7088: : 334it [04:21,  1.02it/s]\u001b[A\n",
      "batch 335, training loss: 3.7088: : 335it [04:21,  1.01it/s]\u001b[A\n",
      "batch 336, training loss: 3.5818: : 335it [04:22,  1.01it/s]\u001b[A\n",
      "batch 336, training loss: 3.5818: : 336it [04:22,  1.01it/s]\u001b[A\n",
      "batch 337, training loss: 3.6461: : 336it [04:23,  1.01it/s]\u001b[A\n",
      "batch 337, training loss: 3.6461: : 337it [04:23,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 3.4866: : 337it [04:24,  1.01s/it]\u001b[A\n",
      "batch 338, training loss: 3.4866: : 338it [04:24,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 3.6009: : 338it [04:25,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 3.6009: : 339it [04:25,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 3.8142: : 339it [04:26,  1.02it/s]\u001b[A\n",
      "batch 340, training loss: 3.8142: : 340it [04:26,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 3.4928: : 340it [04:27,  1.01it/s]\u001b[A\n",
      "batch 341, training loss: 3.4928: : 341it [04:27,  1.00s/it]\u001b[A\n",
      "batch 342, training loss: 3.587: : 341it [04:28,  1.00s/it] \u001b[A\n",
      "batch 342, training loss: 3.587: : 342it [04:28,  1.00s/it]\u001b[A\n",
      "batch 343, training loss: 3.5648: : 342it [04:29,  1.00s/it]\u001b[A\n",
      "batch 343, training loss: 3.5648: : 343it [04:29,  1.00it/s]\u001b[A\n",
      "batch 344, training loss: 3.6919: : 343it [04:30,  1.00it/s]\u001b[A\n",
      "batch 344, training loss: 3.6919: : 344it [04:30,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.6547: : 344it [04:31,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.6547: : 345it [04:31,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 3.5939: : 345it [04:32,  1.01s/it]\u001b[A\n",
      "batch 346, training loss: 3.5939: : 346it [04:32,  1.02s/it]\u001b[A\n",
      "batch 347, training loss: 3.5187: : 346it [04:33,  1.02s/it]\u001b[A\n",
      "batch 347, training loss: 3.5187: : 347it [04:33,  1.02it/s]\u001b[A\n",
      "batch 348, training loss: 3.5964: : 347it [04:34,  1.02it/s]\u001b[A\n",
      "batch 348, training loss: 3.5964: : 348it [04:34,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 3.7429: : 348it [04:35,  1.00it/s]\u001b[A\n",
      "batch 349, training loss: 3.7429: : 349it [04:35,  1.04it/s]\u001b[A\n",
      "batch 350, training loss: 3.5856: : 349it [04:36,  1.04it/s]\u001b[A\n",
      "batch 350, training loss: 3.5856: : 350it [04:36,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.5012: : 350it [04:37,  1.01it/s]\u001b[A\n",
      "batch 351, training loss: 3.5012: : 351it [04:37,  1.00it/s]\u001b[A\n",
      "batch 352, training loss: 3.5628: : 351it [04:38,  1.00it/s]\u001b[A\n",
      "batch 352, training loss: 3.5628: : 352it [04:38,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 3.5983: : 352it [04:39,  1.01it/s]\u001b[A\n",
      "batch 353, training loss: 3.5983: : 353it [04:39,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 3.6601: : 353it [04:40,  1.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 354, training loss: 3.6601: : 354it [04:40,  1.00it/s]\u001b[A\n",
      "batch 355, training loss: 3.6122: : 354it [04:41,  1.00it/s]\u001b[A\n",
      "batch 355, training loss: 3.6122: : 355it [04:41,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.4639: : 355it [04:42,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.4639: : 356it [04:42,  1.02s/it]\u001b[A\n",
      "batch 357, training loss: 3.4907: : 356it [04:43,  1.02s/it]\u001b[A\n",
      "batch 357, training loss: 3.4907: : 357it [04:43,  1.01it/s]\u001b[A\n",
      "batch 358, training loss: 3.6743: : 357it [04:44,  1.01it/s]\u001b[A\n",
      "batch 358, training loss: 3.6743: : 358it [04:44,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 3.4808: : 358it [04:45,  1.00s/it]\u001b[A\n",
      "batch 359, training loss: 3.4808: : 359it [04:45,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 3.6029: : 359it [04:46,  1.01s/it]\u001b[A\n",
      "batch 360, training loss: 3.6029: : 360it [04:46,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.5811: : 360it [04:47,  1.02s/it]\u001b[A\n",
      "batch 361, training loss: 3.5811: : 361it [04:47,  1.02s/it]\u001b[A\n",
      "batch 362, training loss: 3.675: : 361it [04:48,  1.02s/it] \u001b[A\n",
      "batch 362, training loss: 3.675: : 362it [04:48,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.5939: : 362it [04:49,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.5939: : 363it [04:49,  1.02it/s]\u001b[A\n",
      "batch 364, training loss: 3.4919: : 363it [04:50,  1.02it/s]\u001b[A\n",
      "batch 364, training loss: 3.4919: : 364it [04:50,  1.01it/s]\u001b[A\n",
      "batch 365, training loss: 3.4643: : 364it [04:51,  1.01it/s]\u001b[A\n",
      "batch 365, training loss: 3.4643: : 365it [04:51,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 3.6063: : 365it [04:52,  1.00it/s]\u001b[A\n",
      "batch 366, training loss: 3.6063: : 366it [04:52,  1.00s/it]\u001b[A\n",
      "batch 367, training loss: 3.5313: : 366it [04:53,  1.00s/it]\u001b[A\n",
      "batch 367, training loss: 3.5313: : 367it [04:53,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.6119: : 367it [04:54,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.6119: : 368it [04:54,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.6294: : 368it [04:55,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.6294: : 369it [04:55,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.5414: : 369it [04:56,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.5414: : 370it [04:56,  1.01s/it]\u001b[A\n",
      "batch 371, training loss: 3.5618: : 370it [04:57,  1.01s/it]\u001b[A\n",
      "batch 371, training loss: 3.5618: : 371it [04:57,  1.02it/s]\u001b[A\n",
      "batch 372, training loss: 3.522: : 371it [04:58,  1.02it/s] \u001b[A\n",
      "batch 372, training loss: 3.522: : 372it [04:58,  1.01it/s]\u001b[A\n",
      "batch 373, training loss: 3.5491: : 372it [04:59,  1.01it/s]\u001b[A\n",
      "batch 373, training loss: 3.5491: : 373it [04:59,  1.01s/it]\u001b[A\n",
      "batch 374, training loss: 3.5561: : 373it [05:00,  1.01s/it]\u001b[A\n",
      "batch 374, training loss: 3.5561: : 374it [05:00,  1.02s/it]\u001b[A\n",
      "batch 375, training loss: 3.2994: : 374it [05:00,  1.02s/it]\u001b[A\n",
      "batch 375, training loss: 3.2994: : 375it [05:00,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 3.5373: : 375it [05:02,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 3.5373: : 376it [05:02,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 3.6376: : 376it [05:03,  1.07it/s]\u001b[A\n",
      "batch 377, training loss: 3.6376: : 377it [05:03,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 3.548: : 377it [05:04,  1.01s/it] \u001b[A\n",
      "batch 378, training loss: 3.548: : 378it [05:04,  1.05s/it]\u001b[A\n",
      "batch 379, training loss: 3.5025: : 378it [05:05,  1.05s/it]\u001b[A\n",
      "batch 379, training loss: 3.5025: : 379it [05:05,  1.07s/it]\u001b[A\n",
      "batch 380, training loss: 3.4882: : 379it [05:06,  1.07s/it]\u001b[A\n",
      "batch 380, training loss: 3.4882: : 380it [05:06,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 3.5968: : 380it [05:07,  1.07s/it]\u001b[A\n",
      "batch 381, training loss: 3.5968: : 381it [05:07,  1.06s/it]\u001b[A\n",
      "batch 382, training loss: 3.4518: : 381it [05:08,  1.06s/it]\u001b[A\n",
      "batch 382, training loss: 3.4518: : 382it [05:08,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.5074: : 382it [05:09,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.5074: : 383it [05:09,  1.09s/it]\u001b[A\n",
      "batch 384, training loss: 3.5896: : 383it [05:10,  1.09s/it]\u001b[A\n",
      "batch 384, training loss: 3.5896: : 384it [05:10,  1.06s/it]\u001b[A\n",
      "batch 385, training loss: 3.5174: : 384it [05:12,  1.06s/it]\u001b[A\n",
      "batch 385, training loss: 3.5174: : 385it [05:12,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.4896: : 385it [05:13,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.4896: : 386it [05:13,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.5414: : 386it [05:14,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.5414: : 387it [05:14,  1.12s/it]\u001b[A\n",
      "batch 388, training loss: 3.3571: : 387it [05:15,  1.12s/it]\u001b[A\n",
      "batch 388, training loss: 3.3571: : 388it [05:15,  1.12s/it]\u001b[A\n",
      "batch 389, training loss: 3.5108: : 388it [05:16,  1.12s/it]\u001b[A\n",
      "batch 389, training loss: 3.5108: : 389it [05:16,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 3.595: : 389it [05:17,  1.12s/it] \u001b[A\n",
      "batch 390, training loss: 3.595: : 390it [05:17,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 3.6354: : 390it [05:18,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 3.6354: : 391it [05:18,  1.10s/it]\u001b[A\n",
      "batch 392, training loss: 3.574: : 391it [05:19,  1.10s/it] \u001b[A\n",
      "batch 392, training loss: 3.574: : 392it [05:19,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.3902: : 392it [05:20,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.3902: : 393it [05:20,  1.06s/it]\u001b[A\n",
      "batch 394, training loss: 3.3653: : 393it [05:21,  1.06s/it]\u001b[A\n",
      "batch 394, training loss: 3.3653: : 394it [05:21,  1.07s/it]\u001b[A\n",
      "batch 395, training loss: 3.4691: : 394it [05:23,  1.07s/it]\u001b[A\n",
      "batch 395, training loss: 3.4691: : 395it [05:23,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 3.6261: : 395it [05:24,  1.08s/it]\u001b[A\n",
      "batch 396, training loss: 3.6261: : 396it [05:24,  1.09s/it]\u001b[A\n",
      "batch 397, training loss: 3.4438: : 396it [05:25,  1.09s/it]\u001b[A\n",
      "batch 397, training loss: 3.4438: : 397it [05:25,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 3.5569: : 397it [05:26,  1.11s/it]\u001b[A\n",
      "batch 398, training loss: 3.5569: : 398it [05:26,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 3.6586: : 398it [05:27,  1.10s/it]\u001b[A\n",
      "batch 399, training loss: 3.6586: : 399it [05:27,  1.08s/it]\u001b[A\n",
      "batch 400, training loss: 3.4402: : 399it [05:28,  1.08s/it]\u001b[A\n",
      "batch 400, training loss: 3.4402: : 400it [05:28,  1.10s/it]\u001b[A\n",
      "batch 401, training loss: 3.4007: : 400it [05:29,  1.10s/it]\u001b[A\n",
      "batch 401, training loss: 3.4007: : 401it [05:29,  1.10s/it]\u001b[A\n",
      "batch 402, training loss: 3.4764: : 401it [05:30,  1.10s/it]\u001b[A\n",
      "batch 402, training loss: 3.4764: : 402it [05:30,  1.10s/it]\u001b[A\n",
      "batch 403, training loss: 3.6322: : 402it [05:31,  1.10s/it]\u001b[A\n",
      "batch 403, training loss: 3.6322: : 403it [05:31,  1.11s/it]\u001b[A\n",
      "batch 404, training loss: 3.263: : 403it [05:32,  1.11s/it] \u001b[A\n",
      "batch 404, training loss: 3.263: : 404it [05:32,  1.09s/it]\u001b[A\n",
      "batch 405, training loss: 3.5554: : 404it [05:34,  1.09s/it]\u001b[A\n",
      "batch 405, training loss: 3.5554: : 405it [05:34,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 3.4564: : 405it [05:35,  1.10s/it]\u001b[A\n",
      "batch 406, training loss: 3.4564: : 406it [05:35,  1.11s/it]\u001b[A\n",
      "batch 407, training loss: 3.537: : 406it [05:36,  1.11s/it] \u001b[A\n",
      "batch 407, training loss: 3.537: : 407it [05:36,  1.10s/it]\u001b[A\n",
      "batch 408, training loss: 3.3917: : 407it [05:37,  1.10s/it]\u001b[A\n",
      "batch 408, training loss: 3.3917: : 408it [05:37,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 3.5627: : 408it [05:38,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 3.5627: : 409it [05:38,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.4189: : 409it [05:39,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.4189: : 410it [05:39,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.5377: : 410it [05:40,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.5377: : 411it [05:40,  1.12s/it]\u001b[A\n",
      "batch 412, training loss: 3.5338: : 411it [05:41,  1.12s/it]\u001b[A\n",
      "batch 412, training loss: 3.5338: : 412it [05:41,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 3.4859: : 412it [05:42,  1.12s/it]\u001b[A\n",
      "batch 413, training loss: 3.4859: : 413it [05:42,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 3.3991: : 413it [05:44,  1.12s/it]\u001b[A\n",
      "batch 414, training loss: 3.3991: : 414it [05:44,  1.09s/it]\u001b[A\n",
      "batch 415, training loss: 3.3037: : 414it [05:45,  1.09s/it]\u001b[A\n",
      "batch 415, training loss: 3.3037: : 415it [05:45,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.3398: : 415it [05:46,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.3398: : 416it [05:46,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.6014: : 416it [05:47,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.6014: : 417it [05:47,  1.09s/it]\u001b[A\n",
      "batch 418, training loss: 3.4932: : 417it [05:48,  1.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 418, training loss: 3.4932: : 418it [05:48,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.3025: : 418it [05:49,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.3025: : 419it [05:49,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.4246: : 419it [05:50,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.4246: : 420it [05:50,  1.00s/it]\u001b[A\n",
      "batch 421, training loss: 3.5256: : 420it [05:51,  1.00s/it]\u001b[A\n",
      "batch 421, training loss: 3.5256: : 421it [05:51,  1.03s/it]\u001b[A\n",
      "batch 422, training loss: 3.5428: : 421it [05:52,  1.03s/it]\u001b[A\n",
      "batch 422, training loss: 3.5428: : 422it [05:52,  1.03s/it]\u001b[A\n",
      "batch 423, training loss: 3.3439: : 422it [05:53,  1.03s/it]\u001b[A\n",
      "batch 423, training loss: 3.3439: : 423it [05:53,  1.06s/it]\u001b[A\n",
      "batch 424, training loss: 3.517: : 423it [05:54,  1.06s/it] \u001b[A\n",
      "batch 424, training loss: 3.517: : 424it [05:54,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.5509: : 424it [05:55,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.5509: : 425it [05:55,  1.13s/it]\u001b[A\n",
      "batch 426, training loss: 3.505: : 425it [05:57,  1.13s/it] \u001b[A\n",
      "batch 426, training loss: 3.505: : 426it [05:57,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.5297: : 426it [05:58,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.5297: : 427it [05:58,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 3.7064: : 427it [05:59,  1.17s/it]\u001b[A\n",
      "batch 428, training loss: 3.7064: : 428it [05:59,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.5383: : 428it [06:00,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.5383: : 429it [06:00,  1.18s/it]\u001b[A\n",
      "batch 430, training loss: 3.5709: : 429it [06:02,  1.18s/it]\u001b[A\n",
      "batch 430, training loss: 3.5709: : 430it [06:02,  1.19s/it]\u001b[A\n",
      "batch 431, training loss: 3.6161: : 430it [06:03,  1.19s/it]\u001b[A\n",
      "batch 431, training loss: 3.6161: : 431it [06:03,  1.19s/it]\u001b[A\n",
      "batch 432, training loss: 3.4613: : 431it [06:04,  1.19s/it]\u001b[A\n",
      "batch 432, training loss: 3.4613: : 432it [06:04,  1.20s/it]\u001b[A\n",
      "batch 433, training loss: 3.499: : 432it [06:05,  1.20s/it] \u001b[A\n",
      "batch 433, training loss: 3.499: : 433it [06:05,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 3.5026: : 433it [06:06,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 3.5026: : 434it [06:06,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.5022: : 434it [06:08,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.5022: : 435it [06:08,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.5493: : 435it [06:08,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.5493: : 436it [06:08,  1.11s/it]\u001b[A\n",
      "batch 437, training loss: 3.4808: : 436it [06:09,  1.11s/it]\u001b[A\n",
      "batch 437, training loss: 3.4808: : 437it [06:09,  1.01s/it]\u001b[A\n",
      "batch 438, training loss: 3.514: : 437it [06:10,  1.01s/it] \u001b[A\n",
      "batch 438, training loss: 3.514: : 438it [06:10,  1.07s/it]\u001b[A\n",
      "batch 439, training loss: 3.6275: : 438it [06:12,  1.07s/it]\u001b[A\n",
      "batch 439, training loss: 3.6275: : 439it [06:12,  1.10s/it]\u001b[A\n",
      "batch 440, training loss: 3.463: : 439it [06:13,  1.10s/it] \u001b[A\n",
      "batch 440, training loss: 3.463: : 440it [06:13,  1.13s/it]\u001b[A\n",
      "batch 441, training loss: 3.7464: : 440it [06:14,  1.13s/it]\u001b[A\n",
      "batch 441, training loss: 3.7464: : 441it [06:14,  1.15s/it]\u001b[A\n",
      "batch 442, training loss: 3.4219: : 441it [06:15,  1.15s/it]\u001b[A\n",
      "batch 442, training loss: 3.4219: : 442it [06:15,  1.16s/it]\u001b[A\n",
      "batch 443, training loss: 3.5118: : 442it [06:16,  1.16s/it]\u001b[A\n",
      "batch 443, training loss: 3.5118: : 443it [06:16,  1.18s/it]\u001b[A\n",
      "batch 444, training loss: 3.5164: : 443it [06:18,  1.18s/it]\u001b[A\n",
      "batch 444, training loss: 3.5164: : 444it [06:18,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 3.5585: : 444it [06:19,  1.20s/it]\u001b[A\n",
      "batch 445, training loss: 3.5585: : 445it [06:19,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 3.5426: : 445it [06:20,  1.21s/it]\u001b[A\n",
      "batch 446, training loss: 3.5426: : 446it [06:20,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.5139: : 446it [06:21,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.5139: : 447it [06:21,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.4186: : 447it [06:23,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.4186: : 448it [06:23,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 3.5169: : 448it [06:24,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 3.5169: : 449it [06:24,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 3.5775: : 449it [06:25,  1.22s/it]\u001b[A\n",
      "batch 450, training loss: 3.5775: : 450it [06:25,  1.22s/it]\u001b[A\n",
      "batch 451, training loss: 3.5869: : 450it [06:26,  1.22s/it]\u001b[A\n",
      "batch 451, training loss: 3.5869: : 451it [06:26,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 3.5975: : 451it [06:27,  1.22s/it]\u001b[A\n",
      "batch 452, training loss: 3.5975: : 452it [06:27,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.6635: : 452it [06:29,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.6635: : 453it [06:29,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.5249: : 453it [06:30,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.5249: : 454it [06:30,  1.18s/it]\u001b[A\n",
      "batch 455, training loss: 3.4463: : 454it [06:31,  1.18s/it]\u001b[A\n",
      "batch 455, training loss: 3.4463: : 455it [06:31,  1.16s/it]\u001b[A\n",
      "batch 456, training loss: 3.463: : 455it [06:32,  1.16s/it] \u001b[A\n",
      "batch 456, training loss: 3.463: : 456it [06:32,  1.18s/it]\u001b[A\n",
      "batch 457, training loss: 3.5833: : 456it [06:33,  1.18s/it]\u001b[A\n",
      "batch 457, training loss: 3.5833: : 457it [06:33,  1.19s/it]\u001b[A\n",
      "batch 458, training loss: 3.4604: : 457it [06:35,  1.19s/it]\u001b[A\n",
      "batch 458, training loss: 3.4604: : 458it [06:35,  1.19s/it]\u001b[A\n",
      "batch 459, training loss: 3.533: : 458it [06:36,  1.19s/it] \u001b[A\n",
      "batch 459, training loss: 3.533: : 459it [06:36,  1.20s/it]\u001b[A\n",
      "batch 460, training loss: 3.4971: : 459it [06:37,  1.20s/it]\u001b[A\n",
      "batch 460, training loss: 3.4971: : 460it [06:37,  1.22s/it]\u001b[A\n",
      "batch 461, training loss: 3.5954: : 460it [06:38,  1.22s/it]\u001b[A\n",
      "batch 461, training loss: 3.5954: : 461it [06:38,  1.21s/it]\u001b[A\n",
      "batch 462, training loss: 3.5561: : 461it [06:39,  1.21s/it]\u001b[A\n",
      "batch 462, training loss: 3.5561: : 462it [06:39,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.394: : 462it [06:41,  1.23s/it] \u001b[A\n",
      "batch 463, training loss: 3.394: : 463it [06:41,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 3.5416: : 463it [06:42,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 3.5416: : 464it [06:42,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.4902: : 464it [06:43,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.4902: : 465it [06:43,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.3641: : 465it [06:44,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.3641: : 466it [06:44,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.3068: : 466it [06:46,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.3068: : 467it [06:46,  1.23s/it]\u001b[A\n",
      "batch 468, training loss: 3.5006: : 467it [06:47,  1.23s/it]\u001b[A\n",
      "batch 468, training loss: 3.5006: : 468it [06:47,  1.25s/it]\u001b[A\n",
      "batch 469, training loss: 3.4005: : 468it [06:48,  1.25s/it]\u001b[A\n",
      "batch 469, training loss: 3.4005: : 469it [06:48,  1.25s/it]\u001b[A\n",
      "batch 470, training loss: 3.5432: : 469it [06:49,  1.25s/it]\u001b[A\n",
      "batch 470, training loss: 3.5432: : 470it [06:49,  1.26s/it]\u001b[A\n",
      "batch 471, training loss: 3.5048: : 470it [06:51,  1.26s/it]\u001b[A\n",
      "batch 471, training loss: 3.5048: : 471it [06:51,  1.25s/it]\u001b[A\n",
      "batch 472, training loss: 3.4551: : 471it [06:52,  1.25s/it]\u001b[A\n",
      "batch 472, training loss: 3.4551: : 472it [06:52,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.4508: : 472it [06:53,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.4508: : 473it [06:53,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 3.4959: : 473it [06:54,  1.25s/it]\u001b[A\n",
      "batch 474, training loss: 3.4959: : 474it [06:54,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 3.4389: : 474it [06:56,  1.26s/it]\u001b[A\n",
      "batch 475, training loss: 3.4389: : 475it [06:56,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 3.5204: : 475it [06:57,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 3.5204: : 476it [06:57,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.3356: : 476it [06:58,  1.26s/it]\u001b[A\n",
      "batch 477, training loss: 3.3356: : 477it [06:58,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.406: : 477it [06:59,  1.26s/it] \u001b[A\n",
      "batch 478, training loss: 3.406: : 478it [06:59,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.3841: : 478it [07:01,  1.26s/it]\u001b[A\n",
      "batch 479, training loss: 3.3841: : 479it [07:01,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.4316: : 479it [07:02,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.4316: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.4247: : 480it [07:03,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.4247: : 481it [07:03,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.4417: : 481it [07:04,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 482, training loss: 3.4417: : 482it [07:04,  1.25s/it]\u001b[A\n",
      "batch 483, training loss: 3.3717: : 482it [07:06,  1.25s/it]\u001b[A\n",
      "batch 483, training loss: 3.3717: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.4548: : 483it [07:07,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.4548: : 484it [07:07,  1.28s/it]\u001b[A\n",
      "batch 485, training loss: 3.3918: : 484it [07:08,  1.28s/it]\u001b[A\n",
      "batch 485, training loss: 3.3918: : 485it [07:08,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.4809: : 485it [07:10,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.4809: : 486it [07:10,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.5757: : 486it [07:11,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.5757: : 487it [07:11,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.3937: : 487it [07:12,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.3937: : 488it [07:12,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 3.31: : 488it [07:13,  1.25s/it]  \u001b[A\n",
      "batch 489, training loss: 3.31: : 489it [07:13,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.3855: : 489it [07:15,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.3855: : 490it [07:15,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.4442: : 490it [07:16,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.4442: : 491it [07:16,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.3281: : 491it [07:17,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.3281: : 492it [07:17,  1.25s/it]\u001b[A\n",
      "batch 493, training loss: 3.5167: : 492it [07:18,  1.25s/it]\u001b[A\n",
      "batch 493, training loss: 3.5167: : 493it [07:18,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.5331: : 493it [07:20,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.5331: : 494it [07:20,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.3588: : 494it [07:21,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.3588: : 495it [07:21,  1.27s/it]\u001b[A\n",
      "batch 496, training loss: 3.347: : 495it [07:22,  1.27s/it] \u001b[A\n",
      "batch 496, training loss: 3.347: : 496it [07:22,  1.29s/it]\u001b[A\n",
      "batch 497, training loss: 3.2723: : 496it [07:23,  1.29s/it]\u001b[A\n",
      "batch 497, training loss: 3.2723: : 497it [07:23,  1.25s/it]\u001b[A\n",
      "batch 498, training loss: 3.3711: : 497it [07:24,  1.25s/it]\u001b[A\n",
      "batch 498, training loss: 3.3711: : 498it [07:24,  1.20s/it]\u001b[A\n",
      "batch 499, training loss: 3.3311: : 498it [07:25,  1.20s/it]\u001b[A\n",
      "batch 499, training loss: 3.3311: : 499it [07:25,  1.09s/it]\u001b[A\n",
      "batch 500, training loss: 3.6054: : 499it [07:27,  1.09s/it]\u001b[A\n",
      "batch 500, training loss: 3.6054: : 500it [07:27,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 3.4509: : 500it [07:28,  1.18s/it]\u001b[A\n",
      "batch 501, training loss: 3.4509: : 501it [07:28,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.4171: : 501it [07:29,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.4171: : 502it [07:29,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 3.5052: : 502it [07:31,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 3.5052: : 503it [07:31,  1.33s/it]\u001b[A\n",
      "batch 504, training loss: 3.4844: : 503it [07:32,  1.33s/it]\u001b[A\n",
      "batch 504, training loss: 3.4844: : 504it [07:32,  1.30s/it]\u001b[A\n",
      "batch 505, training loss: 3.568: : 504it [07:34,  1.30s/it] \u001b[A\n",
      "batch 505, training loss: 3.568: : 505it [07:34,  1.33s/it]\u001b[A\n",
      "batch 506, training loss: 3.4705: : 505it [07:35,  1.33s/it]\u001b[A\n",
      "batch 506, training loss: 3.4705: : 506it [07:35,  1.36s/it]\u001b[A\n",
      "batch 507, training loss: 3.3749: : 506it [07:36,  1.36s/it]\u001b[A\n",
      "batch 507, training loss: 3.3749: : 507it [07:36,  1.35s/it]\u001b[A\n",
      "batch 508, training loss: 3.5215: : 507it [07:38,  1.35s/it]\u001b[A\n",
      "batch 508, training loss: 3.5215: : 508it [07:38,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.3941: : 508it [07:39,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.3941: : 509it [07:39,  1.36s/it]\u001b[A\n",
      "batch 510, training loss: 3.465: : 509it [07:40,  1.36s/it] \u001b[A\n",
      "batch 510, training loss: 3.465: : 510it [07:40,  1.37s/it]\u001b[A\n",
      "batch 511, training loss: 3.481: : 510it [07:42,  1.37s/it]\u001b[A\n",
      "batch 511, training loss: 3.481: : 511it [07:42,  1.38s/it]\u001b[A\n",
      "batch 512, training loss: 3.4426: : 511it [07:43,  1.38s/it]\u001b[A\n",
      "batch 512, training loss: 3.4426: : 512it [07:43,  1.34s/it]\u001b[A\n",
      "batch 513, training loss: 3.5106: : 512it [07:44,  1.34s/it]\u001b[A\n",
      "batch 513, training loss: 3.5106: : 513it [07:44,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 3.3885: : 513it [07:46,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 3.3885: : 514it [07:46,  1.35s/it]\u001b[A\n",
      "batch 515, training loss: 3.5603: : 514it [07:47,  1.35s/it]\u001b[A\n",
      "batch 515, training loss: 3.5603: : 515it [07:47,  1.36s/it]\u001b[A\n",
      "batch 516, training loss: 3.4379: : 515it [07:49,  1.36s/it]\u001b[A\n",
      "batch 516, training loss: 3.4379: : 516it [07:49,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.4219: : 516it [07:50,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.4219: : 517it [07:50,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 3.5091: : 517it [07:51,  1.37s/it]\u001b[A\n",
      "batch 518, training loss: 3.5091: : 518it [07:51,  1.38s/it]\u001b[A\n",
      "batch 519, training loss: 3.3012: : 518it [07:53,  1.38s/it]\u001b[A\n",
      "batch 519, training loss: 3.3012: : 519it [07:53,  1.38s/it]\u001b[A\n",
      "batch 520, training loss: 3.473: : 519it [07:54,  1.38s/it] \u001b[A\n",
      "batch 520, training loss: 3.473: : 520it [07:54,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.4848: : 520it [07:56,  1.37s/it]\u001b[A\n",
      "batch 521, training loss: 3.4848: : 521it [07:56,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 3.5153: : 521it [07:57,  1.38s/it]\u001b[A\n",
      "batch 522, training loss: 3.5153: : 522it [07:57,  1.35s/it]\u001b[A\n",
      "batch 523, training loss: 3.396: : 522it [07:58,  1.35s/it] \u001b[A\n",
      "batch 523, training loss: 3.396: : 523it [07:58,  1.35s/it]\u001b[A\n",
      "batch 524, training loss: 3.5903: : 523it [08:00,  1.35s/it]\u001b[A\n",
      "batch 524, training loss: 3.5903: : 524it [08:00,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.6331: : 524it [08:01,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.6331: : 525it [08:01,  1.35s/it]\u001b[A\n",
      "batch 526, training loss: 3.4284: : 525it [08:02,  1.35s/it]\u001b[A\n",
      "batch 526, training loss: 3.4284: : 526it [08:02,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 3.4191: : 526it [08:04,  1.37s/it]\u001b[A\n",
      "batch 527, training loss: 3.4191: : 527it [08:04,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 3.4515: : 527it [08:05,  1.33s/it]\u001b[A\n",
      "batch 528, training loss: 3.4515: : 528it [08:05,  1.35s/it]\u001b[A\n",
      "batch 529, training loss: 3.5009: : 528it [08:06,  1.35s/it]\u001b[A\n",
      "batch 529, training loss: 3.5009: : 529it [08:06,  1.39s/it]\u001b[A\n",
      "batch 530, training loss: 3.4691: : 529it [08:08,  1.39s/it]\u001b[A\n",
      "batch 530, training loss: 3.4691: : 530it [08:08,  1.44s/it]\u001b[A\n",
      "batch 531, training loss: 3.4168: : 530it [08:09,  1.44s/it]\u001b[A\n",
      "batch 531, training loss: 3.4168: : 531it [08:09,  1.42s/it]\u001b[A\n",
      "batch 532, training loss: 3.2058: : 531it [08:11,  1.42s/it]\u001b[A\n",
      "batch 532, training loss: 3.2058: : 532it [08:11,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.3754: : 532it [08:12,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.3754: : 533it [08:12,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.5267: : 533it [08:14,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.5267: : 534it [08:14,  1.47s/it]\u001b[A\n",
      "batch 535, training loss: 3.3756: : 534it [08:15,  1.47s/it]\u001b[A\n",
      "batch 535, training loss: 3.3756: : 535it [08:15,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.2351: : 535it [08:17,  1.45s/it]\u001b[A\n",
      "batch 536, training loss: 3.2351: : 536it [08:17,  1.44s/it]\u001b[A\n",
      "batch 537, training loss: 3.3168: : 536it [08:18,  1.44s/it]\u001b[A\n",
      "batch 537, training loss: 3.3168: : 537it [08:18,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.4297: : 537it [08:20,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.4297: : 538it [08:20,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.3954: : 538it [08:21,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.3954: : 539it [08:21,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 3.3376: : 539it [08:23,  1.47s/it]\u001b[A\n",
      "batch 540, training loss: 3.3376: : 540it [08:23,  1.45s/it]\u001b[A\n",
      "batch 541, training loss: 3.4521: : 540it [08:24,  1.45s/it]\u001b[A\n",
      "batch 541, training loss: 3.4521: : 541it [08:24,  1.45s/it]\u001b[A\n",
      "batch 542, training loss: 3.497: : 541it [08:25,  1.45s/it] \u001b[A\n",
      "batch 542, training loss: 3.497: : 542it [08:25,  1.33s/it]\u001b[A\n",
      "batch 543, training loss: 3.3467: : 542it [08:26,  1.33s/it]\u001b[A\n",
      "batch 543, training loss: 3.3467: : 543it [08:26,  1.27s/it]\u001b[A\n",
      "batch 544, training loss: 3.4195: : 543it [08:27,  1.27s/it]\u001b[A\n",
      "batch 544, training loss: 3.4195: : 544it [08:27,  1.21s/it]\u001b[A\n",
      "batch 545, training loss: 3.4199: : 544it [08:28,  1.21s/it]\u001b[A\n",
      "batch 545, training loss: 3.4199: : 545it [08:28,  1.21s/it]\u001b[A\n",
      "batch 546, training loss: 3.3259: : 545it [08:30,  1.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 546, training loss: 3.3259: : 546it [08:30,  1.17s/it]\u001b[A\n",
      "batch 547, training loss: 3.236: : 546it [08:31,  1.17s/it] \u001b[A\n",
      "batch 547, training loss: 3.236: : 547it [08:31,  1.15s/it]\u001b[A\n",
      "batch 548, training loss: 3.1172: : 547it [08:31,  1.15s/it]\u001b[A\n",
      "batch 548, training loss: 3.1172: : 548it [08:31,  1.02s/it]\u001b[A\n",
      "batch 549, training loss: 3.3938: : 548it [08:32,  1.02s/it]\u001b[A\n",
      "batch 549, training loss: 3.3938: : 549it [08:32,  1.05s/it]\u001b[A\n",
      "batch 550, training loss: 3.3301: : 549it [08:34,  1.05s/it]\u001b[A\n",
      "batch 550, training loss: 3.3301: : 550it [08:34,  1.08s/it]\u001b[A\n",
      "batch 551, training loss: 3.3604: : 550it [08:35,  1.08s/it]\u001b[A\n",
      "batch 551, training loss: 3.3604: : 551it [08:35,  1.14s/it]\u001b[A\n",
      "batch 552, training loss: 3.3795: : 551it [08:36,  1.14s/it]\u001b[A\n",
      "batch 552, training loss: 3.3795: : 552it [08:36,  1.13s/it]\u001b[A\n",
      "batch 553, training loss: 3.3718: : 552it [08:37,  1.13s/it]\u001b[A\n",
      "batch 553, training loss: 3.3718: : 553it [08:37,  1.17s/it]\u001b[A\n",
      "batch 554, training loss: 3.1301: : 553it [08:38,  1.17s/it]\u001b[A\n",
      "batch 554, training loss: 3.1301: : 554it [08:38,  1.15s/it]\u001b[A\n",
      "batch 555, training loss: 3.2715: : 554it [08:39,  1.15s/it]\u001b[A\n",
      "batch 555, training loss: 3.2715: : 555it [08:39,  1.14s/it]\u001b[A\n",
      "batch 556, training loss: 3.4409: : 555it [08:41,  1.14s/it]\u001b[A\n",
      "batch 556, training loss: 3.4409: : 556it [08:41,  1.17s/it]\u001b[A\n",
      "batch 557, training loss: 3.2818: : 556it [08:42,  1.17s/it]\u001b[A\n",
      "batch 557, training loss: 3.2818: : 557it [08:42,  1.15s/it]\u001b[A\n",
      "batch 558, training loss: 3.2156: : 557it [08:43,  1.15s/it]\u001b[A\n",
      "batch 558, training loss: 3.2156: : 558it [08:43,  1.19s/it]\u001b[A\n",
      "batch 559, training loss: 3.3144: : 558it [08:44,  1.19s/it]\u001b[A\n",
      "batch 559, training loss: 3.3144: : 559it [08:44,  1.16s/it]\u001b[A\n",
      "batch 560, training loss: 3.3289: : 559it [08:45,  1.16s/it]\u001b[A\n",
      "batch 560, training loss: 3.3289: : 560it [08:45,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.3159: : 560it [08:47,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.3159: : 561it [08:47,  1.18s/it]\u001b[A\n",
      "batch 562, training loss: 3.1613: : 561it [08:48,  1.18s/it]\u001b[A\n",
      "batch 562, training loss: 3.1613: : 562it [08:48,  1.16s/it]\u001b[A\n",
      "batch 563, training loss: 3.3153: : 562it [08:49,  1.16s/it]\u001b[A\n",
      "batch 563, training loss: 3.3153: : 563it [08:49,  1.18s/it]\u001b[A\n",
      "batch 564, training loss: 3.1952: : 563it [08:50,  1.18s/it]\u001b[A\n",
      "batch 564, training loss: 3.1952: : 564it [08:50,  1.01s/it]\u001b[A\n",
      "batch 565, training loss: 3.3362: : 564it [08:51,  1.01s/it]\u001b[A\n",
      "batch 565, training loss: 3.3362: : 565it [08:51,  1.02s/it]\u001b[A\n",
      "batch 566, training loss: 3.4188: : 565it [08:52,  1.02s/it]\u001b[A\n",
      "batch 566, training loss: 3.4188: : 566it [08:52,  1.09s/it]\u001b[A\n",
      "batch 567, training loss: 3.383: : 566it [08:53,  1.09s/it] \u001b[A\n",
      "batch 567, training loss: 3.383: : 567it [08:53,  1.15s/it]\u001b[A\n",
      "batch 568, training loss: 3.4721: : 567it [08:54,  1.15s/it]\u001b[A\n",
      "batch 568, training loss: 3.4721: : 568it [08:54,  1.17s/it]\u001b[A\n",
      "batch 569, training loss: 3.4422: : 568it [08:56,  1.17s/it]\u001b[A\n",
      "batch 569, training loss: 3.4422: : 569it [08:56,  1.24s/it]\u001b[A\n",
      "batch 570, training loss: 3.6069: : 569it [08:57,  1.24s/it]\u001b[A\n",
      "batch 570, training loss: 3.6069: : 570it [08:57,  1.22s/it]\u001b[A\n",
      "batch 571, training loss: 3.4038: : 570it [08:58,  1.22s/it]\u001b[A\n",
      "batch 571, training loss: 3.4038: : 571it [08:58,  1.26s/it]\u001b[A\n",
      "batch 572, training loss: 3.5163: : 571it [08:59,  1.26s/it]\u001b[A\n",
      "batch 572, training loss: 3.5163: : 572it [08:59,  1.25s/it]\u001b[A\n",
      "batch 573, training loss: 3.4157: : 572it [09:01,  1.25s/it]\u001b[A\n",
      "batch 573, training loss: 3.4157: : 573it [09:01,  1.22s/it]\u001b[A\n",
      "batch 574, training loss: 3.5059: : 573it [09:02,  1.22s/it]\u001b[A\n",
      "batch 574, training loss: 3.5059: : 574it [09:02,  1.25s/it]\u001b[A\n",
      "batch 575, training loss: 3.032: : 574it [09:03,  1.25s/it] \u001b[A\n",
      "batch 575, training loss: 3.032: : 575it [09:03,  1.05s/it]\u001b[A\n",
      "batch 576, training loss: 3.4888: : 575it [09:04,  1.05s/it]\u001b[A\n",
      "batch 576, training loss: 3.4888: : 576it [09:04,  1.10s/it]\u001b[A\n",
      "batch 577, training loss: 3.3571: : 576it [09:05,  1.10s/it]\u001b[A\n",
      "batch 577, training loss: 3.3571: : 577it [09:05,  1.21s/it]\u001b[A\n",
      "batch 578, training loss: 3.3293: : 577it [09:07,  1.21s/it]\u001b[A\n",
      "batch 578, training loss: 3.3293: : 578it [09:07,  1.23s/it]\u001b[A\n",
      "batch 579, training loss: 3.391: : 578it [09:08,  1.23s/it] \u001b[A\n",
      "batch 579, training loss: 3.391: : 579it [09:08,  1.31s/it]\u001b[A\n",
      "batch 580, training loss: 3.2967: : 579it [09:09,  1.31s/it]\u001b[A\n",
      "batch 580, training loss: 3.2967: : 580it [09:09,  1.30s/it]\u001b[A\n",
      "batch 581, training loss: 3.3041: : 580it [09:11,  1.30s/it]\u001b[A\n",
      "batch 581, training loss: 3.3041: : 581it [09:11,  1.35s/it]\u001b[A\n",
      "batch 582, training loss: 3.3415: : 581it [09:12,  1.35s/it]\u001b[A\n",
      "batch 582, training loss: 3.3415: : 582it [09:12,  1.34s/it]\u001b[A\n",
      "batch 583, training loss: 2.654: : 582it [09:13,  1.34s/it] \u001b[A\n",
      "batch 583, training loss: 2.654: : 583it [09:13,  1.08s/it]\u001b[A\n",
      "batch 584, training loss: 3.5186: : 583it [09:14,  1.08s/it]\u001b[A\n",
      "batch 584, training loss: 3.5186: : 584it [09:14,  1.21s/it]\u001b[A\n",
      "batch 585, training loss: 3.4559: : 584it [09:15,  1.21s/it]\u001b[A\n",
      "batch 585, training loss: 3.4559: : 585it [09:15,  1.26s/it]\u001b[A\n",
      "batch 586, training loss: 3.5604: : 585it [09:17,  1.26s/it]\u001b[A\n",
      "batch 586, training loss: 3.5604: : 586it [09:17,  1.33s/it]\u001b[A\n",
      "batch 587, training loss: 3.4299: : 586it [09:18,  1.33s/it]\u001b[A\n",
      "batch 587, training loss: 3.4299: : 587it [09:18,  1.34s/it]\u001b[A\n",
      "batch 588, training loss: 3.3391: : 587it [09:20,  1.34s/it]\u001b[A\n",
      "batch 588, training loss: 3.3391: : 588it [09:20,  1.38s/it]\u001b[A\n",
      "batch 589, training loss: 3.4235: : 588it [09:21,  1.38s/it]\u001b[A\n",
      "batch 589, training loss: 3.4235: : 589it [09:21,  1.39s/it]\u001b[A\n",
      "batch 590, training loss: 3.4261: : 589it [09:23,  1.39s/it]\u001b[A\n",
      "batch 590, training loss: 3.4261: : 590it [09:23,  1.44s/it]\u001b[A\n",
      "batch 591, training loss: 3.3082: : 590it [09:24,  1.44s/it]\u001b[A\n",
      "batch 591, training loss: 3.3082: : 591it [09:24,  1.42s/it]\u001b[A\n",
      "batch 592, training loss: 3.342: : 591it [09:26,  1.42s/it] \u001b[A\n",
      "batch 592, training loss: 3.342: : 592it [09:26,  1.44s/it]\u001b[A\n",
      "batch 593, training loss: 3.2717: : 592it [09:27,  1.44s/it]\u001b[A\n",
      "batch 593, training loss: 3.2717: : 593it [09:27,  1.44s/it]\u001b[A\n",
      "batch 594, training loss: 3.3989: : 593it [09:29,  1.44s/it]\u001b[A\n",
      "batch 594, training loss: 3.3989: : 594it [09:29,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.4828: : 594it [09:30,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.4828: : 595it [09:30,  1.39s/it]\u001b[A\n",
      "batch 596, training loss: 3.4426: : 595it [09:31,  1.39s/it]\u001b[A\n",
      "batch 596, training loss: 3.4426: : 596it [09:31,  1.47s/it]\u001b[A\n",
      "batch 597, training loss: 3.2561: : 596it [09:33,  1.47s/it]\u001b[A\n",
      "batch 597, training loss: 3.2561: : 597it [09:33,  1.45s/it]\u001b[A\n",
      "batch 598, training loss: 3.4719: : 597it [09:35,  1.45s/it]\u001b[A\n",
      "batch 598, training loss: 3.4719: : 598it [09:35,  1.53s/it]\u001b[A\n",
      "batch 599, training loss: 3.3427: : 598it [09:36,  1.53s/it]\u001b[A\n",
      "batch 599, training loss: 3.3427: : 599it [09:36,  1.38s/it]\u001b[A\n",
      "batch 600, training loss: 3.2897: : 599it [09:37,  1.38s/it]\u001b[A\n",
      "batch 600, training loss: 3.2897: : 600it [09:37,  1.51s/it]\u001b[A\n",
      "batch 601, training loss: 2.3671: : 600it [09:38,  1.51s/it]\u001b[A\n",
      "batch 601, training loss: 2.3671: : 601it [09:38,  1.25s/it]\u001b[A\n",
      "batch 602, training loss: 3.3392: : 601it [09:40,  1.25s/it]\u001b[A\n",
      "batch 602, training loss: 3.3392: : 602it [09:40,  1.36s/it]\u001b[A\n",
      "batch 603, training loss: 3.3226: : 602it [09:41,  1.36s/it]\u001b[A\n",
      "batch 603, training loss: 3.3226: : 603it [09:41,  1.34s/it]\u001b[A\n",
      "batch 604, training loss: 3.3219: : 603it [09:42,  1.34s/it]\u001b[A\n",
      "batch 604, training loss: 3.3219: : 604it [09:42,  1.33s/it]\u001b[A\n",
      "batch 605, training loss: 3.4446: : 604it [09:44,  1.33s/it]\u001b[A\n",
      "batch 605, training loss: 3.4446: : 605it [09:44,  1.31s/it]\u001b[A\n",
      "batch 606, training loss: 3.2377: : 605it [09:45,  1.31s/it]\u001b[A\n",
      "batch 606, training loss: 3.2377: : 606it [09:45,  1.24s/it]\u001b[A\n",
      "batch 607, training loss: 3.0754: : 606it [09:46,  1.24s/it]\u001b[A\n",
      "batch 607, training loss: 3.0754: : 607it [09:46,  1.18s/it]\u001b[A\n",
      "batch 608, training loss: 3.2341: : 607it [09:47,  1.18s/it]\u001b[A\n",
      "batch 608, training loss: 3.2341: : 608it [09:47,  1.14s/it]\u001b[A\n",
      "batch 609, training loss: 3.2034: : 608it [09:48,  1.14s/it]\u001b[A\n",
      "batch 609, training loss: 3.2034: : 609it [09:48,  1.05s/it]\u001b[A\n",
      "batch 610, training loss: 2.6332: : 609it [09:48,  1.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 610, training loss: 2.6332: : 610it [09:48,  1.01s/it]\u001b[A\n",
      "batch 611, training loss: 2.6703: : 610it [09:49,  1.01s/it]\u001b[A\n",
      "batch 611, training loss: 2.6703: : 611it [09:49,  1.01s/it]\u001b[A\n",
      "batch 612, training loss: 1.9322: : 611it [09:50,  1.01s/it]\u001b[A\n",
      "batch 612, training loss: 1.9322: : 612it [09:50,  1.06it/s]\u001b[A\n",
      "batch 613, training loss: 2.5849: : 612it [09:51,  1.06it/s]\u001b[A\n",
      "batch 613, training loss: 2.5849: : 613it [09:51,  1.16it/s]\u001b[A\n",
      "batch 613, training loss: 2.5849: : 616it [09:51,  1.04it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.8014: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.8014: : 1it [00:00,  2.07it/s]\u001b[A\n",
      "batch 1, dev loss: 3.998: : 1it [00:00,  2.07it/s] \u001b[A\n",
      "batch 1, dev loss: 3.998: : 2it [00:00,  3.37it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5459: : 2it [00:00,  3.37it/s]\u001b[A\n",
      "batch 2, dev loss: 3.5459: : 3it [00:00,  4.23it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6982: : 3it [00:00,  4.23it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6982: : 4it [00:00,  4.74it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6973: : 4it [00:01,  4.74it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6973: : 5it [00:01,  5.12it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7216: : 5it [00:01,  5.12it/s]\u001b[A\n",
      "batch 5, dev loss: 3.7216: : 6it [00:01,  5.37it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7895: : 6it [00:01,  5.37it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7895: : 7it [00:01,  5.50it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5812: : 7it [00:01,  5.50it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5812: : 8it [00:01,  5.48it/s]\u001b[A\n",
      "batch 8, dev loss: 3.799: : 8it [00:01,  5.48it/s] \u001b[A\n",
      "batch 8, dev loss: 3.799: : 9it [00:01,  4.89it/s]\u001b[A\n",
      "batch 9, dev loss: 3.737: : 9it [00:02,  4.89it/s]\u001b[A\n",
      "batch 9, dev loss: 3.737: : 10it [00:02,  4.88it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7782: : 10it [00:02,  4.88it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7782: : 11it [00:02,  4.87it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8262: : 11it [00:02,  4.87it/s]\u001b[A\n",
      "batch 11, dev loss: 3.8262: : 12it [00:02,  4.86it/s]\u001b[A\n",
      "batch 12, dev loss: 3.6426: : 12it [00:02,  4.86it/s]\u001b[A\n",
      "batch 12, dev loss: 3.6426: : 13it [00:02,  4.87it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7963: : 13it [00:02,  4.87it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7963: : 14it [00:02,  5.00it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9579: : 14it [00:03,  5.00it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9579: : 15it [00:03,  5.13it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6732: : 15it [00:03,  5.13it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6732: : 16it [00:03,  5.88it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0688: : 16it [00:03,  5.88it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0688: : 17it [00:03,  5.41it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8369: : 17it [00:03,  5.41it/s]\u001b[A\n",
      "batch 17, dev loss: 3.8369: : 18it [00:03,  4.82it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7356: : 18it [00:03,  4.82it/s]\u001b[A\n",
      "batch 18, dev loss: 3.7356: : 19it [00:03,  4.37it/s]\u001b[A\n",
      "batch 19, dev loss: 3.917: : 19it [00:04,  4.37it/s] \u001b[A\n",
      "batch 19, dev loss: 3.917: : 20it [00:04,  4.41it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7528: : 20it [00:04,  4.41it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7528: : 21it [00:04,  4.51it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6349: : 21it [00:04,  4.51it/s]\u001b[A\n",
      "batch 21, dev loss: 3.6349: : 22it [00:04,  4.58it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8352: : 22it [00:04,  4.58it/s]\u001b[A\n",
      "batch 22, dev loss: 3.8352: : 23it [00:04,  4.77it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8816: : 23it [00:04,  4.77it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7929: : 23it [00:05,  4.77it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7929: : 25it [00:05,  5.37it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7732: : 25it [00:05,  5.37it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7732: : 26it [00:05,  5.01it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7094: : 26it [00:05,  5.01it/s]\u001b[A\n",
      "batch 26, dev loss: 3.7094: : 27it [00:05,  4.87it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6313: : 27it [00:05,  4.87it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6313: : 28it [00:05,  4.25it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9141: : 28it [00:06,  4.25it/s]\u001b[A\n",
      "batch 28, dev loss: 3.9141: : 29it [00:06,  4.15it/s]\u001b[A\n",
      "batch 29, dev loss: 3.817: : 29it [00:06,  4.15it/s] \u001b[A\n",
      "batch 29, dev loss: 3.817: : 30it [00:06,  4.21it/s]\u001b[A\n",
      "batch 30, dev loss: 4.2101: : 30it [00:06,  4.21it/s]\u001b[A\n",
      "batch 31, dev loss: 3.8357: : 30it [00:06,  4.21it/s]\u001b[A\n",
      "batch 31, dev loss: 3.8357: : 32it [00:06,  4.94it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9325: : 32it [00:06,  4.94it/s]\u001b[A\n",
      "batch 32, dev loss: 3.9325: : 33it [00:06,  4.65it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6344: : 33it [00:07,  4.65it/s]\u001b[A\n",
      "batch 33, dev loss: 3.6344: : 34it [00:07,  4.60it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0889: : 34it [00:07,  4.60it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0889: : 35it [00:07,  4.68it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9228: : 35it [00:07,  4.68it/s]\u001b[A\n",
      "batch 35, dev loss: 3.9228: : 36it [00:07,  4.55it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8189: : 36it [00:07,  4.55it/s]\u001b[A\n",
      "batch 36, dev loss: 3.8189: : 37it [00:07,  4.88it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5887: : 37it [00:08,  4.88it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5887: : 38it [00:08,  4.39it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8516: : 38it [00:08,  4.39it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8516: : 39it [00:08,  4.13it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8442: : 39it [00:08,  4.13it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8442: : 40it [00:08,  3.57it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8837: : 40it [00:09,  3.57it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8837: : 41it [00:09,  3.47it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6822: : 41it [00:09,  3.47it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6822: : 42it [00:09,  3.79it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8293: : 42it [00:09,  3.79it/s]\u001b[A\n",
      "batch 42, dev loss: 3.8293: : 43it [00:09,  3.64it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8095: : 43it [00:09,  3.64it/s]\u001b[A\n",
      "batch 43, dev loss: 3.8095: : 44it [00:09,  3.53it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7317: : 44it [00:10,  3.53it/s]\u001b[A\n",
      "batch 44, dev loss: 3.7317: : 45it [00:10,  3.45it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0633: : 45it [00:10,  3.45it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0633: : 46it [00:10,  3.39it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5708: : 46it [00:10,  3.39it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5708: : 47it [00:10,  3.33it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7477: : 47it [00:11,  3.33it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7477: : 48it [00:11,  2.91it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5766: : 48it [00:11,  2.91it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5766: : 49it [00:11,  2.96it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7561: : 49it [00:11,  2.96it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7561: : 50it [00:11,  3.41it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7201: : 50it [00:12,  3.41it/s]\u001b[A\n",
      "batch 50, dev loss: 3.7201: : 51it [00:12,  3.25it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7432: : 51it [00:12,  3.25it/s]\u001b[A\n",
      "batch 51, dev loss: 3.7432: : 52it [00:12,  3.12it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4967: : 52it [00:12,  3.12it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4967: : 53it [00:12,  2.88it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7215: : 53it [00:13,  2.88it/s]\u001b[A\n",
      "batch 53, dev loss: 3.7215: : 54it [00:13,  2.79it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4724: : 54it [00:13,  2.79it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4724: : 55it [00:13,  2.80it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6712: : 55it [00:13,  2.80it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6712: : 56it [00:13,  2.69it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4988: : 56it [00:14,  2.69it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4988: : 57it [00:14,  2.90it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4734: : 57it [00:14,  2.90it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4734: : 58it [00:14,  2.62it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8568: : 58it [00:15,  2.62it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8568: : 59it [00:15,  2.60it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7326: : 59it [00:15,  2.60it/s]\u001b[A\n",
      "batch 59, dev loss: 3.7326: : 60it [00:15,  2.78it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4749: : 60it [00:15,  2.78it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4749: : 61it [00:15,  3.06it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4305: : 61it [00:15,  3.06it/s]\u001b[A\n",
      "batch 61, dev loss: 3.4305: : 62it [00:15,  3.40it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1897: : 62it [00:16,  3.40it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1897: : 63it [00:16,  3.60it/s]\u001b[A\n",
      "batch 63, dev loss: 3.8363: : 63it [00:16,  3.60it/s]\u001b[A\n",
      "batch 63, dev loss: 3.8363: : 64it [00:16,  3.68it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5352: : 64it [00:16,  3.68it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5352: : 65it [00:16,  3.87it/s]\u001b[A\n",
      "batch 65, dev loss: 3.5218: : 65it [00:16,  3.87it/s]\u001b[A\n",
      "batch 65, dev loss: 3.5218: : 66it [00:16,  3.90it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5955: : 66it [00:17,  3.90it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5955: : 67it [00:17,  3.39it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5699: : 67it [00:17,  3.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 67, dev loss: 2.5699: : 68it [00:17,  3.67it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6967: : 68it [00:17,  3.67it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6967: : 69it [00:17,  3.87it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0266: : 69it [00:17,  3.87it/s]\u001b[A\n",
      "batch 69, dev loss: 3.0266: : 70it [00:17,  3.98it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9484: : 70it [00:18,  3.98it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9484: : 71it [00:18,  4.03it/s]\u001b[A\n",
      "batch 71, dev loss: 3.125: : 71it [00:18,  4.03it/s] \u001b[A\n",
      "batch 71, dev loss: 3.125: : 72it [00:18,  4.16it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1113: : 72it [00:18,  4.16it/s]\u001b[A\n",
      "batch 72, dev loss: 4.1113: : 76it [00:18,  4.05it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:00,  1.50it/s]\u001b[A\n",
      "2it [00:01,  1.35it/s]\u001b[A\n",
      "3it [00:02,  1.23it/s]\u001b[A\n",
      "4it [00:03,  1.21it/s]\u001b[A\n",
      "5it [00:04,  1.20it/s]\u001b[A\n",
      "6it [00:04,  1.32it/s]\u001b[A\n",
      "7it [00:05,  1.29it/s]\u001b[A\n",
      "8it [00:06,  1.39it/s]\u001b[A\n",
      "9it [00:07,  1.27it/s]\u001b[A\n",
      "10it [00:08,  1.16it/s]\u001b[A\n",
      "11it [00:09,  1.11it/s]\u001b[A\n",
      "12it [00:10,  1.07it/s]\u001b[A\n",
      "13it [00:11,  1.04it/s]\u001b[A\n",
      "14it [00:12,  1.01it/s]\u001b[A\n",
      "15it [00:13,  1.01it/s]\u001b[A\n",
      "16it [00:13,  1.14it/s]\u001b[A\n",
      "17it [00:14,  1.03it/s]\u001b[A\n",
      "18it [00:15,  1.01it/s]\u001b[A\n",
      "19it [00:17,  1.06s/it]\u001b[A\n",
      "20it [00:18,  1.08s/it]\u001b[A\n",
      "21it [00:19,  1.09s/it]\u001b[A\n",
      "22it [00:20,  1.06s/it]\u001b[A\n",
      "23it [00:21,  1.04s/it]\u001b[A\n",
      "24it [00:21,  1.21it/s]\u001b[A\n",
      "25it [00:23,  1.04it/s]\u001b[A\n",
      "26it [00:24,  1.03s/it]\u001b[A\n",
      "27it [00:25,  1.14s/it]\u001b[A\n",
      "28it [00:26,  1.18s/it]\u001b[A\n",
      "29it [00:28,  1.27s/it]\u001b[A\n",
      "30it [00:29,  1.23s/it]\u001b[A\n",
      "31it [00:30,  1.28s/it]\u001b[A\n",
      "32it [00:32,  1.35s/it]\u001b[A\n",
      "33it [00:33,  1.40s/it]\u001b[A\n",
      "34it [00:35,  1.41s/it]\u001b[A\n",
      "35it [00:36,  1.48s/it]\u001b[A\n",
      "36it [00:37,  1.12s/it]\u001b[A\n",
      "37it [00:39,  1.36s/it]\u001b[A\n",
      "38it [00:40,  1.34s/it]\u001b[A\n",
      "39it [00:42,  1.43s/it]\u001b[A\n",
      "40it [00:43,  1.53s/it]\u001b[A\n",
      "41it [00:44,  1.16s/it]\u001b[A\n",
      "42it [00:46,  1.45s/it]\u001b[A\n",
      "43it [00:48,  1.61s/it]\u001b[A\n",
      "44it [00:50,  1.72s/it]\u001b[A\n",
      "45it [00:51,  1.65s/it]\u001b[A\n",
      "46it [00:53,  1.81s/it]\u001b[A\n",
      "47it [00:55,  1.88s/it]\u001b[A\n",
      "48it [00:58,  2.02s/it]\u001b[A\n",
      "49it [00:58,  1.46s/it]\u001b[A\n",
      "50it [01:00,  1.70s/it]\u001b[A\n",
      "51it [01:02,  1.86s/it]\u001b[A\n",
      "52it [01:04,  1.73s/it]\u001b[A\n",
      "53it [01:06,  1.94s/it]\u001b[A\n",
      "54it [01:08,  1.97s/it]\u001b[A\n",
      "55it [01:11,  2.09s/it]\u001b[A\n",
      "56it [01:12,  1.83s/it]\u001b[A\n",
      "57it [01:14,  1.98s/it]\u001b[A\n",
      "58it [01:16,  1.98s/it]\u001b[A\n",
      "59it [01:18,  1.87s/it]\u001b[A\n",
      "60it [01:19,  1.69s/it]\u001b[A\n",
      "61it [01:20,  1.48s/it]\u001b[A\n",
      "62it [01:21,  1.34s/it]\u001b[A\n",
      "63it [01:22,  1.11s/it]\u001b[A\n",
      "64it [01:22,  1.06it/s]\u001b[A\n",
      "65it [01:23,  1.28it/s]\u001b[A\n",
      "66it [01:23,  1.62it/s]\u001b[A\n",
      "67it [01:23,  1.77it/s]\u001b[A\n",
      "68it [01:24,  2.00it/s]\u001b[A\n",
      "69it [01:24,  2.25it/s]\u001b[A\n",
      "70it [01:24,  1.21s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████                                  | 727/6740 [00:00<00:00, 7267.15it/s]\u001b[A\n",
      " 22%|███████▉                             | 1457/6740 [00:00<00:00, 7285.05it/s]\u001b[A\n",
      " 32%|████████████                         | 2186/6740 [00:00<00:00, 7230.00it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 2910/6740 [00:00<00:00, 6960.08it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 3619/6740 [00:00<00:00, 7002.85it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 4321/6740 [00:00<00:00, 6766.20it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 5005/6740 [00:00<00:00, 6782.31it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 5685/6740 [00:00<00:00, 6775.45it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:00<00:00, 6877.89it/s]\u001b[A\n",
      "Epoch: 11, tfr: 1.0, loss(train/dev): 3.4558/3.701, ppl(dev/test): 40.4878/47.49\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.5278: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.5278: : 1it [00:01,  1.77s/it]\u001b[A\n",
      "batch 2, training loss: 3.4269: : 1it [00:02,  1.77s/it]\u001b[A\n",
      "batch 2, training loss: 3.4269: : 2it [00:02,  1.10s/it]\u001b[A\n",
      "batch 3, training loss: 3.4435: : 2it [00:02,  1.10s/it]\u001b[A\n",
      "batch 3, training loss: 3.4435: : 3it [00:02,  1.24it/s]\u001b[A\n",
      "batch 4, training loss: 3.3831: : 3it [00:03,  1.24it/s]\u001b[A\n",
      "batch 4, training loss: 3.3831: : 4it [00:03,  1.53it/s]\u001b[A\n",
      "batch 5, training loss: 3.2111: : 4it [00:03,  1.53it/s]\u001b[A\n",
      "batch 5, training loss: 3.2111: : 5it [00:03,  1.76it/s]\u001b[A\n",
      "batch 6, training loss: 3.5161: : 5it [00:04,  1.76it/s]\u001b[A\n",
      "batch 6, training loss: 3.5161: : 6it [00:04,  1.94it/s]\u001b[A\n",
      "batch 7, training loss: 3.4996: : 6it [00:04,  1.94it/s]\u001b[A\n",
      "batch 7, training loss: 3.4996: : 7it [00:04,  2.07it/s]\u001b[A\n",
      "batch 8, training loss: 3.5018: : 7it [00:05,  2.07it/s]\u001b[A\n",
      "batch 8, training loss: 3.5018: : 8it [00:05,  2.01it/s]\u001b[A\n",
      "batch 9, training loss: 3.3521: : 8it [00:05,  2.01it/s]\u001b[A\n",
      "batch 9, training loss: 3.3521: : 9it [00:05,  2.32it/s]\u001b[A\n",
      "batch 10, training loss: 3.3621: : 9it [00:05,  2.32it/s]\u001b[A\n",
      "batch 10, training loss: 3.3621: : 10it [00:05,  2.54it/s]\u001b[A\n",
      "batch 11, training loss: 3.426: : 10it [00:06,  2.54it/s] \u001b[A\n",
      "batch 11, training loss: 3.426: : 11it [00:06,  2.35it/s]\u001b[A\n",
      "batch 12, training loss: 3.464: : 11it [00:06,  2.35it/s]\u001b[A\n",
      "batch 12, training loss: 3.464: : 12it [00:06,  2.33it/s]\u001b[A\n",
      "batch 13, training loss: 3.3167: : 12it [00:07,  2.33it/s]\u001b[A\n",
      "batch 13, training loss: 3.3167: : 13it [00:07,  2.33it/s]\u001b[A\n",
      "batch 14, training loss: 3.6013: : 13it [00:07,  2.33it/s]\u001b[A\n",
      "batch 14, training loss: 3.6013: : 14it [00:07,  2.34it/s]\u001b[A\n",
      "batch 15, training loss: 3.4536: : 14it [00:08,  2.34it/s]\u001b[A\n",
      "batch 15, training loss: 3.4536: : 15it [00:08,  2.01it/s]\u001b[A\n",
      "batch 16, training loss: 3.4581: : 15it [00:08,  2.01it/s]\u001b[A\n",
      "batch 16, training loss: 3.4581: : 16it [00:08,  2.11it/s]\u001b[A\n",
      "batch 17, training loss: 3.6066: : 16it [00:08,  2.11it/s]\u001b[A\n",
      "batch 17, training loss: 3.6066: : 17it [00:08,  2.19it/s]\u001b[A\n",
      "batch 18, training loss: 3.437: : 17it [00:09,  2.19it/s] \u001b[A\n",
      "batch 18, training loss: 3.437: : 18it [00:09,  2.27it/s]\u001b[A\n",
      "batch 19, training loss: 3.2099: : 18it [00:09,  2.27it/s]\u001b[A\n",
      "batch 19, training loss: 3.2099: : 19it [00:09,  2.31it/s]\u001b[A\n",
      "batch 20, training loss: 3.3689: : 19it [00:10,  2.31it/s]\u001b[A\n",
      "batch 20, training loss: 3.3689: : 20it [00:10,  2.33it/s]\u001b[A\n",
      "batch 21, training loss: 3.4805: : 20it [00:10,  2.33it/s]\u001b[A\n",
      "batch 21, training loss: 3.4805: : 21it [00:10,  2.10it/s]\u001b[A\n",
      "batch 22, training loss: 3.2736: : 21it [00:11,  2.10it/s]\u001b[A\n",
      "batch 22, training loss: 3.2736: : 22it [00:11,  2.20it/s]\u001b[A\n",
      "batch 23, training loss: 3.4203: : 22it [00:11,  2.20it/s]\u001b[A\n",
      "batch 23, training loss: 3.4203: : 23it [00:11,  2.26it/s]\u001b[A\n",
      "batch 24, training loss: 3.3566: : 23it [00:11,  2.26it/s]\u001b[A\n",
      "batch 24, training loss: 3.3566: : 24it [00:11,  2.31it/s]\u001b[A\n",
      "batch 25, training loss: 3.4432: : 24it [00:12,  2.31it/s]\u001b[A\n",
      "batch 25, training loss: 3.4432: : 25it [00:12,  2.33it/s]\u001b[A\n",
      "batch 26, training loss: 3.2531: : 25it [00:12,  2.33it/s]\u001b[A\n",
      "batch 26, training loss: 3.2531: : 26it [00:12,  2.16it/s]\u001b[A\n",
      "batch 27, training loss: 3.3963: : 26it [00:13,  2.16it/s]\u001b[A\n",
      "batch 27, training loss: 3.3963: : 27it [00:13,  2.21it/s]\u001b[A\n",
      "batch 28, training loss: 3.2508: : 27it [00:13,  2.21it/s]\u001b[A\n",
      "batch 28, training loss: 3.2508: : 28it [00:13,  2.29it/s]\u001b[A\n",
      "batch 29, training loss: 3.4095: : 28it [00:14,  2.29it/s]\u001b[A\n",
      "batch 29, training loss: 3.4095: : 29it [00:14,  2.33it/s]\u001b[A\n",
      "batch 30, training loss: 3.51: : 29it [00:14,  2.33it/s]  \u001b[A\n",
      "batch 30, training loss: 3.51: : 30it [00:14,  2.36it/s]\u001b[A\n",
      "batch 31, training loss: 3.274: : 30it [00:15,  2.36it/s]\u001b[A\n",
      "batch 31, training loss: 3.274: : 31it [00:15,  2.37it/s]\u001b[A\n",
      "batch 32, training loss: 3.4498: : 31it [00:15,  2.37it/s]\u001b[A\n",
      "batch 32, training loss: 3.4498: : 32it [00:15,  2.38it/s]\u001b[A\n",
      "batch 33, training loss: 3.3406: : 32it [00:16,  2.38it/s]\u001b[A\n",
      "batch 33, training loss: 3.3406: : 33it [00:16,  2.14it/s]\u001b[A\n",
      "batch 34, training loss: 3.3519: : 33it [00:16,  2.14it/s]\u001b[A\n",
      "batch 34, training loss: 3.3519: : 34it [00:16,  2.23it/s]\u001b[A\n",
      "batch 35, training loss: 3.4209: : 34it [00:16,  2.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 35, training loss: 3.4209: : 35it [00:16,  2.28it/s]\u001b[A\n",
      "batch 36, training loss: 3.4835: : 35it [00:17,  2.28it/s]\u001b[A\n",
      "batch 36, training loss: 3.4835: : 36it [00:17,  2.33it/s]\u001b[A\n",
      "batch 37, training loss: 3.3534: : 36it [00:17,  2.33it/s]\u001b[A\n",
      "batch 37, training loss: 3.3534: : 37it [00:17,  2.36it/s]\u001b[A\n",
      "batch 38, training loss: 3.1502: : 37it [00:18,  2.36it/s]\u001b[A\n",
      "batch 38, training loss: 3.1502: : 38it [00:18,  2.38it/s]\u001b[A\n",
      "batch 39, training loss: 3.3153: : 38it [00:18,  2.38it/s]\u001b[A\n",
      "batch 39, training loss: 3.3153: : 39it [00:18,  2.14it/s]\u001b[A\n",
      "batch 40, training loss: 3.3603: : 39it [00:19,  2.14it/s]\u001b[A\n",
      "batch 40, training loss: 3.3603: : 40it [00:19,  2.23it/s]\u001b[A\n",
      "batch 41, training loss: 3.5336: : 40it [00:19,  2.23it/s]\u001b[A\n",
      "batch 41, training loss: 3.5336: : 41it [00:19,  2.29it/s]\u001b[A\n",
      "batch 42, training loss: 3.3469: : 41it [00:19,  2.29it/s]\u001b[A\n",
      "batch 42, training loss: 3.3469: : 42it [00:19,  2.33it/s]\u001b[A\n",
      "batch 43, training loss: 3.168: : 42it [00:20,  2.33it/s] \u001b[A\n",
      "batch 43, training loss: 3.168: : 43it [00:20,  2.36it/s]\u001b[A\n",
      "batch 44, training loss: 3.1494: : 43it [00:20,  2.36it/s]\u001b[A\n",
      "batch 44, training loss: 3.1494: : 44it [00:20,  2.38it/s]\u001b[A\n",
      "batch 45, training loss: 3.1968: : 44it [00:21,  2.38it/s]\u001b[A\n",
      "batch 45, training loss: 3.1968: : 45it [00:21,  2.40it/s]\u001b[A\n",
      "batch 46, training loss: 3.3336: : 45it [00:21,  2.40it/s]\u001b[A\n",
      "batch 46, training loss: 3.3336: : 46it [00:21,  2.16it/s]\u001b[A\n",
      "batch 47, training loss: 3.3385: : 46it [00:22,  2.16it/s]\u001b[A\n",
      "batch 47, training loss: 3.3385: : 47it [00:22,  2.25it/s]\u001b[A\n",
      "batch 48, training loss: 3.2171: : 47it [00:22,  2.25it/s]\u001b[A\n",
      "batch 48, training loss: 3.2171: : 48it [00:22,  2.30it/s]\u001b[A\n",
      "batch 49, training loss: 3.5001: : 48it [00:22,  2.30it/s]\u001b[A\n",
      "batch 49, training loss: 3.5001: : 49it [00:22,  2.34it/s]\u001b[A\n",
      "batch 50, training loss: 3.2541: : 49it [00:23,  2.34it/s]\u001b[A\n",
      "batch 50, training loss: 3.2541: : 50it [00:23,  2.36it/s]\u001b[A\n",
      "batch 51, training loss: 3.3558: : 50it [00:23,  2.36it/s]\u001b[A\n",
      "batch 51, training loss: 3.3558: : 51it [00:23,  2.37it/s]\u001b[A\n",
      "batch 52, training loss: 3.1945: : 51it [00:24,  2.37it/s]\u001b[A\n",
      "batch 52, training loss: 3.1945: : 52it [00:24,  2.16it/s]\u001b[A\n",
      "batch 53, training loss: 3.214: : 52it [00:24,  2.16it/s] \u001b[A\n",
      "batch 53, training loss: 3.214: : 53it [00:24,  2.25it/s]\u001b[A\n",
      "batch 54, training loss: 3.1787: : 53it [00:25,  2.25it/s]\u001b[A\n",
      "batch 54, training loss: 3.1787: : 54it [00:25,  2.31it/s]\u001b[A\n",
      "batch 55, training loss: 3.2971: : 54it [00:25,  2.31it/s]\u001b[A\n",
      "batch 55, training loss: 3.2971: : 55it [00:25,  2.34it/s]\u001b[A\n",
      "batch 56, training loss: 3.3221: : 55it [00:25,  2.34it/s]\u001b[A\n",
      "batch 56, training loss: 3.3221: : 56it [00:25,  2.35it/s]\u001b[A\n",
      "batch 57, training loss: 3.288: : 56it [00:26,  2.35it/s] \u001b[A\n",
      "batch 57, training loss: 3.288: : 57it [00:26,  2.37it/s]\u001b[A\n",
      "batch 58, training loss: 3.3535: : 57it [00:26,  2.37it/s]\u001b[A\n",
      "batch 58, training loss: 3.3535: : 58it [00:26,  2.14it/s]\u001b[A\n",
      "batch 59, training loss: 3.2135: : 58it [00:27,  2.14it/s]\u001b[A\n",
      "batch 59, training loss: 3.2135: : 59it [00:27,  2.23it/s]\u001b[A\n",
      "batch 60, training loss: 3.1774: : 59it [00:27,  2.23it/s]\u001b[A\n",
      "batch 60, training loss: 3.1774: : 60it [00:27,  2.29it/s]\u001b[A\n",
      "batch 61, training loss: 3.4028: : 60it [00:28,  2.29it/s]\u001b[A\n",
      "batch 61, training loss: 3.4028: : 61it [00:28,  2.32it/s]\u001b[A\n",
      "batch 62, training loss: 3.2419: : 61it [00:28,  2.32it/s]\u001b[A\n",
      "batch 62, training loss: 3.2419: : 62it [00:28,  2.34it/s]\u001b[A\n",
      "batch 63, training loss: 3.3393: : 62it [00:29,  2.34it/s]\u001b[A\n",
      "batch 63, training loss: 3.3393: : 63it [00:29,  2.19it/s]\u001b[A\n",
      "batch 64, training loss: 3.2962: : 63it [00:29,  2.19it/s]\u001b[A\n",
      "batch 64, training loss: 3.2962: : 64it [00:29,  2.20it/s]\u001b[A\n",
      "batch 65, training loss: 3.3627: : 64it [00:29,  2.20it/s]\u001b[A\n",
      "batch 65, training loss: 3.3627: : 65it [00:29,  2.27it/s]\u001b[A\n",
      "batch 66, training loss: 3.3991: : 65it [00:30,  2.27it/s]\u001b[A\n",
      "batch 66, training loss: 3.3991: : 66it [00:30,  2.32it/s]\u001b[A\n",
      "batch 67, training loss: 3.2601: : 66it [00:30,  2.32it/s]\u001b[A\n",
      "batch 67, training loss: 3.2601: : 67it [00:30,  2.34it/s]\u001b[A\n",
      "batch 68, training loss: 3.3502: : 67it [00:31,  2.34it/s]\u001b[A\n",
      "batch 68, training loss: 3.3502: : 68it [00:31,  2.36it/s]\u001b[A\n",
      "batch 69, training loss: 3.2362: : 68it [00:31,  2.36it/s]\u001b[A\n",
      "batch 69, training loss: 3.2362: : 69it [00:31,  2.38it/s]\u001b[A\n",
      "batch 70, training loss: 3.4327: : 69it [00:32,  2.38it/s]\u001b[A\n",
      "batch 70, training loss: 3.4327: : 70it [00:32,  2.16it/s]\u001b[A\n",
      "batch 71, training loss: 3.2734: : 70it [00:32,  2.16it/s]\u001b[A\n",
      "batch 71, training loss: 3.2734: : 71it [00:32,  2.25it/s]\u001b[A\n",
      "batch 72, training loss: 3.2401: : 71it [00:32,  2.25it/s]\u001b[A\n",
      "batch 72, training loss: 3.2401: : 72it [00:32,  2.29it/s]\u001b[A\n",
      "batch 73, training loss: 3.2819: : 72it [00:33,  2.29it/s]\u001b[A\n",
      "batch 73, training loss: 3.2819: : 73it [00:33,  2.32it/s]\u001b[A\n",
      "batch 74, training loss: 3.1931: : 73it [00:33,  2.32it/s]\u001b[A\n",
      "batch 74, training loss: 3.1931: : 74it [00:33,  2.35it/s]\u001b[A\n",
      "batch 75, training loss: 3.3997: : 74it [00:34,  2.35it/s]\u001b[A\n",
      "batch 75, training loss: 3.3997: : 75it [00:34,  2.38it/s]\u001b[A\n",
      "batch 76, training loss: 3.2108: : 75it [00:34,  2.38it/s]\u001b[A\n",
      "batch 76, training loss: 3.2108: : 76it [00:34,  2.12it/s]\u001b[A\n",
      "batch 77, training loss: 3.3479: : 76it [00:35,  2.12it/s]\u001b[A\n",
      "batch 77, training loss: 3.3479: : 77it [00:35,  2.10it/s]\u001b[A\n",
      "batch 78, training loss: 3.3185: : 77it [00:35,  2.10it/s]\u001b[A\n",
      "batch 78, training loss: 3.3185: : 78it [00:35,  2.12it/s]\u001b[A\n",
      "batch 79, training loss: 3.3632: : 78it [00:36,  2.12it/s]\u001b[A\n",
      "batch 79, training loss: 3.3632: : 79it [00:36,  2.16it/s]\u001b[A\n",
      "batch 80, training loss: 3.3: : 79it [00:36,  2.16it/s]   \u001b[A\n",
      "batch 80, training loss: 3.3: : 80it [00:36,  1.92it/s]\u001b[A\n",
      "batch 81, training loss: 3.2748: : 80it [00:37,  1.92it/s]\u001b[A\n",
      "batch 81, training loss: 3.2748: : 81it [00:37,  2.04it/s]\u001b[A\n",
      "batch 82, training loss: 3.4277: : 81it [00:37,  2.04it/s]\u001b[A\n",
      "batch 82, training loss: 3.4277: : 82it [00:37,  2.14it/s]\u001b[A\n",
      "batch 83, training loss: 3.2869: : 82it [00:38,  2.14it/s]\u001b[A\n",
      "batch 83, training loss: 3.2869: : 83it [00:38,  2.22it/s]\u001b[A\n",
      "batch 84, training loss: 3.4276: : 83it [00:38,  2.22it/s]\u001b[A\n",
      "batch 84, training loss: 3.4276: : 84it [00:38,  2.27it/s]\u001b[A\n",
      "batch 85, training loss: 3.4324: : 84it [00:38,  2.27it/s]\u001b[A\n",
      "batch 85, training loss: 3.4324: : 85it [00:38,  2.31it/s]\u001b[A\n",
      "batch 86, training loss: 3.4169: : 85it [00:39,  2.31it/s]\u001b[A\n",
      "batch 86, training loss: 3.4169: : 86it [00:39,  2.10it/s]\u001b[A\n",
      "batch 87, training loss: 3.4004: : 86it [00:39,  2.10it/s]\u001b[A\n",
      "batch 87, training loss: 3.4004: : 87it [00:39,  2.23it/s]\u001b[A\n",
      "batch 88, training loss: 3.4245: : 87it [00:40,  2.23it/s]\u001b[A\n",
      "batch 88, training loss: 3.4245: : 88it [00:40,  2.21it/s]\u001b[A\n",
      "batch 89, training loss: 3.5322: : 88it [00:40,  2.21it/s]\u001b[A\n",
      "batch 89, training loss: 3.5322: : 89it [00:40,  2.06it/s]\u001b[A\n",
      "batch 90, training loss: 3.4472: : 89it [00:41,  2.06it/s]\u001b[A\n",
      "batch 90, training loss: 3.4472: : 90it [00:41,  2.02it/s]\u001b[A\n",
      "batch 91, training loss: 3.5905: : 90it [00:42,  2.02it/s]\u001b[A\n",
      "batch 91, training loss: 3.5905: : 91it [00:42,  1.83it/s]\u001b[A\n",
      "batch 92, training loss: 3.4172: : 91it [00:42,  1.83it/s]\u001b[A\n",
      "batch 92, training loss: 3.4172: : 92it [00:42,  1.81it/s]\u001b[A\n",
      "batch 93, training loss: 3.3883: : 92it [00:43,  1.81it/s]\u001b[A\n",
      "batch 93, training loss: 3.3883: : 93it [00:43,  1.85it/s]\u001b[A\n",
      "batch 94, training loss: 3.4869: : 93it [00:43,  1.85it/s]\u001b[A\n",
      "batch 94, training loss: 3.4869: : 94it [00:43,  1.88it/s]\u001b[A\n",
      "batch 95, training loss: 3.4348: : 94it [00:44,  1.88it/s]\u001b[A\n",
      "batch 95, training loss: 3.4348: : 95it [00:44,  1.97it/s]\u001b[A\n",
      "batch 96, training loss: 3.4433: : 95it [00:44,  1.97it/s]\u001b[A\n",
      "batch 96, training loss: 3.4433: : 96it [00:44,  1.86it/s]\u001b[A\n",
      "batch 97, training loss: 3.5193: : 96it [00:45,  1.86it/s]\u001b[A\n",
      "batch 97, training loss: 3.5193: : 97it [00:45,  1.84it/s]\u001b[A\n",
      "batch 98, training loss: 3.5292: : 97it [00:45,  1.84it/s]\u001b[A\n",
      "batch 98, training loss: 3.5292: : 98it [00:45,  1.87it/s]\u001b[A\n",
      "batch 99, training loss: 3.3361: : 98it [00:46,  1.87it/s]\u001b[A\n",
      "batch 99, training loss: 3.3361: : 99it [00:46,  1.91it/s]\u001b[A\n",
      "batch 100, training loss: 3.2026: : 99it [00:46,  1.91it/s]\u001b[A\n",
      "batch 100, training loss: 3.2026: : 100it [00:46,  1.76it/s]\u001b[A\n",
      "batch 101, training loss: 3.3576: : 100it [00:47,  1.76it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 101, training loss: 3.3576: : 101it [00:47,  1.79it/s]\u001b[A\n",
      "batch 102, training loss: 3.3762: : 101it [00:48,  1.79it/s]\u001b[A\n",
      "batch 102, training loss: 3.3762: : 102it [00:48,  1.82it/s]\u001b[A\n",
      "batch 103, training loss: 3.3184: : 102it [00:48,  1.82it/s]\u001b[A\n",
      "batch 103, training loss: 3.3184: : 103it [00:48,  1.89it/s]\u001b[A\n",
      "batch 104, training loss: 3.175: : 103it [00:49,  1.89it/s] \u001b[A\n",
      "batch 104, training loss: 3.175: : 104it [00:49,  1.74it/s]\u001b[A\n",
      "batch 105, training loss: 3.3138: : 104it [00:49,  1.74it/s]\u001b[A\n",
      "batch 105, training loss: 3.3138: : 105it [00:49,  1.80it/s]\u001b[A\n",
      "batch 106, training loss: 3.4745: : 105it [00:50,  1.80it/s]\u001b[A\n",
      "batch 106, training loss: 3.4745: : 106it [00:50,  1.81it/s]\u001b[A\n",
      "batch 107, training loss: 3.1756: : 106it [00:50,  1.81it/s]\u001b[A\n",
      "batch 107, training loss: 3.1756: : 107it [00:50,  1.89it/s]\u001b[A\n",
      "batch 108, training loss: 3.4432: : 107it [00:51,  1.89it/s]\u001b[A\n",
      "batch 108, training loss: 3.4432: : 108it [00:51,  1.86it/s]\u001b[A\n",
      "batch 109, training loss: 3.4867: : 108it [00:51,  1.86it/s]\u001b[A\n",
      "batch 109, training loss: 3.4867: : 109it [00:51,  1.77it/s]\u001b[A\n",
      "batch 110, training loss: 3.6217: : 109it [00:52,  1.77it/s]\u001b[A\n",
      "batch 110, training loss: 3.6217: : 110it [00:52,  1.81it/s]\u001b[A\n",
      "batch 111, training loss: 3.3428: : 110it [00:52,  1.81it/s]\u001b[A\n",
      "batch 111, training loss: 3.3428: : 111it [00:52,  1.89it/s]\u001b[A\n",
      "batch 112, training loss: 3.4032: : 111it [00:53,  1.89it/s]\u001b[A\n",
      "batch 112, training loss: 3.4032: : 112it [00:53,  1.86it/s]\u001b[A\n",
      "batch 113, training loss: 3.4789: : 112it [00:53,  1.86it/s]\u001b[A\n",
      "batch 113, training loss: 3.4789: : 113it [00:53,  1.92it/s]\u001b[A\n",
      "batch 114, training loss: 3.465: : 113it [00:54,  1.92it/s] \u001b[A\n",
      "batch 114, training loss: 3.465: : 114it [00:54,  1.74it/s]\u001b[A\n",
      "batch 115, training loss: 3.375: : 114it [00:55,  1.74it/s]\u001b[A\n",
      "batch 115, training loss: 3.375: : 115it [00:55,  1.75it/s]\u001b[A\n",
      "batch 116, training loss: 3.3897: : 115it [00:55,  1.75it/s]\u001b[A\n",
      "batch 116, training loss: 3.3897: : 116it [00:55,  1.82it/s]\u001b[A\n",
      "batch 117, training loss: 3.4241: : 116it [00:56,  1.82it/s]\u001b[A\n",
      "batch 117, training loss: 3.4241: : 117it [00:56,  1.84it/s]\u001b[A\n",
      "batch 118, training loss: 3.5021: : 117it [00:56,  1.84it/s]\u001b[A\n",
      "batch 118, training loss: 3.5021: : 118it [00:56,  1.70it/s]\u001b[A\n",
      "batch 119, training loss: 3.4308: : 118it [00:57,  1.70it/s]\u001b[A\n",
      "batch 119, training loss: 3.4308: : 119it [00:57,  1.77it/s]\u001b[A\n",
      "batch 120, training loss: 3.3647: : 119it [00:57,  1.77it/s]\u001b[A\n",
      "batch 120, training loss: 3.3647: : 120it [00:57,  1.79it/s]\u001b[A\n",
      "batch 121, training loss: 3.6162: : 120it [00:58,  1.79it/s]\u001b[A\n",
      "batch 121, training loss: 3.6162: : 121it [00:58,  1.87it/s]\u001b[A\n",
      "batch 122, training loss: 3.2699: : 121it [00:59,  1.87it/s]\u001b[A\n",
      "batch 122, training loss: 3.2699: : 122it [00:59,  1.69it/s]\u001b[A\n",
      "batch 123, training loss: 3.4513: : 122it [00:59,  1.69it/s]\u001b[A\n",
      "batch 123, training loss: 3.4513: : 123it [00:59,  1.73it/s]\u001b[A\n",
      "batch 124, training loss: 3.4504: : 123it [01:00,  1.73it/s]\u001b[A\n",
      "batch 124, training loss: 3.4504: : 124it [01:00,  1.77it/s]\u001b[A\n",
      "batch 125, training loss: 3.4565: : 124it [01:00,  1.77it/s]\u001b[A\n",
      "batch 125, training loss: 3.4565: : 125it [01:00,  1.81it/s]\u001b[A\n",
      "batch 126, training loss: 3.5431: : 125it [01:01,  1.81it/s]\u001b[A\n",
      "batch 126, training loss: 3.5431: : 126it [01:01,  1.67it/s]\u001b[A\n",
      "batch 127, training loss: 3.2239: : 126it [01:01,  1.67it/s]\u001b[A\n",
      "batch 127, training loss: 3.2239: : 127it [01:01,  1.78it/s]\u001b[A\n",
      "batch 128, training loss: 3.4415: : 127it [01:02,  1.78it/s]\u001b[A\n",
      "batch 128, training loss: 3.4415: : 128it [01:02,  1.78it/s]\u001b[A\n",
      "batch 129, training loss: 3.3943: : 128it [01:03,  1.78it/s]\u001b[A\n",
      "batch 129, training loss: 3.3943: : 129it [01:03,  1.79it/s]\u001b[A\n",
      "batch 130, training loss: 3.3843: : 129it [01:03,  1.79it/s]\u001b[A\n",
      "batch 130, training loss: 3.3843: : 130it [01:03,  1.72it/s]\u001b[A\n",
      "batch 131, training loss: 3.572: : 130it [01:04,  1.72it/s] \u001b[A\n",
      "batch 131, training loss: 3.572: : 131it [01:04,  1.73it/s]\u001b[A\n",
      "batch 132, training loss: 3.3763: : 131it [01:04,  1.73it/s]\u001b[A\n",
      "batch 132, training loss: 3.3763: : 132it [01:04,  1.79it/s]\u001b[A\n",
      "batch 133, training loss: 3.2666: : 132it [01:05,  1.79it/s]\u001b[A\n",
      "batch 133, training loss: 3.2666: : 133it [01:05,  1.85it/s]\u001b[A\n",
      "batch 134, training loss: 3.4512: : 133it [01:05,  1.85it/s]\u001b[A\n",
      "batch 134, training loss: 3.4512: : 134it [01:05,  1.78it/s]\u001b[A\n",
      "batch 135, training loss: 3.3458: : 134it [01:06,  1.78it/s]\u001b[A\n",
      "batch 135, training loss: 3.3458: : 135it [01:06,  1.79it/s]\u001b[A\n",
      "batch 136, training loss: 3.2833: : 135it [01:06,  1.79it/s]\u001b[A\n",
      "batch 136, training loss: 3.2833: : 136it [01:06,  1.85it/s]\u001b[A\n",
      "batch 137, training loss: 3.4591: : 136it [01:07,  1.85it/s]\u001b[A\n",
      "batch 137, training loss: 3.4591: : 137it [01:07,  1.86it/s]\u001b[A\n",
      "batch 138, training loss: 3.4754: : 137it [01:08,  1.86it/s]\u001b[A\n",
      "batch 138, training loss: 3.4754: : 138it [01:08,  1.76it/s]\u001b[A\n",
      "batch 139, training loss: 3.2516: : 138it [01:08,  1.76it/s]\u001b[A\n",
      "batch 139, training loss: 3.2516: : 139it [01:08,  1.77it/s]\u001b[A\n",
      "batch 140, training loss: 3.356: : 139it [01:09,  1.77it/s] \u001b[A\n",
      "batch 140, training loss: 3.356: : 140it [01:09,  1.79it/s]\u001b[A\n",
      "batch 141, training loss: 3.36: : 140it [01:09,  1.79it/s] \u001b[A\n",
      "batch 141, training loss: 3.36: : 141it [01:09,  1.87it/s]\u001b[A\n",
      "batch 142, training loss: 3.3893: : 141it [01:10,  1.87it/s]\u001b[A\n",
      "batch 142, training loss: 3.3893: : 142it [01:10,  1.71it/s]\u001b[A\n",
      "batch 143, training loss: 3.2279: : 142it [01:10,  1.71it/s]\u001b[A\n",
      "batch 143, training loss: 3.2279: : 143it [01:10,  1.97it/s]\u001b[A\n",
      "batch 144, training loss: 3.2763: : 143it [01:11,  1.97it/s]\u001b[A\n",
      "batch 144, training loss: 3.2763: : 144it [01:11,  2.07it/s]\u001b[A\n",
      "batch 145, training loss: 3.3992: : 144it [01:11,  2.07it/s]\u001b[A\n",
      "batch 145, training loss: 3.3992: : 145it [01:11,  2.33it/s]\u001b[A\n",
      "batch 146, training loss: 3.3793: : 145it [01:11,  2.33it/s]\u001b[A\n",
      "batch 146, training loss: 3.3793: : 146it [01:11,  2.32it/s]\u001b[A\n",
      "batch 147, training loss: 3.282: : 146it [01:12,  2.32it/s] \u001b[A\n",
      "batch 147, training loss: 3.282: : 147it [01:12,  2.24it/s]\u001b[A\n",
      "batch 148, training loss: 3.3717: : 147it [01:12,  2.24it/s]\u001b[A\n",
      "batch 148, training loss: 3.3717: : 148it [01:12,  2.12it/s]\u001b[A\n",
      "batch 149, training loss: 3.4517: : 148it [01:13,  2.12it/s]\u001b[A\n",
      "batch 149, training loss: 3.4517: : 149it [01:13,  2.12it/s]\u001b[A\n",
      "batch 150, training loss: 3.4381: : 149it [01:13,  2.12it/s]\u001b[A\n",
      "batch 150, training loss: 3.4381: : 150it [01:13,  2.01it/s]\u001b[A\n",
      "batch 151, training loss: 3.3712: : 150it [01:14,  2.01it/s]\u001b[A\n",
      "batch 151, training loss: 3.3712: : 151it [01:14,  1.87it/s]\u001b[A\n",
      "batch 152, training loss: 3.2875: : 151it [01:15,  1.87it/s]\u001b[A\n",
      "batch 152, training loss: 3.2875: : 152it [01:15,  1.88it/s]\u001b[A\n",
      "batch 153, training loss: 3.316: : 152it [01:15,  1.88it/s] \u001b[A\n",
      "batch 153, training loss: 3.316: : 153it [01:15,  1.93it/s]\u001b[A\n",
      "batch 154, training loss: 3.4364: : 153it [01:16,  1.93it/s]\u001b[A\n",
      "batch 154, training loss: 3.4364: : 154it [01:16,  1.88it/s]\u001b[A\n",
      "batch 155, training loss: 3.569: : 154it [01:16,  1.88it/s] \u001b[A\n",
      "batch 155, training loss: 3.569: : 155it [01:16,  1.70it/s]\u001b[A\n",
      "batch 156, training loss: 3.1575: : 155it [01:17,  1.70it/s]\u001b[A\n",
      "batch 156, training loss: 3.1575: : 156it [01:17,  1.75it/s]\u001b[A\n",
      "batch 157, training loss: 3.3811: : 156it [01:17,  1.75it/s]\u001b[A\n",
      "batch 157, training loss: 3.3811: : 157it [01:17,  1.80it/s]\u001b[A\n",
      "batch 158, training loss: 3.3566: : 157it [01:18,  1.80it/s]\u001b[A\n",
      "batch 158, training loss: 3.3566: : 158it [01:18,  1.82it/s]\u001b[A\n",
      "batch 159, training loss: 3.2809: : 158it [01:19,  1.82it/s]\u001b[A\n",
      "batch 159, training loss: 3.2809: : 159it [01:19,  1.70it/s]\u001b[A\n",
      "batch 160, training loss: 3.4419: : 159it [01:19,  1.70it/s]\u001b[A\n",
      "batch 160, training loss: 3.4419: : 160it [01:19,  1.76it/s]\u001b[A\n",
      "batch 161, training loss: 3.3376: : 160it [01:20,  1.76it/s]\u001b[A\n",
      "batch 161, training loss: 3.3376: : 161it [01:20,  1.78it/s]\u001b[A\n",
      "batch 162, training loss: 3.2968: : 161it [01:20,  1.78it/s]\u001b[A\n",
      "batch 162, training loss: 3.2968: : 162it [01:20,  1.88it/s]\u001b[A\n",
      "batch 163, training loss: 3.5537: : 162it [01:21,  1.88it/s]\u001b[A\n",
      "batch 163, training loss: 3.5537: : 163it [01:21,  1.83it/s]\u001b[A\n",
      "batch 164, training loss: 3.3634: : 163it [01:21,  1.83it/s]\u001b[A\n",
      "batch 164, training loss: 3.3634: : 164it [01:21,  1.78it/s]\u001b[A\n",
      "batch 165, training loss: 3.334: : 164it [01:22,  1.78it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 165, training loss: 3.334: : 165it [01:22,  1.81it/s]\u001b[A\n",
      "batch 166, training loss: 3.3032: : 165it [01:22,  1.81it/s]\u001b[A\n",
      "batch 166, training loss: 3.3032: : 166it [01:22,  1.90it/s]\u001b[A\n",
      "batch 167, training loss: 3.4399: : 166it [01:23,  1.90it/s]\u001b[A\n",
      "batch 167, training loss: 3.4399: : 167it [01:23,  1.86it/s]\u001b[A\n",
      "batch 168, training loss: 3.4135: : 167it [01:23,  1.86it/s]\u001b[A\n",
      "batch 168, training loss: 3.4135: : 168it [01:23,  1.88it/s]\u001b[A\n",
      "batch 169, training loss: 3.4547: : 168it [01:24,  1.88it/s]\u001b[A\n",
      "batch 169, training loss: 3.4547: : 169it [01:24,  1.76it/s]\u001b[A\n",
      "batch 170, training loss: 3.3546: : 169it [01:25,  1.76it/s]\u001b[A\n",
      "batch 170, training loss: 3.3546: : 170it [01:25,  1.84it/s]\u001b[A\n",
      "batch 171, training loss: 2.323: : 170it [01:25,  1.84it/s] \u001b[A\n",
      "batch 171, training loss: 2.323: : 171it [01:25,  2.30it/s]\u001b[A\n",
      "batch 172, training loss: 3.5603: : 171it [01:25,  2.30it/s]\u001b[A\n",
      "batch 172, training loss: 3.5603: : 172it [01:25,  2.10it/s]\u001b[A\n",
      "batch 173, training loss: 3.4034: : 172it [01:26,  2.10it/s]\u001b[A\n",
      "batch 173, training loss: 3.4034: : 173it [01:26,  1.92it/s]\u001b[A\n",
      "batch 174, training loss: 3.5863: : 173it [01:27,  1.92it/s]\u001b[A\n",
      "batch 174, training loss: 3.5863: : 174it [01:27,  1.69it/s]\u001b[A\n",
      "batch 175, training loss: 3.4911: : 174it [01:27,  1.69it/s]\u001b[A\n",
      "batch 175, training loss: 3.4911: : 175it [01:27,  1.67it/s]\u001b[A\n",
      "batch 176, training loss: 3.4623: : 175it [01:28,  1.67it/s]\u001b[A\n",
      "batch 176, training loss: 3.4623: : 176it [01:28,  1.67it/s]\u001b[A\n",
      "batch 177, training loss: 3.4204: : 176it [01:28,  1.67it/s]\u001b[A\n",
      "batch 177, training loss: 3.4204: : 177it [01:28,  1.69it/s]\u001b[A\n",
      "batch 178, training loss: 3.4337: : 177it [01:29,  1.69it/s]\u001b[A\n",
      "batch 178, training loss: 3.4337: : 178it [01:29,  1.60it/s]\u001b[A\n",
      "batch 179, training loss: 3.5186: : 178it [01:30,  1.60it/s]\u001b[A\n",
      "batch 179, training loss: 3.5186: : 179it [01:30,  1.65it/s]\u001b[A\n",
      "batch 180, training loss: 3.5228: : 179it [01:30,  1.65it/s]\u001b[A\n",
      "batch 180, training loss: 3.5228: : 180it [01:30,  1.63it/s]\u001b[A\n",
      "batch 181, training loss: 3.6028: : 180it [01:31,  1.63it/s]\u001b[A\n",
      "batch 181, training loss: 3.6028: : 181it [01:31,  1.64it/s]\u001b[A\n",
      "batch 182, training loss: 3.5074: : 181it [01:32,  1.64it/s]\u001b[A\n",
      "batch 182, training loss: 3.5074: : 182it [01:32,  1.54it/s]\u001b[A\n",
      "batch 183, training loss: 3.6301: : 182it [01:32,  1.54it/s]\u001b[A\n",
      "batch 183, training loss: 3.6301: : 183it [01:32,  1.57it/s]\u001b[A\n",
      "batch 184, training loss: 3.3589: : 183it [01:33,  1.57it/s]\u001b[A\n",
      "batch 184, training loss: 3.3589: : 184it [01:33,  1.61it/s]\u001b[A\n",
      "batch 185, training loss: 3.4039: : 184it [01:33,  1.61it/s]\u001b[A\n",
      "batch 185, training loss: 3.4039: : 185it [01:33,  1.65it/s]\u001b[A\n",
      "batch 186, training loss: 3.5665: : 185it [01:34,  1.65it/s]\u001b[A\n",
      "batch 186, training loss: 3.5665: : 186it [01:34,  1.56it/s]\u001b[A\n",
      "batch 187, training loss: 3.5807: : 186it [01:35,  1.56it/s]\u001b[A\n",
      "batch 187, training loss: 3.5807: : 187it [01:35,  1.58it/s]\u001b[A\n",
      "batch 188, training loss: 3.6409: : 187it [01:35,  1.58it/s]\u001b[A\n",
      "batch 188, training loss: 3.6409: : 188it [01:35,  1.62it/s]\u001b[A\n",
      "batch 189, training loss: 3.6838: : 188it [01:36,  1.62it/s]\u001b[A\n",
      "batch 189, training loss: 3.6838: : 189it [01:36,  1.63it/s]\u001b[A\n",
      "batch 190, training loss: 3.3875: : 189it [01:37,  1.63it/s]\u001b[A\n",
      "batch 190, training loss: 3.3875: : 190it [01:37,  1.51it/s]\u001b[A\n",
      "batch 191, training loss: 3.397: : 190it [01:37,  1.51it/s] \u001b[A\n",
      "batch 191, training loss: 3.397: : 191it [01:37,  1.54it/s]\u001b[A\n",
      "batch 192, training loss: 3.4653: : 191it [01:38,  1.54it/s]\u001b[A\n",
      "batch 192, training loss: 3.4653: : 192it [01:38,  1.58it/s]\u001b[A\n",
      "batch 193, training loss: 3.3871: : 192it [01:39,  1.58it/s]\u001b[A\n",
      "batch 193, training loss: 3.3871: : 193it [01:39,  1.60it/s]\u001b[A\n",
      "batch 194, training loss: 3.4934: : 193it [01:39,  1.60it/s]\u001b[A\n",
      "batch 194, training loss: 3.4934: : 194it [01:39,  1.55it/s]\u001b[A\n",
      "batch 195, training loss: 3.4127: : 194it [01:40,  1.55it/s]\u001b[A\n",
      "batch 195, training loss: 3.4127: : 195it [01:40,  1.62it/s]\u001b[A\n",
      "batch 196, training loss: 3.3384: : 195it [01:40,  1.62it/s]\u001b[A\n",
      "batch 196, training loss: 3.3384: : 196it [01:40,  1.62it/s]\u001b[A\n",
      "batch 197, training loss: 3.5414: : 196it [01:41,  1.62it/s]\u001b[A\n",
      "batch 197, training loss: 3.5414: : 197it [01:41,  1.64it/s]\u001b[A\n",
      "batch 198, training loss: 3.455: : 197it [01:42,  1.64it/s] \u001b[A\n",
      "batch 198, training loss: 3.455: : 198it [01:42,  1.61it/s]\u001b[A\n",
      "batch 199, training loss: 3.4023: : 198it [01:42,  1.61it/s]\u001b[A\n",
      "batch 199, training loss: 3.4023: : 199it [01:42,  1.61it/s]\u001b[A\n",
      "batch 200, training loss: 3.5553: : 199it [01:43,  1.61it/s]\u001b[A\n",
      "batch 200, training loss: 3.5553: : 200it [01:43,  1.64it/s]\u001b[A\n",
      "batch 201, training loss: 3.3843: : 200it [01:44,  1.64it/s]\u001b[A\n",
      "batch 201, training loss: 3.3843: : 201it [01:44,  1.65it/s]\u001b[A\n",
      "batch 202, training loss: 3.2628: : 201it [01:44,  1.65it/s]\u001b[A\n",
      "batch 202, training loss: 3.2628: : 202it [01:44,  1.58it/s]\u001b[A\n",
      "batch 203, training loss: 3.4654: : 202it [01:45,  1.58it/s]\u001b[A\n",
      "batch 203, training loss: 3.4654: : 203it [01:45,  1.65it/s]\u001b[A\n",
      "batch 204, training loss: 3.5095: : 203it [01:45,  1.65it/s]\u001b[A\n",
      "batch 204, training loss: 3.5095: : 204it [01:45,  1.64it/s]\u001b[A\n",
      "batch 205, training loss: 3.4637: : 204it [01:46,  1.64it/s]\u001b[A\n",
      "batch 205, training loss: 3.4637: : 205it [01:46,  1.65it/s]\u001b[A\n",
      "batch 206, training loss: 3.568: : 205it [01:47,  1.65it/s] \u001b[A\n",
      "batch 206, training loss: 3.568: : 206it [01:47,  1.59it/s]\u001b[A\n",
      "batch 207, training loss: 3.3773: : 206it [01:47,  1.59it/s]\u001b[A\n",
      "batch 207, training loss: 3.3773: : 207it [01:47,  1.62it/s]\u001b[A\n",
      "batch 208, training loss: 3.4207: : 207it [01:48,  1.62it/s]\u001b[A\n",
      "batch 208, training loss: 3.4207: : 208it [01:48,  1.66it/s]\u001b[A\n",
      "batch 209, training loss: 3.4054: : 208it [01:48,  1.66it/s]\u001b[A\n",
      "batch 209, training loss: 3.4054: : 209it [01:48,  1.63it/s]\u001b[A\n",
      "batch 210, training loss: 3.4615: : 209it [01:49,  1.63it/s]\u001b[A\n",
      "batch 210, training loss: 3.4615: : 210it [01:49,  1.53it/s]\u001b[A\n",
      "batch 211, training loss: 3.3929: : 210it [01:50,  1.53it/s]\u001b[A\n",
      "batch 211, training loss: 3.3929: : 211it [01:50,  1.55it/s]\u001b[A\n",
      "batch 212, training loss: 3.4185: : 211it [01:50,  1.55it/s]\u001b[A\n",
      "batch 212, training loss: 3.4185: : 212it [01:50,  1.60it/s]\u001b[A\n",
      "batch 213, training loss: 3.6149: : 212it [01:51,  1.60it/s]\u001b[A\n",
      "batch 213, training loss: 3.6149: : 213it [01:51,  1.71it/s]\u001b[A\n",
      "batch 214, training loss: 3.4799: : 213it [01:52,  1.71it/s]\u001b[A\n",
      "batch 214, training loss: 3.4799: : 214it [01:52,  1.55it/s]\u001b[A\n",
      "batch 215, training loss: 3.3331: : 214it [01:52,  1.55it/s]\u001b[A\n",
      "batch 215, training loss: 3.3331: : 215it [01:52,  1.60it/s]\u001b[A\n",
      "batch 216, training loss: 3.4552: : 215it [01:53,  1.60it/s]\u001b[A\n",
      "batch 216, training loss: 3.4552: : 216it [01:53,  1.61it/s]\u001b[A\n",
      "batch 217, training loss: 3.5871: : 216it [01:53,  1.61it/s]\u001b[A\n",
      "batch 217, training loss: 3.5871: : 217it [01:53,  1.62it/s]\u001b[A\n",
      "batch 218, training loss: 3.5165: : 217it [01:54,  1.62it/s]\u001b[A\n",
      "batch 218, training loss: 3.5165: : 218it [01:54,  1.51it/s]\u001b[A\n",
      "batch 219, training loss: 3.6918: : 218it [01:55,  1.51it/s]\u001b[A\n",
      "batch 219, training loss: 3.6918: : 219it [01:55,  1.55it/s]\u001b[A\n",
      "batch 220, training loss: 3.6202: : 219it [01:55,  1.55it/s]\u001b[A\n",
      "batch 220, training loss: 3.6202: : 220it [01:55,  1.59it/s]\u001b[A\n",
      "batch 221, training loss: 3.4752: : 220it [01:56,  1.59it/s]\u001b[A\n",
      "batch 221, training loss: 3.4752: : 221it [01:56,  1.65it/s]\u001b[A\n",
      "batch 222, training loss: 3.5159: : 221it [01:57,  1.65it/s]\u001b[A\n",
      "batch 222, training loss: 3.5159: : 222it [01:57,  1.56it/s]\u001b[A\n",
      "batch 223, training loss: 3.526: : 222it [01:57,  1.56it/s] \u001b[A\n",
      "batch 223, training loss: 3.526: : 223it [01:57,  1.58it/s]\u001b[A\n",
      "batch 224, training loss: 3.3931: : 223it [01:58,  1.58it/s]\u001b[A\n",
      "batch 224, training loss: 3.3931: : 224it [01:58,  1.61it/s]\u001b[A\n",
      "batch 225, training loss: 3.5596: : 224it [01:59,  1.61it/s]\u001b[A\n",
      "batch 225, training loss: 3.5596: : 225it [01:59,  1.62it/s]\u001b[A\n",
      "batch 226, training loss: 3.542: : 225it [01:59,  1.62it/s] \u001b[A\n",
      "batch 226, training loss: 3.542: : 226it [01:59,  1.51it/s]\u001b[A\n",
      "batch 227, training loss: 3.4047: : 226it [02:00,  1.51it/s]\u001b[A\n",
      "batch 227, training loss: 3.4047: : 227it [02:00,  1.54it/s]\u001b[A\n",
      "batch 228, training loss: 3.4856: : 227it [02:00,  1.54it/s]\u001b[A\n",
      "batch 228, training loss: 3.4856: : 228it [02:00,  1.59it/s]\u001b[A\n",
      "batch 229, training loss: 3.4534: : 228it [02:01,  1.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 229, training loss: 3.4534: : 229it [02:01,  1.61it/s]\u001b[A\n",
      "batch 230, training loss: 3.5971: : 229it [02:02,  1.61it/s]\u001b[A\n",
      "batch 230, training loss: 3.5971: : 230it [02:02,  1.50it/s]\u001b[A\n",
      "batch 231, training loss: 3.6322: : 230it [02:02,  1.50it/s]\u001b[A\n",
      "batch 231, training loss: 3.6322: : 231it [02:02,  1.54it/s]\u001b[A\n",
      "batch 232, training loss: 3.4464: : 231it [02:03,  1.54it/s]\u001b[A\n",
      "batch 232, training loss: 3.4464: : 232it [02:03,  1.58it/s]\u001b[A\n",
      "batch 233, training loss: 3.4928: : 232it [02:04,  1.58it/s]\u001b[A\n",
      "batch 233, training loss: 3.4928: : 233it [02:04,  1.59it/s]\u001b[A\n",
      "batch 234, training loss: 3.6483: : 233it [02:04,  1.59it/s]\u001b[A\n",
      "batch 234, training loss: 3.6483: : 234it [02:04,  1.50it/s]\u001b[A\n",
      "batch 235, training loss: 3.4748: : 234it [02:05,  1.50it/s]\u001b[A\n",
      "batch 235, training loss: 3.4748: : 235it [02:05,  1.53it/s]\u001b[A\n",
      "batch 236, training loss: 3.4989: : 235it [02:06,  1.53it/s]\u001b[A\n",
      "batch 236, training loss: 3.4989: : 236it [02:06,  1.57it/s]\u001b[A\n",
      "batch 237, training loss: 3.397: : 236it [02:06,  1.57it/s] \u001b[A\n",
      "batch 237, training loss: 3.397: : 237it [02:06,  1.60it/s]\u001b[A\n",
      "batch 238, training loss: 3.5576: : 237it [02:07,  1.60it/s]\u001b[A\n",
      "batch 238, training loss: 3.5576: : 238it [02:07,  1.50it/s]\u001b[A\n",
      "batch 239, training loss: 3.4613: : 238it [02:08,  1.50it/s]\u001b[A\n",
      "batch 239, training loss: 3.4613: : 239it [02:08,  1.53it/s]\u001b[A\n",
      "batch 240, training loss: 3.4797: : 239it [02:08,  1.53it/s]\u001b[A\n",
      "batch 240, training loss: 3.4797: : 240it [02:08,  1.58it/s]\u001b[A\n",
      "batch 241, training loss: 3.5017: : 240it [02:09,  1.58it/s]\u001b[A\n",
      "batch 241, training loss: 3.5017: : 241it [02:09,  1.62it/s]\u001b[A\n",
      "batch 242, training loss: 3.3689: : 241it [02:10,  1.62it/s]\u001b[A\n",
      "batch 242, training loss: 3.3689: : 242it [02:10,  1.50it/s]\u001b[A\n",
      "batch 243, training loss: 3.5097: : 242it [02:10,  1.50it/s]\u001b[A\n",
      "batch 243, training loss: 3.5097: : 243it [02:10,  1.53it/s]\u001b[A\n",
      "batch 244, training loss: 3.4044: : 243it [02:11,  1.53it/s]\u001b[A\n",
      "batch 244, training loss: 3.4044: : 244it [02:11,  1.57it/s]\u001b[A\n",
      "batch 245, training loss: 3.5517: : 244it [02:11,  1.57it/s]\u001b[A\n",
      "batch 245, training loss: 3.5517: : 245it [02:11,  1.58it/s]\u001b[A\n",
      "batch 246, training loss: 3.479: : 245it [02:12,  1.58it/s] \u001b[A\n",
      "batch 246, training loss: 3.479: : 246it [02:12,  1.50it/s]\u001b[A\n",
      "batch 247, training loss: 3.479: : 246it [02:13,  1.50it/s]\u001b[A\n",
      "batch 247, training loss: 3.479: : 247it [02:13,  1.53it/s]\u001b[A\n",
      "batch 248, training loss: 3.4092: : 247it [02:13,  1.53it/s]\u001b[A\n",
      "batch 248, training loss: 3.4092: : 248it [02:13,  1.58it/s]\u001b[A\n",
      "batch 249, training loss: 3.3698: : 248it [02:14,  1.58it/s]\u001b[A\n",
      "batch 249, training loss: 3.3698: : 249it [02:14,  1.61it/s]\u001b[A\n",
      "batch 250, training loss: 3.5609: : 249it [02:15,  1.61it/s]\u001b[A\n",
      "batch 250, training loss: 3.5609: : 250it [02:15,  1.51it/s]\u001b[A\n",
      "batch 251, training loss: 3.5511: : 250it [02:15,  1.51it/s]\u001b[A\n",
      "batch 251, training loss: 3.5511: : 251it [02:15,  1.54it/s]\u001b[A\n",
      "batch 252, training loss: 2.6402: : 251it [02:16,  1.54it/s]\u001b[A\n",
      "batch 252, training loss: 2.6402: : 252it [02:16,  1.93it/s]\u001b[A\n",
      "batch 253, training loss: 3.414: : 252it [02:16,  1.93it/s] \u001b[A\n",
      "batch 253, training loss: 3.414: : 253it [02:16,  1.82it/s]\u001b[A\n",
      "batch 254, training loss: 3.5359: : 253it [02:17,  1.82it/s]\u001b[A\n",
      "batch 254, training loss: 3.5359: : 254it [02:17,  1.84it/s]\u001b[A\n",
      "batch 255, training loss: 3.3637: : 254it [02:17,  1.84it/s]\u001b[A\n",
      "batch 255, training loss: 3.3637: : 255it [02:17,  2.01it/s]\u001b[A\n",
      "batch 256, training loss: 3.4064: : 255it [02:18,  2.01it/s]\u001b[A\n",
      "batch 256, training loss: 3.4064: : 256it [02:18,  1.86it/s]\u001b[A\n",
      "batch 257, training loss: 3.4884: : 256it [02:18,  1.86it/s]\u001b[A\n",
      "batch 257, training loss: 3.4884: : 257it [02:18,  1.76it/s]\u001b[A\n",
      "batch 258, training loss: 3.5636: : 257it [02:19,  1.76it/s]\u001b[A\n",
      "batch 258, training loss: 3.5636: : 258it [02:19,  1.68it/s]\u001b[A\n",
      "batch 259, training loss: 3.4541: : 258it [02:20,  1.68it/s]\u001b[A\n",
      "batch 259, training loss: 3.4541: : 259it [02:20,  1.54it/s]\u001b[A\n",
      "batch 260, training loss: 3.4405: : 259it [02:20,  1.54it/s]\u001b[A\n",
      "batch 260, training loss: 3.4405: : 260it [02:20,  1.54it/s]\u001b[A\n",
      "batch 261, training loss: 3.5041: : 260it [02:21,  1.54it/s]\u001b[A\n",
      "batch 261, training loss: 3.5041: : 261it [02:21,  1.57it/s]\u001b[A\n",
      "batch 262, training loss: 3.4441: : 261it [02:22,  1.57it/s]\u001b[A\n",
      "batch 262, training loss: 3.4441: : 262it [02:22,  1.55it/s]\u001b[A\n",
      "batch 263, training loss: 3.4215: : 262it [02:22,  1.55it/s]\u001b[A\n",
      "batch 263, training loss: 3.4215: : 263it [02:22,  1.46it/s]\u001b[A\n",
      "batch 264, training loss: 3.5483: : 263it [02:23,  1.46it/s]\u001b[A\n",
      "batch 264, training loss: 3.5483: : 264it [02:23,  1.48it/s]\u001b[A\n",
      "batch 265, training loss: 3.3475: : 264it [02:24,  1.48it/s]\u001b[A\n",
      "batch 265, training loss: 3.3475: : 265it [02:24,  1.46it/s]\u001b[A\n",
      "batch 266, training loss: 3.5335: : 265it [02:25,  1.46it/s]\u001b[A\n",
      "batch 266, training loss: 3.5335: : 266it [02:25,  1.47it/s]\u001b[A\n",
      "batch 267, training loss: 3.5027: : 266it [02:25,  1.47it/s]\u001b[A\n",
      "batch 267, training loss: 3.5027: : 267it [02:25,  1.38it/s]\u001b[A\n",
      "batch 268, training loss: 3.3733: : 267it [02:26,  1.38it/s]\u001b[A\n",
      "batch 268, training loss: 3.3733: : 268it [02:26,  1.46it/s]\u001b[A\n",
      "batch 269, training loss: 3.4287: : 268it [02:27,  1.46it/s]\u001b[A\n",
      "batch 269, training loss: 3.4287: : 269it [02:27,  1.47it/s]\u001b[A\n",
      "batch 270, training loss: 3.4333: : 269it [02:27,  1.47it/s]\u001b[A\n",
      "batch 270, training loss: 3.4333: : 270it [02:27,  1.51it/s]\u001b[A\n",
      "batch 271, training loss: 3.4138: : 270it [02:28,  1.51it/s]\u001b[A\n",
      "batch 271, training loss: 3.4138: : 271it [02:28,  1.40it/s]\u001b[A\n",
      "batch 272, training loss: 3.4354: : 271it [02:29,  1.40it/s]\u001b[A\n",
      "batch 272, training loss: 3.4354: : 272it [02:29,  1.46it/s]\u001b[A\n",
      "batch 273, training loss: 3.5217: : 272it [02:29,  1.46it/s]\u001b[A\n",
      "batch 273, training loss: 3.5217: : 273it [02:29,  1.48it/s]\u001b[A\n",
      "batch 274, training loss: 3.513: : 273it [02:30,  1.48it/s] \u001b[A\n",
      "batch 274, training loss: 3.513: : 274it [02:30,  1.47it/s]\u001b[A\n",
      "batch 275, training loss: 3.3991: : 274it [02:31,  1.47it/s]\u001b[A\n",
      "batch 275, training loss: 3.3991: : 275it [02:31,  1.47it/s]\u001b[A\n",
      "batch 276, training loss: 3.2926: : 275it [02:31,  1.47it/s]\u001b[A\n",
      "batch 276, training loss: 3.2926: : 276it [02:31,  1.47it/s]\u001b[A\n",
      "batch 277, training loss: 3.4463: : 276it [02:32,  1.47it/s]\u001b[A\n",
      "batch 277, training loss: 3.4463: : 277it [02:32,  1.48it/s]\u001b[A\n",
      "batch 278, training loss: 3.3837: : 277it [02:33,  1.48it/s]\u001b[A\n",
      "batch 278, training loss: 3.3837: : 278it [02:33,  1.51it/s]\u001b[A\n",
      "batch 279, training loss: 3.3848: : 278it [02:33,  1.51it/s]\u001b[A\n",
      "batch 279, training loss: 3.3848: : 279it [02:33,  1.51it/s]\u001b[A\n",
      "batch 280, training loss: 3.2911: : 279it [02:34,  1.51it/s]\u001b[A\n",
      "batch 280, training loss: 3.2911: : 280it [02:34,  1.44it/s]\u001b[A\n",
      "batch 281, training loss: 3.4014: : 280it [02:35,  1.44it/s]\u001b[A\n",
      "batch 281, training loss: 3.4014: : 281it [02:35,  1.47it/s]\u001b[A\n",
      "batch 282, training loss: 3.2989: : 281it [02:35,  1.47it/s]\u001b[A\n",
      "batch 282, training loss: 3.2989: : 282it [02:35,  1.51it/s]\u001b[A\n",
      "batch 283, training loss: 3.3961: : 282it [02:36,  1.51it/s]\u001b[A\n",
      "batch 283, training loss: 3.3961: : 283it [02:36,  1.50it/s]\u001b[A\n",
      "batch 284, training loss: 3.3999: : 283it [02:37,  1.50it/s]\u001b[A\n",
      "batch 284, training loss: 3.3999: : 284it [02:37,  1.42it/s]\u001b[A\n",
      "batch 285, training loss: 3.4263: : 284it [02:37,  1.42it/s]\u001b[A\n",
      "batch 285, training loss: 3.4263: : 285it [02:37,  1.46it/s]\u001b[A\n",
      "batch 286, training loss: 3.584: : 285it [02:38,  1.46it/s] \u001b[A\n",
      "batch 286, training loss: 3.584: : 286it [02:38,  1.49it/s]\u001b[A\n",
      "batch 287, training loss: 3.3061: : 286it [02:39,  1.49it/s]\u001b[A\n",
      "batch 287, training loss: 3.3061: : 287it [02:39,  1.50it/s]\u001b[A\n",
      "batch 288, training loss: 3.3371: : 287it [02:40,  1.50it/s]\u001b[A\n",
      "batch 288, training loss: 3.3371: : 288it [02:40,  1.43it/s]\u001b[A\n",
      "batch 289, training loss: 3.5012: : 288it [02:40,  1.43it/s]\u001b[A\n",
      "batch 289, training loss: 3.5012: : 289it [02:40,  1.46it/s]\u001b[A\n",
      "batch 290, training loss: 3.28: : 289it [02:41,  1.46it/s]  \u001b[A\n",
      "batch 290, training loss: 3.28: : 290it [02:41,  1.49it/s]\u001b[A\n",
      "batch 291, training loss: 3.5376: : 290it [02:41,  1.49it/s]\u001b[A\n",
      "batch 291, training loss: 3.5376: : 291it [02:41,  1.51it/s]\u001b[A\n",
      "batch 292, training loss: 3.3537: : 291it [02:42,  1.51it/s]\u001b[A\n",
      "batch 292, training loss: 3.3537: : 292it [02:42,  1.43it/s]\u001b[A\n",
      "batch 293, training loss: 3.4505: : 292it [02:43,  1.43it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 293, training loss: 3.4505: : 293it [02:43,  1.46it/s]\u001b[A\n",
      "batch 294, training loss: 3.5209: : 293it [02:44,  1.46it/s]\u001b[A\n",
      "batch 294, training loss: 3.5209: : 294it [02:44,  1.50it/s]\u001b[A\n",
      "batch 295, training loss: 3.4749: : 294it [02:44,  1.50it/s]\u001b[A\n",
      "batch 295, training loss: 3.4749: : 295it [02:44,  1.51it/s]\u001b[A\n",
      "batch 296, training loss: 3.336: : 295it [02:45,  1.51it/s] \u001b[A\n",
      "batch 296, training loss: 3.336: : 296it [02:45,  1.42it/s]\u001b[A\n",
      "batch 297, training loss: 3.4956: : 296it [02:46,  1.42it/s]\u001b[A\n",
      "batch 297, training loss: 3.4956: : 297it [02:46,  1.46it/s]\u001b[A\n",
      "batch 298, training loss: 3.399: : 297it [02:46,  1.46it/s] \u001b[A\n",
      "batch 298, training loss: 3.399: : 298it [02:46,  1.48it/s]\u001b[A\n",
      "batch 299, training loss: 3.4661: : 298it [02:47,  1.48it/s]\u001b[A\n",
      "batch 299, training loss: 3.4661: : 299it [02:47,  1.50it/s]\u001b[A\n",
      "batch 300, training loss: 3.5371: : 299it [02:48,  1.50it/s]\u001b[A\n",
      "batch 300, training loss: 3.5371: : 300it [02:48,  1.37it/s]\u001b[A\n",
      "batch 301, training loss: 3.4722: : 300it [02:48,  1.37it/s]\u001b[A\n",
      "batch 301, training loss: 3.4722: : 301it [02:48,  1.42it/s]\u001b[A\n",
      "batch 302, training loss: 3.4919: : 301it [02:49,  1.42it/s]\u001b[A\n",
      "batch 302, training loss: 3.4919: : 302it [02:49,  1.44it/s]\u001b[A\n",
      "batch 303, training loss: 3.3879: : 302it [02:50,  1.44it/s]\u001b[A\n",
      "batch 303, training loss: 3.3879: : 303it [02:50,  1.48it/s]\u001b[A\n",
      "batch 304, training loss: 3.4729: : 303it [02:51,  1.48it/s]\u001b[A\n",
      "batch 304, training loss: 3.4729: : 304it [02:51,  1.39it/s]\u001b[A\n",
      "batch 305, training loss: 3.4116: : 304it [02:51,  1.39it/s]\u001b[A\n",
      "batch 305, training loss: 3.4116: : 305it [02:51,  1.45it/s]\u001b[A\n",
      "batch 306, training loss: 3.4747: : 305it [02:52,  1.45it/s]\u001b[A\n",
      "batch 306, training loss: 3.4747: : 306it [02:52,  1.47it/s]\u001b[A\n",
      "batch 307, training loss: 3.6118: : 306it [02:53,  1.47it/s]\u001b[A\n",
      "batch 307, training loss: 3.6118: : 307it [02:53,  1.50it/s]\u001b[A\n",
      "batch 308, training loss: 3.2286: : 307it [02:53,  1.50it/s]\u001b[A\n",
      "batch 308, training loss: 3.2286: : 308it [02:53,  1.51it/s]\u001b[A\n",
      "batch 309, training loss: 3.6191: : 308it [02:54,  1.51it/s]\u001b[A\n",
      "batch 309, training loss: 3.6191: : 309it [02:54,  1.45it/s]\u001b[A\n",
      "batch 310, training loss: 3.3771: : 309it [02:55,  1.45it/s]\u001b[A\n",
      "batch 310, training loss: 3.3771: : 310it [02:55,  1.47it/s]\u001b[A\n",
      "batch 311, training loss: 3.4813: : 310it [02:55,  1.47it/s]\u001b[A\n",
      "batch 311, training loss: 3.4813: : 311it [02:55,  1.51it/s]\u001b[A\n",
      "batch 312, training loss: 3.3208: : 311it [02:56,  1.51it/s]\u001b[A\n",
      "batch 312, training loss: 3.3208: : 312it [02:56,  1.50it/s]\u001b[A\n",
      "batch 313, training loss: 3.4097: : 312it [02:57,  1.50it/s]\u001b[A\n",
      "batch 313, training loss: 3.4097: : 313it [02:57,  1.43it/s]\u001b[A\n",
      "batch 314, training loss: 3.4383: : 313it [02:57,  1.43it/s]\u001b[A\n",
      "batch 314, training loss: 3.4383: : 314it [02:57,  1.46it/s]\u001b[A\n",
      "batch 315, training loss: 3.4773: : 314it [02:58,  1.46it/s]\u001b[A\n",
      "batch 315, training loss: 3.4773: : 315it [02:58,  1.49it/s]\u001b[A\n",
      "batch 316, training loss: 3.6414: : 315it [02:59,  1.49it/s]\u001b[A\n",
      "batch 316, training loss: 3.6414: : 316it [02:59,  1.50it/s]\u001b[A\n",
      "batch 317, training loss: 3.3193: : 316it [02:59,  1.50it/s]\u001b[A\n",
      "batch 317, training loss: 3.3193: : 317it [02:59,  1.65it/s]\u001b[A\n",
      "batch 318, training loss: 3.4988: : 317it [03:00,  1.65it/s]\u001b[A\n",
      "batch 318, training loss: 3.4988: : 318it [03:00,  1.43it/s]\u001b[A\n",
      "batch 319, training loss: 3.5553: : 318it [03:01,  1.43it/s]\u001b[A\n",
      "batch 319, training loss: 3.5553: : 319it [03:01,  1.42it/s]\u001b[A\n",
      "batch 320, training loss: 3.5578: : 319it [03:01,  1.42it/s]\u001b[A\n",
      "batch 320, training loss: 3.5578: : 320it [03:01,  1.39it/s]\u001b[A\n",
      "batch 321, training loss: 3.6256: : 320it [03:02,  1.39it/s]\u001b[A\n",
      "batch 321, training loss: 3.6256: : 321it [03:02,  1.40it/s]\u001b[A\n",
      "batch 322, training loss: 3.658: : 321it [03:03,  1.40it/s] \u001b[A\n",
      "batch 322, training loss: 3.658: : 322it [03:03,  1.29it/s]\u001b[A\n",
      "batch 323, training loss: 3.4204: : 322it [03:04,  1.29it/s]\u001b[A\n",
      "batch 323, training loss: 3.4204: : 323it [03:04,  1.33it/s]\u001b[A\n",
      "batch 324, training loss: 3.5047: : 323it [03:04,  1.33it/s]\u001b[A\n",
      "batch 324, training loss: 3.5047: : 324it [03:04,  1.35it/s]\u001b[A\n",
      "batch 325, training loss: 3.5791: : 324it [03:05,  1.35it/s]\u001b[A\n",
      "batch 325, training loss: 3.5791: : 325it [03:05,  1.34it/s]\u001b[A\n",
      "batch 326, training loss: 3.492: : 325it [03:06,  1.34it/s] \u001b[A\n",
      "batch 326, training loss: 3.492: : 326it [03:06,  1.29it/s]\u001b[A\n",
      "batch 327, training loss: 3.6286: : 326it [03:07,  1.29it/s]\u001b[A\n",
      "batch 327, training loss: 3.6286: : 327it [03:07,  1.33it/s]\u001b[A\n",
      "batch 328, training loss: 3.5223: : 327it [03:08,  1.33it/s]\u001b[A\n",
      "batch 328, training loss: 3.5223: : 328it [03:08,  1.32it/s]\u001b[A\n",
      "batch 329, training loss: 3.4209: : 328it [03:08,  1.32it/s]\u001b[A\n",
      "batch 329, training loss: 3.4209: : 329it [03:08,  1.25it/s]\u001b[A\n",
      "batch 330, training loss: 3.5011: : 329it [03:09,  1.25it/s]\u001b[A\n",
      "batch 330, training loss: 3.5011: : 330it [03:09,  1.29it/s]\u001b[A\n",
      "batch 331, training loss: 3.5397: : 330it [03:10,  1.29it/s]\u001b[A\n",
      "batch 331, training loss: 3.5397: : 331it [03:10,  1.32it/s]\u001b[A\n",
      "batch 332, training loss: 3.4407: : 331it [03:11,  1.32it/s]\u001b[A\n",
      "batch 332, training loss: 3.4407: : 332it [03:11,  1.32it/s]\u001b[A\n",
      "batch 333, training loss: 3.4243: : 332it [03:11,  1.32it/s]\u001b[A\n",
      "batch 333, training loss: 3.4243: : 333it [03:11,  1.27it/s]\u001b[A\n",
      "batch 334, training loss: 3.4824: : 333it [03:12,  1.27it/s]\u001b[A\n",
      "batch 334, training loss: 3.4824: : 334it [03:12,  1.31it/s]\u001b[A\n",
      "batch 335, training loss: 3.648: : 334it [03:13,  1.31it/s] \u001b[A\n",
      "batch 335, training loss: 3.648: : 335it [03:13,  1.30it/s]\u001b[A\n",
      "batch 336, training loss: 3.5396: : 335it [03:14,  1.30it/s]\u001b[A\n",
      "batch 336, training loss: 3.5396: : 336it [03:14,  1.22it/s]\u001b[A\n",
      "batch 337, training loss: 3.5818: : 336it [03:15,  1.22it/s]\u001b[A\n",
      "batch 337, training loss: 3.5818: : 337it [03:15,  1.27it/s]\u001b[A\n",
      "batch 338, training loss: 3.4378: : 337it [03:15,  1.27it/s]\u001b[A\n",
      "batch 338, training loss: 3.4378: : 338it [03:15,  1.31it/s]\u001b[A\n",
      "batch 339, training loss: 3.5352: : 338it [03:16,  1.31it/s]\u001b[A\n",
      "batch 339, training loss: 3.5352: : 339it [03:16,  1.32it/s]\u001b[A\n",
      "batch 340, training loss: 3.7624: : 339it [03:17,  1.32it/s]\u001b[A\n",
      "batch 340, training loss: 3.7624: : 340it [03:17,  1.23it/s]\u001b[A\n",
      "batch 341, training loss: 3.4325: : 340it [03:18,  1.23it/s]\u001b[A\n",
      "batch 341, training loss: 3.4325: : 341it [03:18,  1.28it/s]\u001b[A\n",
      "batch 342, training loss: 3.5383: : 341it [03:18,  1.28it/s]\u001b[A\n",
      "batch 342, training loss: 3.5383: : 342it [03:18,  1.33it/s]\u001b[A\n",
      "batch 343, training loss: 3.5051: : 342it [03:19,  1.33it/s]\u001b[A\n",
      "batch 343, training loss: 3.5051: : 343it [03:19,  1.32it/s]\u001b[A\n",
      "batch 344, training loss: 3.6296: : 343it [03:20,  1.32it/s]\u001b[A\n",
      "batch 344, training loss: 3.6296: : 344it [03:20,  1.28it/s]\u001b[A\n",
      "batch 345, training loss: 3.6274: : 344it [03:21,  1.28it/s]\u001b[A\n",
      "batch 345, training loss: 3.6274: : 345it [03:21,  1.33it/s]\u001b[A\n",
      "batch 346, training loss: 3.5473: : 345it [03:21,  1.33it/s]\u001b[A\n",
      "batch 346, training loss: 3.5473: : 346it [03:21,  1.31it/s]\u001b[A\n",
      "batch 347, training loss: 3.4558: : 346it [03:22,  1.31it/s]\u001b[A\n",
      "batch 347, training loss: 3.4558: : 347it [03:22,  1.23it/s]\u001b[A\n",
      "batch 348, training loss: 3.5449: : 347it [03:23,  1.23it/s]\u001b[A\n",
      "batch 348, training loss: 3.5449: : 348it [03:23,  1.29it/s]\u001b[A\n",
      "batch 349, training loss: 3.6914: : 348it [03:24,  1.29it/s]\u001b[A\n",
      "batch 349, training loss: 3.6914: : 349it [03:24,  1.33it/s]\u001b[A\n",
      "batch 350, training loss: 3.5319: : 349it [03:24,  1.33it/s]\u001b[A\n",
      "batch 350, training loss: 3.5319: : 350it [03:24,  1.41it/s]\u001b[A\n",
      "batch 351, training loss: 3.4475: : 350it [03:25,  1.41it/s]\u001b[A\n",
      "batch 351, training loss: 3.4475: : 351it [03:25,  1.57it/s]\u001b[A\n",
      "batch 352, training loss: 3.5223: : 351it [03:25,  1.57it/s]\u001b[A\n",
      "batch 352, training loss: 3.5223: : 352it [03:25,  1.63it/s]\u001b[A\n",
      "batch 353, training loss: 3.5473: : 352it [03:26,  1.63it/s]\u001b[A\n",
      "batch 353, training loss: 3.5473: : 353it [03:26,  1.53it/s]\u001b[A\n",
      "batch 354, training loss: 3.6018: : 353it [03:27,  1.53it/s]\u001b[A\n",
      "batch 354, training loss: 3.6018: : 354it [03:27,  1.50it/s]\u001b[A\n",
      "batch 355, training loss: 3.5532: : 354it [03:28,  1.50it/s]\u001b[A\n",
      "batch 355, training loss: 3.5532: : 355it [03:28,  1.38it/s]\u001b[A\n",
      "batch 356, training loss: 3.4012: : 355it [03:29,  1.38it/s]\u001b[A\n",
      "batch 356, training loss: 3.4012: : 356it [03:29,  1.36it/s]\u001b[A\n",
      "batch 357, training loss: 3.4104: : 356it [03:29,  1.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 357, training loss: 3.4104: : 357it [03:29,  1.38it/s]\u001b[A\n",
      "batch 358, training loss: 3.6279: : 357it [03:30,  1.38it/s]\u001b[A\n",
      "batch 358, training loss: 3.6279: : 358it [03:30,  1.29it/s]\u001b[A\n",
      "batch 359, training loss: 3.4319: : 358it [03:31,  1.29it/s]\u001b[A\n",
      "batch 359, training loss: 3.4319: : 359it [03:31,  1.30it/s]\u001b[A\n",
      "batch 360, training loss: 3.5433: : 359it [03:32,  1.30it/s]\u001b[A\n",
      "batch 360, training loss: 3.5433: : 360it [03:32,  1.30it/s]\u001b[A\n",
      "batch 361, training loss: 3.5286: : 360it [03:32,  1.30it/s]\u001b[A\n",
      "batch 361, training loss: 3.5286: : 361it [03:32,  1.34it/s]\u001b[A\n",
      "batch 362, training loss: 3.6065: : 361it [03:33,  1.34it/s]\u001b[A\n",
      "batch 362, training loss: 3.6065: : 362it [03:33,  1.27it/s]\u001b[A\n",
      "batch 363, training loss: 3.5309: : 362it [03:34,  1.27it/s]\u001b[A\n",
      "batch 363, training loss: 3.5309: : 363it [03:34,  1.28it/s]\u001b[A\n",
      "batch 364, training loss: 3.4514: : 363it [03:35,  1.28it/s]\u001b[A\n",
      "batch 364, training loss: 3.4514: : 364it [03:35,  1.29it/s]\u001b[A\n",
      "batch 365, training loss: 3.4074: : 364it [03:35,  1.29it/s]\u001b[A\n",
      "batch 365, training loss: 3.4074: : 365it [03:35,  1.33it/s]\u001b[A\n",
      "batch 366, training loss: 3.5455: : 365it [03:36,  1.33it/s]\u001b[A\n",
      "batch 366, training loss: 3.5455: : 366it [03:36,  1.27it/s]\u001b[A\n",
      "batch 367, training loss: 3.4705: : 366it [03:37,  1.27it/s]\u001b[A\n",
      "batch 367, training loss: 3.4705: : 367it [03:37,  1.29it/s]\u001b[A\n",
      "batch 368, training loss: 3.5584: : 367it [03:38,  1.29it/s]\u001b[A\n",
      "batch 368, training loss: 3.5584: : 368it [03:38,  1.32it/s]\u001b[A\n",
      "batch 369, training loss: 3.5788: : 368it [03:38,  1.32it/s]\u001b[A\n",
      "batch 369, training loss: 3.5788: : 369it [03:38,  1.34it/s]\u001b[A\n",
      "batch 370, training loss: 3.5028: : 369it [03:39,  1.34it/s]\u001b[A\n",
      "batch 370, training loss: 3.5028: : 370it [03:39,  1.26it/s]\u001b[A\n",
      "batch 371, training loss: 3.5163: : 370it [03:40,  1.26it/s]\u001b[A\n",
      "batch 371, training loss: 3.5163: : 371it [03:40,  1.27it/s]\u001b[A\n",
      "batch 372, training loss: 3.4636: : 371it [03:41,  1.27it/s]\u001b[A\n",
      "batch 372, training loss: 3.4636: : 372it [03:41,  1.31it/s]\u001b[A\n",
      "batch 373, training loss: 3.4918: : 372it [03:42,  1.31it/s]\u001b[A\n",
      "batch 373, training loss: 3.4918: : 373it [03:42,  1.34it/s]\u001b[A\n",
      "batch 374, training loss: 3.4974: : 373it [03:42,  1.34it/s]\u001b[A\n",
      "batch 374, training loss: 3.4974: : 374it [03:42,  1.25it/s]\u001b[A\n",
      "batch 375, training loss: 3.2025: : 374it [03:43,  1.25it/s]\u001b[A\n",
      "batch 375, training loss: 3.2025: : 375it [03:43,  1.45it/s]\u001b[A\n",
      "batch 376, training loss: 3.4794: : 375it [03:44,  1.45it/s]\u001b[A\n",
      "batch 376, training loss: 3.4794: : 376it [03:44,  1.36it/s]\u001b[A\n",
      "batch 377, training loss: 3.5828: : 376it [03:45,  1.36it/s]\u001b[A\n",
      "batch 377, training loss: 3.5828: : 377it [03:45,  1.22it/s]\u001b[A\n",
      "batch 378, training loss: 3.4979: : 377it [03:46,  1.22it/s]\u001b[A\n",
      "batch 378, training loss: 3.4979: : 378it [03:46,  1.22it/s]\u001b[A\n",
      "batch 379, training loss: 3.4681: : 378it [03:47,  1.22it/s]\u001b[A\n",
      "batch 379, training loss: 3.4681: : 379it [03:47,  1.17it/s]\u001b[A\n",
      "batch 380, training loss: 3.4423: : 379it [03:47,  1.17it/s]\u001b[A\n",
      "batch 380, training loss: 3.4423: : 380it [03:47,  1.20it/s]\u001b[A\n",
      "batch 381, training loss: 3.5446: : 380it [03:48,  1.20it/s]\u001b[A\n",
      "batch 381, training loss: 3.5446: : 381it [03:48,  1.22it/s]\u001b[A\n",
      "batch 382, training loss: 3.4164: : 381it [03:49,  1.22it/s]\u001b[A\n",
      "batch 382, training loss: 3.4164: : 382it [03:49,  1.25it/s]\u001b[A\n",
      "batch 383, training loss: 3.459: : 382it [03:50,  1.25it/s] \u001b[A\n",
      "batch 383, training loss: 3.459: : 383it [03:50,  1.18it/s]\u001b[A\n",
      "batch 384, training loss: 3.526: : 383it [03:51,  1.18it/s]\u001b[A\n",
      "batch 384, training loss: 3.526: : 384it [03:51,  1.18it/s]\u001b[A\n",
      "batch 385, training loss: 3.455: : 384it [03:51,  1.18it/s]\u001b[A\n",
      "batch 385, training loss: 3.455: : 385it [03:51,  1.19it/s]\u001b[A\n",
      "batch 386, training loss: 3.4345: : 385it [03:52,  1.19it/s]\u001b[A\n",
      "batch 386, training loss: 3.4345: : 386it [03:52,  1.18it/s]\u001b[A\n",
      "batch 387, training loss: 3.4876: : 386it [03:53,  1.18it/s]\u001b[A\n",
      "batch 387, training loss: 3.4876: : 387it [03:53,  1.19it/s]\u001b[A\n",
      "batch 388, training loss: 3.3009: : 387it [03:54,  1.19it/s]\u001b[A\n",
      "batch 388, training loss: 3.3009: : 388it [03:54,  1.23it/s]\u001b[A\n",
      "batch 389, training loss: 3.4656: : 388it [03:55,  1.23it/s]\u001b[A\n",
      "batch 389, training loss: 3.4656: : 389it [03:55,  1.16it/s]\u001b[A\n",
      "batch 390, training loss: 3.5458: : 389it [03:56,  1.16it/s]\u001b[A\n",
      "batch 390, training loss: 3.5458: : 390it [03:56,  1.17it/s]\u001b[A\n",
      "batch 391, training loss: 3.5819: : 390it [03:57,  1.17it/s]\u001b[A\n",
      "batch 391, training loss: 3.5819: : 391it [03:57,  1.18it/s]\u001b[A\n",
      "batch 392, training loss: 3.5074: : 391it [03:58,  1.18it/s]\u001b[A\n",
      "batch 392, training loss: 3.5074: : 392it [03:58,  1.14it/s]\u001b[A\n",
      "batch 393, training loss: 3.3247: : 392it [03:58,  1.14it/s]\u001b[A\n",
      "batch 393, training loss: 3.3247: : 393it [03:58,  1.18it/s]\u001b[A\n",
      "batch 394, training loss: 3.3134: : 393it [03:59,  1.18it/s]\u001b[A\n",
      "batch 394, training loss: 3.3134: : 394it [03:59,  1.21it/s]\u001b[A\n",
      "batch 395, training loss: 3.3972: : 394it [04:00,  1.21it/s]\u001b[A\n",
      "batch 395, training loss: 3.3972: : 395it [04:00,  1.24it/s]\u001b[A\n",
      "batch 396, training loss: 3.5652: : 395it [04:01,  1.24it/s]\u001b[A\n",
      "batch 396, training loss: 3.5652: : 396it [04:01,  1.18it/s]\u001b[A\n",
      "batch 397, training loss: 3.3886: : 396it [04:02,  1.18it/s]\u001b[A\n",
      "batch 397, training loss: 3.3886: : 397it [04:02,  1.18it/s]\u001b[A\n",
      "batch 398, training loss: 3.5078: : 397it [04:02,  1.18it/s]\u001b[A\n",
      "batch 398, training loss: 3.5078: : 398it [04:02,  1.19it/s]\u001b[A\n",
      "batch 399, training loss: 3.5979: : 398it [04:03,  1.19it/s]\u001b[A\n",
      "batch 399, training loss: 3.5979: : 399it [04:03,  1.14it/s]\u001b[A\n",
      "batch 400, training loss: 3.3832: : 399it [04:04,  1.14it/s]\u001b[A\n",
      "batch 400, training loss: 3.3832: : 400it [04:04,  1.17it/s]\u001b[A\n",
      "batch 401, training loss: 3.351: : 400it [04:05,  1.17it/s] \u001b[A\n",
      "batch 401, training loss: 3.351: : 401it [04:05,  1.21it/s]\u001b[A\n",
      "batch 402, training loss: 3.4205: : 401it [04:06,  1.21it/s]\u001b[A\n",
      "batch 402, training loss: 3.4205: : 402it [04:06,  1.21it/s]\u001b[A\n",
      "batch 403, training loss: 3.5951: : 402it [04:07,  1.21it/s]\u001b[A\n",
      "batch 403, training loss: 3.5951: : 403it [04:07,  1.18it/s]\u001b[A\n",
      "batch 404, training loss: 3.2074: : 403it [04:08,  1.18it/s]\u001b[A\n",
      "batch 404, training loss: 3.2074: : 404it [04:08,  1.19it/s]\u001b[A\n",
      "batch 405, training loss: 3.4931: : 404it [04:08,  1.19it/s]\u001b[A\n",
      "batch 405, training loss: 3.4931: : 405it [04:08,  1.18it/s]\u001b[A\n",
      "batch 406, training loss: 3.4245: : 405it [04:09,  1.18it/s]\u001b[A\n",
      "batch 406, training loss: 3.4245: : 406it [04:09,  1.19it/s]\u001b[A\n",
      "batch 407, training loss: 3.4788: : 406it [04:10,  1.19it/s]\u001b[A\n",
      "batch 407, training loss: 3.4788: : 407it [04:10,  1.20it/s]\u001b[A\n",
      "batch 408, training loss: 3.327: : 407it [04:11,  1.20it/s] \u001b[A\n",
      "batch 408, training loss: 3.327: : 408it [04:11,  1.24it/s]\u001b[A\n",
      "batch 409, training loss: 3.512: : 408it [04:12,  1.24it/s]\u001b[A\n",
      "batch 409, training loss: 3.512: : 409it [04:12,  1.22it/s]\u001b[A\n",
      "batch 410, training loss: 3.361: : 409it [04:12,  1.22it/s]\u001b[A\n",
      "batch 410, training loss: 3.361: : 410it [04:12,  1.22it/s]\u001b[A\n",
      "batch 411, training loss: 3.4659: : 410it [04:13,  1.22it/s]\u001b[A\n",
      "batch 411, training loss: 3.4659: : 411it [04:13,  1.23it/s]\u001b[A\n",
      "batch 412, training loss: 3.4919: : 411it [04:14,  1.23it/s]\u001b[A\n",
      "batch 412, training loss: 3.4919: : 412it [04:14,  1.25it/s]\u001b[A\n",
      "batch 413, training loss: 3.4227: : 412it [04:15,  1.25it/s]\u001b[A\n",
      "batch 413, training loss: 3.4227: : 413it [04:15,  1.17it/s]\u001b[A\n",
      "batch 414, training loss: 3.3271: : 413it [04:16,  1.17it/s]\u001b[A\n",
      "batch 414, training loss: 3.3271: : 414it [04:16,  1.19it/s]\u001b[A\n",
      "batch 415, training loss: 3.2652: : 414it [04:17,  1.19it/s]\u001b[A\n",
      "batch 415, training loss: 3.2652: : 415it [04:17,  1.19it/s]\u001b[A\n",
      "batch 416, training loss: 3.2873: : 415it [04:18,  1.19it/s]\u001b[A\n",
      "batch 416, training loss: 3.2873: : 416it [04:18,  1.14it/s]\u001b[A\n",
      "batch 417, training loss: 3.5458: : 416it [04:18,  1.14it/s]\u001b[A\n",
      "batch 417, training loss: 3.5458: : 417it [04:18,  1.16it/s]\u001b[A\n",
      "batch 418, training loss: 3.4344: : 417it [04:19,  1.16it/s]\u001b[A\n",
      "batch 418, training loss: 3.4344: : 418it [04:19,  1.20it/s]\u001b[A\n",
      "batch 419, training loss: 3.267: : 418it [04:20,  1.20it/s] \u001b[A\n",
      "batch 419, training loss: 3.267: : 419it [04:20,  1.22it/s]\u001b[A\n",
      "batch 420, training loss: 3.3491: : 419it [04:21,  1.22it/s]\u001b[A\n",
      "batch 420, training loss: 3.3491: : 420it [04:21,  1.19it/s]\u001b[A\n",
      "batch 421, training loss: 3.4731: : 420it [04:22,  1.19it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 421, training loss: 3.4731: : 421it [04:22,  1.20it/s]\u001b[A\n",
      "batch 422, training loss: 3.509: : 421it [04:23,  1.20it/s] \u001b[A\n",
      "batch 422, training loss: 3.509: : 422it [04:23,  1.20it/s]\u001b[A\n",
      "batch 423, training loss: 3.2848: : 422it [04:23,  1.20it/s]\u001b[A\n",
      "batch 423, training loss: 3.2848: : 423it [04:23,  1.19it/s]\u001b[A\n",
      "batch 424, training loss: 3.453: : 423it [04:24,  1.19it/s] \u001b[A\n",
      "batch 424, training loss: 3.453: : 424it [04:24,  1.19it/s]\u001b[A\n",
      "batch 425, training loss: 3.5061: : 424it [04:25,  1.19it/s]\u001b[A\n",
      "batch 425, training loss: 3.5061: : 425it [04:25,  1.21it/s]\u001b[A\n",
      "batch 426, training loss: 3.4522: : 425it [04:26,  1.21it/s]\u001b[A\n",
      "batch 426, training loss: 3.4522: : 426it [04:26,  1.14it/s]\u001b[A\n",
      "batch 427, training loss: 3.4902: : 426it [04:27,  1.14it/s]\u001b[A\n",
      "batch 427, training loss: 3.4902: : 427it [04:27,  1.14it/s]\u001b[A\n",
      "batch 428, training loss: 3.643: : 427it [04:28,  1.14it/s] \u001b[A\n",
      "batch 428, training loss: 3.643: : 428it [04:28,  1.15it/s]\u001b[A\n",
      "batch 429, training loss: 3.474: : 428it [04:29,  1.15it/s]\u001b[A\n",
      "batch 429, training loss: 3.474: : 429it [04:29,  1.10it/s]\u001b[A\n",
      "batch 430, training loss: 3.533: : 429it [04:30,  1.10it/s]\u001b[A\n",
      "batch 430, training loss: 3.533: : 430it [04:30,  1.11it/s]\u001b[A\n",
      "batch 431, training loss: 3.5625: : 430it [04:31,  1.11it/s]\u001b[A\n",
      "batch 431, training loss: 3.5625: : 431it [04:31,  1.06it/s]\u001b[A\n",
      "batch 432, training loss: 3.3952: : 431it [04:32,  1.06it/s]\u001b[A\n",
      "batch 432, training loss: 3.3952: : 432it [04:32,  1.09it/s]\u001b[A\n",
      "batch 433, training loss: 3.4403: : 432it [04:32,  1.09it/s]\u001b[A\n",
      "batch 433, training loss: 3.4403: : 433it [04:32,  1.11it/s]\u001b[A\n",
      "batch 434, training loss: 3.4669: : 433it [04:33,  1.11it/s]\u001b[A\n",
      "batch 434, training loss: 3.4669: : 434it [04:33,  1.14it/s]\u001b[A\n",
      "batch 435, training loss: 3.445: : 434it [04:34,  1.14it/s] \u001b[A\n",
      "batch 435, training loss: 3.445: : 435it [04:34,  1.31it/s]\u001b[A\n",
      "batch 436, training loss: 3.4934: : 435it [04:35,  1.31it/s]\u001b[A\n",
      "batch 436, training loss: 3.4934: : 436it [04:35,  1.23it/s]\u001b[A\n",
      "batch 437, training loss: 3.4228: : 436it [04:36,  1.23it/s]\u001b[A\n",
      "batch 437, training loss: 3.4228: : 437it [04:36,  1.20it/s]\u001b[A\n",
      "batch 438, training loss: 3.4658: : 437it [04:36,  1.20it/s]\u001b[A\n",
      "batch 438, training loss: 3.4658: : 438it [04:36,  1.17it/s]\u001b[A\n",
      "batch 439, training loss: 3.5914: : 438it [04:37,  1.17it/s]\u001b[A\n",
      "batch 439, training loss: 3.5914: : 439it [04:37,  1.11it/s]\u001b[A\n",
      "batch 440, training loss: 3.412: : 439it [04:38,  1.11it/s] \u001b[A\n",
      "batch 440, training loss: 3.412: : 440it [04:38,  1.12it/s]\u001b[A\n",
      "batch 441, training loss: 3.6851: : 440it [04:39,  1.12it/s]\u001b[A\n",
      "batch 441, training loss: 3.6851: : 441it [04:39,  1.06it/s]\u001b[A\n",
      "batch 442, training loss: 3.3561: : 441it [04:40,  1.06it/s]\u001b[A\n",
      "batch 442, training loss: 3.3561: : 442it [04:40,  1.09it/s]\u001b[A\n",
      "batch 443, training loss: 3.4641: : 442it [04:41,  1.09it/s]\u001b[A\n",
      "batch 443, training loss: 3.4641: : 443it [04:41,  1.11it/s]\u001b[A\n",
      "batch 444, training loss: 3.435: : 443it [04:42,  1.11it/s] \u001b[A\n",
      "batch 444, training loss: 3.435: : 444it [04:42,  1.06it/s]\u001b[A\n",
      "batch 445, training loss: 3.5114: : 444it [04:43,  1.06it/s]\u001b[A\n",
      "batch 445, training loss: 3.5114: : 445it [04:43,  1.08it/s]\u001b[A\n",
      "batch 446, training loss: 3.4921: : 445it [04:44,  1.08it/s]\u001b[A\n",
      "batch 446, training loss: 3.4921: : 446it [04:44,  1.04it/s]\u001b[A\n",
      "batch 447, training loss: 3.4542: : 446it [04:45,  1.04it/s]\u001b[A\n",
      "batch 447, training loss: 3.4542: : 447it [04:45,  1.08it/s]\u001b[A\n",
      "batch 448, training loss: 3.3623: : 447it [04:46,  1.08it/s]\u001b[A\n",
      "batch 448, training loss: 3.3623: : 448it [04:46,  1.10it/s]\u001b[A\n",
      "batch 449, training loss: 3.462: : 448it [04:47,  1.10it/s] \u001b[A\n",
      "batch 449, training loss: 3.462: : 449it [04:47,  1.05it/s]\u001b[A\n",
      "batch 450, training loss: 3.5318: : 449it [04:48,  1.05it/s]\u001b[A\n",
      "batch 450, training loss: 3.5318: : 450it [04:48,  1.08it/s]\u001b[A\n",
      "batch 451, training loss: 3.5413: : 450it [04:49,  1.08it/s]\u001b[A\n",
      "batch 451, training loss: 3.5413: : 451it [04:49,  1.10it/s]\u001b[A\n",
      "batch 452, training loss: 3.5357: : 451it [04:50,  1.10it/s]\u001b[A\n",
      "batch 452, training loss: 3.5357: : 452it [04:50,  1.06it/s]\u001b[A\n",
      "batch 453, training loss: 3.6056: : 452it [04:50,  1.06it/s]\u001b[A\n",
      "batch 453, training loss: 3.6056: : 453it [04:50,  1.08it/s]\u001b[A\n",
      "batch 454, training loss: 3.4661: : 453it [04:51,  1.08it/s]\u001b[A\n",
      "batch 454, training loss: 3.4661: : 454it [04:51,  1.10it/s]\u001b[A\n",
      "batch 455, training loss: 3.3789: : 454it [04:52,  1.10it/s]\u001b[A\n",
      "batch 455, training loss: 3.3789: : 455it [04:52,  1.06it/s]\u001b[A\n",
      "batch 456, training loss: 3.4108: : 455it [04:53,  1.06it/s]\u001b[A\n",
      "batch 456, training loss: 3.4108: : 456it [04:53,  1.08it/s]\u001b[A\n",
      "batch 457, training loss: 3.5246: : 456it [04:54,  1.08it/s]\u001b[A\n",
      "batch 457, training loss: 3.5246: : 457it [04:54,  1.05it/s]\u001b[A\n",
      "batch 458, training loss: 3.4211: : 457it [04:55,  1.05it/s]\u001b[A\n",
      "batch 458, training loss: 3.4211: : 458it [04:55,  1.08it/s]\u001b[A\n",
      "batch 459, training loss: 3.4727: : 458it [04:56,  1.08it/s]\u001b[A\n",
      "batch 459, training loss: 3.4727: : 459it [04:56,  1.10it/s]\u001b[A\n",
      "batch 460, training loss: 3.4595: : 459it [04:57,  1.10it/s]\u001b[A\n",
      "batch 460, training loss: 3.4595: : 460it [04:57,  1.05it/s]\u001b[A\n",
      "batch 461, training loss: 3.5405: : 460it [04:58,  1.05it/s]\u001b[A\n",
      "batch 461, training loss: 3.5405: : 461it [04:58,  1.08it/s]\u001b[A\n",
      "batch 462, training loss: 3.5112: : 461it [04:59,  1.08it/s]\u001b[A\n",
      "batch 462, training loss: 3.5112: : 462it [04:59,  1.10it/s]\u001b[A\n",
      "batch 463, training loss: 3.3546: : 462it [05:00,  1.10it/s]\u001b[A\n",
      "batch 463, training loss: 3.3546: : 463it [05:00,  1.05it/s]\u001b[A\n",
      "batch 464, training loss: 3.4895: : 463it [05:01,  1.05it/s]\u001b[A\n",
      "batch 464, training loss: 3.4895: : 464it [05:01,  1.08it/s]\u001b[A\n",
      "batch 465, training loss: 3.4402: : 464it [05:01,  1.08it/s]\u001b[A\n",
      "batch 465, training loss: 3.4402: : 465it [05:01,  1.17it/s]\u001b[A\n",
      "batch 466, training loss: 3.3086: : 465it [05:03,  1.17it/s]\u001b[A\n",
      "batch 466, training loss: 3.3086: : 466it [05:03,  1.08it/s]\u001b[A\n",
      "batch 467, training loss: 3.2524: : 466it [05:03,  1.08it/s]\u001b[A\n",
      "batch 467, training loss: 3.2524: : 467it [05:03,  1.07it/s]\u001b[A\n",
      "batch 468, training loss: 3.4532: : 467it [05:04,  1.07it/s]\u001b[A\n",
      "batch 468, training loss: 3.4532: : 468it [05:04,  1.04it/s]\u001b[A\n",
      "batch 469, training loss: 3.3382: : 468it [05:05,  1.04it/s]\u001b[A\n",
      "batch 469, training loss: 3.3382: : 469it [05:05,  1.05it/s]\u001b[A\n",
      "batch 470, training loss: 3.4938: : 469it [05:06,  1.05it/s]\u001b[A\n",
      "batch 470, training loss: 3.4938: : 470it [05:06,  1.06it/s]\u001b[A\n",
      "batch 471, training loss: 3.4434: : 470it [05:07,  1.06it/s]\u001b[A\n",
      "batch 471, training loss: 3.4434: : 471it [05:07,  1.02it/s]\u001b[A\n",
      "batch 472, training loss: 3.3955: : 471it [05:08,  1.02it/s]\u001b[A\n",
      "batch 472, training loss: 3.3955: : 472it [05:08,  1.04it/s]\u001b[A\n",
      "batch 473, training loss: 3.3914: : 472it [05:09,  1.04it/s]\u001b[A\n",
      "batch 473, training loss: 3.3914: : 473it [05:09,  1.01s/it]\u001b[A\n",
      "batch 474, training loss: 3.4556: : 473it [05:10,  1.01s/it]\u001b[A\n",
      "batch 474, training loss: 3.4556: : 474it [05:10,  1.02it/s]\u001b[A\n",
      "batch 475, training loss: 3.3707: : 474it [05:11,  1.02it/s]\u001b[A\n",
      "batch 475, training loss: 3.3707: : 475it [05:11,  1.05it/s]\u001b[A\n",
      "batch 476, training loss: 3.4698: : 475it [05:12,  1.05it/s]\u001b[A\n",
      "batch 476, training loss: 3.4698: : 476it [05:12,  1.01it/s]\u001b[A\n",
      "batch 477, training loss: 3.289: : 476it [05:13,  1.01it/s] \u001b[A\n",
      "batch 477, training loss: 3.289: : 477it [05:13,  1.05it/s]\u001b[A\n",
      "batch 478, training loss: 3.3541: : 477it [05:14,  1.05it/s]\u001b[A\n",
      "batch 478, training loss: 3.3541: : 478it [05:14,  1.07it/s]\u001b[A\n",
      "batch 479, training loss: 3.3306: : 478it [05:15,  1.07it/s]\u001b[A\n",
      "batch 479, training loss: 3.3306: : 479it [05:15,  1.09it/s]\u001b[A\n",
      "batch 480, training loss: 3.3838: : 479it [05:16,  1.09it/s]\u001b[A\n",
      "batch 480, training loss: 3.3838: : 480it [05:16,  1.04it/s]\u001b[A\n",
      "batch 481, training loss: 3.384: : 480it [05:17,  1.04it/s] \u001b[A\n",
      "batch 481, training loss: 3.384: : 481it [05:17,  1.07it/s]\u001b[A\n",
      "batch 482, training loss: 3.3797: : 481it [05:18,  1.07it/s]\u001b[A\n",
      "batch 482, training loss: 3.3797: : 482it [05:18,  1.08it/s]\u001b[A\n",
      "batch 483, training loss: 3.3197: : 482it [05:19,  1.08it/s]\u001b[A\n",
      "batch 483, training loss: 3.3197: : 483it [05:19,  1.04it/s]\u001b[A\n",
      "batch 484, training loss: 3.3917: : 483it [05:20,  1.04it/s]\u001b[A\n",
      "batch 484, training loss: 3.3917: : 484it [05:20,  1.07it/s]\u001b[A\n",
      "batch 485, training loss: 3.3417: : 484it [05:21,  1.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 485, training loss: 3.3417: : 485it [05:21,  1.08it/s]\u001b[A\n",
      "batch 486, training loss: 3.4306: : 485it [05:22,  1.08it/s]\u001b[A\n",
      "batch 486, training loss: 3.4306: : 486it [05:22,  1.04it/s]\u001b[A\n",
      "batch 487, training loss: 3.5077: : 486it [05:23,  1.04it/s]\u001b[A\n",
      "batch 487, training loss: 3.5077: : 487it [05:23,  1.06it/s]\u001b[A\n",
      "batch 488, training loss: 3.3255: : 487it [05:23,  1.06it/s]\u001b[A\n",
      "batch 488, training loss: 3.3255: : 488it [05:23,  1.08it/s]\u001b[A\n",
      "batch 489, training loss: 3.2643: : 488it [05:25,  1.08it/s]\u001b[A\n",
      "batch 489, training loss: 3.2643: : 489it [05:25,  1.04it/s]\u001b[A\n",
      "batch 490, training loss: 3.3246: : 489it [05:25,  1.04it/s]\u001b[A\n",
      "batch 490, training loss: 3.3246: : 490it [05:25,  1.06it/s]\u001b[A\n",
      "batch 491, training loss: 3.3888: : 490it [05:26,  1.06it/s]\u001b[A\n",
      "batch 491, training loss: 3.3888: : 491it [05:26,  1.08it/s]\u001b[A\n",
      "batch 492, training loss: 3.2718: : 491it [05:27,  1.08it/s]\u001b[A\n",
      "batch 492, training loss: 3.2718: : 492it [05:27,  1.04it/s]\u001b[A\n",
      "batch 493, training loss: 3.4623: : 492it [05:28,  1.04it/s]\u001b[A\n",
      "batch 493, training loss: 3.4623: : 493it [05:28,  1.06it/s]\u001b[A\n",
      "batch 494, training loss: 3.4875: : 493it [05:29,  1.06it/s]\u001b[A\n",
      "batch 494, training loss: 3.4875: : 494it [05:29,  1.08it/s]\u001b[A\n",
      "batch 495, training loss: 3.2869: : 494it [05:30,  1.08it/s]\u001b[A\n",
      "batch 495, training loss: 3.2869: : 495it [05:30,  1.04it/s]\u001b[A\n",
      "batch 496, training loss: 3.303: : 495it [05:31,  1.04it/s] \u001b[A\n",
      "batch 496, training loss: 3.303: : 496it [05:31,  1.06it/s]\u001b[A\n",
      "batch 497, training loss: 3.2169: : 496it [05:32,  1.06it/s]\u001b[A\n",
      "batch 497, training loss: 3.2169: : 497it [05:32,  1.08it/s]\u001b[A\n",
      "batch 498, training loss: 3.3378: : 497it [05:33,  1.08it/s]\u001b[A\n",
      "batch 498, training loss: 3.3378: : 498it [05:33,  1.04it/s]\u001b[A\n",
      "batch 499, training loss: 3.2443: : 498it [05:34,  1.04it/s]\u001b[A\n",
      "batch 499, training loss: 3.2443: : 499it [05:34,  1.18it/s]\u001b[A\n",
      "batch 500, training loss: 3.5499: : 499it [05:35,  1.18it/s]\u001b[A\n",
      "batch 500, training loss: 3.5499: : 500it [05:35,  1.13it/s]\u001b[A\n",
      "batch 501, training loss: 3.4021: : 500it [05:36,  1.13it/s]\u001b[A\n",
      "batch 501, training loss: 3.4021: : 501it [05:36,  1.03it/s]\u001b[A\n",
      "batch 502, training loss: 3.3473: : 501it [05:37,  1.03it/s]\u001b[A\n",
      "batch 502, training loss: 3.3473: : 502it [05:37,  1.04it/s]\u001b[A\n",
      "batch 503, training loss: 3.4484: : 502it [05:38,  1.04it/s]\u001b[A\n",
      "batch 503, training loss: 3.4484: : 503it [05:38,  1.02it/s]\u001b[A\n",
      "batch 504, training loss: 3.4338: : 503it [05:39,  1.02it/s]\u001b[A\n",
      "batch 504, training loss: 3.4338: : 504it [05:39,  1.03s/it]\u001b[A\n",
      "batch 505, training loss: 3.518: : 504it [05:40,  1.03s/it] \u001b[A\n",
      "batch 505, training loss: 3.518: : 505it [05:40,  1.02s/it]\u001b[A\n",
      "batch 506, training loss: 3.4153: : 505it [05:41,  1.02s/it]\u001b[A\n",
      "batch 506, training loss: 3.4153: : 506it [05:41,  1.01it/s]\u001b[A\n",
      "batch 507, training loss: 3.3197: : 506it [05:42,  1.01it/s]\u001b[A\n",
      "batch 507, training loss: 3.3197: : 507it [05:42,  1.02s/it]\u001b[A\n",
      "batch 508, training loss: 3.4626: : 507it [05:43,  1.02s/it]\u001b[A\n",
      "batch 508, training loss: 3.4626: : 508it [05:43,  1.02s/it]\u001b[A\n",
      "batch 509, training loss: 3.3347: : 508it [05:44,  1.02s/it]\u001b[A\n",
      "batch 509, training loss: 3.3347: : 509it [05:44,  1.06it/s]\u001b[A\n",
      "batch 510, training loss: 3.4133: : 509it [05:44,  1.06it/s]\u001b[A\n",
      "batch 510, training loss: 3.4133: : 510it [05:44,  1.20it/s]\u001b[A\n",
      "batch 511, training loss: 3.4225: : 510it [05:45,  1.20it/s]\u001b[A\n",
      "batch 511, training loss: 3.4225: : 511it [05:45,  1.12it/s]\u001b[A\n",
      "batch 512, training loss: 3.417: : 511it [05:46,  1.12it/s] \u001b[A\n",
      "batch 512, training loss: 3.417: : 512it [05:46,  1.09it/s]\u001b[A\n",
      "batch 513, training loss: 3.4589: : 512it [05:47,  1.09it/s]\u001b[A\n",
      "batch 513, training loss: 3.4589: : 513it [05:47,  1.01it/s]\u001b[A\n",
      "batch 514, training loss: 3.3253: : 513it [05:48,  1.01it/s]\u001b[A\n",
      "batch 514, training loss: 3.3253: : 514it [05:48,  1.03it/s]\u001b[A\n",
      "batch 515, training loss: 3.5139: : 514it [05:49,  1.03it/s]\u001b[A\n",
      "batch 515, training loss: 3.5139: : 515it [05:49,  1.03it/s]\u001b[A\n",
      "batch 516, training loss: 3.3882: : 515it [05:50,  1.03it/s]\u001b[A\n",
      "batch 516, training loss: 3.3882: : 516it [05:50,  1.04s/it]\u001b[A\n",
      "batch 517, training loss: 3.3849: : 516it [05:51,  1.04s/it]\u001b[A\n",
      "batch 517, training loss: 3.3849: : 517it [05:51,  1.01s/it]\u001b[A\n",
      "batch 518, training loss: 3.4711: : 517it [05:53,  1.01s/it]\u001b[A\n",
      "batch 518, training loss: 3.4711: : 518it [05:53,  1.06s/it]\u001b[A\n",
      "batch 519, training loss: 3.2636: : 518it [05:54,  1.06s/it]\u001b[A\n",
      "batch 519, training loss: 3.2636: : 519it [05:54,  1.02s/it]\u001b[A\n",
      "batch 520, training loss: 3.4264: : 519it [05:55,  1.02s/it]\u001b[A\n",
      "batch 520, training loss: 3.4264: : 520it [05:55,  1.02s/it]\u001b[A\n",
      "batch 521, training loss: 3.4422: : 520it [05:56,  1.02s/it]\u001b[A\n",
      "batch 521, training loss: 3.4422: : 521it [05:56,  1.05s/it]\u001b[A\n",
      "batch 522, training loss: 3.4632: : 521it [05:57,  1.05s/it]\u001b[A\n",
      "batch 522, training loss: 3.4632: : 522it [05:57,  1.02s/it]\u001b[A\n",
      "batch 523, training loss: 3.3384: : 522it [05:58,  1.02s/it]\u001b[A\n",
      "batch 523, training loss: 3.3384: : 523it [05:58,  1.06s/it]\u001b[A\n",
      "batch 524, training loss: 3.5351: : 523it [05:59,  1.06s/it]\u001b[A\n",
      "batch 524, training loss: 3.5351: : 524it [05:59,  1.02s/it]\u001b[A\n",
      "batch 525, training loss: 3.5826: : 524it [06:00,  1.02s/it]\u001b[A\n",
      "batch 525, training loss: 3.5826: : 525it [06:00,  1.00it/s]\u001b[A\n",
      "batch 526, training loss: 3.3866: : 525it [06:01,  1.00it/s]\u001b[A\n",
      "batch 526, training loss: 3.3866: : 526it [06:01,  1.06s/it]\u001b[A\n",
      "batch 527, training loss: 3.3817: : 526it [06:02,  1.06s/it]\u001b[A\n",
      "batch 527, training loss: 3.3817: : 527it [06:02,  1.01s/it]\u001b[A\n",
      "batch 528, training loss: 3.3935: : 527it [06:03,  1.01s/it]\u001b[A\n",
      "batch 528, training loss: 3.3935: : 528it [06:03,  1.06s/it]\u001b[A\n",
      "batch 529, training loss: 3.4543: : 528it [06:04,  1.06s/it]\u001b[A\n",
      "batch 529, training loss: 3.4543: : 529it [06:04,  1.06s/it]\u001b[A\n",
      "batch 530, training loss: 3.4231: : 529it [06:05,  1.06s/it]\u001b[A\n",
      "batch 530, training loss: 3.4231: : 530it [06:05,  1.05s/it]\u001b[A\n",
      "batch 531, training loss: 3.3675: : 530it [06:06,  1.05s/it]\u001b[A\n",
      "batch 531, training loss: 3.3675: : 531it [06:06,  1.10s/it]\u001b[A\n",
      "batch 532, training loss: 3.1528: : 531it [06:07,  1.10s/it]\u001b[A\n",
      "batch 532, training loss: 3.1528: : 532it [06:07,  1.09s/it]\u001b[A\n",
      "batch 533, training loss: 3.3265: : 532it [06:08,  1.09s/it]\u001b[A\n",
      "batch 533, training loss: 3.3265: : 533it [06:08,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.4673: : 533it [06:10,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.4673: : 534it [06:10,  1.11s/it]\u001b[A\n",
      "batch 535, training loss: 3.3415: : 534it [06:11,  1.11s/it]\u001b[A\n",
      "batch 535, training loss: 3.3415: : 535it [06:11,  1.09s/it]\u001b[A\n",
      "batch 536, training loss: 3.1918: : 535it [06:12,  1.09s/it]\u001b[A\n",
      "batch 536, training loss: 3.1918: : 536it [06:12,  1.14s/it]\u001b[A\n",
      "batch 537, training loss: 3.2685: : 536it [06:13,  1.14s/it]\u001b[A\n",
      "batch 537, training loss: 3.2685: : 537it [06:13,  1.11s/it]\u001b[A\n",
      "batch 538, training loss: 3.3661: : 537it [06:14,  1.11s/it]\u001b[A\n",
      "batch 538, training loss: 3.3661: : 538it [06:14,  1.10s/it]\u001b[A\n",
      "batch 539, training loss: 3.3446: : 538it [06:15,  1.10s/it]\u001b[A\n",
      "batch 539, training loss: 3.3446: : 539it [06:15,  1.12s/it]\u001b[A\n",
      "batch 540, training loss: 3.3014: : 539it [06:16,  1.12s/it]\u001b[A\n",
      "batch 540, training loss: 3.3014: : 540it [06:16,  1.11s/it]\u001b[A\n",
      "batch 541, training loss: 3.4056: : 540it [06:17,  1.11s/it]\u001b[A\n",
      "batch 541, training loss: 3.4056: : 541it [06:17,  1.14s/it]\u001b[A\n",
      "batch 542, training loss: 3.4445: : 541it [06:18,  1.14s/it]\u001b[A\n",
      "batch 542, training loss: 3.4445: : 542it [06:18,  1.12s/it]\u001b[A\n",
      "batch 543, training loss: 3.2928: : 542it [06:20,  1.12s/it]\u001b[A\n",
      "batch 543, training loss: 3.2928: : 543it [06:20,  1.10s/it]\u001b[A\n",
      "batch 544, training loss: 3.354: : 543it [06:21,  1.10s/it] \u001b[A\n",
      "batch 544, training loss: 3.354: : 544it [06:21,  1.14s/it]\u001b[A\n",
      "batch 545, training loss: 3.3741: : 544it [06:22,  1.14s/it]\u001b[A\n",
      "batch 545, training loss: 3.3741: : 545it [06:22,  1.12s/it]\u001b[A\n",
      "batch 546, training loss: 3.2627: : 545it [06:23,  1.12s/it]\u001b[A\n",
      "batch 546, training loss: 3.2627: : 546it [06:23,  1.13s/it]\u001b[A\n",
      "batch 547, training loss: 3.1834: : 546it [06:24,  1.13s/it]\u001b[A\n",
      "batch 547, training loss: 3.1834: : 547it [06:24,  1.12s/it]\u001b[A\n",
      "batch 548, training loss: 3.062: : 547it [06:25,  1.12s/it] \u001b[A\n",
      "batch 548, training loss: 3.062: : 548it [06:25,  1.05it/s]\u001b[A\n",
      "batch 549, training loss: 3.3559: : 548it [06:26,  1.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 549, training loss: 3.3559: : 549it [06:26,  1.06s/it]\u001b[A\n",
      "batch 550, training loss: 3.2854: : 549it [06:27,  1.06s/it]\u001b[A\n",
      "batch 550, training loss: 3.2854: : 550it [06:27,  1.07s/it]\u001b[A\n",
      "batch 551, training loss: 3.2965: : 550it [06:28,  1.07s/it]\u001b[A\n",
      "batch 551, training loss: 3.2965: : 551it [06:28,  1.10s/it]\u001b[A\n",
      "batch 552, training loss: 3.3379: : 551it [06:29,  1.10s/it]\u001b[A\n",
      "batch 552, training loss: 3.3379: : 552it [06:29,  1.12s/it]\u001b[A\n",
      "batch 553, training loss: 3.3396: : 552it [06:31,  1.12s/it]\u001b[A\n",
      "batch 553, training loss: 3.3396: : 553it [06:31,  1.13s/it]\u001b[A\n",
      "batch 554, training loss: 3.0751: : 553it [06:32,  1.13s/it]\u001b[A\n",
      "batch 554, training loss: 3.0751: : 554it [06:32,  1.16s/it]\u001b[A\n",
      "batch 555, training loss: 3.2298: : 554it [06:33,  1.16s/it]\u001b[A\n",
      "batch 555, training loss: 3.2298: : 555it [06:33,  1.15s/it]\u001b[A\n",
      "batch 556, training loss: 3.3991: : 555it [06:34,  1.15s/it]\u001b[A\n",
      "batch 556, training loss: 3.3991: : 556it [06:34,  1.19s/it]\u001b[A\n",
      "batch 557, training loss: 3.235: : 556it [06:35,  1.19s/it] \u001b[A\n",
      "batch 557, training loss: 3.235: : 557it [06:35,  1.16s/it]\u001b[A\n",
      "batch 558, training loss: 3.1755: : 557it [06:36,  1.16s/it]\u001b[A\n",
      "batch 558, training loss: 3.1755: : 558it [06:36,  1.13s/it]\u001b[A\n",
      "batch 559, training loss: 3.2657: : 558it [06:38,  1.13s/it]\u001b[A\n",
      "batch 559, training loss: 3.2657: : 559it [06:38,  1.18s/it]\u001b[A\n",
      "batch 560, training loss: 3.2784: : 559it [06:39,  1.18s/it]\u001b[A\n",
      "batch 560, training loss: 3.2784: : 560it [06:39,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.2556: : 560it [06:40,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.2556: : 561it [06:40,  1.14s/it]\u001b[A\n",
      "batch 562, training loss: 3.1231: : 561it [06:41,  1.14s/it]\u001b[A\n",
      "batch 562, training loss: 3.1231: : 562it [06:41,  1.18s/it]\u001b[A\n",
      "batch 563, training loss: 3.2768: : 562it [06:42,  1.18s/it]\u001b[A\n",
      "batch 563, training loss: 3.2768: : 563it [06:42,  1.16s/it]\u001b[A\n",
      "batch 564, training loss: 3.1483: : 563it [06:43,  1.16s/it]\u001b[A\n",
      "batch 564, training loss: 3.1483: : 564it [06:43,  1.18s/it]\u001b[A\n",
      "batch 565, training loss: 3.2821: : 564it [06:44,  1.18s/it]\u001b[A\n",
      "batch 565, training loss: 3.2821: : 565it [06:44,  1.14s/it]\u001b[A\n",
      "batch 566, training loss: 3.3851: : 565it [06:46,  1.14s/it]\u001b[A\n",
      "batch 566, training loss: 3.3851: : 566it [06:46,  1.15s/it]\u001b[A\n",
      "batch 567, training loss: 3.3317: : 566it [06:47,  1.15s/it]\u001b[A\n",
      "batch 567, training loss: 3.3317: : 567it [06:47,  1.22s/it]\u001b[A\n",
      "batch 568, training loss: 3.417: : 567it [06:48,  1.22s/it] \u001b[A\n",
      "batch 568, training loss: 3.417: : 568it [06:48,  1.21s/it]\u001b[A\n",
      "batch 569, training loss: 3.3991: : 568it [06:50,  1.21s/it]\u001b[A\n",
      "batch 569, training loss: 3.3991: : 569it [06:50,  1.27s/it]\u001b[A\n",
      "batch 570, training loss: 3.5616: : 569it [06:51,  1.27s/it]\u001b[A\n",
      "batch 570, training loss: 3.5616: : 570it [06:51,  1.25s/it]\u001b[A\n",
      "batch 571, training loss: 3.3459: : 570it [06:52,  1.25s/it]\u001b[A\n",
      "batch 571, training loss: 3.3459: : 571it [06:52,  1.28s/it]\u001b[A\n",
      "batch 572, training loss: 3.4677: : 571it [06:53,  1.28s/it]\u001b[A\n",
      "batch 572, training loss: 3.4677: : 572it [06:53,  1.26s/it]\u001b[A\n",
      "batch 573, training loss: 3.3525: : 572it [06:54,  1.26s/it]\u001b[A\n",
      "batch 573, training loss: 3.3525: : 573it [06:54,  1.14s/it]\u001b[A\n",
      "batch 574, training loss: 3.4388: : 573it [06:55,  1.14s/it]\u001b[A\n",
      "batch 574, training loss: 3.4388: : 574it [06:55,  1.00it/s]\u001b[A\n",
      "batch 575, training loss: 2.9282: : 574it [06:55,  1.00it/s]\u001b[A\n",
      "batch 575, training loss: 2.9282: : 575it [06:55,  1.18it/s]\u001b[A\n",
      "batch 576, training loss: 3.4348: : 575it [06:57,  1.18it/s]\u001b[A\n",
      "batch 576, training loss: 3.4348: : 576it [06:57,  1.02it/s]\u001b[A\n",
      "batch 577, training loss: 3.2917: : 576it [06:58,  1.02it/s]\u001b[A\n",
      "batch 577, training loss: 3.2917: : 577it [06:58,  1.05s/it]\u001b[A\n",
      "batch 578, training loss: 3.2726: : 577it [06:59,  1.05s/it]\u001b[A\n",
      "batch 578, training loss: 3.2726: : 578it [06:59,  1.18s/it]\u001b[A\n",
      "batch 579, training loss: 3.3401: : 578it [07:01,  1.18s/it]\u001b[A\n",
      "batch 579, training loss: 3.3401: : 579it [07:01,  1.21s/it]\u001b[A\n",
      "batch 580, training loss: 3.2584: : 579it [07:02,  1.21s/it]\u001b[A\n",
      "batch 580, training loss: 3.2584: : 580it [07:02,  1.25s/it]\u001b[A\n",
      "batch 581, training loss: 3.2532: : 580it [07:03,  1.25s/it]\u001b[A\n",
      "batch 581, training loss: 3.2532: : 581it [07:03,  1.26s/it]\u001b[A\n",
      "batch 582, training loss: 3.302: : 581it [07:05,  1.26s/it] \u001b[A\n",
      "batch 582, training loss: 3.302: : 582it [07:05,  1.25s/it]\u001b[A\n",
      "batch 583, training loss: 2.5256: : 582it [07:05,  1.25s/it]\u001b[A\n",
      "batch 583, training loss: 2.5256: : 583it [07:05,  1.06s/it]\u001b[A\n",
      "batch 584, training loss: 3.4659: : 583it [07:06,  1.06s/it]\u001b[A\n",
      "batch 584, training loss: 3.4659: : 584it [07:06,  1.14s/it]\u001b[A\n",
      "batch 585, training loss: 3.4093: : 584it [07:08,  1.14s/it]\u001b[A\n",
      "batch 585, training loss: 3.4093: : 585it [07:08,  1.20s/it]\u001b[A\n",
      "batch 586, training loss: 3.5049: : 585it [07:09,  1.20s/it]\u001b[A\n",
      "batch 586, training loss: 3.5049: : 586it [07:09,  1.29s/it]\u001b[A\n",
      "batch 587, training loss: 3.3748: : 586it [07:11,  1.29s/it]\u001b[A\n",
      "batch 587, training loss: 3.3748: : 587it [07:11,  1.30s/it]\u001b[A\n",
      "batch 588, training loss: 3.3029: : 587it [07:12,  1.30s/it]\u001b[A\n",
      "batch 588, training loss: 3.3029: : 588it [07:12,  1.36s/it]\u001b[A\n",
      "batch 589, training loss: 3.3655: : 588it [07:14,  1.36s/it]\u001b[A\n",
      "batch 589, training loss: 3.3655: : 589it [07:14,  1.37s/it]\u001b[A\n",
      "batch 590, training loss: 3.3593: : 589it [07:15,  1.37s/it]\u001b[A\n",
      "batch 590, training loss: 3.3593: : 590it [07:15,  1.43s/it]\u001b[A\n",
      "batch 591, training loss: 3.2562: : 590it [07:16,  1.43s/it]\u001b[A\n",
      "batch 591, training loss: 3.2562: : 591it [07:16,  1.41s/it]\u001b[A\n",
      "batch 592, training loss: 3.2994: : 591it [07:18,  1.41s/it]\u001b[A\n",
      "batch 592, training loss: 3.2994: : 592it [07:18,  1.43s/it]\u001b[A\n",
      "batch 593, training loss: 3.2091: : 592it [07:19,  1.43s/it]\u001b[A\n",
      "batch 593, training loss: 3.2091: : 593it [07:19,  1.43s/it]\u001b[A\n",
      "batch 594, training loss: 3.3488: : 593it [07:21,  1.43s/it]\u001b[A\n",
      "batch 594, training loss: 3.3488: : 594it [07:21,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.4192: : 594it [07:22,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.4192: : 595it [07:22,  1.36s/it]\u001b[A\n",
      "batch 596, training loss: 3.3932: : 595it [07:24,  1.36s/it]\u001b[A\n",
      "batch 596, training loss: 3.3932: : 596it [07:24,  1.47s/it]\u001b[A\n",
      "batch 597, training loss: 3.2062: : 596it [07:25,  1.47s/it]\u001b[A\n",
      "batch 597, training loss: 3.2062: : 597it [07:25,  1.45s/it]\u001b[A\n",
      "batch 598, training loss: 3.4269: : 597it [07:27,  1.45s/it]\u001b[A\n",
      "batch 598, training loss: 3.4269: : 598it [07:27,  1.54s/it]\u001b[A\n",
      "batch 599, training loss: 3.2792: : 598it [07:28,  1.54s/it]\u001b[A\n",
      "batch 599, training loss: 3.2792: : 599it [07:28,  1.34s/it]\u001b[A\n",
      "batch 600, training loss: 3.2404: : 599it [07:30,  1.34s/it]\u001b[A\n",
      "batch 600, training loss: 3.2404: : 600it [07:30,  1.48s/it]\u001b[A\n",
      "batch 601, training loss: 2.2882: : 600it [07:30,  1.48s/it]\u001b[A\n",
      "batch 601, training loss: 2.2882: : 601it [07:30,  1.22s/it]\u001b[A\n",
      "batch 602, training loss: 3.2846: : 601it [07:32,  1.22s/it]\u001b[A\n",
      "batch 602, training loss: 3.2846: : 602it [07:32,  1.27s/it]\u001b[A\n",
      "batch 603, training loss: 3.2673: : 602it [07:33,  1.27s/it]\u001b[A\n",
      "batch 603, training loss: 3.2673: : 603it [07:33,  1.31s/it]\u001b[A\n",
      "batch 604, training loss: 3.26: : 603it [07:34,  1.31s/it]  \u001b[A\n",
      "batch 604, training loss: 3.26: : 604it [07:34,  1.31s/it]\u001b[A\n",
      "batch 605, training loss: 3.3832: : 604it [07:36,  1.31s/it]\u001b[A\n",
      "batch 605, training loss: 3.3832: : 605it [07:36,  1.32s/it]\u001b[A\n",
      "batch 606, training loss: 3.1277: : 605it [07:37,  1.32s/it]\u001b[A\n",
      "batch 606, training loss: 3.1277: : 606it [07:37,  1.25s/it]\u001b[A\n",
      "batch 607, training loss: 3.0368: : 606it [07:38,  1.25s/it]\u001b[A\n",
      "batch 607, training loss: 3.0368: : 607it [07:38,  1.19s/it]\u001b[A\n",
      "batch 608, training loss: 3.1249: : 607it [07:39,  1.19s/it]\u001b[A\n",
      "batch 608, training loss: 3.1249: : 608it [07:39,  1.17s/it]\u001b[A\n",
      "batch 609, training loss: 3.1249: : 608it [07:40,  1.17s/it]\u001b[A\n",
      "batch 609, training loss: 3.1249: : 609it [07:40,  1.09s/it]\u001b[A\n",
      "batch 610, training loss: 2.5426: : 609it [07:41,  1.09s/it]\u001b[A\n",
      "batch 610, training loss: 2.5426: : 610it [07:41,  1.04s/it]\u001b[A\n",
      "batch 611, training loss: 2.5477: : 610it [07:42,  1.04s/it]\u001b[A\n",
      "batch 611, training loss: 2.5477: : 611it [07:42,  1.02s/it]\u001b[A\n",
      "batch 612, training loss: 1.7782: : 611it [07:42,  1.02s/it]\u001b[A\n",
      "batch 612, training loss: 1.7782: : 612it [07:42,  1.08it/s]\u001b[A\n",
      "batch 613, training loss: 2.4664: : 612it [07:43,  1.08it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 613, training loss: 2.4664: : 616it [07:44,  1.33it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.7393: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.7393: : 1it [00:00,  3.34it/s]\u001b[A\n",
      "batch 1, dev loss: 3.9256: : 1it [00:00,  3.34it/s]\u001b[A\n",
      "batch 1, dev loss: 3.9256: : 2it [00:00,  4.41it/s]\u001b[A\n",
      "batch 2, dev loss: 3.479: : 2it [00:00,  4.41it/s] \u001b[A\n",
      "batch 2, dev loss: 3.479: : 3it [00:00,  4.83it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6462: : 3it [00:00,  4.83it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6462: : 4it [00:00,  5.08it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6315: : 4it [00:01,  5.08it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6315: : 5it [00:01,  5.23it/s]\u001b[A\n",
      "batch 5, dev loss: 3.6519: : 5it [00:01,  5.23it/s]\u001b[A\n",
      "batch 5, dev loss: 3.6519: : 6it [00:01,  5.32it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7174: : 6it [00:01,  5.32it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7174: : 7it [00:01,  5.51it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5169: : 7it [00:01,  5.51it/s]\u001b[A\n",
      "batch 7, dev loss: 3.5169: : 8it [00:01,  5.57it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7544: : 8it [00:01,  5.57it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7544: : 9it [00:01,  5.21it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6978: : 9it [00:01,  5.21it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6978: : 10it [00:01,  5.41it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7435: : 10it [00:02,  5.41it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7435: : 11it [00:02,  5.54it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7814: : 11it [00:02,  5.54it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7814: : 12it [00:02,  5.81it/s]\u001b[A\n",
      "batch 12, dev loss: 3.5998: : 12it [00:02,  5.81it/s]\u001b[A\n",
      "batch 12, dev loss: 3.5998: : 13it [00:02,  5.71it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7526: : 13it [00:02,  5.71it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7526: : 14it [00:02,  5.91it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9225: : 14it [00:02,  5.91it/s]\u001b[A\n",
      "batch 14, dev loss: 3.9225: : 15it [00:02,  5.77it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6316: : 15it [00:02,  5.77it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6316: : 16it [00:02,  6.51it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0147: : 16it [00:03,  6.51it/s]\u001b[A\n",
      "batch 16, dev loss: 4.0147: : 17it [00:03,  5.91it/s]\u001b[A\n",
      "batch 17, dev loss: 3.7908: : 17it [00:03,  5.91it/s]\u001b[A\n",
      "batch 17, dev loss: 3.7908: : 18it [00:03,  5.58it/s]\u001b[A\n",
      "batch 18, dev loss: 3.6892: : 18it [00:03,  5.58it/s]\u001b[A\n",
      "batch 18, dev loss: 3.6892: : 19it [00:03,  5.42it/s]\u001b[A\n",
      "batch 19, dev loss: 3.8712: : 19it [00:03,  5.42it/s]\u001b[A\n",
      "batch 19, dev loss: 3.8712: : 20it [00:03,  4.84it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7181: : 20it [00:04,  4.84it/s]\u001b[A\n",
      "batch 20, dev loss: 3.7181: : 21it [00:04,  4.52it/s]\u001b[A\n",
      "batch 21, dev loss: 3.593: : 21it [00:04,  4.52it/s] \u001b[A\n",
      "batch 21, dev loss: 3.593: : 22it [00:04,  4.70it/s]\u001b[A\n",
      "batch 22, dev loss: 3.7965: : 22it [00:04,  4.70it/s]\u001b[A\n",
      "batch 22, dev loss: 3.7965: : 23it [00:04,  4.77it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8356: : 23it [00:04,  4.77it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7587: : 23it [00:04,  4.77it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7587: : 25it [00:04,  5.37it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7341: : 25it [00:04,  5.37it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7341: : 26it [00:04,  4.95it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6796: : 26it [00:05,  4.95it/s]\u001b[A\n",
      "batch 26, dev loss: 3.6796: : 27it [00:05,  4.85it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6026: : 27it [00:05,  4.85it/s]\u001b[A\n",
      "batch 27, dev loss: 3.6026: : 28it [00:05,  4.76it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8753: : 28it [00:05,  4.76it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8753: : 29it [00:05,  4.13it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7783: : 29it [00:06,  4.13it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7783: : 30it [00:06,  3.95it/s]\u001b[A\n",
      "batch 30, dev loss: 4.2053: : 30it [00:06,  3.95it/s]\u001b[A\n",
      "batch 30, dev loss: 4.2053: : 31it [00:06,  4.76it/s]\u001b[A\n",
      "batch 31, dev loss: 3.7932: : 31it [00:06,  4.76it/s]\u001b[A\n",
      "batch 31, dev loss: 3.7932: : 32it [00:06,  4.45it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8829: : 32it [00:06,  4.45it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8829: : 33it [00:06,  4.14it/s]\u001b[A\n",
      "batch 33, dev loss: 3.5903: : 33it [00:06,  4.14it/s]\u001b[A\n",
      "batch 33, dev loss: 3.5903: : 34it [00:06,  3.97it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0419: : 34it [00:07,  3.97it/s]\u001b[A\n",
      "batch 34, dev loss: 4.0419: : 35it [00:07,  3.65it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8791: : 35it [00:07,  3.65it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8791: : 36it [00:07,  3.49it/s]\u001b[A\n",
      "batch 36, dev loss: 3.78: : 36it [00:07,  3.49it/s]  \u001b[A\n",
      "batch 36, dev loss: 3.78: : 37it [00:07,  3.88it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5506: : 37it [00:08,  3.88it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5506: : 38it [00:08,  3.73it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8187: : 38it [00:08,  3.73it/s]\u001b[A\n",
      "batch 38, dev loss: 3.8187: : 39it [00:08,  3.63it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8085: : 39it [00:08,  3.63it/s]\u001b[A\n",
      "batch 39, dev loss: 3.8085: : 40it [00:08,  3.55it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8494: : 40it [00:08,  3.55it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8494: : 41it [00:08,  3.54it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6517: : 41it [00:09,  3.54it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6517: : 42it [00:09,  3.42it/s]\u001b[A\n",
      "batch 42, dev loss: 3.7866: : 42it [00:09,  3.42it/s]\u001b[A\n",
      "batch 42, dev loss: 3.7866: : 43it [00:09,  3.33it/s]\u001b[A\n",
      "batch 43, dev loss: 3.7713: : 43it [00:09,  3.33it/s]\u001b[A\n",
      "batch 43, dev loss: 3.7713: : 44it [00:09,  3.32it/s]\u001b[A\n",
      "batch 44, dev loss: 3.6991: : 44it [00:10,  3.32it/s]\u001b[A\n",
      "batch 44, dev loss: 3.6991: : 45it [00:10,  3.38it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0167: : 45it [00:10,  3.38it/s]\u001b[A\n",
      "batch 45, dev loss: 4.0167: : 46it [00:10,  3.33it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5243: : 46it [00:10,  3.33it/s]\u001b[A\n",
      "batch 46, dev loss: 3.5243: : 47it [00:10,  3.28it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7112: : 47it [00:11,  3.28it/s]\u001b[A\n",
      "batch 47, dev loss: 3.7112: : 48it [00:11,  2.88it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5489: : 48it [00:11,  2.88it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5489: : 49it [00:11,  2.91it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7088: : 49it [00:11,  2.91it/s]\u001b[A\n",
      "batch 49, dev loss: 3.7088: : 50it [00:11,  3.40it/s]\u001b[A\n",
      "batch 50, dev loss: 3.6717: : 50it [00:12,  3.40it/s]\u001b[A\n",
      "batch 50, dev loss: 3.6717: : 51it [00:12,  3.22it/s]\u001b[A\n",
      "batch 51, dev loss: 3.6868: : 51it [00:12,  3.22it/s]\u001b[A\n",
      "batch 51, dev loss: 3.6868: : 52it [00:12,  3.10it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4566: : 52it [00:12,  3.10it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4566: : 53it [00:12,  3.05it/s]\u001b[A\n",
      "batch 53, dev loss: 3.6855: : 53it [00:13,  3.05it/s]\u001b[A\n",
      "batch 53, dev loss: 3.6855: : 54it [00:13,  2.82it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4166: : 54it [00:13,  2.82it/s]\u001b[A\n",
      "batch 54, dev loss: 3.4166: : 55it [00:13,  2.82it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6258: : 55it [00:13,  2.82it/s]\u001b[A\n",
      "batch 55, dev loss: 3.6258: : 56it [00:13,  2.72it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4581: : 56it [00:14,  2.72it/s]\u001b[A\n",
      "batch 56, dev loss: 3.4581: : 57it [00:14,  2.85it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4326: : 57it [00:14,  2.85it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4326: : 58it [00:14,  2.72it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8233: : 58it [00:14,  2.72it/s]\u001b[A\n",
      "batch 58, dev loss: 3.8233: : 59it [00:14,  2.81it/s]\u001b[A\n",
      "batch 59, dev loss: 3.6748: : 59it [00:15,  2.81it/s]\u001b[A\n",
      "batch 59, dev loss: 3.6748: : 60it [00:15,  2.94it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4327: : 60it [00:15,  2.94it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4327: : 61it [00:15,  2.78it/s]\u001b[A\n",
      "batch 61, dev loss: 3.3758: : 61it [00:15,  2.78it/s]\u001b[A\n",
      "batch 61, dev loss: 3.3758: : 62it [00:15,  3.06it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1244: : 62it [00:16,  3.06it/s]\u001b[A\n",
      "batch 62, dev loss: 3.1244: : 63it [00:16,  3.32it/s]\u001b[A\n",
      "batch 63, dev loss: 3.782: : 63it [00:16,  3.32it/s] \u001b[A\n",
      "batch 63, dev loss: 3.782: : 64it [00:16,  3.54it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5098: : 64it [00:16,  3.54it/s]\u001b[A\n",
      "batch 64, dev loss: 3.5098: : 65it [00:16,  3.78it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4478: : 65it [00:16,  3.78it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4478: : 66it [00:16,  3.84it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5623: : 66it [00:17,  3.84it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5623: : 67it [00:17,  3.87it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5465: : 67it [00:17,  3.87it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5465: : 68it [00:17,  4.18it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6449: : 68it [00:17,  4.18it/s]\u001b[A\n",
      "batch 68, dev loss: 2.6449: : 69it [00:17,  4.27it/s]\u001b[A\n",
      "batch 69, dev loss: 2.9799: : 69it [00:17,  4.27it/s]\u001b[A\n",
      "batch 69, dev loss: 2.9799: : 70it [00:17,  3.99it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9218: : 70it [00:18,  3.99it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 70, dev loss: 3.9218: : 71it [00:18,  3.83it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1316: : 71it [00:18,  3.83it/s]\u001b[A\n",
      "batch 72, dev loss: 4.085: : 71it [00:18,  3.83it/s] \u001b[A\n",
      "batch 72, dev loss: 4.085: : 76it [00:18,  4.13it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:00,  1.54it/s]\u001b[A\n",
      "2it [00:01,  1.54it/s]\u001b[A\n",
      "3it [00:02,  1.34it/s]\u001b[A\n",
      "4it [00:02,  1.31it/s]\u001b[A\n",
      "5it [00:03,  1.28it/s]\u001b[A\n",
      "6it [00:04,  1.37it/s]\u001b[A\n",
      "7it [00:05,  1.33it/s]\u001b[A\n",
      "8it [00:05,  1.30it/s]\u001b[A\n",
      "9it [00:06,  1.23it/s]\u001b[A\n",
      "10it [00:08,  1.10it/s]\u001b[A\n",
      "11it [00:08,  1.08it/s]\u001b[A\n",
      "12it [00:09,  1.07it/s]\u001b[A\n",
      "13it [00:10,  1.05it/s]\u001b[A\n",
      "14it [00:11,  1.11it/s]\u001b[A\n",
      "15it [00:12,  1.08it/s]\u001b[A\n",
      "16it [00:13,  1.25it/s]\u001b[A\n",
      "17it [00:14,  1.10it/s]\u001b[A\n",
      "18it [00:15,  1.06it/s]\u001b[A\n",
      "19it [00:16,  1.03s/it]\u001b[A\n",
      "20it [00:17,  1.07s/it]\u001b[A\n",
      "21it [00:18,  1.06s/it]\u001b[A\n",
      "22it [00:20,  1.16s/it]\u001b[A\n",
      "23it [00:21,  1.14s/it]\u001b[A\n",
      "24it [00:21,  1.14it/s]\u001b[A\n",
      "25it [00:22,  1.03it/s]\u001b[A\n",
      "26it [00:24,  1.13s/it]\u001b[A\n",
      "27it [00:25,  1.21s/it]\u001b[A\n",
      "28it [00:26,  1.24s/it]\u001b[A\n",
      "29it [00:28,  1.28s/it]\u001b[A\n",
      "30it [00:29,  1.23s/it]\u001b[A\n",
      "31it [00:30,  1.26s/it]\u001b[A\n",
      "32it [00:32,  1.33s/it]\u001b[A\n",
      "33it [00:33,  1.42s/it]\u001b[A\n",
      "34it [00:35,  1.49s/it]\u001b[A\n",
      "35it [00:37,  1.53s/it]\u001b[A\n",
      "36it [00:37,  1.17s/it]\u001b[A\n",
      "37it [00:38,  1.04s/it]\u001b[A\n",
      "38it [00:39,  1.16s/it]\u001b[A\n",
      "39it [00:41,  1.33s/it]\u001b[A\n",
      "40it [00:43,  1.41s/it]\u001b[A\n",
      "41it [00:43,  1.12s/it]\u001b[A\n",
      "42it [00:45,  1.36s/it]\u001b[A\n",
      "43it [00:47,  1.56s/it]\u001b[A\n",
      "44it [00:49,  1.70s/it]\u001b[A\n",
      "45it [00:50,  1.57s/it]\u001b[A\n",
      "46it [00:51,  1.44s/it]\u001b[A\n",
      "47it [00:52,  1.23s/it]\u001b[A\n",
      "48it [00:54,  1.48s/it]\u001b[A\n",
      "49it [00:54,  1.08s/it]\u001b[A\n",
      "50it [00:56,  1.35s/it]\u001b[A\n",
      "51it [00:58,  1.57s/it]\u001b[A\n",
      "52it [01:00,  1.45s/it]\u001b[A\n",
      "53it [01:02,  1.77s/it]\u001b[A\n",
      "54it [01:04,  1.85s/it]\u001b[A\n",
      "55it [01:06,  1.99s/it]\u001b[A\n",
      "56it [01:08,  1.85s/it]\u001b[A\n",
      "57it [01:10,  1.97s/it]\u001b[A\n",
      "58it [01:12,  1.99s/it]\u001b[A\n",
      "59it [01:14,  1.84s/it]\u001b[A\n",
      "60it [01:15,  1.75s/it]\u001b[A\n",
      "61it [01:16,  1.49s/it]\u001b[A\n",
      "62it [01:17,  1.29s/it]\u001b[A\n",
      "63it [01:18,  1.11s/it]\u001b[A\n",
      "64it [01:18,  1.01it/s]\u001b[A\n",
      "65it [01:19,  1.20it/s]\u001b[A\n",
      "66it [01:19,  1.51it/s]\u001b[A\n",
      "67it [01:19,  1.84it/s]\u001b[A\n",
      "68it [01:20,  2.11it/s]\u001b[A\n",
      "69it [01:20,  2.32it/s]\u001b[A\n",
      "70it [01:20,  1.16s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████                                  | 731/6740 [00:00<00:00, 7304.92it/s]\u001b[A\n",
      " 22%|████████                             | 1474/6740 [00:00<00:00, 7374.62it/s]\u001b[A\n",
      " 33%|████████████▏                        | 2214/6740 [00:00<00:00, 7385.78it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 2953/6740 [00:00<00:00, 7137.99it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 3669/6740 [00:00<00:00, 7100.14it/s]\u001b[A\n",
      " 65%|████████████████████████             | 4380/6740 [00:00<00:00, 6967.70it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 5078/6740 [00:00<00:00, 6912.40it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 5770/6740 [00:00<00:00, 6891.00it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:00<00:00, 7002.72it/s]\u001b[A\n",
      "Epoch: 12, tfr: 1.0, loss(train/dev): 3.4007/3.6579, ppl(dev/test): 38.7798/45.2\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.4403: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.4403: : 1it [00:01,  1.90s/it]\u001b[A\n",
      "batch 2, training loss: 3.3497: : 1it [00:02,  1.90s/it]\u001b[A\n",
      "batch 2, training loss: 3.3497: : 2it [00:02,  1.06s/it]\u001b[A\n",
      "batch 3, training loss: 3.4044: : 2it [00:02,  1.06s/it]\u001b[A\n",
      "batch 3, training loss: 3.4044: : 3it [00:02,  1.16it/s]\u001b[A\n",
      "batch 4, training loss: 3.347: : 3it [00:03,  1.16it/s] \u001b[A\n",
      "batch 4, training loss: 3.347: : 4it [00:03,  1.42it/s]\u001b[A\n",
      "batch 5, training loss: 3.1432: : 4it [00:03,  1.42it/s]\u001b[A\n",
      "batch 5, training loss: 3.1432: : 5it [00:03,  1.66it/s]\u001b[A\n",
      "batch 6, training loss: 3.4489: : 5it [00:04,  1.66it/s]\u001b[A\n",
      "batch 6, training loss: 3.4489: : 6it [00:04,  1.85it/s]\u001b[A\n",
      "batch 7, training loss: 3.4303: : 6it [00:04,  1.85it/s]\u001b[A\n",
      "batch 7, training loss: 3.4303: : 7it [00:04,  2.01it/s]\u001b[A\n",
      "batch 8, training loss: 3.4545: : 7it [00:05,  2.01it/s]\u001b[A\n",
      "batch 8, training loss: 3.4545: : 8it [00:05,  1.96it/s]\u001b[A\n",
      "batch 9, training loss: 3.2967: : 8it [00:05,  1.96it/s]\u001b[A\n",
      "batch 9, training loss: 3.2967: : 9it [00:05,  2.05it/s]\u001b[A\n",
      "batch 10, training loss: 3.3241: : 9it [00:06,  2.05it/s]\u001b[A\n",
      "batch 10, training loss: 3.3241: : 10it [00:06,  2.16it/s]\u001b[A\n",
      "batch 11, training loss: 3.3387: : 10it [00:06,  2.16it/s]\u001b[A\n",
      "batch 11, training loss: 3.3387: : 11it [00:06,  2.23it/s]\u001b[A\n",
      "batch 12, training loss: 3.3879: : 11it [00:06,  2.23it/s]\u001b[A\n",
      "batch 12, training loss: 3.3879: : 12it [00:06,  2.28it/s]\u001b[A\n",
      "batch 13, training loss: 3.2422: : 12it [00:07,  2.28it/s]\u001b[A\n",
      "batch 13, training loss: 3.2422: : 13it [00:07,  2.32it/s]\u001b[A\n",
      "batch 14, training loss: 3.5405: : 13it [00:07,  2.32it/s]\u001b[A\n",
      "batch 14, training loss: 3.5405: : 14it [00:07,  2.35it/s]\u001b[A\n",
      "batch 15, training loss: 3.3952: : 14it [00:08,  2.35it/s]\u001b[A\n",
      "batch 15, training loss: 3.3952: : 15it [00:08,  2.13it/s]\u001b[A\n",
      "batch 16, training loss: 3.4047: : 15it [00:08,  2.13it/s]\u001b[A\n",
      "batch 16, training loss: 3.4047: : 16it [00:08,  2.22it/s]\u001b[A\n",
      "batch 17, training loss: 3.5323: : 16it [00:09,  2.22it/s]\u001b[A\n",
      "batch 17, training loss: 3.5323: : 17it [00:09,  2.28it/s]\u001b[A\n",
      "batch 18, training loss: 3.4121: : 17it [00:09,  2.28it/s]\u001b[A\n",
      "batch 18, training loss: 3.4121: : 18it [00:09,  2.32it/s]\u001b[A\n",
      "batch 19, training loss: 3.1635: : 18it [00:09,  2.32it/s]\u001b[A\n",
      "batch 19, training loss: 3.1635: : 19it [00:09,  2.35it/s]\u001b[A\n",
      "batch 20, training loss: 3.3042: : 19it [00:10,  2.35it/s]\u001b[A\n",
      "batch 20, training loss: 3.3042: : 20it [00:10,  2.36it/s]\u001b[A\n",
      "batch 21, training loss: 3.4115: : 20it [00:10,  2.36it/s]\u001b[A\n",
      "batch 21, training loss: 3.4115: : 21it [00:10,  2.39it/s]\u001b[A\n",
      "batch 22, training loss: 3.1995: : 21it [00:11,  2.39it/s]\u001b[A\n",
      "batch 22, training loss: 3.1995: : 22it [00:11,  2.17it/s]\u001b[A\n",
      "batch 23, training loss: 3.3532: : 22it [00:11,  2.17it/s]\u001b[A\n",
      "batch 23, training loss: 3.3532: : 23it [00:11,  2.25it/s]\u001b[A\n",
      "batch 24, training loss: 3.2882: : 23it [00:12,  2.25it/s]\u001b[A\n",
      "batch 24, training loss: 3.2882: : 24it [00:12,  2.30it/s]\u001b[A\n",
      "batch 25, training loss: 3.3811: : 24it [00:12,  2.30it/s]\u001b[A\n",
      "batch 25, training loss: 3.3811: : 25it [00:12,  2.33it/s]\u001b[A\n",
      "batch 26, training loss: 3.1933: : 25it [00:13,  2.33it/s]\u001b[A\n",
      "batch 26, training loss: 3.1933: : 26it [00:13,  2.35it/s]\u001b[A\n",
      "batch 27, training loss: 3.3407: : 26it [00:13,  2.35it/s]\u001b[A\n",
      "batch 27, training loss: 3.3407: : 27it [00:13,  2.13it/s]\u001b[A\n",
      "batch 28, training loss: 3.1855: : 27it [00:13,  2.13it/s]\u001b[A\n",
      "batch 28, training loss: 3.1855: : 28it [00:13,  2.23it/s]\u001b[A\n",
      "batch 29, training loss: 3.3575: : 28it [00:14,  2.23it/s]\u001b[A\n",
      "batch 29, training loss: 3.3575: : 29it [00:14,  2.29it/s]\u001b[A\n",
      "batch 30, training loss: 3.4667: : 29it [00:14,  2.29it/s]\u001b[A\n",
      "batch 30, training loss: 3.4667: : 30it [00:14,  2.33it/s]\u001b[A\n",
      "batch 31, training loss: 3.2382: : 30it [00:15,  2.33it/s]\u001b[A\n",
      "batch 31, training loss: 3.2382: : 31it [00:15,  2.35it/s]\u001b[A\n",
      "batch 32, training loss: 3.413: : 31it [00:15,  2.35it/s] \u001b[A\n",
      "batch 32, training loss: 3.413: : 32it [00:15,  2.37it/s]\u001b[A\n",
      "batch 33, training loss: 3.2802: : 32it [00:16,  2.37it/s]\u001b[A\n",
      "batch 33, training loss: 3.2802: : 33it [00:16,  2.14it/s]\u001b[A\n",
      "batch 34, training loss: 3.3019: : 33it [00:16,  2.14it/s]\u001b[A\n",
      "batch 34, training loss: 3.3019: : 34it [00:16,  2.23it/s]\u001b[A\n",
      "batch 35, training loss: 3.3624: : 34it [00:17,  2.23it/s]\u001b[A\n",
      "batch 35, training loss: 3.3624: : 35it [00:17,  2.28it/s]\u001b[A\n",
      "batch 36, training loss: 3.436: : 35it [00:17,  2.28it/s] \u001b[A\n",
      "batch 36, training loss: 3.436: : 36it [00:17,  2.32it/s]\u001b[A\n",
      "batch 37, training loss: 3.2852: : 36it [00:17,  2.32it/s]\u001b[A\n",
      "batch 37, training loss: 3.2852: : 37it [00:17,  2.34it/s]\u001b[A\n",
      "batch 38, training loss: 3.0983: : 37it [00:18,  2.34it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 38, training loss: 3.0983: : 38it [00:18,  2.37it/s]\u001b[A\n",
      "batch 39, training loss: 3.2724: : 38it [00:18,  2.37it/s]\u001b[A\n",
      "batch 39, training loss: 3.2724: : 39it [00:18,  2.14it/s]\u001b[A\n",
      "batch 40, training loss: 3.3074: : 39it [00:19,  2.14it/s]\u001b[A\n",
      "batch 40, training loss: 3.3074: : 40it [00:19,  2.23it/s]\u001b[A\n",
      "batch 41, training loss: 3.4866: : 40it [00:19,  2.23it/s]\u001b[A\n",
      "batch 41, training loss: 3.4866: : 41it [00:19,  2.28it/s]\u001b[A\n",
      "batch 42, training loss: 3.311: : 41it [00:20,  2.28it/s] \u001b[A\n",
      "batch 42, training loss: 3.311: : 42it [00:20,  2.32it/s]\u001b[A\n",
      "batch 43, training loss: 3.1139: : 42it [00:20,  2.32it/s]\u001b[A\n",
      "batch 43, training loss: 3.1139: : 43it [00:20,  2.34it/s]\u001b[A\n",
      "batch 44, training loss: 3.0915: : 43it [00:21,  2.34it/s]\u001b[A\n",
      "batch 44, training loss: 3.0915: : 44it [00:21,  2.13it/s]\u001b[A\n",
      "batch 45, training loss: 3.1488: : 44it [00:21,  2.13it/s]\u001b[A\n",
      "batch 45, training loss: 3.1488: : 45it [00:21,  2.22it/s]\u001b[A\n",
      "batch 46, training loss: 3.2618: : 45it [00:21,  2.22it/s]\u001b[A\n",
      "batch 46, training loss: 3.2618: : 46it [00:21,  2.28it/s]\u001b[A\n",
      "batch 47, training loss: 3.2965: : 46it [00:22,  2.28it/s]\u001b[A\n",
      "batch 47, training loss: 3.2965: : 47it [00:22,  2.33it/s]\u001b[A\n",
      "batch 48, training loss: 3.176: : 47it [00:22,  2.33it/s] \u001b[A\n",
      "batch 48, training loss: 3.176: : 48it [00:22,  2.35it/s]\u001b[A\n",
      "batch 49, training loss: 3.4422: : 48it [00:23,  2.35it/s]\u001b[A\n",
      "batch 49, training loss: 3.4422: : 49it [00:23,  2.37it/s]\u001b[A\n",
      "batch 50, training loss: 3.2115: : 49it [00:23,  2.37it/s]\u001b[A\n",
      "batch 50, training loss: 3.2115: : 50it [00:23,  2.38it/s]\u001b[A\n",
      "batch 51, training loss: 3.2942: : 50it [00:24,  2.38it/s]\u001b[A\n",
      "batch 51, training loss: 3.2942: : 51it [00:24,  2.15it/s]\u001b[A\n",
      "batch 52, training loss: 3.1445: : 51it [00:24,  2.15it/s]\u001b[A\n",
      "batch 52, training loss: 3.1445: : 52it [00:24,  2.24it/s]\u001b[A\n",
      "batch 53, training loss: 3.1718: : 52it [00:24,  2.24it/s]\u001b[A\n",
      "batch 53, training loss: 3.1718: : 53it [00:24,  2.31it/s]\u001b[A\n",
      "batch 54, training loss: 3.1315: : 53it [00:25,  2.31it/s]\u001b[A\n",
      "batch 54, training loss: 3.1315: : 54it [00:25,  2.42it/s]\u001b[A\n",
      "batch 55, training loss: 3.2525: : 54it [00:25,  2.42it/s]\u001b[A\n",
      "batch 55, training loss: 3.2525: : 55it [00:25,  2.46it/s]\u001b[A\n",
      "batch 56, training loss: 3.2665: : 55it [00:25,  2.46it/s]\u001b[A\n",
      "batch 56, training loss: 3.2665: : 56it [00:25,  2.69it/s]\u001b[A\n",
      "batch 57, training loss: 3.2325: : 56it [00:26,  2.69it/s]\u001b[A\n",
      "batch 57, training loss: 3.2325: : 57it [00:26,  2.28it/s]\u001b[A\n",
      "batch 58, training loss: 3.3056: : 57it [00:26,  2.28it/s]\u001b[A\n",
      "batch 58, training loss: 3.3056: : 58it [00:26,  2.30it/s]\u001b[A\n",
      "batch 59, training loss: 3.1776: : 58it [00:27,  2.30it/s]\u001b[A\n",
      "batch 59, training loss: 3.1776: : 59it [00:27,  2.30it/s]\u001b[A\n",
      "batch 60, training loss: 3.1276: : 59it [00:27,  2.30it/s]\u001b[A\n",
      "batch 60, training loss: 3.1276: : 60it [00:27,  2.31it/s]\u001b[A\n",
      "batch 61, training loss: 3.3665: : 60it [00:28,  2.31it/s]\u001b[A\n",
      "batch 61, training loss: 3.3665: : 61it [00:28,  2.00it/s]\u001b[A\n",
      "batch 62, training loss: 3.1965: : 61it [00:28,  2.00it/s]\u001b[A\n",
      "batch 62, training loss: 3.1965: : 62it [00:28,  2.11it/s]\u001b[A\n",
      "batch 63, training loss: 3.2709: : 62it [00:29,  2.11it/s]\u001b[A\n",
      "batch 63, training loss: 3.2709: : 63it [00:29,  2.18it/s]\u001b[A\n",
      "batch 64, training loss: 3.2524: : 63it [00:29,  2.18it/s]\u001b[A\n",
      "batch 64, training loss: 3.2524: : 64it [00:29,  2.25it/s]\u001b[A\n",
      "batch 65, training loss: 3.2959: : 64it [00:30,  2.25it/s]\u001b[A\n",
      "batch 65, training loss: 3.2959: : 65it [00:30,  2.29it/s]\u001b[A\n",
      "batch 66, training loss: 3.341: : 65it [00:30,  2.29it/s] \u001b[A\n",
      "batch 66, training loss: 3.341: : 66it [00:30,  2.32it/s]\u001b[A\n",
      "batch 67, training loss: 3.215: : 66it [00:31,  2.32it/s]\u001b[A\n",
      "batch 67, training loss: 3.215: : 67it [00:31,  2.11it/s]\u001b[A\n",
      "batch 68, training loss: 3.3003: : 67it [00:31,  2.11it/s]\u001b[A\n",
      "batch 68, training loss: 3.3003: : 68it [00:31,  2.19it/s]\u001b[A\n",
      "batch 69, training loss: 3.1757: : 68it [00:31,  2.19it/s]\u001b[A\n",
      "batch 69, training loss: 3.1757: : 69it [00:31,  2.27it/s]\u001b[A\n",
      "batch 70, training loss: 3.37: : 69it [00:32,  2.27it/s]  \u001b[A\n",
      "batch 70, training loss: 3.37: : 70it [00:32,  2.33it/s]\u001b[A\n",
      "batch 71, training loss: 3.2229: : 70it [00:32,  2.33it/s]\u001b[A\n",
      "batch 71, training loss: 3.2229: : 71it [00:32,  2.35it/s]\u001b[A\n",
      "batch 72, training loss: 3.1806: : 71it [00:33,  2.35it/s]\u001b[A\n",
      "batch 72, training loss: 3.1806: : 72it [00:33,  2.37it/s]\u001b[A\n",
      "batch 73, training loss: 3.2408: : 72it [00:33,  2.37it/s]\u001b[A\n",
      "batch 73, training loss: 3.2408: : 73it [00:33,  2.38it/s]\u001b[A\n",
      "batch 74, training loss: 3.147: : 73it [00:34,  2.38it/s] \u001b[A\n",
      "batch 74, training loss: 3.147: : 74it [00:34,  2.14it/s]\u001b[A\n",
      "batch 75, training loss: 3.3368: : 74it [00:34,  2.14it/s]\u001b[A\n",
      "batch 75, training loss: 3.3368: : 75it [00:34,  2.23it/s]\u001b[A\n",
      "batch 76, training loss: 3.153: : 75it [00:35,  2.23it/s] \u001b[A\n",
      "batch 76, training loss: 3.153: : 76it [00:35,  2.29it/s]\u001b[A\n",
      "batch 77, training loss: 3.2956: : 76it [00:35,  2.29it/s]\u001b[A\n",
      "batch 77, training loss: 3.2956: : 77it [00:35,  2.32it/s]\u001b[A\n",
      "batch 78, training loss: 3.2606: : 77it [00:35,  2.32it/s]\u001b[A\n",
      "batch 78, training loss: 3.2606: : 78it [00:35,  2.35it/s]\u001b[A\n",
      "batch 79, training loss: 3.3138: : 78it [00:36,  2.35it/s]\u001b[A\n",
      "batch 79, training loss: 3.3138: : 79it [00:36,  2.37it/s]\u001b[A\n",
      "batch 80, training loss: 3.2594: : 79it [00:36,  2.37it/s]\u001b[A\n",
      "batch 80, training loss: 3.2594: : 80it [00:36,  2.18it/s]\u001b[A\n",
      "batch 81, training loss: 3.2292: : 80it [00:37,  2.18it/s]\u001b[A\n",
      "batch 81, training loss: 3.2292: : 81it [00:37,  2.22it/s]\u001b[A\n",
      "batch 82, training loss: 3.3663: : 81it [00:37,  2.22it/s]\u001b[A\n",
      "batch 82, training loss: 3.3663: : 82it [00:37,  2.28it/s]\u001b[A\n",
      "batch 83, training loss: 3.2388: : 82it [00:38,  2.28it/s]\u001b[A\n",
      "batch 83, training loss: 3.2388: : 83it [00:38,  2.32it/s]\u001b[A\n",
      "batch 84, training loss: 3.3704: : 83it [00:38,  2.32it/s]\u001b[A\n",
      "batch 84, training loss: 3.3704: : 84it [00:38,  2.34it/s]\u001b[A\n",
      "batch 85, training loss: 3.3853: : 84it [00:38,  2.34it/s]\u001b[A\n",
      "batch 85, training loss: 3.3853: : 85it [00:38,  2.36it/s]\u001b[A\n",
      "batch 86, training loss: 3.3505: : 85it [00:39,  2.36it/s]\u001b[A\n",
      "batch 86, training loss: 3.3505: : 86it [00:39,  2.37it/s]\u001b[A\n",
      "batch 87, training loss: 3.3562: : 86it [00:39,  2.37it/s]\u001b[A\n",
      "batch 87, training loss: 3.3562: : 87it [00:39,  2.24it/s]\u001b[A\n",
      "batch 88, training loss: 3.3894: : 87it [00:40,  2.24it/s]\u001b[A\n",
      "batch 88, training loss: 3.3894: : 88it [00:40,  2.15it/s]\u001b[A\n",
      "batch 89, training loss: 3.4943: : 88it [00:40,  2.15it/s]\u001b[A\n",
      "batch 89, training loss: 3.4943: : 89it [00:40,  2.12it/s]\u001b[A\n",
      "batch 90, training loss: 3.4146: : 89it [00:41,  2.12it/s]\u001b[A\n",
      "batch 90, training loss: 3.4146: : 90it [00:41,  2.01it/s]\u001b[A\n",
      "batch 91, training loss: 3.5328: : 90it [00:41,  2.01it/s]\u001b[A\n",
      "batch 91, training loss: 3.5328: : 91it [00:41,  2.01it/s]\u001b[A\n",
      "batch 92, training loss: 3.3667: : 91it [00:42,  2.01it/s]\u001b[A\n",
      "batch 92, training loss: 3.3667: : 92it [00:42,  1.81it/s]\u001b[A\n",
      "batch 93, training loss: 3.3563: : 92it [00:42,  1.81it/s]\u001b[A\n",
      "batch 93, training loss: 3.3563: : 93it [00:42,  1.91it/s]\u001b[A\n",
      "batch 94, training loss: 3.4408: : 93it [00:43,  1.91it/s]\u001b[A\n",
      "batch 94, training loss: 3.4408: : 94it [00:43,  1.87it/s]\u001b[A\n",
      "batch 95, training loss: 3.3846: : 94it [00:44,  1.87it/s]\u001b[A\n",
      "batch 95, training loss: 3.3846: : 95it [00:44,  1.92it/s]\u001b[A\n",
      "batch 96, training loss: 3.3925: : 95it [00:44,  1.92it/s]\u001b[A\n",
      "batch 96, training loss: 3.3925: : 96it [00:44,  1.74it/s]\u001b[A\n",
      "batch 97, training loss: 3.4788: : 96it [00:45,  1.74it/s]\u001b[A\n",
      "batch 97, training loss: 3.4788: : 97it [00:45,  1.75it/s]\u001b[A\n",
      "batch 98, training loss: 3.4833: : 97it [00:45,  1.75it/s]\u001b[A\n",
      "batch 98, training loss: 3.4833: : 98it [00:45,  1.83it/s]\u001b[A\n",
      "batch 99, training loss: 3.2842: : 98it [00:46,  1.83it/s]\u001b[A\n",
      "batch 99, training loss: 3.2842: : 99it [00:46,  1.84it/s]\u001b[A\n",
      "batch 100, training loss: 3.1695: : 99it [00:46,  1.84it/s]\u001b[A\n",
      "batch 100, training loss: 3.1695: : 100it [00:46,  1.77it/s]\u001b[A\n",
      "batch 101, training loss: 3.3186: : 100it [00:47,  1.77it/s]\u001b[A\n",
      "batch 101, training loss: 3.3186: : 101it [00:47,  1.80it/s]\u001b[A\n",
      "batch 102, training loss: 3.3267: : 101it [00:47,  1.80it/s]\u001b[A\n",
      "batch 102, training loss: 3.3267: : 102it [00:47,  1.84it/s]\u001b[A\n",
      "batch 103, training loss: 3.2717: : 102it [00:48,  1.84it/s]\u001b[A\n",
      "batch 103, training loss: 3.2717: : 103it [00:48,  1.84it/s]\u001b[A\n",
      "batch 104, training loss: 3.1246: : 103it [00:49,  1.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 104, training loss: 3.1246: : 104it [00:49,  1.73it/s]\u001b[A\n",
      "batch 105, training loss: 3.2638: : 104it [00:49,  1.73it/s]\u001b[A\n",
      "batch 105, training loss: 3.2638: : 105it [00:49,  1.76it/s]\u001b[A\n",
      "batch 106, training loss: 3.4263: : 105it [00:50,  1.76it/s]\u001b[A\n",
      "batch 106, training loss: 3.4263: : 106it [00:50,  1.77it/s]\u001b[A\n",
      "batch 107, training loss: 3.1318: : 106it [00:50,  1.77it/s]\u001b[A\n",
      "batch 107, training loss: 3.1318: : 107it [00:50,  1.87it/s]\u001b[A\n",
      "batch 108, training loss: 3.3862: : 107it [00:51,  1.87it/s]\u001b[A\n",
      "batch 108, training loss: 3.3862: : 108it [00:51,  1.73it/s]\u001b[A\n",
      "batch 109, training loss: 3.4237: : 108it [00:51,  1.73it/s]\u001b[A\n",
      "batch 109, training loss: 3.4237: : 109it [00:51,  1.80it/s]\u001b[A\n",
      "batch 110, training loss: 3.5712: : 109it [00:52,  1.80it/s]\u001b[A\n",
      "batch 110, training loss: 3.5712: : 110it [00:52,  1.80it/s]\u001b[A\n",
      "batch 111, training loss: 3.2951: : 110it [00:52,  1.80it/s]\u001b[A\n",
      "batch 111, training loss: 3.2951: : 111it [00:52,  1.88it/s]\u001b[A\n",
      "batch 112, training loss: 3.3656: : 111it [00:53,  1.88it/s]\u001b[A\n",
      "batch 112, training loss: 3.3656: : 112it [00:53,  1.77it/s]\u001b[A\n",
      "batch 113, training loss: 3.4135: : 112it [00:54,  1.77it/s]\u001b[A\n",
      "batch 113, training loss: 3.4135: : 113it [00:54,  1.81it/s]\u001b[A\n",
      "batch 114, training loss: 3.406: : 113it [00:54,  1.81it/s] \u001b[A\n",
      "batch 114, training loss: 3.406: : 114it [00:54,  1.81it/s]\u001b[A\n",
      "batch 115, training loss: 3.3278: : 114it [00:55,  1.81it/s]\u001b[A\n",
      "batch 115, training loss: 3.3278: : 115it [00:55,  1.89it/s]\u001b[A\n",
      "batch 116, training loss: 3.3458: : 115it [00:55,  1.89it/s]\u001b[A\n",
      "batch 116, training loss: 3.3458: : 116it [00:55,  1.86it/s]\u001b[A\n",
      "batch 117, training loss: 3.3971: : 116it [00:56,  1.86it/s]\u001b[A\n",
      "batch 117, training loss: 3.3971: : 117it [00:56,  1.77it/s]\u001b[A\n",
      "batch 118, training loss: 3.4396: : 117it [00:56,  1.77it/s]\u001b[A\n",
      "batch 118, training loss: 3.4396: : 118it [00:56,  1.81it/s]\u001b[A\n",
      "batch 119, training loss: 3.3788: : 118it [00:57,  1.81it/s]\u001b[A\n",
      "batch 119, training loss: 3.3788: : 119it [00:57,  1.89it/s]\u001b[A\n",
      "batch 120, training loss: 3.2993: : 119it [00:57,  1.89it/s]\u001b[A\n",
      "batch 120, training loss: 3.2993: : 120it [00:57,  1.86it/s]\u001b[A\n",
      "batch 121, training loss: 3.5619: : 120it [00:58,  1.86it/s]\u001b[A\n",
      "batch 121, training loss: 3.5619: : 121it [00:58,  1.90it/s]\u001b[A\n",
      "batch 122, training loss: 3.2231: : 121it [00:59,  1.90it/s]\u001b[A\n",
      "batch 122, training loss: 3.2231: : 122it [00:59,  1.75it/s]\u001b[A\n",
      "batch 123, training loss: 3.3964: : 122it [00:59,  1.75it/s]\u001b[A\n",
      "batch 123, training loss: 3.3964: : 123it [00:59,  1.85it/s]\u001b[A\n",
      "batch 124, training loss: 3.3999: : 123it [01:00,  1.85it/s]\u001b[A\n",
      "batch 124, training loss: 3.3999: : 124it [01:00,  1.84it/s]\u001b[A\n",
      "batch 125, training loss: 3.4208: : 124it [01:00,  1.84it/s]\u001b[A\n",
      "batch 125, training loss: 3.4208: : 125it [01:00,  1.89it/s]\u001b[A\n",
      "batch 126, training loss: 3.4902: : 125it [01:01,  1.89it/s]\u001b[A\n",
      "batch 126, training loss: 3.4902: : 126it [01:01,  1.72it/s]\u001b[A\n",
      "batch 127, training loss: 3.1714: : 126it [01:01,  1.72it/s]\u001b[A\n",
      "batch 127, training loss: 3.1714: : 127it [01:01,  1.82it/s]\u001b[A\n",
      "batch 128, training loss: 3.3928: : 127it [01:02,  1.82it/s]\u001b[A\n",
      "batch 128, training loss: 3.3928: : 128it [01:02,  1.81it/s]\u001b[A\n",
      "batch 129, training loss: 3.3258: : 128it [01:02,  1.81it/s]\u001b[A\n",
      "batch 129, training loss: 3.3258: : 129it [01:02,  1.84it/s]\u001b[A\n",
      "batch 130, training loss: 3.3193: : 129it [01:03,  1.84it/s]\u001b[A\n",
      "batch 130, training loss: 3.3193: : 130it [01:03,  1.73it/s]\u001b[A\n",
      "batch 131, training loss: 3.5224: : 130it [01:04,  1.73it/s]\u001b[A\n",
      "batch 131, training loss: 3.5224: : 131it [01:04,  1.81it/s]\u001b[A\n",
      "batch 132, training loss: 3.3164: : 131it [01:04,  1.81it/s]\u001b[A\n",
      "batch 132, training loss: 3.3164: : 132it [01:04,  1.82it/s]\u001b[A\n",
      "batch 133, training loss: 3.2184: : 132it [01:05,  1.82it/s]\u001b[A\n",
      "batch 133, training loss: 3.2184: : 133it [01:05,  1.81it/s]\u001b[A\n",
      "batch 134, training loss: 3.4111: : 133it [01:05,  1.81it/s]\u001b[A\n",
      "batch 134, training loss: 3.4111: : 134it [01:05,  1.72it/s]\u001b[A\n",
      "batch 135, training loss: 3.3052: : 134it [01:06,  1.72it/s]\u001b[A\n",
      "batch 135, training loss: 3.3052: : 135it [01:06,  1.75it/s]\u001b[A\n",
      "batch 136, training loss: 3.2398: : 135it [01:06,  1.75it/s]\u001b[A\n",
      "batch 136, training loss: 3.2398: : 136it [01:06,  1.78it/s]\u001b[A\n",
      "batch 137, training loss: 3.4114: : 136it [01:07,  1.78it/s]\u001b[A\n",
      "batch 137, training loss: 3.4114: : 137it [01:07,  1.86it/s]\u001b[A\n",
      "batch 138, training loss: 3.4165: : 137it [01:07,  1.86it/s]\u001b[A\n",
      "batch 138, training loss: 3.4165: : 138it [01:07,  1.78it/s]\u001b[A\n",
      "batch 139, training loss: 3.1944: : 138it [01:08,  1.78it/s]\u001b[A\n",
      "batch 139, training loss: 3.1944: : 139it [01:08,  1.79it/s]\u001b[A\n",
      "batch 140, training loss: 3.3125: : 139it [01:08,  1.79it/s]\u001b[A\n",
      "batch 140, training loss: 3.3125: : 140it [01:08,  1.86it/s]\u001b[A\n",
      "batch 141, training loss: 3.3146: : 140it [01:09,  1.86it/s]\u001b[A\n",
      "batch 141, training loss: 3.3146: : 141it [01:09,  1.86it/s]\u001b[A\n",
      "batch 142, training loss: 3.3468: : 141it [01:10,  1.86it/s]\u001b[A\n",
      "batch 142, training loss: 3.3468: : 142it [01:10,  1.76it/s]\u001b[A\n",
      "batch 143, training loss: 3.16: : 142it [01:10,  1.76it/s]  \u001b[A\n",
      "batch 143, training loss: 3.16: : 143it [01:10,  1.76it/s]\u001b[A\n",
      "batch 144, training loss: 3.2308: : 143it [01:11,  1.76it/s]\u001b[A\n",
      "batch 144, training loss: 3.2308: : 144it [01:11,  1.84it/s]\u001b[A\n",
      "batch 145, training loss: 3.3366: : 144it [01:11,  1.84it/s]\u001b[A\n",
      "batch 145, training loss: 3.3366: : 145it [01:11,  1.86it/s]\u001b[A\n",
      "batch 146, training loss: 3.3159: : 145it [01:12,  1.86it/s]\u001b[A\n",
      "batch 146, training loss: 3.3159: : 146it [01:12,  1.79it/s]\u001b[A\n",
      "batch 147, training loss: 3.2416: : 146it [01:12,  1.79it/s]\u001b[A\n",
      "batch 147, training loss: 3.2416: : 147it [01:12,  1.90it/s]\u001b[A\n",
      "batch 148, training loss: 3.315: : 147it [01:13,  1.90it/s] \u001b[A\n",
      "batch 148, training loss: 3.315: : 148it [01:13,  1.87it/s]\u001b[A\n",
      "batch 149, training loss: 3.4032: : 148it [01:13,  1.87it/s]\u001b[A\n",
      "batch 149, training loss: 3.4032: : 149it [01:13,  1.92it/s]\u001b[A\n",
      "batch 150, training loss: 3.3771: : 149it [01:14,  1.92it/s]\u001b[A\n",
      "batch 150, training loss: 3.3771: : 150it [01:14,  1.81it/s]\u001b[A\n",
      "batch 151, training loss: 3.3087: : 150it [01:14,  1.81it/s]\u001b[A\n",
      "batch 151, training loss: 3.3087: : 151it [01:14,  1.86it/s]\u001b[A\n",
      "batch 152, training loss: 3.2393: : 151it [01:15,  1.86it/s]\u001b[A\n",
      "batch 152, training loss: 3.2393: : 152it [01:15,  1.86it/s]\u001b[A\n",
      "batch 153, training loss: 3.2516: : 152it [01:16,  1.86it/s]\u001b[A\n",
      "batch 153, training loss: 3.2516: : 153it [01:16,  1.89it/s]\u001b[A\n",
      "batch 154, training loss: 3.3797: : 153it [01:16,  1.89it/s]\u001b[A\n",
      "batch 154, training loss: 3.3797: : 154it [01:16,  1.87it/s]\u001b[A\n",
      "batch 155, training loss: 3.5053: : 154it [01:17,  1.87it/s]\u001b[A\n",
      "batch 155, training loss: 3.5053: : 155it [01:17,  1.78it/s]\u001b[A\n",
      "batch 156, training loss: 3.1006: : 155it [01:17,  1.78it/s]\u001b[A\n",
      "batch 156, training loss: 3.1006: : 156it [01:17,  1.78it/s]\u001b[A\n",
      "batch 157, training loss: 3.3257: : 156it [01:18,  1.78it/s]\u001b[A\n",
      "batch 157, training loss: 3.3257: : 157it [01:18,  1.85it/s]\u001b[A\n",
      "batch 158, training loss: 3.2872: : 157it [01:18,  1.85it/s]\u001b[A\n",
      "batch 158, training loss: 3.2872: : 158it [01:18,  1.86it/s]\u001b[A\n",
      "batch 159, training loss: 3.2235: : 158it [01:19,  1.86it/s]\u001b[A\n",
      "batch 159, training loss: 3.2235: : 159it [01:19,  1.78it/s]\u001b[A\n",
      "batch 160, training loss: 3.4018: : 159it [01:19,  1.78it/s]\u001b[A\n",
      "batch 160, training loss: 3.4018: : 160it [01:19,  1.82it/s]\u001b[A\n",
      "batch 161, training loss: 3.2763: : 160it [01:20,  1.82it/s]\u001b[A\n",
      "batch 161, training loss: 3.2763: : 161it [01:20,  1.85it/s]\u001b[A\n",
      "batch 162, training loss: 3.243: : 161it [01:20,  1.85it/s] \u001b[A\n",
      "batch 162, training loss: 3.243: : 162it [01:20,  1.85it/s]\u001b[A\n",
      "batch 163, training loss: 3.5021: : 162it [01:21,  1.85it/s]\u001b[A\n",
      "batch 163, training loss: 3.5021: : 163it [01:21,  1.93it/s]\u001b[A\n",
      "batch 164, training loss: 3.321: : 163it [01:22,  1.93it/s] \u001b[A\n",
      "batch 164, training loss: 3.321: : 164it [01:22,  1.85it/s]\u001b[A\n",
      "batch 165, training loss: 3.2849: : 164it [01:22,  1.85it/s]\u001b[A\n",
      "batch 165, training loss: 3.2849: : 165it [01:22,  1.82it/s]\u001b[A\n",
      "batch 166, training loss: 3.2532: : 165it [01:23,  1.82it/s]\u001b[A\n",
      "batch 166, training loss: 3.2532: : 166it [01:23,  1.87it/s]\u001b[A\n",
      "batch 167, training loss: 3.3802: : 166it [01:23,  1.87it/s]\u001b[A\n",
      "batch 167, training loss: 3.3802: : 167it [01:23,  1.88it/s]\u001b[A\n",
      "batch 168, training loss: 3.3608: : 167it [01:24,  1.88it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 168, training loss: 3.3608: : 168it [01:24,  1.79it/s]\u001b[A\n",
      "batch 169, training loss: 3.4106: : 168it [01:24,  1.79it/s]\u001b[A\n",
      "batch 169, training loss: 3.4106: : 169it [01:24,  1.79it/s]\u001b[A\n",
      "batch 170, training loss: 3.3003: : 169it [01:25,  1.79it/s]\u001b[A\n",
      "batch 170, training loss: 3.3003: : 170it [01:25,  1.85it/s]\u001b[A\n",
      "batch 171, training loss: 2.2338: : 170it [01:25,  1.85it/s]\u001b[A\n",
      "batch 171, training loss: 2.2338: : 171it [01:25,  2.34it/s]\u001b[A\n",
      "batch 172, training loss: 3.521: : 171it [01:26,  2.34it/s] \u001b[A\n",
      "batch 172, training loss: 3.521: : 172it [01:26,  2.07it/s]\u001b[A\n",
      "batch 173, training loss: 3.3491: : 172it [01:26,  2.07it/s]\u001b[A\n",
      "batch 173, training loss: 3.3491: : 173it [01:26,  1.78it/s]\u001b[A\n",
      "batch 174, training loss: 3.5254: : 173it [01:27,  1.78it/s]\u001b[A\n",
      "batch 174, training loss: 3.5254: : 174it [01:27,  1.72it/s]\u001b[A\n",
      "batch 175, training loss: 3.4493: : 174it [01:28,  1.72it/s]\u001b[A\n",
      "batch 175, training loss: 3.4493: : 175it [01:28,  1.71it/s]\u001b[A\n",
      "batch 176, training loss: 3.417: : 175it [01:28,  1.71it/s] \u001b[A\n",
      "batch 176, training loss: 3.417: : 176it [01:28,  1.67it/s]\u001b[A\n",
      "batch 177, training loss: 3.3668: : 176it [01:29,  1.67it/s]\u001b[A\n",
      "batch 177, training loss: 3.3668: : 177it [01:29,  1.56it/s]\u001b[A\n",
      "batch 178, training loss: 3.3836: : 177it [01:30,  1.56it/s]\u001b[A\n",
      "batch 178, training loss: 3.3836: : 178it [01:30,  1.58it/s]\u001b[A\n",
      "batch 179, training loss: 3.457: : 178it [01:30,  1.58it/s] \u001b[A\n",
      "batch 179, training loss: 3.457: : 179it [01:30,  1.61it/s]\u001b[A\n",
      "batch 180, training loss: 3.4737: : 179it [01:31,  1.61it/s]\u001b[A\n",
      "batch 180, training loss: 3.4737: : 180it [01:31,  1.61it/s]\u001b[A\n",
      "batch 181, training loss: 3.5479: : 180it [01:31,  1.61it/s]\u001b[A\n",
      "batch 181, training loss: 3.5479: : 181it [01:31,  1.72it/s]\u001b[A\n",
      "batch 182, training loss: 3.462: : 181it [01:32,  1.72it/s] \u001b[A\n",
      "batch 182, training loss: 3.462: : 182it [01:32,  1.95it/s]\u001b[A\n",
      "batch 183, training loss: 3.5731: : 182it [01:32,  1.95it/s]\u001b[A\n",
      "batch 183, training loss: 3.5731: : 183it [01:32,  1.99it/s]\u001b[A\n",
      "batch 184, training loss: 3.294: : 183it [01:33,  1.99it/s] \u001b[A\n",
      "batch 184, training loss: 3.294: : 184it [01:33,  1.97it/s]\u001b[A\n",
      "batch 185, training loss: 3.3538: : 184it [01:33,  1.97it/s]\u001b[A\n",
      "batch 185, training loss: 3.3538: : 185it [01:33,  1.90it/s]\u001b[A\n",
      "batch 186, training loss: 3.5179: : 185it [01:34,  1.90it/s]\u001b[A\n",
      "batch 186, training loss: 3.5179: : 186it [01:34,  1.80it/s]\u001b[A\n",
      "batch 187, training loss: 3.5383: : 186it [01:35,  1.80it/s]\u001b[A\n",
      "batch 187, training loss: 3.5383: : 187it [01:35,  1.62it/s]\u001b[A\n",
      "batch 188, training loss: 3.5819: : 187it [01:35,  1.62it/s]\u001b[A\n",
      "batch 188, training loss: 3.5819: : 188it [01:35,  1.61it/s]\u001b[A\n",
      "batch 189, training loss: 3.627: : 188it [01:36,  1.61it/s] \u001b[A\n",
      "batch 189, training loss: 3.627: : 189it [01:36,  1.63it/s]\u001b[A\n",
      "batch 190, training loss: 3.3327: : 189it [01:36,  1.63it/s]\u001b[A\n",
      "batch 190, training loss: 3.3327: : 190it [01:36,  1.63it/s]\u001b[A\n",
      "batch 191, training loss: 3.3445: : 190it [01:37,  1.63it/s]\u001b[A\n",
      "batch 191, training loss: 3.3445: : 191it [01:37,  1.59it/s]\u001b[A\n",
      "batch 192, training loss: 3.4178: : 191it [01:38,  1.59it/s]\u001b[A\n",
      "batch 192, training loss: 3.4178: : 192it [01:38,  1.63it/s]\u001b[A\n",
      "batch 193, training loss: 3.3348: : 192it [01:38,  1.63it/s]\u001b[A\n",
      "batch 193, training loss: 3.3348: : 193it [01:38,  1.62it/s]\u001b[A\n",
      "batch 194, training loss: 3.4433: : 193it [01:39,  1.62it/s]\u001b[A\n",
      "batch 194, training loss: 3.4433: : 194it [01:39,  1.64it/s]\u001b[A\n",
      "batch 195, training loss: 3.3744: : 194it [01:40,  1.64it/s]\u001b[A\n",
      "batch 195, training loss: 3.3744: : 195it [01:40,  1.54it/s]\u001b[A\n",
      "batch 196, training loss: 3.2955: : 195it [01:40,  1.54it/s]\u001b[A\n",
      "batch 196, training loss: 3.2955: : 196it [01:40,  1.62it/s]\u001b[A\n",
      "batch 197, training loss: 3.4946: : 196it [01:41,  1.62it/s]\u001b[A\n",
      "batch 197, training loss: 3.4946: : 197it [01:41,  1.66it/s]\u001b[A\n",
      "batch 198, training loss: 3.3977: : 197it [01:41,  1.66it/s]\u001b[A\n",
      "batch 198, training loss: 3.3977: : 198it [01:41,  1.64it/s]\u001b[A\n",
      "batch 199, training loss: 3.3639: : 198it [01:42,  1.64it/s]\u001b[A\n",
      "batch 199, training loss: 3.3639: : 199it [01:42,  1.53it/s]\u001b[A\n",
      "batch 200, training loss: 3.5027: : 199it [01:43,  1.53it/s]\u001b[A\n",
      "batch 200, training loss: 3.5027: : 200it [01:43,  1.55it/s]\u001b[A\n",
      "batch 201, training loss: 3.335: : 200it [01:43,  1.55it/s] \u001b[A\n",
      "batch 201, training loss: 3.335: : 201it [01:43,  1.59it/s]\u001b[A\n",
      "batch 202, training loss: 3.2131: : 201it [01:44,  1.59it/s]\u001b[A\n",
      "batch 202, training loss: 3.2131: : 202it [01:44,  1.60it/s]\u001b[A\n",
      "batch 203, training loss: 3.4228: : 202it [01:45,  1.60it/s]\u001b[A\n",
      "batch 203, training loss: 3.4228: : 203it [01:45,  1.50it/s]\u001b[A\n",
      "batch 204, training loss: 3.4396: : 203it [01:45,  1.50it/s]\u001b[A\n",
      "batch 204, training loss: 3.4396: : 204it [01:45,  1.53it/s]\u001b[A\n",
      "batch 205, training loss: 3.4106: : 204it [01:46,  1.53it/s]\u001b[A\n",
      "batch 205, training loss: 3.4106: : 205it [01:46,  1.57it/s]\u001b[A\n",
      "batch 206, training loss: 3.51: : 205it [01:47,  1.57it/s]  \u001b[A\n",
      "batch 206, training loss: 3.51: : 206it [01:47,  1.58it/s]\u001b[A\n",
      "batch 207, training loss: 3.3192: : 206it [01:47,  1.58it/s]\u001b[A\n",
      "batch 207, training loss: 3.3192: : 207it [01:47,  1.50it/s]\u001b[A\n",
      "batch 208, training loss: 3.3669: : 207it [01:48,  1.50it/s]\u001b[A\n",
      "batch 208, training loss: 3.3669: : 208it [01:48,  1.53it/s]\u001b[A\n",
      "batch 209, training loss: 3.3583: : 208it [01:48,  1.53it/s]\u001b[A\n",
      "batch 209, training loss: 3.3583: : 209it [01:48,  1.57it/s]\u001b[A\n",
      "batch 210, training loss: 3.4298: : 209it [01:49,  1.57it/s]\u001b[A\n",
      "batch 210, training loss: 3.4298: : 210it [01:49,  1.59it/s]\u001b[A\n",
      "batch 211, training loss: 3.3627: : 210it [01:50,  1.59it/s]\u001b[A\n",
      "batch 211, training loss: 3.3627: : 211it [01:50,  1.49it/s]\u001b[A\n",
      "batch 212, training loss: 3.3725: : 211it [01:50,  1.49it/s]\u001b[A\n",
      "batch 212, training loss: 3.3725: : 212it [01:50,  1.52it/s]\u001b[A\n",
      "batch 213, training loss: 3.5646: : 212it [01:51,  1.52it/s]\u001b[A\n",
      "batch 213, training loss: 3.5646: : 213it [01:51,  1.58it/s]\u001b[A\n",
      "batch 214, training loss: 3.4309: : 213it [01:52,  1.58it/s]\u001b[A\n",
      "batch 214, training loss: 3.4309: : 214it [01:52,  1.59it/s]\u001b[A\n",
      "batch 215, training loss: 3.2836: : 214it [01:52,  1.59it/s]\u001b[A\n",
      "batch 215, training loss: 3.2836: : 215it [01:52,  1.49it/s]\u001b[A\n",
      "batch 216, training loss: 3.4049: : 215it [01:53,  1.49it/s]\u001b[A\n",
      "batch 216, training loss: 3.4049: : 216it [01:53,  1.52it/s]\u001b[A\n",
      "batch 217, training loss: 3.5344: : 216it [01:54,  1.52it/s]\u001b[A\n",
      "batch 217, training loss: 3.5344: : 217it [01:54,  1.57it/s]\u001b[A\n",
      "batch 218, training loss: 3.4548: : 217it [01:54,  1.57it/s]\u001b[A\n",
      "batch 218, training loss: 3.4548: : 218it [01:54,  1.59it/s]\u001b[A\n",
      "batch 219, training loss: 3.6261: : 218it [01:55,  1.59it/s]\u001b[A\n",
      "batch 219, training loss: 3.6261: : 219it [01:55,  1.49it/s]\u001b[A\n",
      "batch 220, training loss: 3.5583: : 219it [01:56,  1.49it/s]\u001b[A\n",
      "batch 220, training loss: 3.5583: : 220it [01:56,  1.53it/s]\u001b[A\n",
      "batch 221, training loss: 3.4194: : 220it [01:56,  1.53it/s]\u001b[A\n",
      "batch 221, training loss: 3.4194: : 221it [01:56,  1.57it/s]\u001b[A\n",
      "batch 222, training loss: 3.464: : 221it [01:57,  1.57it/s] \u001b[A\n",
      "batch 222, training loss: 3.464: : 222it [01:57,  1.59it/s]\u001b[A\n",
      "batch 223, training loss: 3.4747: : 222it [01:58,  1.59it/s]\u001b[A\n",
      "batch 223, training loss: 3.4747: : 223it [01:58,  1.49it/s]\u001b[A\n",
      "batch 224, training loss: 3.3193: : 223it [01:58,  1.49it/s]\u001b[A\n",
      "batch 224, training loss: 3.3193: : 224it [01:58,  1.53it/s]\u001b[A\n",
      "batch 225, training loss: 3.512: : 224it [01:59,  1.53it/s] \u001b[A\n",
      "batch 225, training loss: 3.512: : 225it [01:59,  1.57it/s]\u001b[A\n",
      "batch 226, training loss: 3.4776: : 225it [01:59,  1.57it/s]\u001b[A\n",
      "batch 226, training loss: 3.4776: : 226it [01:59,  1.58it/s]\u001b[A\n",
      "batch 227, training loss: 3.3408: : 226it [02:00,  1.58it/s]\u001b[A\n",
      "batch 227, training loss: 3.3408: : 227it [02:00,  1.49it/s]\u001b[A\n",
      "batch 228, training loss: 3.435: : 227it [02:01,  1.49it/s] \u001b[A\n",
      "batch 228, training loss: 3.435: : 228it [02:01,  1.53it/s]\u001b[A\n",
      "batch 229, training loss: 3.397: : 228it [02:01,  1.53it/s]\u001b[A\n",
      "batch 229, training loss: 3.397: : 229it [02:01,  1.58it/s]\u001b[A\n",
      "batch 230, training loss: 3.5459: : 229it [02:02,  1.58it/s]\u001b[A\n",
      "batch 230, training loss: 3.5459: : 230it [02:02,  1.59it/s]\u001b[A\n",
      "batch 231, training loss: 3.5792: : 230it [02:03,  1.59it/s]\u001b[A\n",
      "batch 231, training loss: 3.5792: : 231it [02:03,  1.50it/s]\u001b[A\n",
      "batch 232, training loss: 3.4002: : 231it [02:03,  1.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 232, training loss: 3.4002: : 232it [02:03,  1.53it/s]\u001b[A\n",
      "batch 233, training loss: 3.4344: : 232it [02:04,  1.53it/s]\u001b[A\n",
      "batch 233, training loss: 3.4344: : 233it [02:04,  1.57it/s]\u001b[A\n",
      "batch 234, training loss: 3.5801: : 233it [02:05,  1.57it/s]\u001b[A\n",
      "batch 234, training loss: 3.5801: : 234it [02:05,  1.57it/s]\u001b[A\n",
      "batch 235, training loss: 3.4111: : 234it [02:05,  1.57it/s]\u001b[A\n",
      "batch 235, training loss: 3.4111: : 235it [02:05,  1.50it/s]\u001b[A\n",
      "batch 236, training loss: 3.4423: : 235it [02:06,  1.50it/s]\u001b[A\n",
      "batch 236, training loss: 3.4423: : 236it [02:06,  1.52it/s]\u001b[A\n",
      "batch 237, training loss: 3.3452: : 236it [02:07,  1.52it/s]\u001b[A\n",
      "batch 237, training loss: 3.3452: : 237it [02:07,  1.57it/s]\u001b[A\n",
      "batch 238, training loss: 3.5113: : 237it [02:07,  1.57it/s]\u001b[A\n",
      "batch 238, training loss: 3.5113: : 238it [02:07,  1.58it/s]\u001b[A\n",
      "batch 239, training loss: 3.4098: : 238it [02:08,  1.58it/s]\u001b[A\n",
      "batch 239, training loss: 3.4098: : 239it [02:08,  1.49it/s]\u001b[A\n",
      "batch 240, training loss: 3.4128: : 239it [02:09,  1.49it/s]\u001b[A\n",
      "batch 240, training loss: 3.4128: : 240it [02:09,  1.53it/s]\u001b[A\n",
      "batch 241, training loss: 3.4592: : 240it [02:09,  1.53it/s]\u001b[A\n",
      "batch 241, training loss: 3.4592: : 241it [02:09,  1.58it/s]\u001b[A\n",
      "batch 242, training loss: 3.3142: : 241it [02:10,  1.58it/s]\u001b[A\n",
      "batch 242, training loss: 3.3142: : 242it [02:10,  1.58it/s]\u001b[A\n",
      "batch 243, training loss: 3.4519: : 242it [02:11,  1.58it/s]\u001b[A\n",
      "batch 243, training loss: 3.4519: : 243it [02:11,  1.50it/s]\u001b[A\n",
      "batch 244, training loss: 3.34: : 243it [02:11,  1.50it/s]  \u001b[A\n",
      "batch 244, training loss: 3.34: : 244it [02:11,  1.53it/s]\u001b[A\n",
      "batch 245, training loss: 3.5073: : 244it [02:12,  1.53it/s]\u001b[A\n",
      "batch 245, training loss: 3.5073: : 245it [02:12,  1.58it/s]\u001b[A\n",
      "batch 246, training loss: 3.4402: : 245it [02:12,  1.58it/s]\u001b[A\n",
      "batch 246, training loss: 3.4402: : 246it [02:12,  1.58it/s]\u001b[A\n",
      "batch 247, training loss: 3.4167: : 246it [02:13,  1.58it/s]\u001b[A\n",
      "batch 247, training loss: 3.4167: : 247it [02:13,  1.50it/s]\u001b[A\n",
      "batch 248, training loss: 3.3597: : 247it [02:14,  1.50it/s]\u001b[A\n",
      "batch 248, training loss: 3.3597: : 248it [02:14,  1.53it/s]\u001b[A\n",
      "batch 249, training loss: 3.3007: : 248it [02:14,  1.53it/s]\u001b[A\n",
      "batch 249, training loss: 3.3007: : 249it [02:14,  1.59it/s]\u001b[A\n",
      "batch 250, training loss: 3.5054: : 249it [02:15,  1.59it/s]\u001b[A\n",
      "batch 250, training loss: 3.5054: : 250it [02:15,  1.61it/s]\u001b[A\n",
      "batch 251, training loss: 3.4967: : 250it [02:16,  1.61it/s]\u001b[A\n",
      "batch 251, training loss: 3.4967: : 251it [02:16,  1.51it/s]\u001b[A\n",
      "batch 252, training loss: 2.4976: : 251it [02:16,  1.51it/s]\u001b[A\n",
      "batch 252, training loss: 2.4976: : 252it [02:16,  1.85it/s]\u001b[A\n",
      "batch 253, training loss: 3.3656: : 252it [02:17,  1.85it/s]\u001b[A\n",
      "batch 253, training loss: 3.3656: : 253it [02:17,  1.78it/s]\u001b[A\n",
      "batch 254, training loss: 3.4772: : 253it [02:17,  1.78it/s]\u001b[A\n",
      "batch 254, training loss: 3.4772: : 254it [02:17,  1.68it/s]\u001b[A\n",
      "batch 255, training loss: 3.2988: : 254it [02:18,  1.68it/s]\u001b[A\n",
      "batch 255, training loss: 3.2988: : 255it [02:18,  1.54it/s]\u001b[A\n",
      "batch 256, training loss: 3.3633: : 255it [02:19,  1.54it/s]\u001b[A\n",
      "batch 256, training loss: 3.3633: : 256it [02:19,  1.54it/s]\u001b[A\n",
      "batch 257, training loss: 3.4501: : 256it [02:19,  1.54it/s]\u001b[A\n",
      "batch 257, training loss: 3.4501: : 257it [02:19,  1.55it/s]\u001b[A\n",
      "batch 258, training loss: 3.5243: : 257it [02:20,  1.55it/s]\u001b[A\n",
      "batch 258, training loss: 3.5243: : 258it [02:20,  1.54it/s]\u001b[A\n",
      "batch 259, training loss: 3.4044: : 258it [02:21,  1.54it/s]\u001b[A\n",
      "batch 259, training loss: 3.4044: : 259it [02:21,  1.45it/s]\u001b[A\n",
      "batch 260, training loss: 3.3856: : 259it [02:21,  1.45it/s]\u001b[A\n",
      "batch 260, training loss: 3.3856: : 260it [02:21,  1.48it/s]\u001b[A\n",
      "batch 261, training loss: 3.4535: : 260it [02:22,  1.48it/s]\u001b[A\n",
      "batch 261, training loss: 3.4535: : 261it [02:22,  1.50it/s]\u001b[A\n",
      "batch 262, training loss: 3.4088: : 261it [02:23,  1.50it/s]\u001b[A\n",
      "batch 262, training loss: 3.4088: : 262it [02:23,  1.51it/s]\u001b[A\n",
      "batch 263, training loss: 3.38: : 262it [02:23,  1.51it/s]  \u001b[A\n",
      "batch 263, training loss: 3.38: : 263it [02:24,  1.44it/s]\u001b[A\n",
      "batch 264, training loss: 3.5053: : 263it [02:24,  1.44it/s]\u001b[A\n",
      "batch 264, training loss: 3.5053: : 264it [02:24,  1.46it/s]\u001b[A\n",
      "batch 265, training loss: 3.2921: : 264it [02:25,  1.46it/s]\u001b[A\n",
      "batch 265, training loss: 3.2921: : 265it [02:25,  1.50it/s]\u001b[A\n",
      "batch 266, training loss: 3.4816: : 265it [02:25,  1.50it/s]\u001b[A\n",
      "batch 266, training loss: 3.4816: : 266it [02:25,  1.51it/s]\u001b[A\n",
      "batch 267, training loss: 3.454: : 266it [02:26,  1.51it/s] \u001b[A\n",
      "batch 267, training loss: 3.454: : 267it [02:26,  1.43it/s]\u001b[A\n",
      "batch 268, training loss: 3.3287: : 267it [02:27,  1.43it/s]\u001b[A\n",
      "batch 268, training loss: 3.3287: : 268it [02:27,  1.47it/s]\u001b[A\n",
      "batch 269, training loss: 3.3794: : 268it [02:28,  1.47it/s]\u001b[A\n",
      "batch 269, training loss: 3.3794: : 269it [02:28,  1.49it/s]\u001b[A\n",
      "batch 270, training loss: 3.3968: : 269it [02:28,  1.49it/s]\u001b[A\n",
      "batch 270, training loss: 3.3968: : 270it [02:28,  1.51it/s]\u001b[A\n",
      "batch 271, training loss: 3.379: : 270it [02:29,  1.51it/s] \u001b[A\n",
      "batch 271, training loss: 3.379: : 271it [02:29,  1.41it/s]\u001b[A\n",
      "batch 272, training loss: 3.3795: : 271it [02:30,  1.41it/s]\u001b[A\n",
      "batch 272, training loss: 3.3795: : 272it [02:30,  1.46it/s]\u001b[A\n",
      "batch 273, training loss: 3.4891: : 272it [02:30,  1.46it/s]\u001b[A\n",
      "batch 273, training loss: 3.4891: : 273it [02:30,  1.49it/s]\u001b[A\n",
      "batch 274, training loss: 3.4658: : 273it [02:31,  1.49it/s]\u001b[A\n",
      "batch 274, training loss: 3.4658: : 274it [02:31,  1.51it/s]\u001b[A\n",
      "batch 275, training loss: 3.3547: : 274it [02:32,  1.51it/s]\u001b[A\n",
      "batch 275, training loss: 3.3547: : 275it [02:32,  1.38it/s]\u001b[A\n",
      "batch 276, training loss: 3.2404: : 275it [02:32,  1.38it/s]\u001b[A\n",
      "batch 276, training loss: 3.2404: : 276it [02:32,  1.43it/s]\u001b[A\n",
      "batch 277, training loss: 3.3734: : 276it [02:33,  1.43it/s]\u001b[A\n",
      "batch 277, training loss: 3.3734: : 277it [02:33,  1.44it/s]\u001b[A\n",
      "batch 278, training loss: 3.3336: : 277it [02:34,  1.44it/s]\u001b[A\n",
      "batch 278, training loss: 3.3336: : 278it [02:34,  1.38it/s]\u001b[A\n",
      "batch 279, training loss: 3.3291: : 278it [02:35,  1.38it/s]\u001b[A\n",
      "batch 279, training loss: 3.3291: : 279it [02:35,  1.42it/s]\u001b[A\n",
      "batch 280, training loss: 3.2494: : 279it [02:35,  1.42it/s]\u001b[A\n",
      "batch 280, training loss: 3.2494: : 280it [02:35,  1.47it/s]\u001b[A\n",
      "batch 281, training loss: 3.3513: : 280it [02:36,  1.47it/s]\u001b[A\n",
      "batch 281, training loss: 3.3513: : 281it [02:36,  1.48it/s]\u001b[A\n",
      "batch 282, training loss: 3.2309: : 281it [02:37,  1.48it/s]\u001b[A\n",
      "batch 282, training loss: 3.2309: : 282it [02:37,  1.41it/s]\u001b[A\n",
      "batch 283, training loss: 3.3491: : 282it [02:37,  1.41it/s]\u001b[A\n",
      "batch 283, training loss: 3.3491: : 283it [02:37,  1.45it/s]\u001b[A\n",
      "batch 284, training loss: 3.3565: : 283it [02:38,  1.45it/s]\u001b[A\n",
      "batch 284, training loss: 3.3565: : 284it [02:38,  1.53it/s]\u001b[A\n",
      "batch 285, training loss: 3.3847: : 284it [02:38,  1.53it/s]\u001b[A\n",
      "batch 285, training loss: 3.3847: : 285it [02:38,  1.68it/s]\u001b[A\n",
      "batch 286, training loss: 3.5432: : 285it [02:39,  1.68it/s]\u001b[A\n",
      "batch 286, training loss: 3.5432: : 286it [02:39,  1.86it/s]\u001b[A\n",
      "batch 287, training loss: 3.2532: : 286it [02:39,  1.86it/s]\u001b[A\n",
      "batch 287, training loss: 3.2532: : 287it [02:39,  1.90it/s]\u001b[A\n",
      "batch 288, training loss: 3.2944: : 287it [02:40,  1.90it/s]\u001b[A\n",
      "batch 288, training loss: 3.2944: : 288it [02:40,  1.75it/s]\u001b[A\n",
      "batch 289, training loss: 3.4431: : 288it [02:40,  1.75it/s]\u001b[A\n",
      "batch 289, training loss: 3.4431: : 289it [02:40,  1.68it/s]\u001b[A\n",
      "batch 290, training loss: 3.2316: : 289it [02:41,  1.68it/s]\u001b[A\n",
      "batch 290, training loss: 3.2316: : 290it [02:41,  1.62it/s]\u001b[A\n",
      "batch 291, training loss: 3.4849: : 290it [02:42,  1.62it/s]\u001b[A\n",
      "batch 291, training loss: 3.4849: : 291it [02:42,  1.50it/s]\u001b[A\n",
      "batch 292, training loss: 3.3033: : 291it [02:43,  1.50it/s]\u001b[A\n",
      "batch 292, training loss: 3.3033: : 292it [02:43,  1.51it/s]\u001b[A\n",
      "batch 293, training loss: 3.3941: : 292it [02:43,  1.51it/s]\u001b[A\n",
      "batch 293, training loss: 3.3941: : 293it [02:43,  1.47it/s]\u001b[A\n",
      "batch 294, training loss: 3.4595: : 293it [02:44,  1.47it/s]\u001b[A\n",
      "batch 294, training loss: 3.4595: : 294it [02:44,  1.48it/s]\u001b[A\n",
      "batch 295, training loss: 3.427: : 294it [02:45,  1.48it/s] \u001b[A\n",
      "batch 295, training loss: 3.427: : 295it [02:45,  1.39it/s]\u001b[A\n",
      "batch 296, training loss: 3.2842: : 295it [02:45,  1.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 296, training loss: 3.2842: : 296it [02:45,  1.44it/s]\u001b[A\n",
      "batch 297, training loss: 3.4291: : 296it [02:46,  1.44it/s]\u001b[A\n",
      "batch 297, training loss: 3.4291: : 297it [02:46,  1.46it/s]\u001b[A\n",
      "batch 298, training loss: 3.3457: : 297it [02:47,  1.46it/s]\u001b[A\n",
      "batch 298, training loss: 3.3457: : 298it [02:47,  1.40it/s]\u001b[A\n",
      "batch 299, training loss: 3.4165: : 298it [02:48,  1.40it/s]\u001b[A\n",
      "batch 299, training loss: 3.4165: : 299it [02:48,  1.44it/s]\u001b[A\n",
      "batch 300, training loss: 3.4785: : 299it [02:48,  1.44it/s]\u001b[A\n",
      "batch 300, training loss: 3.4785: : 300it [02:48,  1.44it/s]\u001b[A\n",
      "batch 301, training loss: 3.4182: : 300it [02:49,  1.44it/s]\u001b[A\n",
      "batch 301, training loss: 3.4182: : 301it [02:49,  1.48it/s]\u001b[A\n",
      "batch 302, training loss: 3.4331: : 301it [02:50,  1.48it/s]\u001b[A\n",
      "batch 302, training loss: 3.4331: : 302it [02:50,  1.41it/s]\u001b[A\n",
      "batch 303, training loss: 3.3379: : 302it [02:50,  1.41it/s]\u001b[A\n",
      "batch 303, training loss: 3.3379: : 303it [02:50,  1.46it/s]\u001b[A\n",
      "batch 304, training loss: 3.4402: : 303it [02:51,  1.46it/s]\u001b[A\n",
      "batch 304, training loss: 3.4402: : 304it [02:51,  1.44it/s]\u001b[A\n",
      "batch 305, training loss: 3.3539: : 304it [02:52,  1.44it/s]\u001b[A\n",
      "batch 305, training loss: 3.3539: : 305it [02:52,  1.46it/s]\u001b[A\n",
      "batch 306, training loss: 3.4144: : 305it [02:52,  1.46it/s]\u001b[A\n",
      "batch 306, training loss: 3.4144: : 306it [02:52,  1.38it/s]\u001b[A\n",
      "batch 307, training loss: 3.555: : 306it [02:53,  1.38it/s] \u001b[A\n",
      "batch 307, training loss: 3.555: : 307it [02:53,  1.43it/s]\u001b[A\n",
      "batch 308, training loss: 3.1878: : 307it [02:54,  1.43it/s]\u001b[A\n",
      "batch 308, training loss: 3.1878: : 308it [02:54,  1.45it/s]\u001b[A\n",
      "batch 309, training loss: 3.5776: : 308it [02:55,  1.45it/s]\u001b[A\n",
      "batch 309, training loss: 3.5776: : 309it [02:55,  1.39it/s]\u001b[A\n",
      "batch 310, training loss: 3.316: : 309it [02:55,  1.39it/s] \u001b[A\n",
      "batch 310, training loss: 3.316: : 310it [02:55,  1.43it/s]\u001b[A\n",
      "batch 311, training loss: 3.4392: : 310it [02:56,  1.43it/s]\u001b[A\n",
      "batch 311, training loss: 3.4392: : 311it [02:56,  1.47it/s]\u001b[A\n",
      "batch 312, training loss: 3.2656: : 311it [02:57,  1.47it/s]\u001b[A\n",
      "batch 312, training loss: 3.2656: : 312it [02:57,  1.49it/s]\u001b[A\n",
      "batch 313, training loss: 3.3589: : 312it [02:57,  1.49it/s]\u001b[A\n",
      "batch 313, training loss: 3.3589: : 313it [02:57,  1.43it/s]\u001b[A\n",
      "batch 314, training loss: 3.3916: : 313it [02:58,  1.43it/s]\u001b[A\n",
      "batch 314, training loss: 3.3916: : 314it [02:58,  1.46it/s]\u001b[A\n",
      "batch 315, training loss: 3.4309: : 314it [02:59,  1.46it/s]\u001b[A\n",
      "batch 315, training loss: 3.4309: : 315it [02:59,  1.48it/s]\u001b[A\n",
      "batch 316, training loss: 3.5793: : 315it [02:59,  1.48it/s]\u001b[A\n",
      "batch 316, training loss: 3.5793: : 316it [02:59,  1.50it/s]\u001b[A\n",
      "batch 317, training loss: 3.2491: : 316it [03:00,  1.50it/s]\u001b[A\n",
      "batch 317, training loss: 3.2491: : 317it [03:00,  1.64it/s]\u001b[A\n",
      "batch 318, training loss: 3.4506: : 317it [03:01,  1.64it/s]\u001b[A\n",
      "batch 318, training loss: 3.4506: : 318it [03:01,  1.44it/s]\u001b[A\n",
      "batch 319, training loss: 3.5116: : 318it [03:01,  1.44it/s]\u001b[A\n",
      "batch 319, training loss: 3.5116: : 319it [03:01,  1.45it/s]\u001b[A\n",
      "batch 320, training loss: 3.5035: : 319it [03:02,  1.45it/s]\u001b[A\n",
      "batch 320, training loss: 3.5035: : 320it [03:02,  1.44it/s]\u001b[A\n",
      "batch 321, training loss: 3.5745: : 320it [03:03,  1.44it/s]\u001b[A\n",
      "batch 321, training loss: 3.5745: : 321it [03:03,  1.30it/s]\u001b[A\n",
      "batch 322, training loss: 3.6009: : 321it [03:04,  1.30it/s]\u001b[A\n",
      "batch 322, training loss: 3.6009: : 322it [03:04,  1.31it/s]\u001b[A\n",
      "batch 323, training loss: 3.3705: : 322it [03:04,  1.31it/s]\u001b[A\n",
      "batch 323, training loss: 3.3705: : 323it [03:04,  1.34it/s]\u001b[A\n",
      "batch 324, training loss: 3.4492: : 323it [03:05,  1.34it/s]\u001b[A\n",
      "batch 324, training loss: 3.4492: : 324it [03:05,  1.36it/s]\u001b[A\n",
      "batch 325, training loss: 3.5456: : 324it [03:06,  1.36it/s]\u001b[A\n",
      "batch 325, training loss: 3.5456: : 325it [03:06,  1.26it/s]\u001b[A\n",
      "batch 326, training loss: 3.4481: : 325it [03:07,  1.26it/s]\u001b[A\n",
      "batch 326, training loss: 3.4481: : 326it [03:07,  1.31it/s]\u001b[A\n",
      "batch 327, training loss: 3.5829: : 326it [03:07,  1.31it/s]\u001b[A\n",
      "batch 327, training loss: 3.5829: : 327it [03:07,  1.34it/s]\u001b[A\n",
      "batch 328, training loss: 3.4732: : 327it [03:08,  1.34it/s]\u001b[A\n",
      "batch 328, training loss: 3.4732: : 328it [03:08,  1.24it/s]\u001b[A\n",
      "batch 329, training loss: 3.3696: : 328it [03:09,  1.24it/s]\u001b[A\n",
      "batch 329, training loss: 3.3696: : 329it [03:09,  1.27it/s]\u001b[A\n",
      "batch 330, training loss: 3.4645: : 329it [03:10,  1.27it/s]\u001b[A\n",
      "batch 330, training loss: 3.4645: : 330it [03:10,  1.32it/s]\u001b[A\n",
      "batch 331, training loss: 3.4954: : 330it [03:11,  1.32it/s]\u001b[A\n",
      "batch 331, training loss: 3.4954: : 331it [03:11,  1.34it/s]\u001b[A\n",
      "batch 332, training loss: 3.3871: : 331it [03:11,  1.34it/s]\u001b[A\n",
      "batch 332, training loss: 3.3871: : 332it [03:11,  1.24it/s]\u001b[A\n",
      "batch 333, training loss: 3.3818: : 332it [03:12,  1.24it/s]\u001b[A\n",
      "batch 333, training loss: 3.3818: : 333it [03:12,  1.25it/s]\u001b[A\n",
      "batch 334, training loss: 3.4428: : 333it [03:13,  1.25it/s]\u001b[A\n",
      "batch 334, training loss: 3.4428: : 334it [03:13,  1.30it/s]\u001b[A\n",
      "batch 335, training loss: 3.6042: : 334it [03:14,  1.30it/s]\u001b[A\n",
      "batch 335, training loss: 3.6042: : 335it [03:14,  1.24it/s]\u001b[A\n",
      "batch 336, training loss: 3.4787: : 335it [03:15,  1.24it/s]\u001b[A\n",
      "batch 336, training loss: 3.4787: : 336it [03:15,  1.26it/s]\u001b[A\n",
      "batch 337, training loss: 3.5466: : 336it [03:15,  1.26it/s]\u001b[A\n",
      "batch 337, training loss: 3.5466: : 337it [03:15,  1.32it/s]\u001b[A\n",
      "batch 338, training loss: 3.3947: : 337it [03:16,  1.32it/s]\u001b[A\n",
      "batch 338, training loss: 3.3947: : 338it [03:16,  1.35it/s]\u001b[A\n",
      "batch 339, training loss: 3.4967: : 338it [03:17,  1.35it/s]\u001b[A\n",
      "batch 339, training loss: 3.4967: : 339it [03:17,  1.26it/s]\u001b[A\n",
      "batch 340, training loss: 3.7103: : 339it [03:18,  1.26it/s]\u001b[A\n",
      "batch 340, training loss: 3.7103: : 340it [03:18,  1.27it/s]\u001b[A\n",
      "batch 341, training loss: 3.3956: : 340it [03:18,  1.27it/s]\u001b[A\n",
      "batch 341, training loss: 3.3956: : 341it [03:18,  1.31it/s]\u001b[A\n",
      "batch 342, training loss: 3.4942: : 341it [03:19,  1.31it/s]\u001b[A\n",
      "batch 342, training loss: 3.4942: : 342it [03:19,  1.31it/s]\u001b[A\n",
      "batch 343, training loss: 3.4314: : 342it [03:20,  1.31it/s]\u001b[A\n",
      "batch 343, training loss: 3.4314: : 343it [03:20,  1.33it/s]\u001b[A\n",
      "batch 344, training loss: 3.5719: : 343it [03:21,  1.33it/s]\u001b[A\n",
      "batch 344, training loss: 3.5719: : 344it [03:21,  1.35it/s]\u001b[A\n",
      "batch 345, training loss: 3.5661: : 344it [03:21,  1.35it/s]\u001b[A\n",
      "batch 345, training loss: 3.5661: : 345it [03:21,  1.33it/s]\u001b[A\n",
      "batch 346, training loss: 3.4868: : 345it [03:22,  1.33it/s]\u001b[A\n",
      "batch 346, training loss: 3.4868: : 346it [03:22,  1.24it/s]\u001b[A\n",
      "batch 347, training loss: 3.4103: : 346it [03:23,  1.24it/s]\u001b[A\n",
      "batch 347, training loss: 3.4103: : 347it [03:23,  1.30it/s]\u001b[A\n",
      "batch 348, training loss: 3.4959: : 347it [03:24,  1.30it/s]\u001b[A\n",
      "batch 348, training loss: 3.4959: : 348it [03:24,  1.32it/s]\u001b[A\n",
      "batch 349, training loss: 3.6619: : 348it [03:24,  1.32it/s]\u001b[A\n",
      "batch 349, training loss: 3.6619: : 349it [03:24,  1.32it/s]\u001b[A\n",
      "batch 350, training loss: 3.4666: : 349it [03:25,  1.32it/s]\u001b[A\n",
      "batch 350, training loss: 3.4666: : 350it [03:25,  1.28it/s]\u001b[A\n",
      "batch 351, training loss: 3.4199: : 350it [03:26,  1.28it/s]\u001b[A\n",
      "batch 351, training loss: 3.4199: : 351it [03:26,  1.32it/s]\u001b[A\n",
      "batch 352, training loss: 3.4714: : 351it [03:27,  1.32it/s]\u001b[A\n",
      "batch 352, training loss: 3.4714: : 352it [03:27,  1.32it/s]\u001b[A\n",
      "batch 353, training loss: 3.4729: : 352it [03:28,  1.32it/s]\u001b[A\n",
      "batch 353, training loss: 3.4729: : 353it [03:28,  1.23it/s]\u001b[A\n",
      "batch 354, training loss: 3.5486: : 353it [03:28,  1.23it/s]\u001b[A\n",
      "batch 354, training loss: 3.5486: : 354it [03:28,  1.29it/s]\u001b[A\n",
      "batch 355, training loss: 3.4958: : 354it [03:29,  1.29it/s]\u001b[A\n",
      "batch 355, training loss: 3.4958: : 355it [03:29,  1.32it/s]\u001b[A\n",
      "batch 356, training loss: 3.3563: : 355it [03:30,  1.32it/s]\u001b[A\n",
      "batch 356, training loss: 3.3563: : 356it [03:30,  1.31it/s]\u001b[A\n",
      "batch 357, training loss: 3.3847: : 356it [03:31,  1.31it/s]\u001b[A\n",
      "batch 357, training loss: 3.3847: : 357it [03:31,  1.28it/s]\u001b[A\n",
      "batch 358, training loss: 3.5718: : 357it [03:31,  1.28it/s]\u001b[A\n",
      "batch 358, training loss: 3.5718: : 358it [03:31,  1.31it/s]\u001b[A\n",
      "batch 359, training loss: 3.3794: : 358it [03:32,  1.31it/s]\u001b[A\n",
      "batch 359, training loss: 3.3794: : 359it [03:32,  1.31it/s]\u001b[A\n",
      "batch 360, training loss: 3.4831: : 359it [03:33,  1.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 360, training loss: 3.4831: : 360it [03:33,  1.35it/s]\u001b[A\n",
      "batch 361, training loss: 3.4755: : 360it [03:34,  1.35it/s]\u001b[A\n",
      "batch 361, training loss: 3.4755: : 361it [03:34,  1.29it/s]\u001b[A\n",
      "batch 362, training loss: 3.5347: : 361it [03:34,  1.29it/s]\u001b[A\n",
      "batch 362, training loss: 3.5347: : 362it [03:34,  1.30it/s]\u001b[A\n",
      "batch 363, training loss: 3.4878: : 362it [03:35,  1.30it/s]\u001b[A\n",
      "batch 363, training loss: 3.4878: : 363it [03:35,  1.34it/s]\u001b[A\n",
      "batch 364, training loss: 3.402: : 363it [03:36,  1.34it/s] \u001b[A\n",
      "batch 364, training loss: 3.402: : 364it [03:36,  1.28it/s]\u001b[A\n",
      "batch 365, training loss: 3.3575: : 364it [03:37,  1.28it/s]\u001b[A\n",
      "batch 365, training loss: 3.3575: : 365it [03:37,  1.28it/s]\u001b[A\n",
      "batch 366, training loss: 3.4943: : 365it [03:38,  1.28it/s]\u001b[A\n",
      "batch 366, training loss: 3.4943: : 366it [03:38,  1.33it/s]\u001b[A\n",
      "batch 367, training loss: 3.4344: : 366it [03:38,  1.33it/s]\u001b[A\n",
      "batch 367, training loss: 3.4344: : 367it [03:38,  1.36it/s]\u001b[A\n",
      "batch 368, training loss: 3.5034: : 367it [03:39,  1.36it/s]\u001b[A\n",
      "batch 368, training loss: 3.5034: : 368it [03:39,  1.27it/s]\u001b[A\n",
      "batch 369, training loss: 3.5193: : 368it [03:40,  1.27it/s]\u001b[A\n",
      "batch 369, training loss: 3.5193: : 369it [03:40,  1.28it/s]\u001b[A\n",
      "batch 370, training loss: 3.4582: : 369it [03:41,  1.28it/s]\u001b[A\n",
      "batch 370, training loss: 3.4582: : 370it [03:41,  1.31it/s]\u001b[A\n",
      "batch 371, training loss: 3.4682: : 370it [03:41,  1.31it/s]\u001b[A\n",
      "batch 371, training loss: 3.4682: : 371it [03:41,  1.26it/s]\u001b[A\n",
      "batch 372, training loss: 3.4101: : 371it [03:42,  1.26it/s]\u001b[A\n",
      "batch 372, training loss: 3.4101: : 372it [03:42,  1.27it/s]\u001b[A\n",
      "batch 373, training loss: 3.4347: : 372it [03:43,  1.27it/s]\u001b[A\n",
      "batch 373, training loss: 3.4347: : 373it [03:43,  1.31it/s]\u001b[A\n",
      "batch 374, training loss: 3.449: : 373it [03:44,  1.31it/s] \u001b[A\n",
      "batch 374, training loss: 3.449: : 374it [03:44,  1.31it/s]\u001b[A\n",
      "batch 375, training loss: 3.1049: : 374it [03:44,  1.31it/s]\u001b[A\n",
      "batch 375, training loss: 3.1049: : 375it [03:44,  1.42it/s]\u001b[A\n",
      "batch 376, training loss: 3.4431: : 375it [03:45,  1.42it/s]\u001b[A\n",
      "batch 376, training loss: 3.4431: : 376it [03:45,  1.39it/s]\u001b[A\n",
      "batch 377, training loss: 3.525: : 376it [03:46,  1.39it/s] \u001b[A\n",
      "batch 377, training loss: 3.525: : 377it [03:46,  1.33it/s]\u001b[A\n",
      "batch 378, training loss: 3.4526: : 377it [03:47,  1.33it/s]\u001b[A\n",
      "batch 378, training loss: 3.4526: : 378it [03:47,  1.31it/s]\u001b[A\n",
      "batch 379, training loss: 3.4021: : 378it [03:47,  1.31it/s]\u001b[A\n",
      "batch 379, training loss: 3.4021: : 379it [03:47,  1.41it/s]\u001b[A\n",
      "batch 380, training loss: 3.3804: : 379it [03:48,  1.41it/s]\u001b[A\n",
      "batch 380, training loss: 3.3804: : 380it [03:48,  1.48it/s]\u001b[A\n",
      "batch 381, training loss: 3.4818: : 380it [03:48,  1.48it/s]\u001b[A\n",
      "batch 381, training loss: 3.4818: : 381it [03:48,  1.58it/s]\u001b[A\n",
      "batch 382, training loss: 3.3549: : 381it [03:49,  1.58it/s]\u001b[A\n",
      "batch 382, training loss: 3.3549: : 382it [03:49,  1.63it/s]\u001b[A\n",
      "batch 383, training loss: 3.4065: : 382it [03:49,  1.63it/s]\u001b[A\n",
      "batch 383, training loss: 3.4065: : 383it [03:49,  1.71it/s]\u001b[A\n",
      "batch 384, training loss: 3.4656: : 383it [03:50,  1.71it/s]\u001b[A\n",
      "batch 384, training loss: 3.4656: : 384it [03:50,  1.71it/s]\u001b[A\n",
      "batch 385, training loss: 3.4065: : 384it [03:51,  1.71it/s]\u001b[A\n",
      "batch 385, training loss: 3.4065: : 385it [03:51,  1.78it/s]\u001b[A\n",
      "batch 386, training loss: 3.3825: : 385it [03:51,  1.78it/s]\u001b[A\n",
      "batch 386, training loss: 3.3825: : 386it [03:51,  1.77it/s]\u001b[A\n",
      "batch 387, training loss: 3.4463: : 386it [03:52,  1.77it/s]\u001b[A\n",
      "batch 387, training loss: 3.4463: : 387it [03:52,  1.79it/s]\u001b[A\n",
      "batch 388, training loss: 3.2376: : 387it [03:52,  1.79it/s]\u001b[A\n",
      "batch 388, training loss: 3.2376: : 388it [03:52,  1.80it/s]\u001b[A\n",
      "batch 389, training loss: 3.4063: : 388it [03:53,  1.80it/s]\u001b[A\n",
      "batch 389, training loss: 3.4063: : 389it [03:53,  1.83it/s]\u001b[A\n",
      "batch 390, training loss: 3.4889: : 389it [03:53,  1.83it/s]\u001b[A\n",
      "batch 390, training loss: 3.4889: : 390it [03:53,  1.81it/s]\u001b[A\n",
      "batch 391, training loss: 3.5102: : 390it [03:54,  1.81it/s]\u001b[A\n",
      "batch 391, training loss: 3.5102: : 391it [03:54,  1.84it/s]\u001b[A\n",
      "batch 392, training loss: 3.4657: : 391it [03:54,  1.84it/s]\u001b[A\n",
      "batch 392, training loss: 3.4657: : 392it [03:54,  1.81it/s]\u001b[A\n",
      "batch 393, training loss: 3.2839: : 392it [03:55,  1.81it/s]\u001b[A\n",
      "batch 393, training loss: 3.2839: : 393it [03:55,  1.84it/s]\u001b[A\n",
      "batch 394, training loss: 3.254: : 393it [03:55,  1.84it/s] \u001b[A\n",
      "batch 394, training loss: 3.254: : 394it [03:55,  1.83it/s]\u001b[A\n",
      "batch 395, training loss: 3.3397: : 394it [03:56,  1.83it/s]\u001b[A\n",
      "batch 395, training loss: 3.3397: : 395it [03:56,  1.85it/s]\u001b[A\n",
      "batch 396, training loss: 3.517: : 395it [03:57,  1.85it/s] \u001b[A\n",
      "batch 396, training loss: 3.517: : 396it [03:57,  1.82it/s]\u001b[A\n",
      "batch 397, training loss: 3.3527: : 396it [03:57,  1.82it/s]\u001b[A\n",
      "batch 397, training loss: 3.3527: : 397it [03:57,  1.83it/s]\u001b[A\n",
      "batch 398, training loss: 3.4451: : 397it [03:58,  1.83it/s]\u001b[A\n",
      "batch 398, training loss: 3.4451: : 398it [03:58,  1.81it/s]\u001b[A\n",
      "batch 399, training loss: 3.5401: : 398it [03:58,  1.81it/s]\u001b[A\n",
      "batch 399, training loss: 3.5401: : 399it [03:58,  1.84it/s]\u001b[A\n",
      "batch 400, training loss: 3.3387: : 399it [03:59,  1.84it/s]\u001b[A\n",
      "batch 400, training loss: 3.3387: : 400it [03:59,  1.83it/s]\u001b[A\n",
      "batch 401, training loss: 3.2843: : 400it [03:59,  1.83it/s]\u001b[A\n",
      "batch 401, training loss: 3.2843: : 401it [03:59,  1.85it/s]\u001b[A\n",
      "batch 402, training loss: 3.3836: : 401it [04:00,  1.85it/s]\u001b[A\n",
      "batch 402, training loss: 3.3836: : 402it [04:00,  1.83it/s]\u001b[A\n",
      "batch 403, training loss: 3.541: : 402it [04:00,  1.83it/s] \u001b[A\n",
      "batch 403, training loss: 3.541: : 403it [04:00,  1.85it/s]\u001b[A\n",
      "batch 404, training loss: 3.1709: : 403it [04:01,  1.85it/s]\u001b[A\n",
      "batch 404, training loss: 3.1709: : 404it [04:01,  1.82it/s]\u001b[A\n",
      "batch 405, training loss: 3.4459: : 404it [04:01,  1.82it/s]\u001b[A\n",
      "batch 405, training loss: 3.4459: : 405it [04:01,  1.84it/s]\u001b[A\n",
      "batch 406, training loss: 3.3564: : 405it [04:02,  1.84it/s]\u001b[A\n",
      "batch 406, training loss: 3.3564: : 406it [04:02,  1.81it/s]\u001b[A\n",
      "batch 407, training loss: 3.4337: : 406it [04:03,  1.81it/s]\u001b[A\n",
      "batch 407, training loss: 3.4337: : 407it [04:03,  1.84it/s]\u001b[A\n",
      "batch 408, training loss: 3.2855: : 407it [04:03,  1.84it/s]\u001b[A\n",
      "batch 408, training loss: 3.2855: : 408it [04:03,  1.81it/s]\u001b[A\n",
      "batch 409, training loss: 3.431: : 408it [04:04,  1.81it/s] \u001b[A\n",
      "batch 409, training loss: 3.431: : 409it [04:04,  1.85it/s]\u001b[A\n",
      "batch 410, training loss: 3.3107: : 409it [04:04,  1.85it/s]\u001b[A\n",
      "batch 410, training loss: 3.3107: : 410it [04:04,  1.82it/s]\u001b[A\n",
      "batch 411, training loss: 3.4197: : 410it [04:05,  1.82it/s]\u001b[A\n",
      "batch 411, training loss: 3.4197: : 411it [04:05,  1.85it/s]\u001b[A\n",
      "batch 412, training loss: 3.4456: : 411it [04:05,  1.85it/s]\u001b[A\n",
      "batch 412, training loss: 3.4456: : 412it [04:05,  1.85it/s]\u001b[A\n",
      "batch 413, training loss: 3.3612: : 412it [04:06,  1.85it/s]\u001b[A\n",
      "batch 413, training loss: 3.3612: : 413it [04:06,  1.86it/s]\u001b[A\n",
      "batch 414, training loss: 3.2946: : 413it [04:06,  1.86it/s]\u001b[A\n",
      "batch 414, training loss: 3.2946: : 414it [04:06,  1.83it/s]\u001b[A\n",
      "batch 415, training loss: 3.1897: : 414it [04:07,  1.83it/s]\u001b[A\n",
      "batch 415, training loss: 3.1897: : 415it [04:07,  1.84it/s]\u001b[A\n",
      "batch 416, training loss: 3.2388: : 415it [04:07,  1.84it/s]\u001b[A\n",
      "batch 416, training loss: 3.2388: : 416it [04:07,  1.85it/s]\u001b[A\n",
      "batch 417, training loss: 3.5038: : 416it [04:08,  1.85it/s]\u001b[A\n",
      "batch 417, training loss: 3.5038: : 417it [04:08,  1.86it/s]\u001b[A\n",
      "batch 418, training loss: 3.3712: : 417it [04:09,  1.86it/s]\u001b[A\n",
      "batch 418, training loss: 3.3712: : 418it [04:09,  1.84it/s]\u001b[A\n",
      "batch 419, training loss: 3.2058: : 418it [04:09,  1.84it/s]\u001b[A\n",
      "batch 419, training loss: 3.2058: : 419it [04:09,  1.87it/s]\u001b[A\n",
      "batch 420, training loss: 3.3123: : 419it [04:10,  1.87it/s]\u001b[A\n",
      "batch 420, training loss: 3.3123: : 420it [04:10,  1.83it/s]\u001b[A\n",
      "batch 421, training loss: 3.4238: : 420it [04:10,  1.83it/s]\u001b[A\n",
      "batch 421, training loss: 3.4238: : 421it [04:10,  1.86it/s]\u001b[A\n",
      "batch 422, training loss: 3.4354: : 421it [04:11,  1.86it/s]\u001b[A\n",
      "batch 422, training loss: 3.4354: : 422it [04:11,  1.84it/s]\u001b[A\n",
      "batch 423, training loss: 3.2203: : 422it [04:11,  1.84it/s]\u001b[A\n",
      "batch 423, training loss: 3.2203: : 423it [04:11,  1.86it/s]\u001b[A\n",
      "batch 424, training loss: 3.4079: : 423it [04:12,  1.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 424, training loss: 3.4079: : 424it [04:12,  1.80it/s]\u001b[A\n",
      "batch 425, training loss: 3.453: : 424it [04:12,  1.80it/s] \u001b[A\n",
      "batch 425, training loss: 3.453: : 425it [04:12,  1.80it/s]\u001b[A\n",
      "batch 426, training loss: 3.3961: : 425it [04:13,  1.80it/s]\u001b[A\n",
      "batch 426, training loss: 3.3961: : 426it [04:13,  1.77it/s]\u001b[A\n",
      "batch 427, training loss: 3.4378: : 426it [04:13,  1.77it/s]\u001b[A\n",
      "batch 427, training loss: 3.4378: : 427it [04:13,  1.79it/s]\u001b[A\n",
      "batch 428, training loss: 3.5879: : 427it [04:14,  1.79it/s]\u001b[A\n",
      "batch 428, training loss: 3.5879: : 428it [04:14,  1.76it/s]\u001b[A\n",
      "batch 429, training loss: 3.4204: : 428it [04:15,  1.76it/s]\u001b[A\n",
      "batch 429, training loss: 3.4204: : 429it [04:15,  1.78it/s]\u001b[A\n",
      "batch 430, training loss: 3.4668: : 429it [04:15,  1.78it/s]\u001b[A\n",
      "batch 430, training loss: 3.4668: : 430it [04:15,  1.75it/s]\u001b[A\n",
      "batch 431, training loss: 3.5225: : 430it [04:16,  1.75it/s]\u001b[A\n",
      "batch 431, training loss: 3.5225: : 431it [04:16,  1.76it/s]\u001b[A\n",
      "batch 432, training loss: 3.3439: : 431it [04:16,  1.76it/s]\u001b[A\n",
      "batch 432, training loss: 3.3439: : 432it [04:16,  1.73it/s]\u001b[A\n",
      "batch 433, training loss: 3.3854: : 432it [04:17,  1.73it/s]\u001b[A\n",
      "batch 433, training loss: 3.3854: : 433it [04:17,  1.75it/s]\u001b[A\n",
      "batch 434, training loss: 3.4187: : 433it [04:18,  1.75it/s]\u001b[A\n",
      "batch 434, training loss: 3.4187: : 434it [04:18,  1.71it/s]\u001b[A\n",
      "batch 435, training loss: 3.3981: : 434it [04:18,  1.71it/s]\u001b[A\n",
      "batch 435, training loss: 3.3981: : 435it [04:18,  1.73it/s]\u001b[A\n",
      "batch 436, training loss: 3.4351: : 435it [04:19,  1.73it/s]\u001b[A\n",
      "batch 436, training loss: 3.4351: : 436it [04:19,  1.71it/s]\u001b[A\n",
      "batch 437, training loss: 3.378: : 436it [04:19,  1.71it/s] \u001b[A\n",
      "batch 437, training loss: 3.378: : 437it [04:19,  1.74it/s]\u001b[A\n",
      "batch 438, training loss: 3.4107: : 437it [04:20,  1.74it/s]\u001b[A\n",
      "batch 438, training loss: 3.4107: : 438it [04:20,  1.71it/s]\u001b[A\n",
      "batch 439, training loss: 3.5301: : 438it [04:20,  1.71it/s]\u001b[A\n",
      "batch 439, training loss: 3.5301: : 439it [04:20,  1.73it/s]\u001b[A\n",
      "batch 440, training loss: 3.3567: : 439it [04:21,  1.73it/s]\u001b[A\n",
      "batch 440, training loss: 3.3567: : 440it [04:21,  1.72it/s]\u001b[A\n",
      "batch 441, training loss: 3.6322: : 440it [04:22,  1.72it/s]\u001b[A\n",
      "batch 441, training loss: 3.6322: : 441it [04:22,  1.74it/s]\u001b[A\n",
      "batch 442, training loss: 3.3142: : 441it [04:22,  1.74it/s]\u001b[A\n",
      "batch 442, training loss: 3.3142: : 442it [04:22,  1.74it/s]\u001b[A\n",
      "batch 443, training loss: 3.4077: : 442it [04:23,  1.74it/s]\u001b[A\n",
      "batch 443, training loss: 3.4077: : 443it [04:23,  1.75it/s]\u001b[A\n",
      "batch 444, training loss: 3.4072: : 443it [04:23,  1.75it/s]\u001b[A\n",
      "batch 444, training loss: 3.4072: : 444it [04:23,  1.72it/s]\u001b[A\n",
      "batch 445, training loss: 3.4669: : 444it [04:24,  1.72it/s]\u001b[A\n",
      "batch 445, training loss: 3.4669: : 445it [04:24,  1.74it/s]\u001b[A\n",
      "batch 446, training loss: 3.4445: : 445it [04:24,  1.74it/s]\u001b[A\n",
      "batch 446, training loss: 3.4445: : 446it [04:24,  1.71it/s]\u001b[A\n",
      "batch 447, training loss: 3.3991: : 446it [04:25,  1.71it/s]\u001b[A\n",
      "batch 447, training loss: 3.3991: : 447it [04:25,  1.73it/s]\u001b[A\n",
      "batch 448, training loss: 3.3124: : 447it [04:26,  1.73it/s]\u001b[A\n",
      "batch 448, training loss: 3.3124: : 448it [04:26,  1.72it/s]\u001b[A\n",
      "batch 449, training loss: 3.394: : 448it [04:26,  1.72it/s] \u001b[A\n",
      "batch 449, training loss: 3.394: : 449it [04:26,  1.75it/s]\u001b[A\n",
      "batch 450, training loss: 3.4742: : 449it [04:27,  1.75it/s]\u001b[A\n",
      "batch 450, training loss: 3.4742: : 450it [04:27,  1.73it/s]\u001b[A\n",
      "batch 451, training loss: 3.4884: : 450it [04:27,  1.73it/s]\u001b[A\n",
      "batch 451, training loss: 3.4884: : 451it [04:27,  1.74it/s]\u001b[A\n",
      "batch 452, training loss: 3.466: : 451it [04:28,  1.74it/s] \u001b[A\n",
      "batch 452, training loss: 3.466: : 452it [04:28,  1.71it/s]\u001b[A\n",
      "batch 453, training loss: 3.5541: : 452it [04:29,  1.71it/s]\u001b[A\n",
      "batch 453, training loss: 3.5541: : 453it [04:29,  1.73it/s]\u001b[A\n",
      "batch 454, training loss: 3.4285: : 453it [04:29,  1.73it/s]\u001b[A\n",
      "batch 454, training loss: 3.4285: : 454it [04:29,  1.72it/s]\u001b[A\n",
      "batch 455, training loss: 3.3199: : 454it [04:30,  1.72it/s]\u001b[A\n",
      "batch 455, training loss: 3.3199: : 455it [04:30,  1.73it/s]\u001b[A\n",
      "batch 456, training loss: 3.3557: : 455it [04:30,  1.73it/s]\u001b[A\n",
      "batch 456, training loss: 3.3557: : 456it [04:30,  1.71it/s]\u001b[A\n",
      "batch 457, training loss: 3.4587: : 456it [04:31,  1.71it/s]\u001b[A\n",
      "batch 457, training loss: 3.4587: : 457it [04:31,  1.79it/s]\u001b[A\n",
      "batch 458, training loss: 3.3485: : 457it [04:31,  1.79it/s]\u001b[A\n",
      "batch 458, training loss: 3.3485: : 458it [04:31,  1.83it/s]\u001b[A\n",
      "batch 459, training loss: 3.4187: : 458it [04:32,  1.83it/s]\u001b[A\n",
      "batch 459, training loss: 3.4187: : 459it [04:32,  1.73it/s]\u001b[A\n",
      "batch 460, training loss: 3.4142: : 459it [04:32,  1.73it/s]\u001b[A\n",
      "batch 460, training loss: 3.4142: : 460it [04:32,  1.81it/s]\u001b[A\n",
      "batch 461, training loss: 3.4966: : 460it [04:33,  1.81it/s]\u001b[A\n",
      "batch 461, training loss: 3.4966: : 461it [04:33,  1.79it/s]\u001b[A\n",
      "batch 462, training loss: 3.4675: : 461it [04:34,  1.79it/s]\u001b[A\n",
      "batch 462, training loss: 3.4675: : 462it [04:34,  1.84it/s]\u001b[A\n",
      "batch 463, training loss: 3.2969: : 462it [04:34,  1.84it/s]\u001b[A\n",
      "batch 463, training loss: 3.2969: : 463it [04:34,  1.83it/s]\u001b[A\n",
      "batch 464, training loss: 3.4408: : 463it [04:35,  1.83it/s]\u001b[A\n",
      "batch 464, training loss: 3.4408: : 464it [04:35,  1.88it/s]\u001b[A\n",
      "batch 465, training loss: 3.3769: : 464it [04:35,  1.88it/s]\u001b[A\n",
      "batch 465, training loss: 3.3769: : 465it [04:35,  1.91it/s]\u001b[A\n",
      "batch 466, training loss: 3.267: : 465it [04:36,  1.91it/s] \u001b[A\n",
      "batch 466, training loss: 3.267: : 466it [04:36,  1.92it/s]\u001b[A\n",
      "batch 467, training loss: 3.2045: : 466it [04:36,  1.92it/s]\u001b[A\n",
      "batch 467, training loss: 3.2045: : 467it [04:36,  1.85it/s]\u001b[A\n",
      "batch 468, training loss: 3.4147: : 467it [04:37,  1.85it/s]\u001b[A\n",
      "batch 468, training loss: 3.4147: : 468it [04:37,  1.85it/s]\u001b[A\n",
      "batch 469, training loss: 3.3131: : 468it [04:37,  1.85it/s]\u001b[A\n",
      "batch 469, training loss: 3.3131: : 469it [04:37,  1.85it/s]\u001b[A\n",
      "batch 470, training loss: 3.4414: : 469it [04:38,  1.85it/s]\u001b[A\n",
      "batch 470, training loss: 3.4414: : 470it [04:38,  1.75it/s]\u001b[A\n",
      "batch 471, training loss: 3.3884: : 470it [04:38,  1.75it/s]\u001b[A\n",
      "batch 471, training loss: 3.3884: : 471it [04:38,  1.79it/s]\u001b[A\n",
      "batch 472, training loss: 3.3592: : 471it [04:39,  1.79it/s]\u001b[A\n",
      "batch 472, training loss: 3.3592: : 472it [04:39,  1.78it/s]\u001b[A\n",
      "batch 473, training loss: 3.3532: : 472it [04:40,  1.78it/s]\u001b[A\n",
      "batch 473, training loss: 3.3532: : 473it [04:40,  1.69it/s]\u001b[A\n",
      "batch 474, training loss: 3.405: : 473it [04:40,  1.69it/s] \u001b[A\n",
      "batch 474, training loss: 3.405: : 474it [04:40,  1.73it/s]\u001b[A\n",
      "batch 475, training loss: 3.3145: : 474it [04:41,  1.73it/s]\u001b[A\n",
      "batch 475, training loss: 3.3145: : 475it [04:41,  1.73it/s]\u001b[A\n",
      "batch 476, training loss: 3.4199: : 475it [04:41,  1.73it/s]\u001b[A\n",
      "batch 476, training loss: 3.4199: : 476it [04:41,  1.65it/s]\u001b[A\n",
      "batch 477, training loss: 3.2377: : 476it [04:42,  1.65it/s]\u001b[A\n",
      "batch 477, training loss: 3.2377: : 477it [04:42,  1.71it/s]\u001b[A\n",
      "batch 478, training loss: 3.3059: : 477it [04:43,  1.71it/s]\u001b[A\n",
      "batch 478, training loss: 3.3059: : 478it [04:43,  1.71it/s]\u001b[A\n",
      "batch 479, training loss: 3.2926: : 478it [04:43,  1.71it/s]\u001b[A\n",
      "batch 479, training loss: 3.2926: : 479it [04:43,  1.76it/s]\u001b[A\n",
      "batch 480, training loss: 3.319: : 479it [04:44,  1.76it/s] \u001b[A\n",
      "batch 480, training loss: 3.319: : 480it [04:44,  1.79it/s]\u001b[A\n",
      "batch 481, training loss: 3.3442: : 480it [04:44,  1.79it/s]\u001b[A\n",
      "batch 481, training loss: 3.3442: : 481it [04:44,  1.76it/s]\u001b[A\n",
      "batch 482, training loss: 3.3198: : 481it [04:45,  1.76it/s]\u001b[A\n",
      "batch 482, training loss: 3.3198: : 482it [04:45,  1.79it/s]\u001b[A\n",
      "batch 483, training loss: 3.2699: : 482it [04:45,  1.79it/s]\u001b[A\n",
      "batch 483, training loss: 3.2699: : 483it [04:45,  1.77it/s]\u001b[A\n",
      "batch 484, training loss: 3.3515: : 483it [04:46,  1.77it/s]\u001b[A\n",
      "batch 484, training loss: 3.3515: : 484it [04:46,  1.70it/s]\u001b[A\n",
      "batch 485, training loss: 3.2865: : 484it [04:47,  1.70it/s]\u001b[A\n",
      "batch 485, training loss: 3.2865: : 485it [04:47,  1.76it/s]\u001b[A\n",
      "batch 486, training loss: 3.3642: : 485it [04:47,  1.76it/s]\u001b[A\n",
      "batch 486, training loss: 3.3642: : 486it [04:47,  1.74it/s]\u001b[A\n",
      "batch 487, training loss: 3.4795: : 486it [04:48,  1.74it/s]\u001b[A\n",
      "batch 487, training loss: 3.4795: : 487it [04:48,  1.77it/s]\u001b[A\n",
      "batch 488, training loss: 3.2788: : 487it [04:48,  1.77it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 488, training loss: 3.2788: : 488it [04:48,  1.78it/s]\u001b[A\n",
      "batch 489, training loss: 3.2329: : 488it [04:49,  1.78it/s]\u001b[A\n",
      "batch 489, training loss: 3.2329: : 489it [04:49,  1.68it/s]\u001b[A\n",
      "batch 490, training loss: 3.2756: : 489it [04:49,  1.68it/s]\u001b[A\n",
      "batch 490, training loss: 3.2756: : 490it [04:49,  1.74it/s]\u001b[A\n",
      "batch 491, training loss: 3.3328: : 490it [04:50,  1.74it/s]\u001b[A\n",
      "batch 491, training loss: 3.3328: : 491it [04:50,  1.72it/s]\u001b[A\n",
      "batch 492, training loss: 3.2297: : 491it [04:51,  1.72it/s]\u001b[A\n",
      "batch 492, training loss: 3.2297: : 492it [04:51,  1.63it/s]\u001b[A\n",
      "batch 493, training loss: 3.4039: : 492it [04:51,  1.63it/s]\u001b[A\n",
      "batch 493, training loss: 3.4039: : 493it [04:51,  1.71it/s]\u001b[A\n",
      "batch 494, training loss: 3.4268: : 493it [04:52,  1.71it/s]\u001b[A\n",
      "batch 494, training loss: 3.4268: : 494it [04:52,  1.71it/s]\u001b[A\n",
      "batch 495, training loss: 3.2505: : 494it [04:52,  1.71it/s]\u001b[A\n",
      "batch 495, training loss: 3.2505: : 495it [04:52,  1.77it/s]\u001b[A\n",
      "batch 496, training loss: 3.2684: : 495it [04:53,  1.77it/s]\u001b[A\n",
      "batch 496, training loss: 3.2684: : 496it [04:53,  1.80it/s]\u001b[A\n",
      "batch 497, training loss: 3.1645: : 496it [04:53,  1.80it/s]\u001b[A\n",
      "batch 497, training loss: 3.1645: : 497it [04:53,  1.76it/s]\u001b[A\n",
      "batch 498, training loss: 3.2757: : 497it [04:54,  1.76it/s]\u001b[A\n",
      "batch 498, training loss: 3.2757: : 498it [04:54,  1.80it/s]\u001b[A\n",
      "batch 499, training loss: 3.1695: : 498it [04:55,  1.80it/s]\u001b[A\n",
      "batch 499, training loss: 3.1695: : 499it [04:55,  1.76it/s]\u001b[A\n",
      "batch 500, training loss: 3.4966: : 499it [04:56,  1.76it/s]\u001b[A\n",
      "batch 500, training loss: 3.4966: : 500it [04:56,  1.47it/s]\u001b[A\n",
      "batch 501, training loss: 3.3554: : 500it [04:57,  1.47it/s]\u001b[A\n",
      "batch 501, training loss: 3.3554: : 501it [04:57,  1.29it/s]\u001b[A\n",
      "batch 502, training loss: 3.3031: : 501it [04:58,  1.29it/s]\u001b[A\n",
      "batch 502, training loss: 3.3031: : 502it [04:58,  1.15it/s]\u001b[A\n",
      "batch 503, training loss: 3.4014: : 502it [04:59,  1.15it/s]\u001b[A\n",
      "batch 503, training loss: 3.4014: : 503it [04:59,  1.12it/s]\u001b[A\n",
      "batch 504, training loss: 3.3853: : 503it [05:00,  1.12it/s]\u001b[A\n",
      "batch 504, training loss: 3.3853: : 504it [05:00,  1.08it/s]\u001b[A\n",
      "batch 505, training loss: 3.457: : 504it [05:01,  1.08it/s] \u001b[A\n",
      "batch 505, training loss: 3.457: : 505it [05:01,  1.01it/s]\u001b[A\n",
      "batch 506, training loss: 3.3624: : 505it [05:02,  1.01it/s]\u001b[A\n",
      "batch 506, training loss: 3.3624: : 506it [05:02,  1.00it/s]\u001b[A\n",
      "batch 507, training loss: 3.2738: : 506it [05:03,  1.00it/s]\u001b[A\n",
      "batch 507, training loss: 3.2738: : 507it [05:03,  1.03s/it]\u001b[A\n",
      "batch 508, training loss: 3.4107: : 507it [05:04,  1.03s/it]\u001b[A\n",
      "batch 508, training loss: 3.4107: : 508it [05:04,  1.02s/it]\u001b[A\n",
      "batch 509, training loss: 3.2929: : 508it [05:05,  1.02s/it]\u001b[A\n",
      "batch 509, training loss: 3.2929: : 509it [05:05,  1.01s/it]\u001b[A\n",
      "batch 510, training loss: 3.3635: : 509it [05:06,  1.01s/it]\u001b[A\n",
      "batch 510, training loss: 3.3635: : 510it [05:06,  1.06s/it]\u001b[A\n",
      "batch 511, training loss: 3.3688: : 510it [05:07,  1.06s/it]\u001b[A\n",
      "batch 511, training loss: 3.3688: : 511it [05:07,  1.02s/it]\u001b[A\n",
      "batch 512, training loss: 3.3632: : 511it [05:08,  1.02s/it]\u001b[A\n",
      "batch 512, training loss: 3.3632: : 512it [05:08,  1.02s/it]\u001b[A\n",
      "batch 513, training loss: 3.4184: : 512it [05:09,  1.02s/it]\u001b[A\n",
      "batch 513, training loss: 3.4184: : 513it [05:09,  1.05s/it]\u001b[A\n",
      "batch 514, training loss: 3.2793: : 513it [05:10,  1.05s/it]\u001b[A\n",
      "batch 514, training loss: 3.2793: : 514it [05:10,  1.04s/it]\u001b[A\n",
      "batch 515, training loss: 3.4542: : 514it [05:11,  1.04s/it]\u001b[A\n",
      "batch 515, training loss: 3.4542: : 515it [05:11,  1.06s/it]\u001b[A\n",
      "batch 516, training loss: 3.3281: : 515it [05:12,  1.06s/it]\u001b[A\n",
      "batch 516, training loss: 3.3281: : 516it [05:12,  1.06s/it]\u001b[A\n",
      "batch 517, training loss: 3.3269: : 516it [05:13,  1.06s/it]\u001b[A\n",
      "batch 517, training loss: 3.3269: : 517it [05:13,  1.03s/it]\u001b[A\n",
      "batch 518, training loss: 3.4155: : 517it [05:14,  1.03s/it]\u001b[A\n",
      "batch 518, training loss: 3.4155: : 518it [05:14,  1.07s/it]\u001b[A\n",
      "batch 519, training loss: 3.217: : 518it [05:15,  1.07s/it] \u001b[A\n",
      "batch 519, training loss: 3.217: : 519it [05:15,  1.03s/it]\u001b[A\n",
      "batch 520, training loss: 3.3779: : 519it [05:16,  1.03s/it]\u001b[A\n",
      "batch 520, training loss: 3.3779: : 520it [05:16,  1.03s/it]\u001b[A\n",
      "batch 521, training loss: 3.3958: : 520it [05:17,  1.03s/it]\u001b[A\n",
      "batch 521, training loss: 3.3958: : 521it [05:17,  1.06s/it]\u001b[A\n",
      "batch 522, training loss: 3.4218: : 521it [05:18,  1.06s/it]\u001b[A\n",
      "batch 522, training loss: 3.4218: : 522it [05:18,  1.02s/it]\u001b[A\n",
      "batch 523, training loss: 3.3017: : 522it [05:20,  1.02s/it]\u001b[A\n",
      "batch 523, training loss: 3.3017: : 523it [05:20,  1.07s/it]\u001b[A\n",
      "batch 524, training loss: 3.4839: : 523it [05:20,  1.07s/it]\u001b[A\n",
      "batch 524, training loss: 3.4839: : 524it [05:20,  1.03s/it]\u001b[A\n",
      "batch 525, training loss: 3.524: : 524it [05:21,  1.03s/it] \u001b[A\n",
      "batch 525, training loss: 3.524: : 525it [05:21,  1.02s/it]\u001b[A\n",
      "batch 526, training loss: 3.3161: : 525it [05:23,  1.02s/it]\u001b[A\n",
      "batch 526, training loss: 3.3161: : 526it [05:23,  1.06s/it]\u001b[A\n",
      "batch 527, training loss: 3.3167: : 526it [05:23,  1.06s/it]\u001b[A\n",
      "batch 527, training loss: 3.3167: : 527it [05:23,  1.01s/it]\u001b[A\n",
      "batch 528, training loss: 3.3329: : 527it [05:25,  1.01s/it]\u001b[A\n",
      "batch 528, training loss: 3.3329: : 528it [05:25,  1.07s/it]\u001b[A\n",
      "batch 529, training loss: 3.4097: : 528it [05:26,  1.07s/it]\u001b[A\n",
      "batch 529, training loss: 3.4097: : 529it [05:26,  1.06s/it]\u001b[A\n",
      "batch 530, training loss: 3.3737: : 529it [05:27,  1.06s/it]\u001b[A\n",
      "batch 530, training loss: 3.3737: : 530it [05:27,  1.05s/it]\u001b[A\n",
      "batch 531, training loss: 3.2872: : 530it [05:28,  1.05s/it]\u001b[A\n",
      "batch 531, training loss: 3.2872: : 531it [05:28,  1.10s/it]\u001b[A\n",
      "batch 532, training loss: 3.1166: : 531it [05:29,  1.10s/it]\u001b[A\n",
      "batch 532, training loss: 3.1166: : 532it [05:29,  1.09s/it]\u001b[A\n",
      "batch 533, training loss: 3.2812: : 532it [05:30,  1.09s/it]\u001b[A\n",
      "batch 533, training loss: 3.2812: : 533it [05:30,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.4283: : 533it [05:31,  1.07s/it]\u001b[A\n",
      "batch 534, training loss: 3.4283: : 534it [05:31,  1.11s/it]\u001b[A\n",
      "batch 535, training loss: 3.2812: : 534it [05:32,  1.11s/it]\u001b[A\n",
      "batch 535, training loss: 3.2812: : 535it [05:32,  1.09s/it]\u001b[A\n",
      "batch 536, training loss: 3.1403: : 535it [05:33,  1.09s/it]\u001b[A\n",
      "batch 536, training loss: 3.1403: : 536it [05:33,  1.07s/it]\u001b[A\n",
      "batch 537, training loss: 3.2104: : 536it [05:35,  1.07s/it]\u001b[A\n",
      "batch 537, training loss: 3.2104: : 537it [05:35,  1.11s/it]\u001b[A\n",
      "batch 538, training loss: 3.3225: : 537it [05:36,  1.11s/it]\u001b[A\n",
      "batch 538, training loss: 3.3225: : 538it [05:36,  1.09s/it]\u001b[A\n",
      "batch 539, training loss: 3.2791: : 538it [05:37,  1.09s/it]\u001b[A\n",
      "batch 539, training loss: 3.2791: : 539it [05:37,  1.07s/it]\u001b[A\n",
      "batch 540, training loss: 3.2473: : 539it [05:38,  1.07s/it]\u001b[A\n",
      "batch 540, training loss: 3.2473: : 540it [05:38,  1.11s/it]\u001b[A\n",
      "batch 541, training loss: 3.3422: : 540it [05:39,  1.11s/it]\u001b[A\n",
      "batch 541, training loss: 3.3422: : 541it [05:39,  1.10s/it]\u001b[A\n",
      "batch 542, training loss: 3.3834: : 541it [05:40,  1.10s/it]\u001b[A\n",
      "batch 542, training loss: 3.3834: : 542it [05:40,  1.07s/it]\u001b[A\n",
      "batch 543, training loss: 3.2476: : 542it [05:41,  1.07s/it]\u001b[A\n",
      "batch 543, training loss: 3.2476: : 543it [05:41,  1.10s/it]\u001b[A\n",
      "batch 544, training loss: 3.3068: : 543it [05:42,  1.10s/it]\u001b[A\n",
      "batch 544, training loss: 3.3068: : 544it [05:42,  1.10s/it]\u001b[A\n",
      "batch 545, training loss: 3.3142: : 544it [05:43,  1.10s/it]\u001b[A\n",
      "batch 545, training loss: 3.3142: : 545it [05:43,  1.13s/it]\u001b[A\n",
      "batch 546, training loss: 3.2249: : 545it [05:44,  1.13s/it]\u001b[A\n",
      "batch 546, training loss: 3.2249: : 546it [05:44,  1.12s/it]\u001b[A\n",
      "batch 547, training loss: 3.1436: : 546it [05:45,  1.12s/it]\u001b[A\n",
      "batch 547, training loss: 3.1436: : 547it [05:45,  1.09s/it]\u001b[A\n",
      "batch 548, training loss: 2.9757: : 547it [05:46,  1.09s/it]\u001b[A\n",
      "batch 548, training loss: 2.9757: : 548it [05:46,  1.07it/s]\u001b[A\n",
      "batch 549, training loss: 3.3142: : 548it [05:47,  1.07it/s]\u001b[A\n",
      "batch 549, training loss: 3.3142: : 549it [05:47,  1.02s/it]\u001b[A\n",
      "batch 550, training loss: 3.2303: : 549it [05:48,  1.02s/it]\u001b[A\n",
      "batch 550, training loss: 3.2303: : 550it [05:48,  1.06s/it]\u001b[A\n",
      "batch 551, training loss: 3.2631: : 550it [05:50,  1.06s/it]\u001b[A\n",
      "batch 551, training loss: 3.2631: : 551it [05:50,  1.12s/it]\u001b[A\n",
      "batch 552, training loss: 3.3021: : 551it [05:51,  1.12s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 552, training loss: 3.3021: : 552it [05:51,  1.12s/it]\u001b[A\n",
      "batch 553, training loss: 3.2827: : 552it [05:52,  1.12s/it]\u001b[A\n",
      "batch 553, training loss: 3.2827: : 553it [05:52,  1.12s/it]\u001b[A\n",
      "batch 554, training loss: 3.0275: : 553it [05:53,  1.12s/it]\u001b[A\n",
      "batch 554, training loss: 3.0275: : 554it [05:53,  1.16s/it]\u001b[A\n",
      "batch 555, training loss: 3.1819: : 554it [05:54,  1.16s/it]\u001b[A\n",
      "batch 555, training loss: 3.1819: : 555it [05:54,  1.14s/it]\u001b[A\n",
      "batch 556, training loss: 3.349: : 555it [05:56,  1.14s/it] \u001b[A\n",
      "batch 556, training loss: 3.349: : 556it [05:56,  1.18s/it]\u001b[A\n",
      "batch 557, training loss: 3.1725: : 556it [05:57,  1.18s/it]\u001b[A\n",
      "batch 557, training loss: 3.1725: : 557it [05:57,  1.15s/it]\u001b[A\n",
      "batch 558, training loss: 3.1305: : 557it [05:58,  1.15s/it]\u001b[A\n",
      "batch 558, training loss: 3.1305: : 558it [05:58,  1.18s/it]\u001b[A\n",
      "batch 559, training loss: 3.2225: : 558it [05:59,  1.18s/it]\u001b[A\n",
      "batch 559, training loss: 3.2225: : 559it [05:59,  1.17s/it]\u001b[A\n",
      "batch 560, training loss: 3.2406: : 559it [06:00,  1.17s/it]\u001b[A\n",
      "batch 560, training loss: 3.2406: : 560it [06:00,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.1937: : 560it [06:01,  1.15s/it]\u001b[A\n",
      "batch 561, training loss: 3.1937: : 561it [06:01,  1.18s/it]\u001b[A\n",
      "batch 562, training loss: 3.0823: : 561it [06:02,  1.18s/it]\u001b[A\n",
      "batch 562, training loss: 3.0823: : 562it [06:02,  1.16s/it]\u001b[A\n",
      "batch 563, training loss: 3.2238: : 562it [06:04,  1.16s/it]\u001b[A\n",
      "batch 563, training loss: 3.2238: : 563it [06:04,  1.19s/it]\u001b[A\n",
      "batch 564, training loss: 3.0773: : 563it [06:04,  1.19s/it]\u001b[A\n",
      "batch 564, training loss: 3.0773: : 564it [06:04,  1.05s/it]\u001b[A\n",
      "batch 565, training loss: 3.2417: : 564it [06:05,  1.05s/it]\u001b[A\n",
      "batch 565, training loss: 3.2417: : 565it [06:05,  1.05it/s]\u001b[A\n",
      "batch 566, training loss: 3.3142: : 565it [06:06,  1.05it/s]\u001b[A\n",
      "batch 566, training loss: 3.3142: : 566it [06:06,  1.02s/it]\u001b[A\n",
      "batch 567, training loss: 3.2998: : 566it [06:07,  1.02s/it]\u001b[A\n",
      "batch 567, training loss: 3.2998: : 567it [06:07,  1.05s/it]\u001b[A\n",
      "batch 568, training loss: 3.3655: : 567it [06:09,  1.05s/it]\u001b[A\n",
      "batch 568, training loss: 3.3655: : 568it [06:09,  1.14s/it]\u001b[A\n",
      "batch 569, training loss: 3.3441: : 568it [06:10,  1.14s/it]\u001b[A\n",
      "batch 569, training loss: 3.3441: : 569it [06:10,  1.14s/it]\u001b[A\n",
      "batch 570, training loss: 3.5126: : 569it [06:11,  1.14s/it]\u001b[A\n",
      "batch 570, training loss: 3.5126: : 570it [06:11,  1.20s/it]\u001b[A\n",
      "batch 571, training loss: 3.3061: : 570it [06:13,  1.20s/it]\u001b[A\n",
      "batch 571, training loss: 3.3061: : 571it [06:13,  1.21s/it]\u001b[A\n",
      "batch 572, training loss: 3.4242: : 571it [06:14,  1.21s/it]\u001b[A\n",
      "batch 572, training loss: 3.4242: : 572it [06:14,  1.27s/it]\u001b[A\n",
      "batch 573, training loss: 3.3124: : 572it [06:15,  1.27s/it]\u001b[A\n",
      "batch 573, training loss: 3.3124: : 573it [06:15,  1.25s/it]\u001b[A\n",
      "batch 574, training loss: 3.3994: : 573it [06:16,  1.25s/it]\u001b[A\n",
      "batch 574, training loss: 3.3994: : 574it [06:16,  1.27s/it]\u001b[A\n",
      "batch 575, training loss: 2.7994: : 574it [06:17,  1.27s/it]\u001b[A\n",
      "batch 575, training loss: 2.7994: : 575it [06:17,  1.06s/it]\u001b[A\n",
      "batch 576, training loss: 3.3918: : 575it [06:18,  1.06s/it]\u001b[A\n",
      "batch 576, training loss: 3.3918: : 576it [06:18,  1.11s/it]\u001b[A\n",
      "batch 577, training loss: 3.2407: : 576it [06:20,  1.11s/it]\u001b[A\n",
      "batch 577, training loss: 3.2407: : 577it [06:20,  1.22s/it]\u001b[A\n",
      "batch 578, training loss: 3.2343: : 577it [06:21,  1.22s/it]\u001b[A\n",
      "batch 578, training loss: 3.2343: : 578it [06:21,  1.24s/it]\u001b[A\n",
      "batch 579, training loss: 3.3026: : 578it [06:23,  1.24s/it]\u001b[A\n",
      "batch 579, training loss: 3.3026: : 579it [06:23,  1.31s/it]\u001b[A\n",
      "batch 580, training loss: 3.2112: : 579it [06:24,  1.31s/it]\u001b[A\n",
      "batch 580, training loss: 3.2112: : 580it [06:24,  1.31s/it]\u001b[A\n",
      "batch 581, training loss: 3.2016: : 580it [06:25,  1.31s/it]\u001b[A\n",
      "batch 581, training loss: 3.2016: : 581it [06:25,  1.33s/it]\u001b[A\n",
      "batch 582, training loss: 3.2517: : 581it [06:27,  1.33s/it]\u001b[A\n",
      "batch 582, training loss: 3.2517: : 582it [06:27,  1.32s/it]\u001b[A\n",
      "batch 583, training loss: 2.391: : 582it [06:27,  1.32s/it] \u001b[A\n",
      "batch 583, training loss: 2.391: : 583it [06:27,  1.05s/it]\u001b[A\n",
      "batch 584, training loss: 3.4166: : 583it [06:28,  1.05s/it]\u001b[A\n",
      "batch 584, training loss: 3.4166: : 584it [06:28,  1.16s/it]\u001b[A\n",
      "batch 585, training loss: 3.3551: : 584it [06:30,  1.16s/it]\u001b[A\n",
      "batch 585, training loss: 3.3551: : 585it [06:30,  1.22s/it]\u001b[A\n",
      "batch 586, training loss: 3.453: : 585it [06:31,  1.22s/it] \u001b[A\n",
      "batch 586, training loss: 3.453: : 586it [06:31,  1.30s/it]\u001b[A\n",
      "batch 587, training loss: 3.3358: : 586it [06:33,  1.30s/it]\u001b[A\n",
      "batch 587, training loss: 3.3358: : 587it [06:33,  1.31s/it]\u001b[A\n",
      "batch 588, training loss: 3.2427: : 587it [06:34,  1.31s/it]\u001b[A\n",
      "batch 588, training loss: 3.2427: : 588it [06:34,  1.37s/it]\u001b[A\n",
      "batch 589, training loss: 3.3031: : 588it [06:35,  1.37s/it]\u001b[A\n",
      "batch 589, training loss: 3.3031: : 589it [06:35,  1.38s/it]\u001b[A\n",
      "batch 590, training loss: 3.3112: : 589it [06:37,  1.38s/it]\u001b[A\n",
      "batch 590, training loss: 3.3112: : 590it [06:37,  1.43s/it]\u001b[A\n",
      "batch 591, training loss: 3.2144: : 590it [06:38,  1.43s/it]\u001b[A\n",
      "batch 591, training loss: 3.2144: : 591it [06:38,  1.42s/it]\u001b[A\n",
      "batch 592, training loss: 3.2425: : 591it [06:40,  1.42s/it]\u001b[A\n",
      "batch 592, training loss: 3.2425: : 592it [06:40,  1.42s/it]\u001b[A\n",
      "batch 593, training loss: 3.1625: : 592it [06:41,  1.42s/it]\u001b[A\n",
      "batch 593, training loss: 3.1625: : 593it [06:41,  1.44s/it]\u001b[A\n",
      "batch 594, training loss: 3.3008: : 593it [06:43,  1.44s/it]\u001b[A\n",
      "batch 594, training loss: 3.3008: : 594it [06:43,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.3753: : 594it [06:44,  1.48s/it]\u001b[A\n",
      "batch 595, training loss: 3.3753: : 595it [06:44,  1.33s/it]\u001b[A\n",
      "batch 596, training loss: 3.3605: : 595it [06:46,  1.33s/it]\u001b[A\n",
      "batch 596, training loss: 3.3605: : 596it [06:46,  1.44s/it]\u001b[A\n",
      "batch 597, training loss: 3.1711: : 596it [06:47,  1.44s/it]\u001b[A\n",
      "batch 597, training loss: 3.1711: : 597it [06:47,  1.42s/it]\u001b[A\n",
      "batch 598, training loss: 3.3711: : 597it [06:49,  1.42s/it]\u001b[A\n",
      "batch 598, training loss: 3.3711: : 598it [06:49,  1.52s/it]\u001b[A\n",
      "batch 599, training loss: 3.1967: : 598it [06:50,  1.52s/it]\u001b[A\n",
      "batch 599, training loss: 3.1967: : 599it [06:50,  1.35s/it]\u001b[A\n",
      "batch 600, training loss: 3.2018: : 599it [06:51,  1.35s/it]\u001b[A\n",
      "batch 600, training loss: 3.2018: : 600it [06:51,  1.50s/it]\u001b[A\n",
      "batch 601, training loss: 2.1665: : 600it [06:52,  1.50s/it]\u001b[A\n",
      "batch 601, training loss: 2.1665: : 601it [06:52,  1.22s/it]\u001b[A\n",
      "batch 602, training loss: 3.2209: : 601it [06:54,  1.22s/it]\u001b[A\n",
      "batch 602, training loss: 3.2209: : 602it [06:54,  1.35s/it]\u001b[A\n",
      "batch 603, training loss: 3.2218: : 602it [06:55,  1.35s/it]\u001b[A\n",
      "batch 603, training loss: 3.2218: : 603it [06:55,  1.32s/it]\u001b[A\n",
      "batch 604, training loss: 3.1701: : 603it [06:56,  1.32s/it]\u001b[A\n",
      "batch 604, training loss: 3.1701: : 604it [06:56,  1.36s/it]\u001b[A\n",
      "batch 605, training loss: 3.321: : 604it [06:58,  1.36s/it] \u001b[A\n",
      "batch 605, training loss: 3.321: : 605it [06:58,  1.31s/it]\u001b[A\n",
      "batch 606, training loss: 3.0688: : 605it [06:59,  1.31s/it]\u001b[A\n",
      "batch 606, training loss: 3.0688: : 606it [06:59,  1.25s/it]\u001b[A\n",
      "batch 607, training loss: 2.9298: : 606it [07:00,  1.25s/it]\u001b[A\n",
      "batch 607, training loss: 2.9298: : 607it [07:00,  1.23s/it]\u001b[A\n",
      "batch 608, training loss: 3.0219: : 607it [07:01,  1.23s/it]\u001b[A\n",
      "batch 608, training loss: 3.0219: : 608it [07:01,  1.15s/it]\u001b[A\n",
      "batch 609, training loss: 3.0363: : 608it [07:02,  1.15s/it]\u001b[A\n",
      "batch 609, training loss: 3.0363: : 609it [07:02,  1.08s/it]\u001b[A\n",
      "batch 610, training loss: 2.4288: : 609it [07:03,  1.08s/it]\u001b[A\n",
      "batch 610, training loss: 2.4288: : 610it [07:03,  1.07s/it]\u001b[A\n",
      "batch 611, training loss: 2.4735: : 610it [07:04,  1.07s/it]\u001b[A\n",
      "batch 611, training loss: 2.4735: : 611it [07:04,  1.00it/s]\u001b[A\n",
      "batch 612, training loss: 1.7125: : 611it [07:04,  1.00it/s]\u001b[A\n",
      "batch 612, training loss: 1.7125: : 612it [07:04,  1.09it/s]\u001b[A\n",
      "batch 613, training loss: 2.255: : 612it [07:05,  1.09it/s] \u001b[A\n",
      "batch 613, training loss: 2.255: : 613it [07:05,  1.17it/s]\u001b[A\n",
      "batch 613, training loss: 2.255: : 615it [07:05,  1.97it/s]\u001b[A\n",
      "batch 613, training loss: 2.255: : 616it [07:06,  1.44it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.7227: : 0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, dev loss: 3.7227: : 1it [00:00,  1.65it/s]\u001b[A\n",
      "batch 1, dev loss: 3.906: : 1it [00:00,  1.65it/s] \u001b[A\n",
      "batch 1, dev loss: 3.906: : 2it [00:00,  2.87it/s]\u001b[A\n",
      "batch 2, dev loss: 3.463: : 2it [00:00,  2.87it/s]\u001b[A\n",
      "batch 2, dev loss: 3.463: : 3it [00:00,  3.72it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6344: : 3it [00:01,  3.72it/s]\u001b[A\n",
      "batch 3, dev loss: 3.6344: : 4it [00:01,  4.27it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6272: : 4it [00:01,  4.27it/s]\u001b[A\n",
      "batch 4, dev loss: 3.6272: : 5it [00:01,  4.83it/s]\u001b[A\n",
      "batch 5, dev loss: 3.6381: : 5it [00:01,  4.83it/s]\u001b[A\n",
      "batch 5, dev loss: 3.6381: : 6it [00:01,  5.07it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7056: : 6it [00:01,  5.07it/s]\u001b[A\n",
      "batch 6, dev loss: 3.7056: : 7it [00:01,  5.30it/s]\u001b[A\n",
      "batch 7, dev loss: 3.4997: : 7it [00:01,  5.30it/s]\u001b[A\n",
      "batch 7, dev loss: 3.4997: : 8it [00:01,  5.59it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7191: : 8it [00:01,  5.59it/s]\u001b[A\n",
      "batch 8, dev loss: 3.7191: : 9it [00:01,  5.43it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6618: : 9it [00:02,  5.43it/s]\u001b[A\n",
      "batch 9, dev loss: 3.6618: : 10it [00:02,  4.84it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7167: : 10it [00:02,  4.84it/s]\u001b[A\n",
      "batch 10, dev loss: 3.7167: : 11it [00:02,  4.64it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7412: : 11it [00:02,  4.64it/s]\u001b[A\n",
      "batch 11, dev loss: 3.7412: : 12it [00:02,  4.72it/s]\u001b[A\n",
      "batch 12, dev loss: 3.5674: : 12it [00:02,  4.72it/s]\u001b[A\n",
      "batch 12, dev loss: 3.5674: : 13it [00:02,  4.78it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7311: : 13it [00:03,  4.78it/s]\u001b[A\n",
      "batch 13, dev loss: 3.7311: : 14it [00:03,  4.84it/s]\u001b[A\n",
      "batch 14, dev loss: 3.906: : 14it [00:03,  4.84it/s] \u001b[A\n",
      "batch 14, dev loss: 3.906: : 15it [00:03,  4.96it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6105: : 15it [00:03,  4.96it/s]\u001b[A\n",
      "batch 15, dev loss: 3.6105: : 16it [00:03,  5.65it/s]\u001b[A\n",
      "batch 16, dev loss: 3.9869: : 16it [00:03,  5.65it/s]\u001b[A\n",
      "batch 16, dev loss: 3.9869: : 17it [00:03,  5.40it/s]\u001b[A\n",
      "batch 17, dev loss: 3.7541: : 17it [00:03,  5.40it/s]\u001b[A\n",
      "batch 17, dev loss: 3.7541: : 18it [00:03,  5.29it/s]\u001b[A\n",
      "batch 18, dev loss: 3.6348: : 18it [00:04,  5.29it/s]\u001b[A\n",
      "batch 18, dev loss: 3.6348: : 19it [00:04,  4.69it/s]\u001b[A\n",
      "batch 19, dev loss: 3.8257: : 19it [00:04,  4.69it/s]\u001b[A\n",
      "batch 19, dev loss: 3.8257: : 20it [00:04,  4.36it/s]\u001b[A\n",
      "batch 20, dev loss: 3.687: : 20it [00:04,  4.36it/s] \u001b[A\n",
      "batch 20, dev loss: 3.687: : 21it [00:04,  4.37it/s]\u001b[A\n",
      "batch 21, dev loss: 3.5399: : 21it [00:04,  4.37it/s]\u001b[A\n",
      "batch 21, dev loss: 3.5399: : 22it [00:04,  4.49it/s]\u001b[A\n",
      "batch 22, dev loss: 3.7579: : 22it [00:04,  4.49it/s]\u001b[A\n",
      "batch 22, dev loss: 3.7579: : 23it [00:04,  4.62it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8403: : 23it [00:05,  4.62it/s]\u001b[A\n",
      "batch 23, dev loss: 3.8403: : 24it [00:05,  5.48it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7221: : 24it [00:05,  5.48it/s]\u001b[A\n",
      "batch 24, dev loss: 3.7221: : 25it [00:05,  4.95it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7092: : 25it [00:05,  4.95it/s]\u001b[A\n",
      "batch 25, dev loss: 3.7092: : 26it [00:05,  4.69it/s]\u001b[A\n",
      "batch 26, dev loss: 3.661: : 26it [00:05,  4.69it/s] \u001b[A\n",
      "batch 26, dev loss: 3.661: : 27it [00:05,  4.05it/s]\u001b[A\n",
      "batch 27, dev loss: 3.5753: : 27it [00:06,  4.05it/s]\u001b[A\n",
      "batch 27, dev loss: 3.5753: : 28it [00:06,  3.86it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8655: : 28it [00:06,  3.86it/s]\u001b[A\n",
      "batch 28, dev loss: 3.8655: : 29it [00:06,  3.86it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7434: : 29it [00:06,  3.86it/s]\u001b[A\n",
      "batch 29, dev loss: 3.7434: : 30it [00:06,  3.97it/s]\u001b[A\n",
      "batch 30, dev loss: 4.1796: : 30it [00:06,  3.97it/s]\u001b[A\n",
      "batch 31, dev loss: 3.7382: : 30it [00:07,  3.97it/s]\u001b[A\n",
      "batch 31, dev loss: 3.7382: : 32it [00:07,  4.56it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8605: : 32it [00:07,  4.56it/s]\u001b[A\n",
      "batch 32, dev loss: 3.8605: : 33it [00:07,  4.45it/s]\u001b[A\n",
      "batch 33, dev loss: 3.5384: : 33it [00:07,  4.45it/s]\u001b[A\n",
      "batch 33, dev loss: 3.5384: : 34it [00:07,  4.24it/s]\u001b[A\n",
      "batch 34, dev loss: 3.9888: : 34it [00:07,  4.24it/s]\u001b[A\n",
      "batch 34, dev loss: 3.9888: : 35it [00:07,  4.23it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8474: : 35it [00:07,  4.23it/s]\u001b[A\n",
      "batch 35, dev loss: 3.8474: : 36it [00:07,  4.41it/s]\u001b[A\n",
      "batch 36, dev loss: 3.7307: : 36it [00:08,  4.41it/s]\u001b[A\n",
      "batch 36, dev loss: 3.7307: : 37it [00:08,  5.16it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5258: : 37it [00:08,  5.16it/s]\u001b[A\n",
      "batch 37, dev loss: 3.5258: : 38it [00:08,  4.89it/s]\u001b[A\n",
      "batch 38, dev loss: 3.7807: : 38it [00:08,  4.89it/s]\u001b[A\n",
      "batch 38, dev loss: 3.7807: : 39it [00:08,  4.91it/s]\u001b[A\n",
      "batch 39, dev loss: 3.7757: : 39it [00:08,  4.91it/s]\u001b[A\n",
      "batch 39, dev loss: 3.7757: : 40it [00:08,  5.12it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8208: : 40it [00:08,  5.12it/s]\u001b[A\n",
      "batch 40, dev loss: 3.8208: : 41it [00:08,  4.66it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6314: : 41it [00:09,  4.66it/s]\u001b[A\n",
      "batch 41, dev loss: 3.6314: : 42it [00:09,  5.04it/s]\u001b[A\n",
      "batch 42, dev loss: 3.7365: : 42it [00:09,  5.04it/s]\u001b[A\n",
      "batch 42, dev loss: 3.7365: : 43it [00:09,  4.42it/s]\u001b[A\n",
      "batch 43, dev loss: 3.7261: : 43it [00:09,  4.42it/s]\u001b[A\n",
      "batch 43, dev loss: 3.7261: : 44it [00:09,  4.05it/s]\u001b[A\n",
      "batch 44, dev loss: 3.6521: : 44it [00:09,  4.05it/s]\u001b[A\n",
      "batch 44, dev loss: 3.6521: : 45it [00:09,  3.91it/s]\u001b[A\n",
      "batch 45, dev loss: 3.9719: : 45it [00:10,  3.91it/s]\u001b[A\n",
      "batch 45, dev loss: 3.9719: : 46it [00:10,  3.66it/s]\u001b[A\n",
      "batch 46, dev loss: 3.4675: : 46it [00:10,  3.66it/s]\u001b[A\n",
      "batch 46, dev loss: 3.4675: : 47it [00:10,  3.14it/s]\u001b[A\n",
      "batch 47, dev loss: 3.6635: : 47it [00:11,  3.14it/s]\u001b[A\n",
      "batch 47, dev loss: 3.6635: : 48it [00:11,  3.19it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5146: : 48it [00:11,  3.19it/s]\u001b[A\n",
      "batch 48, dev loss: 3.5146: : 49it [00:11,  3.21it/s]\u001b[A\n",
      "batch 49, dev loss: 3.6622: : 49it [00:11,  3.21it/s]\u001b[A\n",
      "batch 49, dev loss: 3.6622: : 50it [00:11,  3.72it/s]\u001b[A\n",
      "batch 50, dev loss: 3.6135: : 50it [00:11,  3.72it/s]\u001b[A\n",
      "batch 50, dev loss: 3.6135: : 51it [00:11,  3.44it/s]\u001b[A\n",
      "batch 51, dev loss: 3.6401: : 51it [00:12,  3.44it/s]\u001b[A\n",
      "batch 51, dev loss: 3.6401: : 52it [00:12,  3.34it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4008: : 52it [00:12,  3.34it/s]\u001b[A\n",
      "batch 52, dev loss: 3.4008: : 53it [00:12,  3.29it/s]\u001b[A\n",
      "batch 53, dev loss: 3.6505: : 53it [00:12,  3.29it/s]\u001b[A\n",
      "batch 53, dev loss: 3.6505: : 54it [00:12,  2.89it/s]\u001b[A\n",
      "batch 54, dev loss: 3.3757: : 54it [00:13,  2.89it/s]\u001b[A\n",
      "batch 54, dev loss: 3.3757: : 55it [00:13,  2.94it/s]\u001b[A\n",
      "batch 55, dev loss: 3.5836: : 55it [00:13,  2.94it/s]\u001b[A\n",
      "batch 55, dev loss: 3.5836: : 56it [00:13,  2.89it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3996: : 56it [00:13,  2.89it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3996: : 57it [00:13,  3.00it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4043: : 57it [00:14,  3.00it/s]\u001b[A\n",
      "batch 57, dev loss: 3.4043: : 58it [00:14,  2.74it/s]\u001b[A\n",
      "batch 58, dev loss: 3.7643: : 58it [00:14,  2.74it/s]\u001b[A\n",
      "batch 58, dev loss: 3.7643: : 59it [00:14,  2.67it/s]\u001b[A\n",
      "batch 59, dev loss: 3.6285: : 59it [00:15,  2.67it/s]\u001b[A\n",
      "batch 59, dev loss: 3.6285: : 60it [00:15,  2.79it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4022: : 60it [00:15,  2.79it/s]\u001b[A\n",
      "batch 60, dev loss: 3.4022: : 61it [00:15,  3.00it/s]\u001b[A\n",
      "batch 61, dev loss: 3.3407: : 61it [00:15,  3.00it/s]\u001b[A\n",
      "batch 61, dev loss: 3.3407: : 62it [00:15,  3.30it/s]\u001b[A\n",
      "batch 62, dev loss: 3.0877: : 62it [00:15,  3.30it/s]\u001b[A\n",
      "batch 62, dev loss: 3.0877: : 63it [00:15,  3.65it/s]\u001b[A\n",
      "batch 63, dev loss: 3.7455: : 63it [00:16,  3.65it/s]\u001b[A\n",
      "batch 63, dev loss: 3.7455: : 64it [00:16,  3.77it/s]\u001b[A\n",
      "batch 64, dev loss: 3.4221: : 64it [00:16,  3.77it/s]\u001b[A\n",
      "batch 64, dev loss: 3.4221: : 65it [00:16,  3.82it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4038: : 65it [00:16,  3.82it/s]\u001b[A\n",
      "batch 65, dev loss: 3.4038: : 66it [00:16,  3.93it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5797: : 66it [00:16,  3.93it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5797: : 67it [00:16,  3.78it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5345: : 67it [00:17,  3.78it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5345: : 68it [00:17,  3.57it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5638: : 68it [00:17,  3.57it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5638: : 69it [00:17,  3.84it/s]\u001b[A\n",
      "batch 69, dev loss: 2.9713: : 69it [00:17,  3.84it/s]\u001b[A\n",
      "batch 69, dev loss: 2.9713: : 70it [00:17,  4.01it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9547: : 70it [00:17,  4.01it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9547: : 71it [00:17,  4.05it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1166: : 71it [00:18,  4.05it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1166: : 72it [00:18,  4.12it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0212: : 72it [00:18,  4.12it/s]\u001b[A\n",
      "batch 72, dev loss: 4.0212: : 76it [00:18,  4.14it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:00,  1.26it/s]\u001b[A\n",
      "2it [00:01,  1.26it/s]\u001b[A\n",
      "3it [00:02,  1.55it/s]\u001b[A\n",
      "4it [00:02,  1.45it/s]\u001b[A\n",
      "5it [00:03,  1.31it/s]\u001b[A\n",
      "6it [00:04,  1.30it/s]\u001b[A\n",
      "7it [00:05,  1.24it/s]\u001b[A\n",
      "8it [00:06,  1.25it/s]\u001b[A\n",
      "9it [00:07,  1.14it/s]\u001b[A\n",
      "10it [00:08,  1.12it/s]\u001b[A\n",
      "11it [00:09,  1.06it/s]\u001b[A\n",
      "12it [00:10,  1.08it/s]\u001b[A\n",
      "13it [00:11,  1.05it/s]\u001b[A\n",
      "14it [00:12,  1.05it/s]\u001b[A\n",
      "15it [00:13,  1.05s/it]\u001b[A\n",
      "16it [00:13,  1.12it/s]\u001b[A\n",
      "17it [00:15,  1.03it/s]\u001b[A\n",
      "18it [00:16,  1.07s/it]\u001b[A\n",
      "19it [00:17,  1.10s/it]\u001b[A\n",
      "20it [00:18,  1.10s/it]\u001b[A\n",
      "21it [00:19,  1.10s/it]\u001b[A\n",
      "22it [00:20,  1.13s/it]\u001b[A\n",
      "23it [00:21,  1.03s/it]\u001b[A\n",
      "24it [00:22,  1.22it/s]\u001b[A\n",
      "25it [00:23,  1.01it/s]\u001b[A\n",
      "26it [00:24,  1.12s/it]\u001b[A\n",
      "27it [00:26,  1.14s/it]\u001b[A\n",
      "28it [00:27,  1.21s/it]\u001b[A\n",
      "29it [00:28,  1.27s/it]\u001b[A\n",
      "30it [00:29,  1.16s/it]\u001b[A\n",
      "31it [00:31,  1.26s/it]\u001b[A\n",
      "32it [00:32,  1.28s/it]\u001b[A\n",
      "33it [00:33,  1.31s/it]\u001b[A\n",
      "34it [00:35,  1.37s/it]\u001b[A\n",
      "35it [00:36,  1.38s/it]\u001b[A\n",
      "36it [00:37,  1.05s/it]\u001b[A\n",
      "37it [00:38,  1.26s/it]\u001b[A\n",
      "38it [00:40,  1.46s/it]\u001b[A\n",
      "39it [00:42,  1.40s/it]\u001b[A\n",
      "40it [00:42,  1.14s/it]\u001b[A\n",
      "41it [00:42,  1.17it/s]\u001b[A\n",
      "42it [00:44,  1.09s/it]\u001b[A\n",
      "43it [00:46,  1.35s/it]\u001b[A\n",
      "44it [00:48,  1.59s/it]\u001b[A\n",
      "45it [00:50,  1.56s/it]\u001b[A\n",
      "46it [00:52,  1.73s/it]\u001b[A\n",
      "47it [00:54,  1.92s/it]\u001b[A\n",
      "48it [00:56,  1.98s/it]\u001b[A\n",
      "49it [00:56,  1.42s/it]\u001b[A\n",
      "50it [00:59,  1.68s/it]\u001b[A\n",
      "51it [01:01,  1.78s/it]\u001b[A\n",
      "52it [01:02,  1.67s/it]\u001b[A\n",
      "53it [01:04,  1.84s/it]\u001b[A\n",
      "54it [01:06,  1.90s/it]\u001b[A\n",
      "55it [01:09,  2.07s/it]\u001b[A\n",
      "56it [01:10,  1.82s/it]\u001b[A\n",
      "57it [01:12,  1.94s/it]\u001b[A\n",
      "58it [01:14,  1.90s/it]\u001b[A\n",
      "59it [01:15,  1.76s/it]\u001b[A\n",
      "60it [01:17,  1.62s/it]\u001b[A\n",
      "61it [01:18,  1.39s/it]\u001b[A\n",
      "62it [01:18,  1.21s/it]\u001b[A\n",
      "63it [01:19,  1.05s/it]\u001b[A\n",
      "64it [01:20,  1.05it/s]\u001b[A\n",
      "65it [01:20,  1.30it/s]\u001b[A\n",
      "66it [01:20,  1.65it/s]\u001b[A\n",
      "67it [01:21,  1.93it/s]\u001b[A\n",
      "68it [01:21,  2.19it/s]\u001b[A\n",
      "69it [01:21,  2.37it/s]\u001b[A\n",
      "70it [01:22,  1.17s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▏                                 | 740/6740 [00:00<00:00, 7391.14it/s]\u001b[A\n",
      " 22%|████████▏                            | 1485/6740 [00:00<00:00, 7422.52it/s]\u001b[A\n",
      " 33%|████████████▎                        | 2235/6740 [00:00<00:00, 7456.08it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 2981/6740 [00:00<00:00, 7236.07it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 3706/6740 [00:00<00:00, 7191.78it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 4426/6740 [00:00<00:00, 7023.54it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 5130/6740 [00:00<00:00, 6981.90it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 5829/6740 [00:00<00:00, 6944.96it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:00<00:00, 7016.26it/s]\u001b[A\n",
      "Epoch: 13, tfr: 1.0, loss(train/dev): 3.3477/3.6243, ppl(dev/test): 37.4985/43.9\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.405: : 0it [00:01, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.405: : 1it [00:01,  1.71s/it]\u001b[A\n",
      "batch 2, training loss: 3.3369: : 1it [00:02,  1.71s/it]\u001b[A\n",
      "batch 2, training loss: 3.3369: : 2it [00:02,  1.05it/s]\u001b[A\n",
      "batch 3, training loss: 3.3803: : 2it [00:02,  1.05it/s]\u001b[A\n",
      "batch 3, training loss: 3.3803: : 3it [00:02,  1.42it/s]\u001b[A\n",
      "batch 4, training loss: 3.3285: : 3it [00:02,  1.42it/s]\u001b[A\n",
      "batch 4, training loss: 3.3285: : 4it [00:02,  1.68it/s]\u001b[A\n",
      "batch 5, training loss: 3.1338: : 4it [00:03,  1.68it/s]\u001b[A\n",
      "batch 5, training loss: 3.1338: : 5it [00:03,  1.70it/s]\u001b[A\n",
      "batch 6, training loss: 3.414: : 5it [00:03,  1.70it/s] \u001b[A\n",
      "batch 6, training loss: 3.414: : 6it [00:03,  1.91it/s]\u001b[A\n",
      "batch 7, training loss: 3.3612: : 6it [00:04,  1.91it/s]\u001b[A\n",
      "batch 7, training loss: 3.3612: : 7it [00:04,  2.06it/s]\u001b[A\n",
      "batch 8, training loss: 3.409: : 7it [00:04,  2.06it/s] \u001b[A\n",
      "batch 8, training loss: 3.409: : 8it [00:04,  2.17it/s]\u001b[A\n",
      "batch 9, training loss: 3.271: : 8it [00:05,  2.17it/s]\u001b[A\n",
      "batch 9, training loss: 3.271: : 9it [00:05,  2.24it/s]\u001b[A\n",
      "batch 10, training loss: 3.3013: : 9it [00:05,  2.24it/s]\u001b[A\n",
      "batch 10, training loss: 3.3013: : 10it [00:05,  2.28it/s]\u001b[A\n",
      "batch 11, training loss: 3.3103: : 10it [00:06,  2.28it/s]\u001b[A\n",
      "batch 11, training loss: 3.3103: : 11it [00:06,  2.31it/s]\u001b[A\n",
      "batch 12, training loss: 3.3305: : 11it [00:06,  2.31it/s]\u001b[A\n",
      "batch 12, training loss: 3.3305: : 12it [00:06,  2.11it/s]\u001b[A\n",
      "batch 13, training loss: 3.2167: : 12it [00:06,  2.11it/s]\u001b[A\n",
      "batch 13, training loss: 3.2167: : 13it [00:06,  2.21it/s]\u001b[A\n",
      "batch 14, training loss: 3.5073: : 13it [00:07,  2.21it/s]\u001b[A\n",
      "batch 14, training loss: 3.5073: : 14it [00:07,  2.28it/s]\u001b[A\n",
      "batch 15, training loss: 3.3537: : 14it [00:07,  2.28it/s]\u001b[A\n",
      "batch 15, training loss: 3.3537: : 15it [00:07,  2.31it/s]\u001b[A\n",
      "batch 16, training loss: 3.3538: : 15it [00:08,  2.31it/s]\u001b[A\n",
      "batch 16, training loss: 3.3538: : 16it [00:08,  2.33it/s]\u001b[A\n",
      "batch 17, training loss: 3.4813: : 16it [00:08,  2.33it/s]\u001b[A\n",
      "batch 17, training loss: 3.4813: : 17it [00:08,  2.36it/s]\u001b[A\n",
      "batch 18, training loss: 3.3678: : 17it [00:09,  2.36it/s]\u001b[A\n",
      "batch 18, training loss: 3.3678: : 18it [00:09,  2.14it/s]\u001b[A\n",
      "batch 19, training loss: 3.1445: : 18it [00:09,  2.14it/s]\u001b[A\n",
      "batch 19, training loss: 3.1445: : 19it [00:09,  2.22it/s]\u001b[A\n",
      "batch 20, training loss: 3.2615: : 19it [00:10,  2.22it/s]\u001b[A\n",
      "batch 20, training loss: 3.2615: : 20it [00:10,  2.28it/s]\u001b[A\n",
      "batch 21, training loss: 3.368: : 20it [00:10,  2.28it/s] \u001b[A\n",
      "batch 21, training loss: 3.368: : 21it [00:10,  2.31it/s]\u001b[A\n",
      "batch 22, training loss: 3.1526: : 21it [00:10,  2.31it/s]\u001b[A\n",
      "batch 22, training loss: 3.1526: : 22it [00:10,  2.34it/s]\u001b[A\n",
      "batch 23, training loss: 3.3242: : 22it [00:11,  2.34it/s]\u001b[A\n",
      "batch 23, training loss: 3.3242: : 23it [00:11,  2.35it/s]\u001b[A\n",
      "batch 24, training loss: 3.2571: : 23it [00:11,  2.35it/s]\u001b[A\n",
      "batch 24, training loss: 3.2571: : 24it [00:11,  2.19it/s]\u001b[A\n",
      "batch 25, training loss: 3.3409: : 24it [00:12,  2.19it/s]\u001b[A\n",
      "batch 25, training loss: 3.3409: : 25it [00:12,  2.49it/s]\u001b[A\n",
      "batch 26, training loss: 3.1488: : 25it [00:12,  2.49it/s]\u001b[A\n",
      "batch 26, training loss: 3.1488: : 26it [00:12,  2.43it/s]\u001b[A\n",
      "batch 27, training loss: 3.3035: : 26it [00:12,  2.43it/s]\u001b[A\n",
      "batch 27, training loss: 3.3035: : 27it [00:12,  2.66it/s]\u001b[A\n",
      "batch 28, training loss: 3.1252: : 27it [00:13,  2.66it/s]\u001b[A\n",
      "batch 28, training loss: 3.1252: : 28it [00:13,  2.55it/s]\u001b[A\n",
      "batch 29, training loss: 3.3017: : 28it [00:13,  2.55it/s]\u001b[A\n",
      "batch 29, training loss: 3.3017: : 29it [00:13,  2.43it/s]\u001b[A\n",
      "batch 30, training loss: 3.4182: : 29it [00:14,  2.43it/s]\u001b[A\n",
      "batch 30, training loss: 3.4182: : 30it [00:14,  2.09it/s]\u001b[A\n",
      "batch 31, training loss: 3.1701: : 30it [00:14,  2.09it/s]\u001b[A\n",
      "batch 31, training loss: 3.1701: : 31it [00:14,  2.19it/s]\u001b[A\n",
      "batch 32, training loss: 3.3485: : 31it [00:15,  2.19it/s]\u001b[A\n",
      "batch 32, training loss: 3.3485: : 32it [00:15,  2.25it/s]\u001b[A\n",
      "batch 33, training loss: 3.2403: : 32it [00:15,  2.25it/s]\u001b[A\n",
      "batch 33, training loss: 3.2403: : 33it [00:15,  2.29it/s]\u001b[A\n",
      "batch 34, training loss: 3.2683: : 33it [00:16,  2.29it/s]\u001b[A\n",
      "batch 34, training loss: 3.2683: : 34it [00:16,  2.32it/s]\u001b[A\n",
      "batch 35, training loss: 3.3109: : 34it [00:16,  2.32it/s]\u001b[A\n",
      "batch 35, training loss: 3.3109: : 35it [00:16,  2.34it/s]\u001b[A\n",
      "batch 36, training loss: 3.4005: : 35it [00:17,  2.34it/s]\u001b[A\n",
      "batch 36, training loss: 3.4005: : 36it [00:17,  2.05it/s]\u001b[A\n",
      "batch 37, training loss: 3.2556: : 36it [00:17,  2.05it/s]\u001b[A\n",
      "batch 37, training loss: 3.2556: : 37it [00:17,  2.06it/s]\u001b[A\n",
      "batch 38, training loss: 3.0427: : 37it [00:17,  2.06it/s]\u001b[A\n",
      "batch 38, training loss: 3.0427: : 38it [00:17,  2.11it/s]\u001b[A\n",
      "batch 39, training loss: 3.2053: : 38it [00:18,  2.11it/s]\u001b[A\n",
      "batch 39, training loss: 3.2053: : 39it [00:18,  2.16it/s]\u001b[A\n",
      "batch 40, training loss: 3.2668: : 39it [00:19,  2.16it/s]\u001b[A\n",
      "batch 40, training loss: 3.2668: : 40it [00:19,  1.93it/s]\u001b[A\n",
      "batch 41, training loss: 3.4291: : 40it [00:19,  1.93it/s]\u001b[A\n",
      "batch 41, training loss: 3.4291: : 41it [00:19,  2.03it/s]\u001b[A\n",
      "batch 42, training loss: 3.2654: : 41it [00:19,  2.03it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 42, training loss: 3.2654: : 42it [00:19,  2.13it/s]\u001b[A\n",
      "batch 43, training loss: 3.0879: : 42it [00:20,  2.13it/s]\u001b[A\n",
      "batch 43, training loss: 3.0879: : 43it [00:20,  2.20it/s]\u001b[A\n",
      "batch 44, training loss: 3.0515: : 43it [00:20,  2.20it/s]\u001b[A\n",
      "batch 44, training loss: 3.0515: : 44it [00:20,  2.26it/s]\u001b[A\n",
      "batch 45, training loss: 3.0929: : 44it [00:21,  2.26it/s]\u001b[A\n",
      "batch 45, training loss: 3.0929: : 45it [00:21,  2.31it/s]\u001b[A\n",
      "batch 46, training loss: 3.2185: : 45it [00:21,  2.31it/s]\u001b[A\n",
      "batch 46, training loss: 3.2185: : 46it [00:21,  2.10it/s]\u001b[A\n",
      "batch 47, training loss: 3.2347: : 46it [00:22,  2.10it/s]\u001b[A\n",
      "batch 47, training loss: 3.2347: : 47it [00:22,  2.20it/s]\u001b[A\n",
      "batch 48, training loss: 3.1186: : 47it [00:22,  2.20it/s]\u001b[A\n",
      "batch 48, training loss: 3.1186: : 48it [00:22,  2.25it/s]\u001b[A\n",
      "batch 49, training loss: 3.3904: : 48it [00:22,  2.25it/s]\u001b[A\n",
      "batch 49, training loss: 3.3904: : 49it [00:22,  2.29it/s]\u001b[A\n",
      "batch 50, training loss: 3.1604: : 49it [00:23,  2.29it/s]\u001b[A\n",
      "batch 50, training loss: 3.1604: : 50it [00:23,  2.33it/s]\u001b[A\n",
      "batch 51, training loss: 3.2425: : 50it [00:23,  2.33it/s]\u001b[A\n",
      "batch 51, training loss: 3.2425: : 51it [00:23,  2.12it/s]\u001b[A\n",
      "batch 52, training loss: 3.0955: : 51it [00:24,  2.12it/s]\u001b[A\n",
      "batch 52, training loss: 3.0955: : 52it [00:24,  2.20it/s]\u001b[A\n",
      "batch 53, training loss: 3.113: : 52it [00:24,  2.20it/s] \u001b[A\n",
      "batch 53, training loss: 3.113: : 53it [00:24,  2.28it/s]\u001b[A\n",
      "batch 54, training loss: 3.0893: : 53it [00:25,  2.28it/s]\u001b[A\n",
      "batch 54, training loss: 3.0893: : 54it [00:25,  2.32it/s]\u001b[A\n",
      "batch 55, training loss: 3.2086: : 54it [00:25,  2.32it/s]\u001b[A\n",
      "batch 55, training loss: 3.2086: : 55it [00:25,  2.34it/s]\u001b[A\n",
      "batch 56, training loss: 3.2258: : 55it [00:26,  2.34it/s]\u001b[A\n",
      "batch 56, training loss: 3.2258: : 56it [00:26,  2.36it/s]\u001b[A\n",
      "batch 57, training loss: 3.1821: : 56it [00:26,  2.36it/s]\u001b[A\n",
      "batch 57, training loss: 3.1821: : 57it [00:26,  2.13it/s]\u001b[A\n",
      "batch 58, training loss: 3.2725: : 57it [00:26,  2.13it/s]\u001b[A\n",
      "batch 58, training loss: 3.2725: : 58it [00:26,  2.23it/s]\u001b[A\n",
      "batch 59, training loss: 3.1191: : 58it [00:27,  2.23it/s]\u001b[A\n",
      "batch 59, training loss: 3.1191: : 59it [00:27,  2.28it/s]\u001b[A\n",
      "batch 60, training loss: 3.0801: : 59it [00:27,  2.28it/s]\u001b[A\n",
      "batch 60, training loss: 3.0801: : 60it [00:27,  2.32it/s]\u001b[A\n",
      "batch 61, training loss: 3.31: : 60it [00:28,  2.32it/s]  \u001b[A\n",
      "batch 61, training loss: 3.31: : 61it [00:28,  2.34it/s]\u001b[A\n",
      "batch 62, training loss: 3.167: : 61it [00:28,  2.34it/s]\u001b[A\n",
      "batch 62, training loss: 3.167: : 62it [00:28,  2.36it/s]\u001b[A\n",
      "batch 63, training loss: 3.2439: : 62it [00:29,  2.36it/s]\u001b[A\n",
      "batch 63, training loss: 3.2439: : 63it [00:29,  2.13it/s]\u001b[A\n",
      "batch 64, training loss: 3.2174: : 63it [00:29,  2.13it/s]\u001b[A\n",
      "batch 64, training loss: 3.2174: : 64it [00:29,  2.22it/s]\u001b[A\n",
      "batch 65, training loss: 3.2755: : 64it [00:30,  2.22it/s]\u001b[A\n",
      "batch 65, training loss: 3.2755: : 65it [00:30,  2.27it/s]\u001b[A\n",
      "batch 66, training loss: 3.3013: : 65it [00:30,  2.27it/s]\u001b[A\n",
      "batch 66, training loss: 3.3013: : 66it [00:30,  2.31it/s]\u001b[A\n",
      "batch 67, training loss: 3.1548: : 66it [00:30,  2.31it/s]\u001b[A\n",
      "batch 67, training loss: 3.1548: : 67it [00:30,  2.33it/s]\u001b[A\n",
      "batch 68, training loss: 3.2456: : 67it [00:31,  2.33it/s]\u001b[A\n",
      "batch 68, training loss: 3.2456: : 68it [00:31,  2.09it/s]\u001b[A\n",
      "batch 69, training loss: 3.1171: : 68it [00:31,  2.09it/s]\u001b[A\n",
      "batch 69, training loss: 3.1171: : 69it [00:31,  2.07it/s]\u001b[A\n",
      "batch 70, training loss: 3.3315: : 69it [00:32,  2.07it/s]\u001b[A\n",
      "batch 70, training loss: 3.3315: : 70it [00:32,  2.08it/s]\u001b[A\n",
      "batch 71, training loss: 3.1864: : 70it [00:32,  2.08it/s]\u001b[A\n",
      "batch 71, training loss: 3.1864: : 71it [00:32,  2.13it/s]\u001b[A\n",
      "batch 72, training loss: 3.1376: : 71it [00:33,  2.13it/s]\u001b[A\n",
      "batch 72, training loss: 3.1376: : 72it [00:33,  1.92it/s]\u001b[A\n",
      "batch 73, training loss: 3.202: : 72it [00:33,  1.92it/s] \u001b[A\n",
      "batch 73, training loss: 3.202: : 73it [00:33,  2.01it/s]\u001b[A\n",
      "batch 74, training loss: 3.1122: : 73it [00:34,  2.01it/s]\u001b[A\n",
      "batch 74, training loss: 3.1122: : 74it [00:34,  2.11it/s]\u001b[A\n",
      "batch 75, training loss: 3.291: : 74it [00:34,  2.11it/s] \u001b[A\n",
      "batch 75, training loss: 3.291: : 75it [00:34,  2.19it/s]\u001b[A\n",
      "batch 76, training loss: 3.1003: : 75it [00:35,  2.19it/s]\u001b[A\n",
      "batch 76, training loss: 3.1003: : 76it [00:35,  2.25it/s]\u001b[A\n",
      "batch 77, training loss: 3.243: : 76it [00:35,  2.25it/s] \u001b[A\n",
      "batch 77, training loss: 3.243: : 77it [00:35,  2.11it/s]\u001b[A\n",
      "batch 78, training loss: 3.2041: : 77it [00:36,  2.11it/s]\u001b[A\n",
      "batch 78, training loss: 3.2041: : 78it [00:36,  2.16it/s]\u001b[A\n",
      "batch 79, training loss: 3.2665: : 78it [00:36,  2.16it/s]\u001b[A\n",
      "batch 79, training loss: 3.2665: : 79it [00:36,  2.24it/s]\u001b[A\n",
      "batch 80, training loss: 3.2211: : 79it [00:37,  2.24it/s]\u001b[A\n",
      "batch 80, training loss: 3.2211: : 80it [00:37,  2.29it/s]\u001b[A\n",
      "batch 81, training loss: 3.1767: : 80it [00:37,  2.29it/s]\u001b[A\n",
      "batch 81, training loss: 3.1767: : 81it [00:37,  2.33it/s]\u001b[A\n",
      "batch 82, training loss: 3.3135: : 81it [00:37,  2.33it/s]\u001b[A\n",
      "batch 82, training loss: 3.3135: : 82it [00:37,  2.35it/s]\u001b[A\n",
      "batch 83, training loss: 3.1842: : 82it [00:38,  2.35it/s]\u001b[A\n",
      "batch 83, training loss: 3.1842: : 83it [00:38,  2.36it/s]\u001b[A\n",
      "batch 84, training loss: 3.3318: : 83it [00:38,  2.36it/s]\u001b[A\n",
      "batch 84, training loss: 3.3318: : 84it [00:38,  2.13it/s]\u001b[A\n",
      "batch 85, training loss: 3.3323: : 84it [00:39,  2.13it/s]\u001b[A\n",
      "batch 85, training loss: 3.3323: : 85it [00:39,  2.22it/s]\u001b[A\n",
      "batch 86, training loss: 3.315: : 85it [00:39,  2.22it/s] \u001b[A\n",
      "batch 86, training loss: 3.315: : 86it [00:39,  2.25it/s]\u001b[A\n",
      "batch 87, training loss: 3.2889: : 86it [00:40,  2.25it/s]\u001b[A\n",
      "batch 87, training loss: 3.2889: : 87it [00:40,  2.32it/s]\u001b[A\n",
      "batch 88, training loss: 3.3422: : 87it [00:40,  2.32it/s]\u001b[A\n",
      "batch 88, training loss: 3.3422: : 88it [00:40,  2.29it/s]\u001b[A\n",
      "batch 89, training loss: 3.4464: : 88it [00:41,  2.29it/s]\u001b[A\n",
      "batch 89, training loss: 3.4464: : 89it [00:41,  1.93it/s]\u001b[A\n",
      "batch 90, training loss: 3.3545: : 89it [00:41,  1.93it/s]\u001b[A\n",
      "batch 90, training loss: 3.3545: : 90it [00:41,  1.96it/s]\u001b[A\n",
      "batch 91, training loss: 3.4674: : 90it [00:42,  1.96it/s]\u001b[A\n",
      "batch 91, training loss: 3.4674: : 91it [00:42,  1.93it/s]\u001b[A\n",
      "batch 92, training loss: 3.3216: : 91it [00:42,  1.93it/s]\u001b[A\n",
      "batch 92, training loss: 3.3216: : 92it [00:42,  1.98it/s]\u001b[A\n",
      "batch 93, training loss: 3.3118: : 92it [00:43,  1.98it/s]\u001b[A\n",
      "batch 93, training loss: 3.3118: : 93it [00:43,  1.92it/s]\u001b[A\n",
      "batch 94, training loss: 3.3967: : 93it [00:43,  1.92it/s]\u001b[A\n",
      "batch 94, training loss: 3.3967: : 94it [00:43,  1.80it/s]\u001b[A\n",
      "batch 95, training loss: 3.333: : 94it [00:44,  1.80it/s] \u001b[A\n",
      "batch 95, training loss: 3.333: : 95it [00:44,  1.83it/s]\u001b[A\n",
      "batch 96, training loss: 3.3502: : 95it [00:44,  1.83it/s]\u001b[A\n",
      "batch 96, training loss: 3.3502: : 96it [00:44,  1.90it/s]\u001b[A\n",
      "batch 97, training loss: 3.4168: : 96it [00:45,  1.90it/s]\u001b[A\n",
      "batch 97, training loss: 3.4168: : 97it [00:45,  1.86it/s]\u001b[A\n",
      "batch 98, training loss: 3.4324: : 97it [00:45,  1.86it/s]\u001b[A\n",
      "batch 98, training loss: 3.4324: : 98it [00:45,  1.91it/s]\u001b[A\n",
      "batch 99, training loss: 3.2294: : 98it [00:46,  1.91it/s]\u001b[A\n",
      "batch 99, training loss: 3.2294: : 99it [00:46,  1.74it/s]\u001b[A\n",
      "batch 100, training loss: 3.1247: : 99it [00:47,  1.74it/s]\u001b[A\n",
      "batch 100, training loss: 3.1247: : 100it [00:47,  1.81it/s]\u001b[A\n",
      "batch 101, training loss: 3.2505: : 100it [00:47,  1.81it/s]\u001b[A\n",
      "batch 101, training loss: 3.2505: : 101it [00:47,  1.81it/s]\u001b[A\n",
      "batch 102, training loss: 3.2905: : 101it [00:48,  1.81it/s]\u001b[A\n",
      "batch 102, training loss: 3.2905: : 102it [00:48,  1.86it/s]\u001b[A\n",
      "batch 103, training loss: 3.234: : 102it [00:48,  1.86it/s] \u001b[A\n",
      "batch 103, training loss: 3.234: : 103it [00:48,  1.73it/s]\u001b[A\n",
      "batch 104, training loss: 3.0616: : 103it [00:49,  1.73it/s]\u001b[A\n",
      "batch 104, training loss: 3.0616: : 104it [00:49,  1.86it/s]\u001b[A\n",
      "batch 105, training loss: 3.2225: : 104it [00:49,  1.86it/s]\u001b[A\n",
      "batch 105, training loss: 3.2225: : 105it [00:49,  1.84it/s]\u001b[A\n",
      "batch 106, training loss: 3.3868: : 105it [00:50,  1.84it/s]\u001b[A\n",
      "batch 106, training loss: 3.3868: : 106it [00:50,  1.89it/s]\u001b[A\n",
      "batch 107, training loss: 3.0683: : 106it [00:51,  1.89it/s]\u001b[A\n",
      "batch 107, training loss: 3.0683: : 107it [00:51,  1.82it/s]\u001b[A\n",
      "batch 108, training loss: 3.3409: : 107it [00:51,  1.82it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 108, training loss: 3.3409: : 108it [00:51,  1.83it/s]\u001b[A\n",
      "batch 109, training loss: 3.3613: : 108it [00:52,  1.83it/s]\u001b[A\n",
      "batch 109, training loss: 3.3613: : 109it [00:52,  1.93it/s]\u001b[A\n",
      "batch 110, training loss: 3.5243: : 109it [00:52,  1.93it/s]\u001b[A\n",
      "batch 110, training loss: 3.5243: : 110it [00:52,  1.89it/s]\u001b[A\n",
      "batch 111, training loss: 3.2476: : 110it [00:53,  1.89it/s]\u001b[A\n",
      "batch 111, training loss: 3.2476: : 111it [00:53,  1.93it/s]\u001b[A\n",
      "batch 112, training loss: 3.3169: : 111it [00:53,  1.93it/s]\u001b[A\n",
      "batch 112, training loss: 3.3169: : 112it [00:53,  1.82it/s]\u001b[A\n",
      "batch 113, training loss: 3.3658: : 112it [00:54,  1.82it/s]\u001b[A\n",
      "batch 113, training loss: 3.3658: : 113it [00:54,  1.86it/s]\u001b[A\n",
      "batch 114, training loss: 3.3483: : 113it [00:54,  1.86it/s]\u001b[A\n",
      "batch 114, training loss: 3.3483: : 114it [00:54,  1.91it/s]\u001b[A\n",
      "batch 115, training loss: 3.2619: : 114it [00:55,  1.91it/s]\u001b[A\n",
      "batch 115, training loss: 3.2619: : 115it [00:55,  1.87it/s]\u001b[A\n",
      "batch 116, training loss: 3.3075: : 115it [00:55,  1.87it/s]\u001b[A\n",
      "batch 116, training loss: 3.3075: : 116it [00:55,  1.84it/s]\u001b[A\n",
      "batch 117, training loss: 3.323: : 116it [00:56,  1.84it/s] \u001b[A\n",
      "batch 117, training loss: 3.323: : 117it [00:56,  1.77it/s]\u001b[A\n",
      "batch 118, training loss: 3.4092: : 117it [00:56,  1.77it/s]\u001b[A\n",
      "batch 118, training loss: 3.4092: : 118it [00:56,  1.89it/s]\u001b[A\n",
      "batch 119, training loss: 3.3375: : 118it [00:57,  1.89it/s]\u001b[A\n",
      "batch 119, training loss: 3.3375: : 119it [00:57,  1.85it/s]\u001b[A\n",
      "batch 120, training loss: 3.2695: : 119it [00:57,  1.85it/s]\u001b[A\n",
      "batch 120, training loss: 3.2695: : 120it [00:57,  1.90it/s]\u001b[A\n",
      "batch 121, training loss: 3.4997: : 120it [00:58,  1.90it/s]\u001b[A\n",
      "batch 121, training loss: 3.4997: : 121it [00:58,  1.83it/s]\u001b[A\n",
      "batch 122, training loss: 3.1755: : 121it [00:59,  1.83it/s]\u001b[A\n",
      "batch 122, training loss: 3.1755: : 122it [00:59,  1.83it/s]\u001b[A\n",
      "batch 123, training loss: 3.3566: : 122it [00:59,  1.83it/s]\u001b[A\n",
      "batch 123, training loss: 3.3566: : 123it [00:59,  1.85it/s]\u001b[A\n",
      "batch 124, training loss: 3.3577: : 123it [01:00,  1.85it/s]\u001b[A\n",
      "batch 124, training loss: 3.3577: : 124it [01:00,  1.87it/s]\u001b[A\n",
      "batch 125, training loss: 3.3794: : 124it [01:00,  1.87it/s]\u001b[A\n",
      "batch 125, training loss: 3.3794: : 125it [01:00,  1.86it/s]\u001b[A\n",
      "batch 126, training loss: 3.4477: : 125it [01:01,  1.86it/s]\u001b[A\n",
      "batch 126, training loss: 3.4477: : 126it [01:01,  1.75it/s]\u001b[A\n",
      "batch 127, training loss: 3.1292: : 126it [01:01,  1.75it/s]\u001b[A\n",
      "batch 127, training loss: 3.1292: : 127it [01:01,  1.76it/s]\u001b[A\n",
      "batch 128, training loss: 3.3483: : 127it [01:02,  1.76it/s]\u001b[A\n",
      "batch 128, training loss: 3.3483: : 128it [01:02,  1.77it/s]\u001b[A\n",
      "batch 129, training loss: 3.2907: : 128it [01:02,  1.77it/s]\u001b[A\n",
      "batch 129, training loss: 3.2907: : 129it [01:02,  1.86it/s]\u001b[A\n",
      "batch 130, training loss: 3.2791: : 129it [01:03,  1.86it/s]\u001b[A\n",
      "batch 130, training loss: 3.2791: : 130it [01:03,  1.68it/s]\u001b[A\n",
      "batch 131, training loss: 3.4699: : 130it [01:04,  1.68it/s]\u001b[A\n",
      "batch 131, training loss: 3.4699: : 131it [01:04,  1.80it/s]\u001b[A\n",
      "batch 132, training loss: 3.2752: : 131it [01:04,  1.80it/s]\u001b[A\n",
      "batch 132, training loss: 3.2752: : 132it [01:04,  1.82it/s]\u001b[A\n",
      "batch 133, training loss: 3.1775: : 132it [01:05,  1.82it/s]\u001b[A\n",
      "batch 133, training loss: 3.1775: : 133it [01:05,  1.91it/s]\u001b[A\n",
      "batch 134, training loss: 3.3723: : 133it [01:05,  1.91it/s]\u001b[A\n",
      "batch 134, training loss: 3.3723: : 134it [01:05,  1.86it/s]\u001b[A\n",
      "batch 135, training loss: 3.2502: : 134it [01:06,  1.86it/s]\u001b[A\n",
      "batch 135, training loss: 3.2502: : 135it [01:06,  1.77it/s]\u001b[A\n",
      "batch 136, training loss: 3.1823: : 135it [01:06,  1.77it/s]\u001b[A\n",
      "batch 136, training loss: 3.1823: : 136it [01:06,  1.79it/s]\u001b[A\n",
      "batch 137, training loss: 3.3535: : 136it [01:07,  1.79it/s]\u001b[A\n",
      "batch 137, training loss: 3.3535: : 137it [01:07,  1.80it/s]\u001b[A\n",
      "batch 138, training loss: 3.3622: : 137it [01:07,  1.80it/s]\u001b[A\n",
      "batch 138, training loss: 3.3622: : 138it [01:07,  1.84it/s]\u001b[A\n",
      "batch 139, training loss: 3.1508: : 138it [01:08,  1.84it/s]\u001b[A\n",
      "batch 139, training loss: 3.1508: : 139it [01:08,  1.69it/s]\u001b[A\n",
      "batch 140, training loss: 3.2685: : 139it [01:09,  1.69it/s]\u001b[A\n",
      "batch 140, training loss: 3.2685: : 140it [01:09,  1.74it/s]\u001b[A\n",
      "batch 141, training loss: 3.269: : 140it [01:09,  1.74it/s] \u001b[A\n",
      "batch 141, training loss: 3.269: : 141it [01:09,  1.80it/s]\u001b[A\n",
      "batch 142, training loss: 3.2892: : 141it [01:10,  1.80it/s]\u001b[A\n",
      "batch 142, training loss: 3.2892: : 142it [01:10,  1.81it/s]\u001b[A\n",
      "batch 143, training loss: 3.1243: : 142it [01:10,  1.81it/s]\u001b[A\n",
      "batch 143, training loss: 3.1243: : 143it [01:10,  1.67it/s]\u001b[A\n",
      "batch 144, training loss: 3.1949: : 143it [01:11,  1.67it/s]\u001b[A\n",
      "batch 144, training loss: 3.1949: : 144it [01:11,  1.75it/s]\u001b[A\n",
      "batch 145, training loss: 3.2893: : 144it [01:11,  1.75it/s]\u001b[A\n",
      "batch 145, training loss: 3.2893: : 145it [01:11,  1.78it/s]\u001b[A\n",
      "batch 146, training loss: 3.2694: : 145it [01:12,  1.78it/s]\u001b[A\n",
      "batch 146, training loss: 3.2694: : 146it [01:12,  1.87it/s]\u001b[A\n",
      "batch 147, training loss: 3.1862: : 146it [01:12,  1.87it/s]\u001b[A\n",
      "batch 147, training loss: 3.1862: : 147it [01:12,  1.85it/s]\u001b[A\n",
      "batch 148, training loss: 3.2686: : 147it [01:13,  1.85it/s]\u001b[A\n",
      "batch 148, training loss: 3.2686: : 148it [01:13,  1.69it/s]\u001b[A\n",
      "batch 149, training loss: 3.337: : 148it [01:14,  1.69it/s] \u001b[A\n",
      "batch 149, training loss: 3.337: : 149it [01:14,  1.80it/s]\u001b[A\n",
      "batch 150, training loss: 3.333: : 149it [01:14,  1.80it/s]\u001b[A\n",
      "batch 150, training loss: 3.333: : 150it [01:14,  1.79it/s]\u001b[A\n",
      "batch 151, training loss: 3.2616: : 150it [01:15,  1.79it/s]\u001b[A\n",
      "batch 151, training loss: 3.2616: : 151it [01:15,  1.86it/s]\u001b[A\n",
      "batch 152, training loss: 3.197: : 151it [01:15,  1.86it/s] \u001b[A\n",
      "batch 152, training loss: 3.197: : 152it [01:15,  1.71it/s]\u001b[A\n",
      "batch 153, training loss: 3.2197: : 152it [01:16,  1.71it/s]\u001b[A\n",
      "batch 153, training loss: 3.2197: : 153it [01:16,  1.80it/s]\u001b[A\n",
      "batch 154, training loss: 3.3358: : 153it [01:16,  1.80it/s]\u001b[A\n",
      "batch 154, training loss: 3.3358: : 154it [01:16,  1.79it/s]\u001b[A\n",
      "batch 155, training loss: 3.4626: : 154it [01:17,  1.79it/s]\u001b[A\n",
      "batch 155, training loss: 3.4626: : 155it [01:17,  1.80it/s]\u001b[A\n",
      "batch 156, training loss: 3.0509: : 155it [01:18,  1.80it/s]\u001b[A\n",
      "batch 156, training loss: 3.0509: : 156it [01:18,  1.81it/s]\u001b[A\n",
      "batch 157, training loss: 3.264: : 156it [01:18,  1.81it/s] \u001b[A\n",
      "batch 157, training loss: 3.264: : 157it [01:18,  2.09it/s]\u001b[A\n",
      "batch 158, training loss: 3.2657: : 157it [01:18,  2.09it/s]\u001b[A\n",
      "batch 158, training loss: 3.2657: : 158it [01:18,  2.09it/s]\u001b[A\n",
      "batch 159, training loss: 3.1687: : 158it [01:19,  2.09it/s]\u001b[A\n",
      "batch 159, training loss: 3.1687: : 159it [01:19,  2.10it/s]\u001b[A\n",
      "batch 160, training loss: 3.3542: : 159it [01:19,  2.10it/s]\u001b[A\n",
      "batch 160, training loss: 3.3542: : 160it [01:19,  2.09it/s]\u001b[A\n",
      "batch 161, training loss: 3.2308: : 160it [01:20,  2.09it/s]\u001b[A\n",
      "batch 161, training loss: 3.2308: : 161it [01:20,  1.93it/s]\u001b[A\n",
      "batch 162, training loss: 3.2026: : 161it [01:20,  1.93it/s]\u001b[A\n",
      "batch 162, training loss: 3.2026: : 162it [01:20,  1.92it/s]\u001b[A\n",
      "batch 163, training loss: 3.4625: : 162it [01:21,  1.92it/s]\u001b[A\n",
      "batch 163, training loss: 3.4625: : 163it [01:21,  1.90it/s]\u001b[A\n",
      "batch 164, training loss: 3.2799: : 163it [01:21,  1.90it/s]\u001b[A\n",
      "batch 164, training loss: 3.2799: : 164it [01:21,  1.90it/s]\u001b[A\n",
      "batch 165, training loss: 3.22: : 164it [01:22,  1.90it/s]  \u001b[A\n",
      "batch 165, training loss: 3.22: : 165it [01:22,  1.89it/s]\u001b[A\n",
      "batch 166, training loss: 3.2072: : 165it [01:23,  1.89it/s]\u001b[A\n",
      "batch 166, training loss: 3.2072: : 166it [01:23,  1.74it/s]\u001b[A\n",
      "batch 167, training loss: 3.3392: : 166it [01:23,  1.74it/s]\u001b[A\n",
      "batch 167, training loss: 3.3392: : 167it [01:23,  1.79it/s]\u001b[A\n",
      "batch 168, training loss: 3.3144: : 167it [01:24,  1.79it/s]\u001b[A\n",
      "batch 168, training loss: 3.3144: : 168it [01:24,  1.80it/s]\u001b[A\n",
      "batch 169, training loss: 3.3622: : 168it [01:24,  1.80it/s]\u001b[A\n",
      "batch 169, training loss: 3.3622: : 169it [01:24,  1.89it/s]\u001b[A\n",
      "batch 170, training loss: 3.2572: : 169it [01:25,  1.89it/s]\u001b[A\n",
      "batch 170, training loss: 3.2572: : 170it [01:25,  1.85it/s]\u001b[A\n",
      "batch 171, training loss: 2.1173: : 170it [01:25,  1.85it/s]\u001b[A\n",
      "batch 171, training loss: 2.1173: : 171it [01:25,  2.09it/s]\u001b[A\n",
      "batch 172, training loss: 3.4724: : 171it [01:26,  2.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 172, training loss: 3.4724: : 172it [01:26,  1.90it/s]\u001b[A\n",
      "batch 173, training loss: 3.2971: : 172it [01:26,  1.90it/s]\u001b[A\n",
      "batch 173, training loss: 3.2971: : 173it [01:26,  1.84it/s]\u001b[A\n",
      "batch 174, training loss: 3.4804: : 173it [01:27,  1.84it/s]\u001b[A\n",
      "batch 174, training loss: 3.4804: : 174it [01:27,  1.75it/s]\u001b[A\n",
      "batch 175, training loss: 3.3874: : 174it [01:28,  1.75it/s]\u001b[A\n",
      "batch 175, training loss: 3.3874: : 175it [01:28,  1.60it/s]\u001b[A\n",
      "batch 176, training loss: 3.365: : 175it [01:28,  1.60it/s] \u001b[A\n",
      "batch 176, training loss: 3.365: : 176it [01:28,  1.60it/s]\u001b[A\n",
      "batch 177, training loss: 3.3248: : 176it [01:29,  1.60it/s]\u001b[A\n",
      "batch 177, training loss: 3.3248: : 177it [01:29,  1.63it/s]\u001b[A\n",
      "batch 178, training loss: 3.3458: : 177it [01:30,  1.63it/s]\u001b[A\n",
      "batch 178, training loss: 3.3458: : 178it [01:30,  1.63it/s]\u001b[A\n",
      "batch 179, training loss: 3.4178: : 178it [01:30,  1.63it/s]\u001b[A\n",
      "batch 179, training loss: 3.4178: : 179it [01:30,  1.52it/s]\u001b[A\n",
      "batch 180, training loss: 3.4051: : 179it [01:31,  1.52it/s]\u001b[A\n",
      "batch 180, training loss: 3.4051: : 180it [01:31,  1.54it/s]\u001b[A\n",
      "batch 181, training loss: 3.4938: : 180it [01:32,  1.54it/s]\u001b[A\n",
      "batch 181, training loss: 3.4938: : 181it [01:32,  1.58it/s]\u001b[A\n",
      "batch 182, training loss: 3.4118: : 181it [01:32,  1.58it/s]\u001b[A\n",
      "batch 182, training loss: 3.4118: : 182it [01:32,  1.59it/s]\u001b[A\n",
      "batch 183, training loss: 3.5517: : 182it [01:33,  1.59it/s]\u001b[A\n",
      "batch 183, training loss: 3.5517: : 183it [01:33,  1.56it/s]\u001b[A\n",
      "batch 184, training loss: 3.2456: : 183it [01:33,  1.56it/s]\u001b[A\n",
      "batch 184, training loss: 3.2456: : 184it [01:33,  1.62it/s]\u001b[A\n",
      "batch 185, training loss: 3.2912: : 184it [01:34,  1.62it/s]\u001b[A\n",
      "batch 185, training loss: 3.2912: : 185it [01:34,  1.61it/s]\u001b[A\n",
      "batch 186, training loss: 3.4676: : 185it [01:35,  1.61it/s]\u001b[A\n",
      "batch 186, training loss: 3.4676: : 186it [01:35,  1.63it/s]\u001b[A\n",
      "batch 187, training loss: 3.4945: : 186it [01:35,  1.63it/s]\u001b[A\n",
      "batch 187, training loss: 3.4945: : 187it [01:35,  1.53it/s]\u001b[A\n",
      "batch 188, training loss: 3.5307: : 187it [01:36,  1.53it/s]\u001b[A\n",
      "batch 188, training loss: 3.5307: : 188it [01:36,  1.60it/s]\u001b[A\n",
      "batch 189, training loss: 3.5743: : 188it [01:36,  1.60it/s]\u001b[A\n",
      "batch 189, training loss: 3.5743: : 189it [01:36,  1.66it/s]\u001b[A\n",
      "batch 190, training loss: 3.2869: : 189it [01:37,  1.66it/s]\u001b[A\n",
      "batch 190, training loss: 3.2869: : 190it [01:37,  1.63it/s]\u001b[A\n",
      "batch 191, training loss: 3.2979: : 190it [01:38,  1.63it/s]\u001b[A\n",
      "batch 191, training loss: 3.2979: : 191it [01:38,  1.53it/s]\u001b[A\n",
      "batch 192, training loss: 3.3691: : 191it [01:39,  1.53it/s]\u001b[A\n",
      "batch 192, training loss: 3.3691: : 192it [01:39,  1.55it/s]\u001b[A\n",
      "batch 193, training loss: 3.2818: : 192it [01:39,  1.55it/s]\u001b[A\n",
      "batch 193, training loss: 3.2818: : 193it [01:39,  1.59it/s]\u001b[A\n",
      "batch 194, training loss: 3.3969: : 193it [01:40,  1.59it/s]\u001b[A\n",
      "batch 194, training loss: 3.3969: : 194it [01:40,  1.59it/s]\u001b[A\n",
      "batch 195, training loss: 3.3128: : 194it [01:40,  1.59it/s]\u001b[A\n",
      "batch 195, training loss: 3.3128: : 195it [01:40,  1.51it/s]\u001b[A\n",
      "batch 196, training loss: 3.2344: : 195it [01:41,  1.51it/s]\u001b[A\n",
      "batch 196, training loss: 3.2344: : 196it [01:41,  1.54it/s]\u001b[A\n",
      "batch 197, training loss: 3.4461: : 196it [01:42,  1.54it/s]\u001b[A\n",
      "batch 197, training loss: 3.4461: : 197it [01:42,  1.58it/s]\u001b[A\n",
      "batch 198, training loss: 3.3465: : 197it [01:42,  1.58it/s]\u001b[A\n",
      "batch 198, training loss: 3.3465: : 198it [01:42,  1.60it/s]\u001b[A\n",
      "batch 199, training loss: 3.3171: : 198it [01:43,  1.60it/s]\u001b[A\n",
      "batch 199, training loss: 3.3171: : 199it [01:43,  1.50it/s]\u001b[A\n",
      "batch 200, training loss: 3.4427: : 199it [01:44,  1.50it/s]\u001b[A\n",
      "batch 200, training loss: 3.4427: : 200it [01:44,  1.53it/s]\u001b[A\n",
      "batch 201, training loss: 3.296: : 200it [01:44,  1.53it/s] \u001b[A\n",
      "batch 201, training loss: 3.296: : 201it [01:44,  1.58it/s]\u001b[A\n",
      "batch 202, training loss: 3.1736: : 201it [01:45,  1.58it/s]\u001b[A\n",
      "batch 202, training loss: 3.1736: : 202it [01:45,  1.60it/s]\u001b[A\n",
      "batch 203, training loss: 3.3599: : 202it [01:46,  1.60it/s]\u001b[A\n",
      "batch 203, training loss: 3.3599: : 203it [01:46,  1.50it/s]\u001b[A\n",
      "batch 204, training loss: 3.3996: : 203it [01:46,  1.50it/s]\u001b[A\n",
      "batch 204, training loss: 3.3996: : 204it [01:46,  1.53it/s]\u001b[A\n",
      "batch 205, training loss: 3.3531: : 204it [01:47,  1.53it/s]\u001b[A\n",
      "batch 205, training loss: 3.3531: : 205it [01:47,  1.57it/s]\u001b[A\n",
      "batch 206, training loss: 3.4525: : 205it [01:47,  1.57it/s]\u001b[A\n",
      "batch 206, training loss: 3.4525: : 206it [01:47,  1.58it/s]\u001b[A\n",
      "batch 207, training loss: 3.2795: : 206it [01:48,  1.58it/s]\u001b[A\n",
      "batch 207, training loss: 3.2795: : 207it [01:48,  1.54it/s]\u001b[A\n",
      "batch 208, training loss: 3.3177: : 207it [01:49,  1.54it/s]\u001b[A\n",
      "batch 208, training loss: 3.3177: : 208it [01:49,  1.62it/s]\u001b[A\n",
      "batch 209, training loss: 3.2995: : 208it [01:49,  1.62it/s]\u001b[A\n",
      "batch 209, training loss: 3.2995: : 209it [01:49,  1.60it/s]\u001b[A\n",
      "batch 210, training loss: 3.3797: : 209it [01:50,  1.60it/s]\u001b[A\n",
      "batch 210, training loss: 3.3797: : 210it [01:50,  1.63it/s]\u001b[A\n",
      "batch 211, training loss: 3.2993: : 210it [01:51,  1.63it/s]\u001b[A\n",
      "batch 211, training loss: 3.2993: : 211it [01:51,  1.53it/s]\u001b[A\n",
      "batch 212, training loss: 3.316: : 211it [01:51,  1.53it/s] \u001b[A\n",
      "batch 212, training loss: 3.316: : 212it [01:51,  1.57it/s]\u001b[A\n",
      "batch 213, training loss: 3.4994: : 212it [01:52,  1.57it/s]\u001b[A\n",
      "batch 213, training loss: 3.4994: : 213it [01:52,  1.60it/s]\u001b[A\n",
      "batch 214, training loss: 3.3823: : 213it [01:52,  1.60it/s]\u001b[A\n",
      "batch 214, training loss: 3.3823: : 214it [01:52,  1.65it/s]\u001b[A\n",
      "batch 215, training loss: 3.25: : 214it [01:53,  1.65it/s]  \u001b[A\n",
      "batch 215, training loss: 3.25: : 215it [01:53,  1.55it/s]\u001b[A\n",
      "batch 216, training loss: 3.3514: : 215it [01:54,  1.55it/s]\u001b[A\n",
      "batch 216, training loss: 3.3514: : 216it [01:54,  1.57it/s]\u001b[A\n",
      "batch 217, training loss: 3.4858: : 216it [01:54,  1.57it/s]\u001b[A\n",
      "batch 217, training loss: 3.4858: : 217it [01:54,  1.60it/s]\u001b[A\n",
      "batch 218, training loss: 3.4123: : 217it [01:55,  1.60it/s]\u001b[A\n",
      "batch 218, training loss: 3.4123: : 218it [01:55,  1.60it/s]\u001b[A\n",
      "batch 219, training loss: 3.5734: : 218it [01:56,  1.60it/s]\u001b[A\n",
      "batch 219, training loss: 3.5734: : 219it [01:56,  1.55it/s]\u001b[A\n",
      "batch 220, training loss: 3.5174: : 219it [01:56,  1.55it/s]\u001b[A\n",
      "batch 220, training loss: 3.5174: : 220it [01:56,  1.59it/s]\u001b[A\n",
      "batch 221, training loss: 3.3745: : 220it [01:57,  1.59it/s]\u001b[A\n",
      "batch 221, training loss: 3.3745: : 221it [01:57,  1.61it/s]\u001b[A\n",
      "batch 222, training loss: 3.4168: : 221it [01:58,  1.61it/s]\u001b[A\n",
      "batch 222, training loss: 3.4168: : 222it [01:58,  1.61it/s]\u001b[A\n",
      "batch 223, training loss: 3.4288: : 222it [01:58,  1.61it/s]\u001b[A\n",
      "batch 223, training loss: 3.4288: : 223it [01:58,  1.66it/s]\u001b[A\n",
      "batch 224, training loss: 3.2998: : 223it [01:59,  1.66it/s]\u001b[A\n",
      "batch 224, training loss: 3.2998: : 224it [01:59,  1.65it/s]\u001b[A\n",
      "batch 225, training loss: 3.4675: : 224it [01:59,  1.65it/s]\u001b[A\n",
      "batch 225, training loss: 3.4675: : 225it [01:59,  1.69it/s]\u001b[A\n",
      "batch 226, training loss: 3.425: : 225it [02:00,  1.69it/s] \u001b[A\n",
      "batch 226, training loss: 3.425: : 226it [02:00,  1.66it/s]\u001b[A\n",
      "batch 227, training loss: 3.3049: : 226it [02:00,  1.66it/s]\u001b[A\n",
      "batch 227, training loss: 3.3049: : 227it [02:00,  1.69it/s]\u001b[A\n",
      "batch 228, training loss: 3.3837: : 227it [02:01,  1.69it/s]\u001b[A\n",
      "batch 228, training loss: 3.3837: : 228it [02:01,  1.68it/s]\u001b[A\n",
      "batch 229, training loss: 3.3492: : 228it [02:02,  1.68it/s]\u001b[A\n",
      "batch 229, training loss: 3.3492: : 229it [02:02,  1.71it/s]\u001b[A\n",
      "batch 230, training loss: 3.5083: : 229it [02:02,  1.71it/s]\u001b[A\n",
      "batch 230, training loss: 3.5083: : 230it [02:02,  1.68it/s]\u001b[A\n",
      "batch 231, training loss: 3.5325: : 230it [02:03,  1.68it/s]\u001b[A\n",
      "batch 231, training loss: 3.5325: : 231it [02:03,  1.70it/s]\u001b[A\n",
      "batch 232, training loss: 3.3473: : 231it [02:03,  1.70it/s]\u001b[A\n",
      "batch 232, training loss: 3.3473: : 232it [02:03,  1.68it/s]\u001b[A\n",
      "batch 233, training loss: 3.3717: : 232it [02:04,  1.68it/s]\u001b[A\n",
      "batch 233, training loss: 3.3717: : 233it [02:04,  1.70it/s]\u001b[A\n",
      "batch 234, training loss: 3.5453: : 233it [02:05,  1.70it/s]\u001b[A\n",
      "batch 234, training loss: 3.5453: : 234it [02:05,  1.68it/s]\u001b[A\n",
      "batch 235, training loss: 3.3736: : 234it [02:05,  1.68it/s]\u001b[A\n",
      "batch 235, training loss: 3.3736: : 235it [02:05,  1.70it/s]\u001b[A\n",
      "batch 236, training loss: 3.3924: : 235it [02:06,  1.70it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 236, training loss: 3.3924: : 236it [02:06,  1.68it/s]\u001b[A\n",
      "batch 237, training loss: 3.2974: : 236it [02:06,  1.68it/s]\u001b[A\n",
      "batch 237, training loss: 3.2974: : 237it [02:06,  1.70it/s]\u001b[A\n",
      "batch 238, training loss: 3.4606: : 237it [02:07,  1.70it/s]\u001b[A\n",
      "batch 238, training loss: 3.4606: : 238it [02:07,  1.69it/s]\u001b[A\n",
      "batch 239, training loss: 3.3544: : 238it [02:08,  1.69it/s]\u001b[A\n",
      "batch 239, training loss: 3.3544: : 239it [02:08,  1.70it/s]\u001b[A\n",
      "batch 240, training loss: 3.3627: : 239it [02:08,  1.70it/s]\u001b[A\n",
      "batch 240, training loss: 3.3627: : 240it [02:08,  1.68it/s]\u001b[A\n",
      "batch 241, training loss: 3.4059: : 240it [02:09,  1.68it/s]\u001b[A\n",
      "batch 241, training loss: 3.4059: : 241it [02:09,  1.71it/s]\u001b[A\n",
      "batch 242, training loss: 3.2562: : 241it [02:09,  1.71it/s]\u001b[A\n",
      "batch 242, training loss: 3.2562: : 242it [02:09,  1.69it/s]\u001b[A\n",
      "batch 243, training loss: 3.4127: : 242it [02:10,  1.69it/s]\u001b[A\n",
      "batch 243, training loss: 3.4127: : 243it [02:10,  1.71it/s]\u001b[A\n",
      "batch 244, training loss: 3.3004: : 243it [02:10,  1.71it/s]\u001b[A\n",
      "batch 244, training loss: 3.3004: : 244it [02:10,  1.68it/s]\u001b[A\n",
      "batch 245, training loss: 3.4723: : 244it [02:11,  1.68it/s]\u001b[A\n",
      "batch 245, training loss: 3.4723: : 245it [02:11,  1.71it/s]\u001b[A\n",
      "batch 246, training loss: 3.3879: : 245it [02:12,  1.71it/s]\u001b[A\n",
      "batch 246, training loss: 3.3879: : 246it [02:12,  1.68it/s]\u001b[A\n",
      "batch 247, training loss: 3.3729: : 246it [02:12,  1.68it/s]\u001b[A\n",
      "batch 247, training loss: 3.3729: : 247it [02:12,  1.71it/s]\u001b[A\n",
      "batch 248, training loss: 3.3049: : 247it [02:13,  1.71it/s]\u001b[A\n",
      "batch 248, training loss: 3.3049: : 248it [02:13,  1.69it/s]\u001b[A\n",
      "batch 249, training loss: 3.2725: : 248it [02:13,  1.69it/s]\u001b[A\n",
      "batch 249, training loss: 3.2725: : 249it [02:13,  1.72it/s]\u001b[A\n",
      "batch 250, training loss: 3.4496: : 249it [02:14,  1.72it/s]\u001b[A\n",
      "batch 250, training loss: 3.4496: : 250it [02:14,  1.70it/s]\u001b[A\n",
      "batch 251, training loss: 3.4521: : 250it [02:15,  1.70it/s]\u001b[A\n",
      "batch 251, training loss: 3.4521: : 251it [02:15,  1.72it/s]\u001b[A\n",
      "batch 252, training loss: 2.3743: : 251it [02:15,  1.72it/s]\u001b[A\n",
      "batch 252, training loss: 2.3743: : 252it [02:15,  2.08it/s]\u001b[A\n",
      "batch 253, training loss: 3.3139: : 252it [02:15,  2.08it/s]\u001b[A\n",
      "batch 253, training loss: 3.3139: : 253it [02:15,  1.93it/s]\u001b[A\n",
      "batch 254, training loss: 3.4365: : 253it [02:16,  1.93it/s]\u001b[A\n",
      "batch 254, training loss: 3.4365: : 254it [02:16,  1.80it/s]\u001b[A\n",
      "batch 255, training loss: 3.2534: : 254it [02:17,  1.80it/s]\u001b[A\n",
      "batch 255, training loss: 3.2534: : 255it [02:17,  1.69it/s]\u001b[A\n",
      "batch 256, training loss: 3.3237: : 255it [02:17,  1.69it/s]\u001b[A\n",
      "batch 256, training loss: 3.3237: : 256it [02:17,  1.66it/s]\u001b[A\n",
      "batch 257, training loss: 3.3836: : 256it [02:18,  1.66it/s]\u001b[A\n",
      "batch 257, training loss: 3.3836: : 257it [02:18,  1.62it/s]\u001b[A\n",
      "batch 258, training loss: 3.4612: : 257it [02:19,  1.62it/s]\u001b[A\n",
      "batch 258, training loss: 3.4612: : 258it [02:19,  1.64it/s]\u001b[A\n",
      "batch 259, training loss: 3.3597: : 258it [02:19,  1.64it/s]\u001b[A\n",
      "batch 259, training loss: 3.3597: : 259it [02:19,  1.62it/s]\u001b[A\n",
      "batch 260, training loss: 3.3309: : 259it [02:20,  1.62it/s]\u001b[A\n",
      "batch 260, training loss: 3.3309: : 260it [02:20,  1.58it/s]\u001b[A\n",
      "batch 261, training loss: 3.3998: : 260it [02:21,  1.58it/s]\u001b[A\n",
      "batch 261, training loss: 3.3998: : 261it [02:21,  1.57it/s]\u001b[A\n",
      "batch 262, training loss: 3.3563: : 261it [02:21,  1.57it/s]\u001b[A\n",
      "batch 262, training loss: 3.3563: : 262it [02:21,  1.73it/s]\u001b[A\n",
      "batch 263, training loss: 3.3278: : 262it [02:21,  1.73it/s]\u001b[A\n",
      "batch 263, training loss: 3.3278: : 263it [02:21,  1.93it/s]\u001b[A\n",
      "batch 264, training loss: 3.4621: : 263it [02:22,  1.93it/s]\u001b[A\n",
      "batch 264, training loss: 3.4621: : 264it [02:22,  2.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.2372: : 264it [02:22,  2.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.2372: : 265it [02:22,  1.86it/s]\u001b[A\n",
      "batch 266, training loss: 3.4192: : 265it [02:23,  1.86it/s]\u001b[A\n",
      "batch 266, training loss: 3.4192: : 266it [02:23,  1.77it/s]\u001b[A\n",
      "batch 267, training loss: 3.4059: : 266it [02:24,  1.77it/s]\u001b[A\n",
      "batch 267, training loss: 3.4059: : 267it [02:24,  1.70it/s]\u001b[A\n",
      "batch 268, training loss: 3.2684: : 267it [02:24,  1.70it/s]\u001b[A\n",
      "batch 268, training loss: 3.2684: : 268it [02:24,  1.70it/s]\u001b[A\n",
      "batch 269, training loss: 3.3292: : 268it [02:25,  1.70it/s]\u001b[A\n",
      "batch 269, training loss: 3.3292: : 269it [02:25,  1.66it/s]\u001b[A\n",
      "batch 270, training loss: 3.3593: : 269it [02:26,  1.66it/s]\u001b[A\n",
      "batch 270, training loss: 3.3593: : 270it [02:26,  1.59it/s]\u001b[A\n",
      "batch 271, training loss: 3.3071: : 270it [02:26,  1.59it/s]\u001b[A\n",
      "batch 271, training loss: 3.3071: : 271it [02:26,  1.59it/s]\u001b[A\n",
      "batch 272, training loss: 3.3323: : 271it [02:27,  1.59it/s]\u001b[A\n",
      "batch 272, training loss: 3.3323: : 272it [02:27,  1.57it/s]\u001b[A\n",
      "batch 273, training loss: 3.4324: : 272it [02:28,  1.57it/s]\u001b[A\n",
      "batch 273, training loss: 3.4324: : 273it [02:28,  1.60it/s]\u001b[A\n",
      "batch 274, training loss: 3.4132: : 273it [02:28,  1.60it/s]\u001b[A\n",
      "batch 274, training loss: 3.4132: : 274it [02:28,  1.59it/s]\u001b[A\n",
      "batch 275, training loss: 3.3051: : 274it [02:29,  1.59it/s]\u001b[A\n",
      "batch 275, training loss: 3.3051: : 275it [02:29,  1.61it/s]\u001b[A\n",
      "batch 276, training loss: 3.1876: : 275it [02:29,  1.61it/s]\u001b[A\n",
      "batch 276, training loss: 3.1876: : 276it [02:29,  1.61it/s]\u001b[A\n",
      "batch 277, training loss: 3.3376: : 276it [02:30,  1.61it/s]\u001b[A\n",
      "batch 277, training loss: 3.3376: : 277it [02:30,  1.62it/s]\u001b[A\n",
      "batch 278, training loss: 3.2829: : 277it [02:31,  1.62it/s]\u001b[A\n",
      "batch 278, training loss: 3.2829: : 278it [02:31,  1.61it/s]\u001b[A\n",
      "batch 279, training loss: 3.278: : 278it [02:31,  1.61it/s] \u001b[A\n",
      "batch 279, training loss: 3.278: : 279it [02:31,  1.56it/s]\u001b[A\n",
      "batch 280, training loss: 3.2011: : 279it [02:32,  1.56it/s]\u001b[A\n",
      "batch 280, training loss: 3.2011: : 280it [02:32,  1.56it/s]\u001b[A\n",
      "batch 281, training loss: 3.3108: : 280it [02:33,  1.56it/s]\u001b[A\n",
      "batch 281, training loss: 3.3108: : 281it [02:33,  1.56it/s]\u001b[A\n",
      "batch 282, training loss: 3.184: : 281it [02:33,  1.56it/s] \u001b[A\n",
      "batch 282, training loss: 3.184: : 282it [02:33,  1.59it/s]\u001b[A\n",
      "batch 283, training loss: 3.2928: : 282it [02:34,  1.59it/s]\u001b[A\n",
      "batch 283, training loss: 3.2928: : 283it [02:34,  1.58it/s]\u001b[A\n",
      "batch 284, training loss: 3.2971: : 283it [02:34,  1.58it/s]\u001b[A\n",
      "batch 284, training loss: 3.2971: : 284it [02:34,  1.60it/s]\u001b[A\n",
      "batch 285, training loss: 3.3221: : 284it [02:35,  1.60it/s]\u001b[A\n",
      "batch 285, training loss: 3.3221: : 285it [02:35,  1.60it/s]\u001b[A\n",
      "batch 286, training loss: 3.4992: : 285it [02:36,  1.60it/s]\u001b[A\n",
      "batch 286, training loss: 3.4992: : 286it [02:36,  1.62it/s]\u001b[A\n",
      "batch 287, training loss: 3.2029: : 286it [02:36,  1.62it/s]\u001b[A\n",
      "batch 287, training loss: 3.2029: : 287it [02:36,  1.60it/s]\u001b[A\n",
      "batch 288, training loss: 3.2359: : 287it [02:37,  1.60it/s]\u001b[A\n",
      "batch 288, training loss: 3.2359: : 288it [02:37,  1.63it/s]\u001b[A\n",
      "batch 289, training loss: 3.3948: : 288it [02:38,  1.63it/s]\u001b[A\n",
      "batch 289, training loss: 3.3948: : 289it [02:38,  1.61it/s]\u001b[A\n",
      "batch 290, training loss: 3.1787: : 289it [02:38,  1.61it/s]\u001b[A\n",
      "batch 290, training loss: 3.1787: : 290it [02:38,  1.63it/s]\u001b[A\n",
      "batch 291, training loss: 3.4491: : 290it [02:39,  1.63it/s]\u001b[A\n",
      "batch 291, training loss: 3.4491: : 291it [02:39,  1.61it/s]\u001b[A\n",
      "batch 292, training loss: 3.2466: : 291it [02:39,  1.61it/s]\u001b[A\n",
      "batch 292, training loss: 3.2466: : 292it [02:39,  1.63it/s]\u001b[A\n",
      "batch 293, training loss: 3.3554: : 292it [02:40,  1.63it/s]\u001b[A\n",
      "batch 293, training loss: 3.3554: : 293it [02:40,  1.61it/s]\u001b[A\n",
      "batch 294, training loss: 3.3956: : 293it [02:41,  1.61it/s]\u001b[A\n",
      "batch 294, training loss: 3.3956: : 294it [02:41,  1.56it/s]\u001b[A\n",
      "batch 295, training loss: 3.3716: : 294it [02:41,  1.56it/s]\u001b[A\n",
      "batch 295, training loss: 3.3716: : 295it [02:41,  1.57it/s]\u001b[A\n",
      "batch 296, training loss: 3.2285: : 295it [02:42,  1.57it/s]\u001b[A\n",
      "batch 296, training loss: 3.2285: : 296it [02:42,  1.56it/s]\u001b[A\n",
      "batch 297, training loss: 3.3736: : 296it [02:43,  1.56it/s]\u001b[A\n",
      "batch 297, training loss: 3.3736: : 297it [02:43,  1.58it/s]\u001b[A\n",
      "batch 298, training loss: 3.294: : 297it [02:43,  1.58it/s] \u001b[A\n",
      "batch 298, training loss: 3.294: : 298it [02:43,  1.57it/s]\u001b[A\n",
      "batch 299, training loss: 3.377: : 298it [02:44,  1.57it/s]\u001b[A\n",
      "batch 299, training loss: 3.377: : 299it [02:44,  1.60it/s]\u001b[A\n",
      "batch 300, training loss: 3.4461: : 299it [02:44,  1.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 300, training loss: 3.4461: : 300it [02:44,  1.59it/s]\u001b[A\n",
      "batch 301, training loss: 3.371: : 300it [02:45,  1.59it/s] \u001b[A\n",
      "batch 301, training loss: 3.371: : 301it [02:45,  1.55it/s]\u001b[A\n",
      "batch 302, training loss: 3.3825: : 301it [02:46,  1.55it/s]\u001b[A\n",
      "batch 302, training loss: 3.3825: : 302it [02:46,  1.56it/s]\u001b[A\n",
      "batch 303, training loss: 3.2865: : 302it [02:46,  1.56it/s]\u001b[A\n",
      "batch 303, training loss: 3.2865: : 303it [02:46,  1.56it/s]\u001b[A\n",
      "batch 304, training loss: 3.368: : 303it [02:47,  1.56it/s] \u001b[A\n",
      "batch 304, training loss: 3.368: : 304it [02:47,  1.59it/s]\u001b[A\n",
      "batch 305, training loss: 3.3019: : 304it [02:48,  1.59it/s]\u001b[A\n",
      "batch 305, training loss: 3.3019: : 305it [02:48,  1.59it/s]\u001b[A\n",
      "batch 306, training loss: 3.3668: : 305it [02:48,  1.59it/s]\u001b[A\n",
      "batch 306, training loss: 3.3668: : 306it [02:48,  1.61it/s]\u001b[A\n",
      "batch 307, training loss: 3.5128: : 306it [02:49,  1.61it/s]\u001b[A\n",
      "batch 307, training loss: 3.5128: : 307it [02:49,  1.60it/s]\u001b[A\n",
      "batch 308, training loss: 3.1313: : 307it [02:50,  1.60it/s]\u001b[A\n",
      "batch 308, training loss: 3.1313: : 308it [02:50,  1.56it/s]\u001b[A\n",
      "batch 309, training loss: 3.5252: : 308it [02:50,  1.56it/s]\u001b[A\n",
      "batch 309, training loss: 3.5252: : 309it [02:50,  1.57it/s]\u001b[A\n",
      "batch 310, training loss: 3.2843: : 309it [02:51,  1.57it/s]\u001b[A\n",
      "batch 310, training loss: 3.2843: : 310it [02:51,  1.56it/s]\u001b[A\n",
      "batch 311, training loss: 3.3802: : 310it [02:51,  1.56it/s]\u001b[A\n",
      "batch 311, training loss: 3.3802: : 311it [02:51,  1.59it/s]\u001b[A\n",
      "batch 312, training loss: 3.2224: : 311it [02:52,  1.59it/s]\u001b[A\n",
      "batch 312, training loss: 3.2224: : 312it [02:52,  1.58it/s]\u001b[A\n",
      "batch 313, training loss: 3.3096: : 312it [02:53,  1.58it/s]\u001b[A\n",
      "batch 313, training loss: 3.3096: : 313it [02:53,  1.61it/s]\u001b[A\n",
      "batch 314, training loss: 3.3432: : 313it [02:53,  1.61it/s]\u001b[A\n",
      "batch 314, training loss: 3.3432: : 314it [02:53,  1.60it/s]\u001b[A\n",
      "batch 315, training loss: 3.3848: : 314it [02:54,  1.60it/s]\u001b[A\n",
      "batch 315, training loss: 3.3848: : 315it [02:54,  1.62it/s]\u001b[A\n",
      "batch 316, training loss: 3.5209: : 315it [02:55,  1.62it/s]\u001b[A\n",
      "batch 316, training loss: 3.5209: : 316it [02:55,  1.61it/s]\u001b[A\n",
      "batch 317, training loss: 3.1926: : 316it [02:55,  1.61it/s]\u001b[A\n",
      "batch 317, training loss: 3.1926: : 317it [02:55,  1.69it/s]\u001b[A\n",
      "batch 318, training loss: 3.4111: : 317it [02:56,  1.69it/s]\u001b[A\n",
      "batch 318, training loss: 3.4111: : 318it [02:56,  1.61it/s]\u001b[A\n",
      "batch 319, training loss: 3.458: : 318it [02:57,  1.61it/s] \u001b[A\n",
      "batch 319, training loss: 3.458: : 319it [02:57,  1.51it/s]\u001b[A\n",
      "batch 320, training loss: 3.4567: : 319it [02:57,  1.51it/s]\u001b[A\n",
      "batch 320, training loss: 3.4567: : 320it [02:57,  1.46it/s]\u001b[A\n",
      "batch 321, training loss: 3.5218: : 320it [02:58,  1.46it/s]\u001b[A\n",
      "batch 321, training loss: 3.5218: : 321it [02:58,  1.47it/s]\u001b[A\n",
      "batch 322, training loss: 3.5511: : 321it [02:59,  1.47it/s]\u001b[A\n",
      "batch 322, training loss: 3.5511: : 322it [02:59,  1.47it/s]\u001b[A\n",
      "batch 323, training loss: 3.3168: : 322it [02:59,  1.47it/s]\u001b[A\n",
      "batch 323, training loss: 3.3168: : 323it [02:59,  1.42it/s]\u001b[A\n",
      "batch 324, training loss: 3.4224: : 323it [03:00,  1.42it/s]\u001b[A\n",
      "batch 324, training loss: 3.4224: : 324it [03:00,  1.40it/s]\u001b[A\n",
      "batch 325, training loss: 3.4891: : 324it [03:01,  1.40it/s]\u001b[A\n",
      "batch 325, training loss: 3.4891: : 325it [03:01,  1.42it/s]\u001b[A\n",
      "batch 326, training loss: 3.383: : 325it [03:01,  1.42it/s] \u001b[A\n",
      "batch 326, training loss: 3.383: : 326it [03:01,  1.44it/s]\u001b[A\n",
      "batch 327, training loss: 3.5361: : 326it [03:02,  1.44it/s]\u001b[A\n",
      "batch 327, training loss: 3.5361: : 327it [03:02,  1.40it/s]\u001b[A\n",
      "batch 328, training loss: 3.419: : 327it [03:03,  1.40it/s] \u001b[A\n",
      "batch 328, training loss: 3.419: : 328it [03:03,  1.38it/s]\u001b[A\n",
      "batch 329, training loss: 3.3112: : 328it [03:04,  1.38it/s]\u001b[A\n",
      "batch 329, training loss: 3.3112: : 329it [03:04,  1.41it/s]\u001b[A\n",
      "batch 330, training loss: 3.4039: : 329it [03:04,  1.41it/s]\u001b[A\n",
      "batch 330, training loss: 3.4039: : 330it [03:04,  1.43it/s]\u001b[A\n",
      "batch 331, training loss: 3.4499: : 330it [03:05,  1.43it/s]\u001b[A\n",
      "batch 331, training loss: 3.4499: : 331it [03:05,  1.39it/s]\u001b[A\n",
      "batch 332, training loss: 3.3398: : 331it [03:06,  1.39it/s]\u001b[A\n",
      "batch 332, training loss: 3.3398: : 332it [03:06,  1.38it/s]\u001b[A\n",
      "batch 333, training loss: 3.3354: : 332it [03:06,  1.38it/s]\u001b[A\n",
      "batch 333, training loss: 3.3354: : 333it [03:06,  1.41it/s]\u001b[A\n",
      "batch 334, training loss: 3.3691: : 333it [03:07,  1.41it/s]\u001b[A\n",
      "batch 334, training loss: 3.3691: : 334it [03:07,  1.42it/s]\u001b[A\n",
      "batch 335, training loss: 3.5501: : 334it [03:08,  1.42it/s]\u001b[A\n",
      "batch 335, training loss: 3.5501: : 335it [03:08,  1.38it/s]\u001b[A\n",
      "batch 336, training loss: 3.4266: : 335it [03:09,  1.38it/s]\u001b[A\n",
      "batch 336, training loss: 3.4266: : 336it [03:09,  1.37it/s]\u001b[A\n",
      "batch 337, training loss: 3.4907: : 336it [03:09,  1.37it/s]\u001b[A\n",
      "batch 337, training loss: 3.4907: : 337it [03:09,  1.41it/s]\u001b[A\n",
      "batch 338, training loss: 3.34: : 337it [03:10,  1.41it/s]  \u001b[A\n",
      "batch 338, training loss: 3.34: : 338it [03:10,  1.43it/s]\u001b[A\n",
      "batch 339, training loss: 3.4505: : 338it [03:11,  1.43it/s]\u001b[A\n",
      "batch 339, training loss: 3.4505: : 339it [03:11,  1.40it/s]\u001b[A\n",
      "batch 340, training loss: 3.6579: : 339it [03:12,  1.40it/s]\u001b[A\n",
      "batch 340, training loss: 3.6579: : 340it [03:12,  1.38it/s]\u001b[A\n",
      "batch 341, training loss: 3.3484: : 340it [03:12,  1.38it/s]\u001b[A\n",
      "batch 341, training loss: 3.3484: : 341it [03:12,  1.40it/s]\u001b[A\n",
      "batch 342, training loss: 3.4334: : 341it [03:13,  1.40it/s]\u001b[A\n",
      "batch 342, training loss: 3.4334: : 342it [03:13,  1.42it/s]\u001b[A\n",
      "batch 343, training loss: 3.4112: : 342it [03:14,  1.42it/s]\u001b[A\n",
      "batch 343, training loss: 3.4112: : 343it [03:14,  1.39it/s]\u001b[A\n",
      "batch 344, training loss: 3.5244: : 343it [03:14,  1.39it/s]\u001b[A\n",
      "batch 344, training loss: 3.5244: : 344it [03:14,  1.38it/s]\u001b[A\n",
      "batch 345, training loss: 3.5099: : 344it [03:15,  1.38it/s]\u001b[A\n",
      "batch 345, training loss: 3.5099: : 345it [03:15,  1.40it/s]\u001b[A\n",
      "batch 346, training loss: 3.4445: : 345it [03:16,  1.40it/s]\u001b[A\n",
      "batch 346, training loss: 3.4445: : 346it [03:16,  1.42it/s]\u001b[A\n",
      "batch 347, training loss: 3.3732: : 346it [03:17,  1.42it/s]\u001b[A\n",
      "batch 347, training loss: 3.3732: : 347it [03:17,  1.40it/s]\u001b[A\n",
      "batch 348, training loss: 3.4366: : 347it [03:17,  1.40it/s]\u001b[A\n",
      "batch 348, training loss: 3.4366: : 348it [03:17,  1.38it/s]\u001b[A\n",
      "batch 349, training loss: 3.6116: : 348it [03:18,  1.38it/s]\u001b[A\n",
      "batch 349, training loss: 3.6116: : 349it [03:18,  1.41it/s]\u001b[A\n",
      "batch 350, training loss: 3.436: : 349it [03:19,  1.41it/s] \u001b[A\n",
      "batch 350, training loss: 3.436: : 350it [03:19,  1.42it/s]\u001b[A\n",
      "batch 351, training loss: 3.368: : 350it [03:19,  1.42it/s]\u001b[A\n",
      "batch 351, training loss: 3.368: : 351it [03:19,  1.40it/s]\u001b[A\n",
      "batch 352, training loss: 3.4117: : 351it [03:20,  1.40it/s]\u001b[A\n",
      "batch 352, training loss: 3.4117: : 352it [03:20,  1.38it/s]\u001b[A\n",
      "batch 353, training loss: 3.4295: : 352it [03:21,  1.38it/s]\u001b[A\n",
      "batch 353, training loss: 3.4295: : 353it [03:21,  1.41it/s]\u001b[A\n",
      "batch 354, training loss: 3.4961: : 353it [03:21,  1.41it/s]\u001b[A\n",
      "batch 354, training loss: 3.4961: : 354it [03:21,  1.43it/s]\u001b[A\n",
      "batch 355, training loss: 3.4478: : 354it [03:22,  1.43it/s]\u001b[A\n",
      "batch 355, training loss: 3.4478: : 355it [03:22,  1.40it/s]\u001b[A\n",
      "batch 356, training loss: 3.2972: : 355it [03:23,  1.40it/s]\u001b[A\n",
      "batch 356, training loss: 3.2972: : 356it [03:23,  1.60it/s]\u001b[A\n",
      "batch 357, training loss: 3.3174: : 356it [03:23,  1.60it/s]\u001b[A\n",
      "batch 357, training loss: 3.3174: : 357it [03:23,  1.78it/s]\u001b[A\n",
      "batch 358, training loss: 3.5263: : 357it [03:24,  1.78it/s]\u001b[A\n",
      "batch 358, training loss: 3.5263: : 358it [03:24,  1.75it/s]\u001b[A\n",
      "batch 359, training loss: 3.3561: : 358it [03:24,  1.75it/s]\u001b[A\n",
      "batch 359, training loss: 3.3561: : 359it [03:24,  1.60it/s]\u001b[A\n",
      "batch 360, training loss: 3.4259: : 359it [03:25,  1.60it/s]\u001b[A\n",
      "batch 360, training loss: 3.4259: : 360it [03:25,  1.51it/s]\u001b[A\n",
      "batch 361, training loss: 3.4362: : 360it [03:26,  1.51it/s]\u001b[A\n",
      "batch 361, training loss: 3.4362: : 361it [03:26,  1.50it/s]\u001b[A\n",
      "batch 362, training loss: 3.4916: : 361it [03:26,  1.50it/s]\u001b[A\n",
      "batch 362, training loss: 3.4916: : 362it [03:26,  1.49it/s]\u001b[A\n",
      "batch 363, training loss: 3.4419: : 362it [03:27,  1.49it/s]\u001b[A\n",
      "batch 363, training loss: 3.4419: : 363it [03:27,  1.45it/s]\u001b[A\n",
      "batch 364, training loss: 3.3461: : 363it [03:28,  1.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 364, training loss: 3.3461: : 364it [03:28,  1.47it/s]\u001b[A\n",
      "batch 365, training loss: 3.299: : 364it [03:29,  1.47it/s] \u001b[A\n",
      "batch 365, training loss: 3.299: : 365it [03:29,  1.47it/s]\u001b[A\n",
      "batch 366, training loss: 3.436: : 365it [03:29,  1.47it/s]\u001b[A\n",
      "batch 366, training loss: 3.436: : 366it [03:29,  1.42it/s]\u001b[A\n",
      "batch 367, training loss: 3.3875: : 366it [03:30,  1.42it/s]\u001b[A\n",
      "batch 367, training loss: 3.3875: : 367it [03:30,  1.40it/s]\u001b[A\n",
      "batch 368, training loss: 3.4643: : 367it [03:31,  1.40it/s]\u001b[A\n",
      "batch 368, training loss: 3.4643: : 368it [03:31,  1.42it/s]\u001b[A\n",
      "batch 369, training loss: 3.4725: : 368it [03:31,  1.42it/s]\u001b[A\n",
      "batch 369, training loss: 3.4725: : 369it [03:31,  1.44it/s]\u001b[A\n",
      "batch 370, training loss: 3.4029: : 369it [03:32,  1.44it/s]\u001b[A\n",
      "batch 370, training loss: 3.4029: : 370it [03:32,  1.41it/s]\u001b[A\n",
      "batch 371, training loss: 3.3972: : 370it [03:33,  1.41it/s]\u001b[A\n",
      "batch 371, training loss: 3.3972: : 371it [03:33,  1.38it/s]\u001b[A\n",
      "batch 372, training loss: 3.3573: : 371it [03:34,  1.38it/s]\u001b[A\n",
      "batch 372, training loss: 3.3573: : 372it [03:34,  1.41it/s]\u001b[A\n",
      "batch 373, training loss: 3.3762: : 372it [03:34,  1.41it/s]\u001b[A\n",
      "batch 373, training loss: 3.3762: : 373it [03:34,  1.42it/s]\u001b[A\n",
      "batch 374, training loss: 3.4061: : 373it [03:35,  1.42it/s]\u001b[A\n",
      "batch 374, training loss: 3.4061: : 374it [03:35,  1.39it/s]\u001b[A\n",
      "batch 375, training loss: 3.0276: : 374it [03:35,  1.39it/s]\u001b[A\n",
      "batch 375, training loss: 3.0276: : 375it [03:35,  1.60it/s]\u001b[A\n",
      "batch 376, training loss: 3.3918: : 375it [03:36,  1.60it/s]\u001b[A\n",
      "batch 376, training loss: 3.3918: : 376it [03:36,  1.47it/s]\u001b[A\n",
      "batch 377, training loss: 3.4664: : 376it [03:37,  1.47it/s]\u001b[A\n",
      "batch 377, training loss: 3.4664: : 377it [03:37,  1.39it/s]\u001b[A\n",
      "batch 378, training loss: 3.3997: : 377it [03:38,  1.39it/s]\u001b[A\n",
      "batch 378, training loss: 3.3997: : 378it [03:38,  1.33it/s]\u001b[A\n",
      "batch 379, training loss: 3.3605: : 378it [03:39,  1.33it/s]\u001b[A\n",
      "batch 379, training loss: 3.3605: : 379it [03:39,  1.30it/s]\u001b[A\n",
      "batch 380, training loss: 3.3387: : 379it [03:39,  1.30it/s]\u001b[A\n",
      "batch 380, training loss: 3.3387: : 380it [03:39,  1.28it/s]\u001b[A\n",
      "batch 381, training loss: 3.4396: : 380it [03:40,  1.28it/s]\u001b[A\n",
      "batch 381, training loss: 3.4396: : 381it [03:40,  1.28it/s]\u001b[A\n",
      "batch 382, training loss: 3.3082: : 381it [03:41,  1.28it/s]\u001b[A\n",
      "batch 382, training loss: 3.3082: : 382it [03:41,  1.27it/s]\u001b[A\n",
      "batch 383, training loss: 3.3633: : 382it [03:42,  1.27it/s]\u001b[A\n",
      "batch 383, training loss: 3.3633: : 383it [03:42,  1.26it/s]\u001b[A\n",
      "batch 384, training loss: 3.4248: : 383it [03:43,  1.26it/s]\u001b[A\n",
      "batch 384, training loss: 3.4248: : 384it [03:43,  1.25it/s]\u001b[A\n",
      "batch 385, training loss: 3.3733: : 384it [03:43,  1.25it/s]\u001b[A\n",
      "batch 385, training loss: 3.3733: : 385it [03:43,  1.25it/s]\u001b[A\n",
      "batch 386, training loss: 3.3395: : 385it [03:44,  1.25it/s]\u001b[A\n",
      "batch 386, training loss: 3.3395: : 386it [03:44,  1.24it/s]\u001b[A\n",
      "batch 387, training loss: 3.3798: : 386it [03:45,  1.24it/s]\u001b[A\n",
      "batch 387, training loss: 3.3798: : 387it [03:45,  1.23it/s]\u001b[A\n",
      "batch 388, training loss: 3.2074: : 387it [03:46,  1.23it/s]\u001b[A\n",
      "batch 388, training loss: 3.2074: : 388it [03:46,  1.23it/s]\u001b[A\n",
      "batch 389, training loss: 3.3564: : 388it [03:47,  1.23it/s]\u001b[A\n",
      "batch 389, training loss: 3.3564: : 389it [03:47,  1.22it/s]\u001b[A\n",
      "batch 390, training loss: 3.4414: : 389it [03:48,  1.22it/s]\u001b[A\n",
      "batch 390, training loss: 3.4414: : 390it [03:48,  1.22it/s]\u001b[A\n",
      "batch 391, training loss: 3.4553: : 390it [03:48,  1.22it/s]\u001b[A\n",
      "batch 391, training loss: 3.4553: : 391it [03:48,  1.28it/s]\u001b[A\n",
      "batch 392, training loss: 3.4205: : 391it [03:49,  1.28it/s]\u001b[A\n",
      "batch 392, training loss: 3.4205: : 392it [03:49,  1.31it/s]\u001b[A\n",
      "batch 393, training loss: 3.2355: : 392it [03:50,  1.31it/s]\u001b[A\n",
      "batch 393, training loss: 3.2355: : 393it [03:50,  1.30it/s]\u001b[A\n",
      "batch 394, training loss: 3.2125: : 393it [03:51,  1.30it/s]\u001b[A\n",
      "batch 394, training loss: 3.2125: : 394it [03:51,  1.29it/s]\u001b[A\n",
      "batch 395, training loss: 3.2937: : 394it [03:51,  1.29it/s]\u001b[A\n",
      "batch 395, training loss: 3.2937: : 395it [03:51,  1.27it/s]\u001b[A\n",
      "batch 396, training loss: 3.4583: : 395it [03:52,  1.27it/s]\u001b[A\n",
      "batch 396, training loss: 3.4583: : 396it [03:52,  1.26it/s]\u001b[A\n",
      "batch 397, training loss: 3.3015: : 396it [03:53,  1.26it/s]\u001b[A\n",
      "batch 397, training loss: 3.3015: : 397it [03:53,  1.25it/s]\u001b[A\n",
      "batch 398, training loss: 3.4091: : 397it [03:54,  1.25it/s]\u001b[A\n",
      "batch 398, training loss: 3.4091: : 398it [03:54,  1.24it/s]\u001b[A\n",
      "batch 399, training loss: 3.5211: : 398it [03:55,  1.24it/s]\u001b[A\n",
      "batch 399, training loss: 3.5211: : 399it [03:55,  1.24it/s]\u001b[A\n",
      "batch 400, training loss: 3.29: : 399it [03:55,  1.24it/s]  \u001b[A\n",
      "batch 400, training loss: 3.29: : 400it [03:55,  1.23it/s]\u001b[A\n",
      "batch 401, training loss: 3.2405: : 400it [03:56,  1.23it/s]\u001b[A\n",
      "batch 401, training loss: 3.2405: : 401it [03:56,  1.24it/s]\u001b[A\n",
      "batch 402, training loss: 3.3326: : 401it [03:57,  1.24it/s]\u001b[A\n",
      "batch 402, training loss: 3.3326: : 402it [03:57,  1.23it/s]\u001b[A\n",
      "batch 403, training loss: 3.4879: : 402it [03:58,  1.23it/s]\u001b[A\n",
      "batch 403, training loss: 3.4879: : 403it [03:58,  1.23it/s]\u001b[A\n",
      "batch 404, training loss: 3.1215: : 403it [03:59,  1.23it/s]\u001b[A\n",
      "batch 404, training loss: 3.1215: : 404it [03:59,  1.23it/s]\u001b[A\n",
      "batch 405, training loss: 3.4088: : 404it [04:00,  1.23it/s]\u001b[A\n",
      "batch 405, training loss: 3.4088: : 405it [04:00,  1.10it/s]\u001b[A\n",
      "batch 406, training loss: 3.3099: : 405it [04:01,  1.10it/s]\u001b[A\n",
      "batch 406, training loss: 3.3099: : 406it [04:01,  1.05it/s]\u001b[A\n",
      "batch 407, training loss: 3.3813: : 406it [04:02,  1.05it/s]\u001b[A\n",
      "batch 407, training loss: 3.3813: : 407it [04:02,  1.00it/s]\u001b[A\n",
      "batch 408, training loss: 3.2325: : 407it [04:03,  1.00it/s]\u001b[A\n",
      "batch 408, training loss: 3.2325: : 408it [04:03,  1.04s/it]\u001b[A\n",
      "batch 409, training loss: 3.3858: : 408it [04:04,  1.04s/it]\u001b[A\n",
      "batch 409, training loss: 3.3858: : 409it [04:04,  1.06s/it]\u001b[A\n",
      "batch 410, training loss: 3.2688: : 409it [04:05,  1.06s/it]\u001b[A\n",
      "batch 410, training loss: 3.2688: : 410it [04:05,  1.05s/it]\u001b[A\n",
      "batch 411, training loss: 3.3698: : 410it [04:06,  1.05s/it]\u001b[A\n",
      "batch 411, training loss: 3.3698: : 411it [04:06,  1.07s/it]\u001b[A\n",
      "batch 412, training loss: 3.408: : 411it [04:08,  1.07s/it] \u001b[A\n",
      "batch 412, training loss: 3.408: : 412it [04:08,  1.08s/it]\u001b[A\n",
      "batch 413, training loss: 3.3153: : 412it [04:09,  1.08s/it]\u001b[A\n",
      "batch 413, training loss: 3.3153: : 413it [04:09,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.2342: : 413it [04:10,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.2342: : 414it [04:10,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.1532: : 414it [04:11,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.1532: : 415it [04:11,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 3.181: : 415it [04:12,  1.09s/it] \u001b[A\n",
      "batch 416, training loss: 3.181: : 416it [04:12,  1.08s/it]\u001b[A\n",
      "batch 417, training loss: 3.4478: : 416it [04:13,  1.08s/it]\u001b[A\n",
      "batch 417, training loss: 3.4478: : 417it [04:13,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.3213: : 417it [04:14,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.3213: : 418it [04:14,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.1569: : 418it [04:15,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.1569: : 419it [04:15,  1.09s/it]\u001b[A\n",
      "batch 420, training loss: 3.255: : 419it [04:16,  1.09s/it] \u001b[A\n",
      "batch 420, training loss: 3.255: : 420it [04:16,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.3718: : 420it [04:17,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.3718: : 421it [04:17,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.4001: : 421it [04:19,  1.09s/it]\u001b[A\n",
      "batch 422, training loss: 3.4001: : 422it [04:19,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.1855: : 422it [04:19,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.1855: : 423it [04:19,  1.06s/it]\u001b[A\n",
      "batch 424, training loss: 3.3696: : 423it [04:21,  1.06s/it]\u001b[A\n",
      "batch 424, training loss: 3.3696: : 424it [04:21,  1.12s/it]\u001b[A\n",
      "batch 425, training loss: 3.4215: : 424it [04:22,  1.12s/it]\u001b[A\n",
      "batch 425, training loss: 3.4215: : 425it [04:22,  1.14s/it]\u001b[A\n",
      "batch 426, training loss: 3.357: : 425it [04:23,  1.14s/it] \u001b[A\n",
      "batch 426, training loss: 3.357: : 426it [04:23,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 3.403: : 426it [04:24,  1.15s/it]\u001b[A\n",
      "batch 427, training loss: 3.403: : 427it [04:24,  1.15s/it]\u001b[A\n",
      "batch 428, training loss: 3.5477: : 427it [04:25,  1.15s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 428, training loss: 3.5477: : 428it [04:25,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 3.3711: : 428it [04:27,  1.18s/it]\u001b[A\n",
      "batch 429, training loss: 3.3711: : 429it [04:27,  1.18s/it]\u001b[A\n",
      "batch 430, training loss: 3.424: : 429it [04:28,  1.18s/it] \u001b[A\n",
      "batch 430, training loss: 3.424: : 430it [04:28,  1.18s/it]\u001b[A\n",
      "batch 431, training loss: 3.4741: : 430it [04:29,  1.18s/it]\u001b[A\n",
      "batch 431, training loss: 3.4741: : 431it [04:29,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 3.2997: : 431it [04:30,  1.20s/it]\u001b[A\n",
      "batch 432, training loss: 3.2997: : 432it [04:30,  1.20s/it]\u001b[A\n",
      "batch 433, training loss: 3.3535: : 432it [04:32,  1.20s/it]\u001b[A\n",
      "batch 433, training loss: 3.3535: : 433it [04:32,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 3.3647: : 433it [04:33,  1.21s/it]\u001b[A\n",
      "batch 434, training loss: 3.3647: : 434it [04:33,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.3606: : 434it [04:34,  1.21s/it]\u001b[A\n",
      "batch 435, training loss: 3.3606: : 435it [04:34,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3955: : 435it [04:35,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3955: : 436it [04:35,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.3338: : 436it [04:36,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.3338: : 437it [04:36,  1.22s/it]\u001b[A\n",
      "batch 438, training loss: 3.3645: : 437it [04:37,  1.22s/it]\u001b[A\n",
      "batch 438, training loss: 3.3645: : 438it [04:37,  1.18s/it]\u001b[A\n",
      "batch 439, training loss: 3.5018: : 438it [04:38,  1.18s/it]\u001b[A\n",
      "batch 439, training loss: 3.5018: : 439it [04:38,  1.07s/it]\u001b[A\n",
      "batch 440, training loss: 3.312: : 439it [04:39,  1.07s/it] \u001b[A\n",
      "batch 440, training loss: 3.312: : 440it [04:39,  1.10s/it]\u001b[A\n",
      "batch 441, training loss: 3.5904: : 440it [04:41,  1.10s/it]\u001b[A\n",
      "batch 441, training loss: 3.5904: : 441it [04:41,  1.14s/it]\u001b[A\n",
      "batch 442, training loss: 3.2675: : 441it [04:42,  1.14s/it]\u001b[A\n",
      "batch 442, training loss: 3.2675: : 442it [04:42,  1.13s/it]\u001b[A\n",
      "batch 443, training loss: 3.3609: : 442it [04:43,  1.13s/it]\u001b[A\n",
      "batch 443, training loss: 3.3609: : 443it [04:43,  1.17s/it]\u001b[A\n",
      "batch 444, training loss: 3.3515: : 443it [04:44,  1.17s/it]\u001b[A\n",
      "batch 444, training loss: 3.3515: : 444it [04:44,  1.17s/it]\u001b[A\n",
      "batch 445, training loss: 3.4058: : 444it [04:45,  1.17s/it]\u001b[A\n",
      "batch 445, training loss: 3.4058: : 445it [04:45,  1.18s/it]\u001b[A\n",
      "batch 446, training loss: 3.3797: : 445it [04:47,  1.18s/it]\u001b[A\n",
      "batch 446, training loss: 3.3797: : 446it [04:47,  1.18s/it]\u001b[A\n",
      "batch 447, training loss: 3.3644: : 446it [04:48,  1.18s/it]\u001b[A\n",
      "batch 447, training loss: 3.3644: : 447it [04:48,  1.19s/it]\u001b[A\n",
      "batch 448, training loss: 3.2584: : 447it [04:49,  1.19s/it]\u001b[A\n",
      "batch 448, training loss: 3.2584: : 448it [04:49,  1.19s/it]\u001b[A\n",
      "batch 449, training loss: 3.3477: : 448it [04:50,  1.19s/it]\u001b[A\n",
      "batch 449, training loss: 3.3477: : 449it [04:50,  1.19s/it]\u001b[A\n",
      "batch 450, training loss: 3.44: : 449it [04:51,  1.19s/it]  \u001b[A\n",
      "batch 450, training loss: 3.44: : 450it [04:51,  1.21s/it]\u001b[A\n",
      "batch 451, training loss: 3.4456: : 450it [04:53,  1.21s/it]\u001b[A\n",
      "batch 451, training loss: 3.4456: : 451it [04:53,  1.21s/it]\u001b[A\n",
      "batch 452, training loss: 3.4185: : 451it [04:54,  1.21s/it]\u001b[A\n",
      "batch 452, training loss: 3.4185: : 452it [04:54,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.5001: : 452it [04:55,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.5001: : 453it [04:55,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.384: : 453it [04:56,  1.22s/it] \u001b[A\n",
      "batch 454, training loss: 3.384: : 454it [04:56,  1.22s/it]\u001b[A\n",
      "batch 455, training loss: 3.292: : 454it [04:58,  1.22s/it]\u001b[A\n",
      "batch 455, training loss: 3.292: : 455it [04:58,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.3021: : 455it [04:59,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.3021: : 456it [04:59,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 3.4175: : 456it [05:00,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 3.4175: : 457it [05:00,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 3.3123: : 457it [05:01,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 3.3123: : 458it [05:01,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.3669: : 458it [05:03,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.3669: : 459it [05:03,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.354: : 459it [05:04,  1.23s/it] \u001b[A\n",
      "batch 460, training loss: 3.354: : 460it [05:04,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 3.4509: : 460it [05:05,  1.23s/it]\u001b[A\n",
      "batch 461, training loss: 3.4509: : 461it [05:05,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.4114: : 461it [05:06,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.4114: : 462it [05:06,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.2464: : 462it [05:08,  1.24s/it]\u001b[A\n",
      "batch 463, training loss: 3.2464: : 463it [05:08,  1.24s/it]\u001b[A\n",
      "batch 464, training loss: 3.3875: : 463it [05:09,  1.24s/it]\u001b[A\n",
      "batch 464, training loss: 3.3875: : 464it [05:09,  1.24s/it]\u001b[A\n",
      "batch 465, training loss: 3.3251: : 464it [05:10,  1.24s/it]\u001b[A\n",
      "batch 465, training loss: 3.3251: : 465it [05:10,  1.16s/it]\u001b[A\n",
      "batch 466, training loss: 3.206: : 465it [05:11,  1.16s/it] \u001b[A\n",
      "batch 466, training loss: 3.206: : 466it [05:11,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.1615: : 466it [05:12,  1.20s/it]\u001b[A\n",
      "batch 467, training loss: 3.1615: : 467it [05:12,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 3.3585: : 467it [05:13,  1.20s/it]\u001b[A\n",
      "batch 468, training loss: 3.3585: : 468it [05:13,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.2473: : 468it [05:15,  1.22s/it]\u001b[A\n",
      "batch 469, training loss: 3.2473: : 469it [05:15,  1.23s/it]\u001b[A\n",
      "batch 470, training loss: 3.391: : 469it [05:16,  1.23s/it] \u001b[A\n",
      "batch 470, training loss: 3.391: : 470it [05:16,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 3.3421: : 470it [05:17,  1.24s/it]\u001b[A\n",
      "batch 471, training loss: 3.3421: : 471it [05:17,  1.22s/it]\u001b[A\n",
      "batch 472, training loss: 3.2937: : 471it [05:18,  1.22s/it]\u001b[A\n",
      "batch 472, training loss: 3.2937: : 472it [05:18,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 3.3106: : 472it [05:20,  1.25s/it]\u001b[A\n",
      "batch 473, training loss: 3.3106: : 473it [05:20,  1.23s/it]\u001b[A\n",
      "batch 474, training loss: 3.3518: : 473it [05:21,  1.23s/it]\u001b[A\n",
      "batch 474, training loss: 3.3518: : 474it [05:21,  1.25s/it]\u001b[A\n",
      "batch 475, training loss: 3.256: : 474it [05:22,  1.25s/it] \u001b[A\n",
      "batch 475, training loss: 3.256: : 475it [05:22,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 3.3653: : 475it [05:23,  1.25s/it]\u001b[A\n",
      "batch 476, training loss: 3.3653: : 476it [05:23,  1.25s/it]\u001b[A\n",
      "batch 477, training loss: 3.1804: : 476it [05:25,  1.25s/it]\u001b[A\n",
      "batch 477, training loss: 3.1804: : 477it [05:25,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.2539: : 477it [05:26,  1.25s/it]\u001b[A\n",
      "batch 478, training loss: 3.2539: : 478it [05:26,  1.27s/it]\u001b[A\n",
      "batch 479, training loss: 3.2288: : 478it [05:27,  1.27s/it]\u001b[A\n",
      "batch 479, training loss: 3.2288: : 479it [05:27,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 3.2701: : 479it [05:29,  1.25s/it]\u001b[A\n",
      "batch 480, training loss: 3.2701: : 480it [05:29,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.2883: : 480it [05:30,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.2883: : 481it [05:30,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.2708: : 481it [05:31,  1.25s/it]\u001b[A\n",
      "batch 482, training loss: 3.2708: : 482it [05:31,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 3.2187: : 482it [05:32,  1.27s/it]\u001b[A\n",
      "batch 483, training loss: 3.2187: : 483it [05:32,  1.25s/it]\u001b[A\n",
      "batch 484, training loss: 3.2973: : 483it [05:34,  1.25s/it]\u001b[A\n",
      "batch 484, training loss: 3.2973: : 484it [05:34,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.2428: : 484it [05:35,  1.25s/it]\u001b[A\n",
      "batch 485, training loss: 3.2428: : 485it [05:35,  1.25s/it]\u001b[A\n",
      "batch 486, training loss: 3.3261: : 485it [05:36,  1.25s/it]\u001b[A\n",
      "batch 486, training loss: 3.3261: : 486it [05:36,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.412: : 486it [05:37,  1.26s/it] \u001b[A\n",
      "batch 487, training loss: 3.412: : 487it [05:37,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.2335: : 487it [05:39,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.2335: : 488it [05:39,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 3.1646: : 488it [05:40,  1.25s/it]\u001b[A\n",
      "batch 489, training loss: 3.1646: : 489it [05:40,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.2159: : 489it [05:41,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.2159: : 490it [05:41,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.2738: : 490it [05:42,  1.25s/it]\u001b[A\n",
      "batch 491, training loss: 3.2738: : 491it [05:42,  1.26s/it]\u001b[A\n",
      "batch 492, training loss: 3.1776: : 491it [05:44,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 492, training loss: 3.1776: : 492it [05:44,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.351: : 492it [05:45,  1.26s/it] \u001b[A\n",
      "batch 493, training loss: 3.351: : 493it [05:45,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.3746: : 493it [05:46,  1.26s/it]\u001b[A\n",
      "batch 494, training loss: 3.3746: : 494it [05:46,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.1884: : 494it [05:47,  1.25s/it]\u001b[A\n",
      "batch 495, training loss: 3.1884: : 495it [05:47,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.2166: : 495it [05:49,  1.26s/it]\u001b[A\n",
      "batch 496, training loss: 3.2166: : 496it [05:49,  1.27s/it]\u001b[A\n",
      "batch 497, training loss: 3.1118: : 496it [05:50,  1.27s/it]\u001b[A\n",
      "batch 497, training loss: 3.1118: : 497it [05:50,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.2365: : 497it [05:51,  1.26s/it]\u001b[A\n",
      "batch 498, training loss: 3.2365: : 498it [05:51,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.0886: : 498it [05:52,  1.26s/it]\u001b[A\n",
      "batch 499, training loss: 3.0886: : 499it [05:52,  1.14s/it]\u001b[A\n",
      "batch 500, training loss: 3.4502: : 499it [05:53,  1.14s/it]\u001b[A\n",
      "batch 500, training loss: 3.4502: : 500it [05:53,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 3.2984: : 500it [05:55,  1.17s/it]\u001b[A\n",
      "batch 501, training loss: 3.2984: : 501it [05:55,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.2316: : 501it [05:56,  1.24s/it]\u001b[A\n",
      "batch 502, training loss: 3.2316: : 502it [05:56,  1.29s/it]\u001b[A\n",
      "batch 503, training loss: 3.338: : 502it [05:57,  1.29s/it] \u001b[A\n",
      "batch 503, training loss: 3.338: : 503it [05:57,  1.31s/it]\u001b[A\n",
      "batch 504, training loss: 3.3377: : 503it [05:59,  1.31s/it]\u001b[A\n",
      "batch 504, training loss: 3.3377: : 504it [05:59,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 3.4139: : 504it [06:00,  1.34s/it]\u001b[A\n",
      "batch 505, training loss: 3.4139: : 505it [06:00,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.3111: : 505it [06:02,  1.34s/it]\u001b[A\n",
      "batch 506, training loss: 3.3111: : 506it [06:02,  1.35s/it]\u001b[A\n",
      "batch 507, training loss: 3.2455: : 506it [06:03,  1.35s/it]\u001b[A\n",
      "batch 507, training loss: 3.2455: : 507it [06:03,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 3.3614: : 507it [06:04,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 3.3614: : 508it [06:04,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.2469: : 508it [06:06,  1.36s/it]\u001b[A\n",
      "batch 509, training loss: 3.2469: : 509it [06:06,  1.38s/it]\u001b[A\n",
      "batch 510, training loss: 3.321: : 509it [06:07,  1.38s/it] \u001b[A\n",
      "batch 510, training loss: 3.321: : 510it [06:07,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.3168: : 510it [06:08,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.3168: : 511it [06:08,  1.37s/it]\u001b[A\n",
      "batch 512, training loss: 3.2993: : 511it [06:10,  1.37s/it]\u001b[A\n",
      "batch 512, training loss: 3.2993: : 512it [06:10,  1.39s/it]\u001b[A\n",
      "batch 513, training loss: 3.3506: : 512it [06:11,  1.39s/it]\u001b[A\n",
      "batch 513, training loss: 3.3506: : 513it [06:11,  1.39s/it]\u001b[A\n",
      "batch 514, training loss: 3.2508: : 513it [06:12,  1.39s/it]\u001b[A\n",
      "batch 514, training loss: 3.2508: : 514it [06:12,  1.27s/it]\u001b[A\n",
      "batch 515, training loss: 3.4101: : 514it [06:14,  1.27s/it]\u001b[A\n",
      "batch 515, training loss: 3.4101: : 515it [06:14,  1.30s/it]\u001b[A\n",
      "batch 516, training loss: 3.2701: : 515it [06:15,  1.30s/it]\u001b[A\n",
      "batch 516, training loss: 3.2701: : 516it [06:15,  1.33s/it]\u001b[A\n",
      "batch 517, training loss: 3.269: : 516it [06:16,  1.33s/it] \u001b[A\n",
      "batch 517, training loss: 3.269: : 517it [06:16,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.3783: : 517it [06:18,  1.36s/it]\u001b[A\n",
      "batch 518, training loss: 3.3783: : 518it [06:18,  1.33s/it]\u001b[A\n",
      "batch 519, training loss: 3.1699: : 518it [06:19,  1.33s/it]\u001b[A\n",
      "batch 519, training loss: 3.1699: : 519it [06:19,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 3.3406: : 519it [06:21,  1.36s/it]\u001b[A\n",
      "batch 520, training loss: 3.3406: : 520it [06:21,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 3.3484: : 520it [06:22,  1.38s/it]\u001b[A\n",
      "batch 521, training loss: 3.3484: : 521it [06:22,  1.35s/it]\u001b[A\n",
      "batch 522, training loss: 3.36: : 521it [06:23,  1.35s/it]  \u001b[A\n",
      "batch 522, training loss: 3.36: : 522it [06:23,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 3.2551: : 522it [06:25,  1.37s/it]\u001b[A\n",
      "batch 523, training loss: 3.2551: : 523it [06:25,  1.38s/it]\u001b[A\n",
      "batch 524, training loss: 3.4354: : 523it [06:26,  1.38s/it]\u001b[A\n",
      "batch 524, training loss: 3.4354: : 524it [06:26,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.4758: : 524it [06:27,  1.37s/it]\u001b[A\n",
      "batch 525, training loss: 3.4758: : 525it [06:27,  1.38s/it]\u001b[A\n",
      "batch 526, training loss: 3.2762: : 525it [06:29,  1.38s/it]\u001b[A\n",
      "batch 526, training loss: 3.2762: : 526it [06:29,  1.39s/it]\u001b[A\n",
      "batch 527, training loss: 3.2663: : 526it [06:30,  1.39s/it]\u001b[A\n",
      "batch 527, training loss: 3.2663: : 527it [06:30,  1.35s/it]\u001b[A\n",
      "batch 528, training loss: 3.2948: : 527it [06:32,  1.35s/it]\u001b[A\n",
      "batch 528, training loss: 3.2948: : 528it [06:32,  1.40s/it]\u001b[A\n",
      "batch 529, training loss: 3.3544: : 528it [06:33,  1.40s/it]\u001b[A\n",
      "batch 529, training loss: 3.3544: : 529it [06:33,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.3185: : 529it [06:34,  1.40s/it]\u001b[A\n",
      "batch 530, training loss: 3.3185: : 530it [06:34,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 3.2593: : 530it [06:36,  1.42s/it]\u001b[A\n",
      "batch 531, training loss: 3.2593: : 531it [06:36,  1.44s/it]\u001b[A\n",
      "batch 532, training loss: 3.0814: : 531it [06:38,  1.44s/it]\u001b[A\n",
      "batch 532, training loss: 3.0814: : 532it [06:38,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 3.2237: : 532it [06:39,  1.47s/it]\u001b[A\n",
      "batch 533, training loss: 3.2237: : 533it [06:39,  1.48s/it]\u001b[A\n",
      "batch 534, training loss: 3.3757: : 533it [06:41,  1.48s/it]\u001b[A\n",
      "batch 534, training loss: 3.3757: : 534it [06:41,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.2291: : 534it [06:42,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.2291: : 535it [06:42,  1.44s/it]\u001b[A\n",
      "batch 536, training loss: 3.0914: : 535it [06:43,  1.44s/it]\u001b[A\n",
      "batch 536, training loss: 3.0914: : 536it [06:43,  1.46s/it]\u001b[A\n",
      "batch 537, training loss: 3.1708: : 536it [06:45,  1.46s/it]\u001b[A\n",
      "batch 537, training loss: 3.1708: : 537it [06:45,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.2792: : 537it [06:46,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.2792: : 538it [06:46,  1.43s/it]\u001b[A\n",
      "batch 539, training loss: 3.237: : 538it [06:48,  1.43s/it] \u001b[A\n",
      "batch 539, training loss: 3.237: : 539it [06:48,  1.46s/it]\u001b[A\n",
      "batch 540, training loss: 3.1963: : 539it [06:49,  1.46s/it]\u001b[A\n",
      "batch 540, training loss: 3.1963: : 540it [06:49,  1.47s/it]\u001b[A\n",
      "batch 541, training loss: 3.304: : 540it [06:51,  1.47s/it] \u001b[A\n",
      "batch 541, training loss: 3.304: : 541it [06:51,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.363: : 541it [06:52,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.363: : 542it [06:52,  1.44s/it]\u001b[A\n",
      "batch 543, training loss: 3.1771: : 542it [06:54,  1.44s/it]\u001b[A\n",
      "batch 543, training loss: 3.1771: : 543it [06:54,  1.46s/it]\u001b[A\n",
      "batch 544, training loss: 3.257: : 543it [06:55,  1.46s/it] \u001b[A\n",
      "batch 544, training loss: 3.257: : 544it [06:55,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 3.2634: : 544it [06:57,  1.49s/it]\u001b[A\n",
      "batch 545, training loss: 3.2634: : 545it [06:57,  1.48s/it]\u001b[A\n",
      "batch 546, training loss: 3.1829: : 545it [06:58,  1.48s/it]\u001b[A\n",
      "batch 546, training loss: 3.1829: : 546it [06:58,  1.47s/it]\u001b[A\n",
      "batch 547, training loss: 3.0973: : 546it [07:00,  1.47s/it]\u001b[A\n",
      "batch 547, training loss: 3.0973: : 547it [07:00,  1.48s/it]\u001b[A\n",
      "batch 548, training loss: 2.8667: : 547it [07:00,  1.48s/it]\u001b[A\n",
      "batch 548, training loss: 2.8667: : 548it [07:00,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.2558: : 548it [07:02,  1.29s/it]\u001b[A\n",
      "batch 549, training loss: 3.2558: : 549it [07:02,  1.38s/it]\u001b[A\n",
      "batch 550, training loss: 3.2038: : 549it [07:04,  1.38s/it]\u001b[A\n",
      "batch 550, training loss: 3.2038: : 550it [07:04,  1.45s/it]\u001b[A\n",
      "batch 551, training loss: 3.2129: : 550it [07:05,  1.45s/it]\u001b[A\n",
      "batch 551, training loss: 3.2129: : 551it [07:05,  1.51s/it]\u001b[A\n",
      "batch 552, training loss: 3.2643: : 551it [07:07,  1.51s/it]\u001b[A\n",
      "batch 552, training loss: 3.2643: : 552it [07:07,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.2481: : 552it [07:08,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.2481: : 553it [07:08,  1.57s/it]\u001b[A\n",
      "batch 554, training loss: 2.9982: : 553it [07:10,  1.57s/it]\u001b[A\n",
      "batch 554, training loss: 2.9982: : 554it [07:10,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 3.1245: : 554it [07:12,  1.58s/it]\u001b[A\n",
      "batch 555, training loss: 3.1245: : 555it [07:12,  1.57s/it]\u001b[A\n",
      "batch 556, training loss: 3.3014: : 555it [07:13,  1.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 556, training loss: 3.3014: : 556it [07:13,  1.56s/it]\u001b[A\n",
      "batch 557, training loss: 3.1359: : 556it [07:15,  1.56s/it]\u001b[A\n",
      "batch 557, training loss: 3.1359: : 557it [07:15,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.0929: : 557it [07:16,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.0929: : 558it [07:16,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.1868: : 558it [07:18,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.1868: : 559it [07:18,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.1924: : 559it [07:19,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.1924: : 560it [07:19,  1.57s/it]\u001b[A\n",
      "batch 561, training loss: 3.1755: : 560it [07:21,  1.57s/it]\u001b[A\n",
      "batch 561, training loss: 3.1755: : 561it [07:21,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 3.0528: : 561it [07:23,  1.59s/it]\u001b[A\n",
      "batch 562, training loss: 3.0528: : 562it [07:23,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.1774: : 562it [07:24,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.1774: : 563it [07:24,  1.59s/it]\u001b[A\n",
      "batch 564, training loss: 3.0373: : 563it [07:26,  1.59s/it]\u001b[A\n",
      "batch 564, training loss: 3.0373: : 564it [07:26,  1.58s/it]\u001b[A\n",
      "batch 565, training loss: 3.1857: : 564it [07:27,  1.58s/it]\u001b[A\n",
      "batch 565, training loss: 3.1857: : 565it [07:27,  1.50s/it]\u001b[A\n",
      "batch 566, training loss: 3.2904: : 565it [07:29,  1.50s/it]\u001b[A\n",
      "batch 566, training loss: 3.2904: : 566it [07:29,  1.55s/it]\u001b[A\n",
      "batch 567, training loss: 3.2552: : 566it [07:30,  1.55s/it]\u001b[A\n",
      "batch 567, training loss: 3.2552: : 567it [07:30,  1.57s/it]\u001b[A\n",
      "batch 568, training loss: 3.3094: : 567it [07:32,  1.57s/it]\u001b[A\n",
      "batch 568, training loss: 3.3094: : 568it [07:32,  1.61s/it]\u001b[A\n",
      "batch 569, training loss: 3.2969: : 568it [07:34,  1.61s/it]\u001b[A\n",
      "batch 569, training loss: 3.2969: : 569it [07:34,  1.64s/it]\u001b[A\n",
      "batch 570, training loss: 3.4748: : 569it [07:35,  1.64s/it]\u001b[A\n",
      "batch 570, training loss: 3.4748: : 570it [07:35,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 3.2526: : 570it [07:37,  1.65s/it]\u001b[A\n",
      "batch 571, training loss: 3.2526: : 571it [07:37,  1.66s/it]\u001b[A\n",
      "batch 572, training loss: 3.3828: : 571it [07:39,  1.66s/it]\u001b[A\n",
      "batch 572, training loss: 3.3828: : 572it [07:39,  1.68s/it]\u001b[A\n",
      "batch 573, training loss: 3.2781: : 572it [07:40,  1.68s/it]\u001b[A\n",
      "batch 573, training loss: 3.2781: : 573it [07:40,  1.66s/it]\u001b[A\n",
      "batch 574, training loss: 3.3511: : 573it [07:42,  1.66s/it]\u001b[A\n",
      "batch 574, training loss: 3.3511: : 574it [07:42,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 2.7221: : 574it [07:43,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 2.7221: : 575it [07:43,  1.43s/it]\u001b[A\n",
      "batch 576, training loss: 3.3548: : 575it [07:45,  1.43s/it]\u001b[A\n",
      "batch 576, training loss: 3.3548: : 576it [07:45,  1.53s/it]\u001b[A\n",
      "batch 577, training loss: 3.2077: : 576it [07:46,  1.53s/it]\u001b[A\n",
      "batch 577, training loss: 3.2077: : 577it [07:46,  1.45s/it]\u001b[A\n",
      "batch 578, training loss: 3.1843: : 577it [07:48,  1.45s/it]\u001b[A\n",
      "batch 578, training loss: 3.1843: : 578it [07:48,  1.48s/it]\u001b[A\n",
      "batch 579, training loss: 3.2703: : 578it [07:49,  1.48s/it]\u001b[A\n",
      "batch 579, training loss: 3.2703: : 579it [07:49,  1.56s/it]\u001b[A\n",
      "batch 580, training loss: 3.1732: : 579it [07:51,  1.56s/it]\u001b[A\n",
      "batch 580, training loss: 3.1732: : 580it [07:51,  1.60s/it]\u001b[A\n",
      "batch 581, training loss: 3.1631: : 580it [07:53,  1.60s/it]\u001b[A\n",
      "batch 581, training loss: 3.1631: : 581it [07:53,  1.63s/it]\u001b[A\n",
      "batch 582, training loss: 3.2173: : 581it [07:54,  1.63s/it]\u001b[A\n",
      "batch 582, training loss: 3.2173: : 582it [07:54,  1.66s/it]\u001b[A\n",
      "batch 583, training loss: 2.2558: : 582it [07:55,  1.66s/it]\u001b[A\n",
      "batch 583, training loss: 2.2558: : 583it [07:55,  1.36s/it]\u001b[A\n",
      "batch 584, training loss: 3.379: : 583it [07:57,  1.36s/it] \u001b[A\n",
      "batch 584, training loss: 3.379: : 584it [07:57,  1.52s/it]\u001b[A\n",
      "batch 585, training loss: 3.3157: : 584it [07:59,  1.52s/it]\u001b[A\n",
      "batch 585, training loss: 3.3157: : 585it [07:59,  1.63s/it]\u001b[A\n",
      "batch 586, training loss: 3.4094: : 585it [08:01,  1.63s/it]\u001b[A\n",
      "batch 586, training loss: 3.4094: : 586it [08:01,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 3.2922: : 586it [08:03,  1.71s/it]\u001b[A\n",
      "batch 587, training loss: 3.2922: : 587it [08:03,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.1896: : 587it [08:05,  1.77s/it]\u001b[A\n",
      "batch 588, training loss: 3.1896: : 588it [08:05,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.2689: : 588it [08:07,  1.80s/it]\u001b[A\n",
      "batch 589, training loss: 3.2689: : 589it [08:07,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.2703: : 589it [08:08,  1.85s/it]\u001b[A\n",
      "batch 590, training loss: 3.2703: : 590it [08:08,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.1833: : 590it [08:10,  1.87s/it]\u001b[A\n",
      "batch 591, training loss: 3.1833: : 591it [08:10,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.1888: : 591it [08:12,  1.90s/it]\u001b[A\n",
      "batch 592, training loss: 3.1888: : 592it [08:12,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.1224: : 592it [08:14,  1.87s/it]\u001b[A\n",
      "batch 593, training loss: 3.1224: : 593it [08:14,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 3.2637: : 593it [08:16,  1.90s/it]\u001b[A\n",
      "batch 594, training loss: 3.2637: : 594it [08:16,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 3.3086: : 594it [08:18,  1.96s/it]\u001b[A\n",
      "batch 595, training loss: 3.3086: : 595it [08:18,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.3261: : 595it [08:20,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.3261: : 596it [08:20,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.1299: : 596it [08:22,  1.95s/it]\u001b[A\n",
      "batch 597, training loss: 3.1299: : 597it [08:22,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 3.3267: : 597it [08:24,  1.97s/it]\u001b[A\n",
      "batch 598, training loss: 3.3267: : 598it [08:24,  2.07s/it]\u001b[A\n",
      "batch 599, training loss: 3.1455: : 598it [08:26,  2.07s/it]\u001b[A\n",
      "batch 599, training loss: 3.1455: : 599it [08:26,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.1708: : 599it [08:28,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.1708: : 600it [08:28,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 2.0937: : 600it [08:29,  2.01s/it]\u001b[A\n",
      "batch 601, training loss: 2.0937: : 601it [08:29,  1.66s/it]\u001b[A\n",
      "batch 602, training loss: 3.1598: : 601it [08:31,  1.66s/it]\u001b[A\n",
      "batch 602, training loss: 3.1598: : 602it [08:31,  1.78s/it]\u001b[A\n",
      "batch 603, training loss: 3.181: : 602it [08:33,  1.78s/it] \u001b[A\n",
      "batch 603, training loss: 3.181: : 603it [08:33,  1.77s/it]\u001b[A\n",
      "batch 604, training loss: 3.1245: : 603it [08:35,  1.77s/it]\u001b[A\n",
      "batch 604, training loss: 3.1245: : 604it [08:35,  1.80s/it]\u001b[A\n",
      "batch 605, training loss: 3.2634: : 604it [08:36,  1.80s/it]\u001b[A\n",
      "batch 605, training loss: 3.2634: : 605it [08:36,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 2.9924: : 605it [08:38,  1.78s/it]\u001b[A\n",
      "batch 606, training loss: 2.9924: : 606it [08:38,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 2.8514: : 606it [08:40,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 2.8514: : 607it [08:40,  1.69s/it]\u001b[A\n",
      "batch 608, training loss: 2.955: : 607it [08:41,  1.69s/it] \u001b[A\n",
      "batch 608, training loss: 2.955: : 608it [08:41,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 2.9469: : 608it [08:42,  1.57s/it]\u001b[A\n",
      "batch 609, training loss: 2.9469: : 609it [08:42,  1.51s/it]\u001b[A\n",
      "batch 610, training loss: 2.3255: : 609it [08:44,  1.51s/it]\u001b[A\n",
      "batch 610, training loss: 2.3255: : 610it [08:44,  1.45s/it]\u001b[A\n",
      "batch 611, training loss: 2.403: : 610it [08:45,  1.45s/it] \u001b[A\n",
      "batch 611, training loss: 2.403: : 611it [08:45,  1.38s/it]\u001b[A\n",
      "batch 612, training loss: 1.6139: : 611it [08:46,  1.38s/it]\u001b[A\n",
      "batch 612, training loss: 1.6139: : 612it [08:46,  1.30s/it]\u001b[A\n",
      "batch 613, training loss: 2.1419: : 612it [08:47,  1.30s/it]\u001b[A\n",
      "batch 613, training loss: 2.1419: : 613it [08:47,  1.24s/it]\u001b[A\n",
      "batch 613, training loss: 2.1419: : 616it [08:48,  1.17it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.6328: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.6328: : 1it [00:00,  3.29it/s]\u001b[A\n",
      "batch 1, dev loss: 3.8104: : 1it [00:00,  3.29it/s]\u001b[A\n",
      "batch 1, dev loss: 3.8104: : 2it [00:00,  3.96it/s]\u001b[A\n",
      "batch 2, dev loss: 3.4004: : 2it [00:00,  3.96it/s]\u001b[A\n",
      "batch 2, dev loss: 3.4004: : 3it [00:00,  4.13it/s]\u001b[A\n",
      "batch 3, dev loss: 3.5881: : 3it [00:00,  4.13it/s]\u001b[A\n",
      "batch 3, dev loss: 3.5881: : 4it [00:00,  4.42it/s]\u001b[A\n",
      "batch 4, dev loss: 3.5647: : 4it [00:01,  4.42it/s]\u001b[A\n",
      "batch 4, dev loss: 3.5647: : 5it [00:01,  4.80it/s]\u001b[A\n",
      "batch 5, dev loss: 3.5622: : 5it [00:01,  4.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5, dev loss: 3.5622: : 6it [00:01,  4.78it/s]\u001b[A\n",
      "batch 6, dev loss: 3.6392: : 6it [00:01,  4.78it/s]\u001b[A\n",
      "batch 6, dev loss: 3.6392: : 7it [00:01,  4.89it/s]\u001b[A\n",
      "batch 7, dev loss: 3.4386: : 7it [00:01,  4.89it/s]\u001b[A\n",
      "batch 7, dev loss: 3.4386: : 8it [00:01,  5.23it/s]\u001b[A\n",
      "batch 8, dev loss: 3.6003: : 8it [00:01,  5.23it/s]\u001b[A\n",
      "batch 8, dev loss: 3.6003: : 9it [00:01,  5.14it/s]\u001b[A\n",
      "batch 9, dev loss: 3.5364: : 9it [00:02,  5.14it/s]\u001b[A\n",
      "batch 9, dev loss: 3.5364: : 10it [00:02,  4.75it/s]\u001b[A\n",
      "batch 10, dev loss: 3.5901: : 10it [00:02,  4.75it/s]\u001b[A\n",
      "batch 10, dev loss: 3.5901: : 11it [00:02,  4.76it/s]\u001b[A\n",
      "batch 11, dev loss: 3.6088: : 11it [00:02,  4.76it/s]\u001b[A\n",
      "batch 11, dev loss: 3.6088: : 12it [00:02,  4.58it/s]\u001b[A\n",
      "batch 12, dev loss: 3.4521: : 12it [00:02,  4.58it/s]\u001b[A\n",
      "batch 12, dev loss: 3.4521: : 13it [00:02,  4.55it/s]\u001b[A\n",
      "batch 13, dev loss: 3.6146: : 13it [00:03,  4.55it/s]\u001b[A\n",
      "batch 13, dev loss: 3.6146: : 14it [00:03,  4.69it/s]\u001b[A\n",
      "batch 14, dev loss: 3.7638: : 14it [00:03,  4.69it/s]\u001b[A\n",
      "batch 14, dev loss: 3.7638: : 15it [00:03,  4.53it/s]\u001b[A\n",
      "batch 15, dev loss: 3.5155: : 15it [00:03,  4.53it/s]\u001b[A\n",
      "batch 15, dev loss: 3.5155: : 16it [00:03,  5.03it/s]\u001b[A\n",
      "batch 16, dev loss: 3.8539: : 16it [00:03,  5.03it/s]\u001b[A\n",
      "batch 16, dev loss: 3.8539: : 17it [00:03,  4.50it/s]\u001b[A\n",
      "batch 17, dev loss: 3.6401: : 17it [00:03,  4.50it/s]\u001b[A\n",
      "batch 17, dev loss: 3.6401: : 18it [00:03,  4.06it/s]\u001b[A\n",
      "batch 18, dev loss: 3.5329: : 18it [00:04,  4.06it/s]\u001b[A\n",
      "batch 18, dev loss: 3.5329: : 19it [00:04,  3.78it/s]\u001b[A\n",
      "batch 19, dev loss: 3.7149: : 19it [00:04,  3.78it/s]\u001b[A\n",
      "batch 19, dev loss: 3.7149: : 20it [00:04,  3.87it/s]\u001b[A\n",
      "batch 20, dev loss: 3.5811: : 20it [00:04,  3.87it/s]\u001b[A\n",
      "batch 20, dev loss: 3.5811: : 21it [00:04,  3.86it/s]\u001b[A\n",
      "batch 21, dev loss: 3.4295: : 21it [00:05,  3.86it/s]\u001b[A\n",
      "batch 21, dev loss: 3.4295: : 22it [00:05,  3.64it/s]\u001b[A\n",
      "batch 22, dev loss: 3.6376: : 22it [00:05,  3.64it/s]\u001b[A\n",
      "batch 22, dev loss: 3.6376: : 23it [00:05,  3.58it/s]\u001b[A\n",
      "batch 23, dev loss: 3.7131: : 23it [00:05,  3.58it/s]\u001b[A\n",
      "batch 23, dev loss: 3.7131: : 24it [00:05,  4.07it/s]\u001b[A\n",
      "batch 24, dev loss: 3.6128: : 24it [00:05,  4.07it/s]\u001b[A\n",
      "batch 24, dev loss: 3.6128: : 25it [00:05,  3.77it/s]\u001b[A\n",
      "batch 25, dev loss: 3.5423: : 25it [00:06,  3.77it/s]\u001b[A\n",
      "batch 25, dev loss: 3.5423: : 26it [00:06,  3.59it/s]\u001b[A\n",
      "batch 26, dev loss: 3.5322: : 26it [00:06,  3.59it/s]\u001b[A\n",
      "batch 26, dev loss: 3.5322: : 27it [00:06,  3.43it/s]\u001b[A\n",
      "batch 27, dev loss: 3.4426: : 27it [00:06,  3.43it/s]\u001b[A\n",
      "batch 27, dev loss: 3.4426: : 28it [00:06,  3.37it/s]\u001b[A\n",
      "batch 28, dev loss: 3.7264: : 28it [00:07,  3.37it/s]\u001b[A\n",
      "batch 28, dev loss: 3.7264: : 29it [00:07,  3.32it/s]\u001b[A\n",
      "batch 29, dev loss: 3.6174: : 29it [00:07,  3.32it/s]\u001b[A\n",
      "batch 29, dev loss: 3.6174: : 30it [00:07,  3.43it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0368: : 30it [00:07,  3.43it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0368: : 31it [00:07,  4.07it/s]\u001b[A\n",
      "batch 31, dev loss: 3.62: : 31it [00:07,  4.07it/s]  \u001b[A\n",
      "batch 31, dev loss: 3.62: : 32it [00:07,  3.57it/s]\u001b[A\n",
      "batch 32, dev loss: 3.7203: : 32it [00:08,  3.57it/s]\u001b[A\n",
      "batch 32, dev loss: 3.7203: : 33it [00:08,  3.57it/s]\u001b[A\n",
      "batch 33, dev loss: 3.4443: : 33it [00:08,  3.57it/s]\u001b[A\n",
      "batch 33, dev loss: 3.4443: : 34it [00:08,  3.42it/s]\u001b[A\n",
      "batch 34, dev loss: 3.8658: : 34it [00:08,  3.42it/s]\u001b[A\n",
      "batch 34, dev loss: 3.8658: : 35it [00:08,  3.25it/s]\u001b[A\n",
      "batch 35, dev loss: 3.7164: : 35it [00:09,  3.25it/s]\u001b[A\n",
      "batch 35, dev loss: 3.7164: : 36it [00:09,  3.13it/s]\u001b[A\n",
      "batch 36, dev loss: 3.6144: : 36it [00:09,  3.13it/s]\u001b[A\n",
      "batch 36, dev loss: 3.6144: : 37it [00:09,  3.29it/s]\u001b[A\n",
      "batch 37, dev loss: 3.3826: : 37it [00:09,  3.29it/s]\u001b[A\n",
      "batch 37, dev loss: 3.3826: : 38it [00:09,  3.04it/s]\u001b[A\n",
      "batch 38, dev loss: 3.6638: : 38it [00:10,  3.04it/s]\u001b[A\n",
      "batch 38, dev loss: 3.6638: : 39it [00:10,  2.96it/s]\u001b[A\n",
      "batch 39, dev loss: 3.6475: : 39it [00:10,  2.96it/s]\u001b[A\n",
      "batch 39, dev loss: 3.6475: : 40it [00:10,  2.82it/s]\u001b[A\n",
      "batch 40, dev loss: 3.6941: : 40it [00:10,  2.82it/s]\u001b[A\n",
      "batch 40, dev loss: 3.6941: : 41it [00:10,  2.76it/s]\u001b[A\n",
      "batch 41, dev loss: 3.4769: : 41it [00:11,  2.76it/s]\u001b[A\n",
      "batch 41, dev loss: 3.4769: : 42it [00:11,  3.07it/s]\u001b[A\n",
      "batch 42, dev loss: 3.6108: : 42it [00:11,  3.07it/s]\u001b[A\n",
      "batch 42, dev loss: 3.6108: : 43it [00:11,  2.85it/s]\u001b[A\n",
      "batch 43, dev loss: 3.6084: : 43it [00:12,  2.85it/s]\u001b[A\n",
      "batch 43, dev loss: 3.6084: : 44it [00:12,  2.65it/s]\u001b[A\n",
      "batch 44, dev loss: 3.5358: : 44it [00:12,  2.65it/s]\u001b[A\n",
      "batch 44, dev loss: 3.5358: : 45it [00:12,  2.61it/s]\u001b[A\n",
      "batch 45, dev loss: 3.8139: : 45it [00:12,  2.61it/s]\u001b[A\n",
      "batch 45, dev loss: 3.8139: : 46it [00:12,  2.57it/s]\u001b[A\n",
      "batch 46, dev loss: 3.3751: : 46it [00:13,  2.57it/s]\u001b[A\n",
      "batch 46, dev loss: 3.3751: : 47it [00:13,  2.50it/s]\u001b[A\n",
      "batch 47, dev loss: 3.5359: : 47it [00:13,  2.50it/s]\u001b[A\n",
      "batch 47, dev loss: 3.5359: : 48it [00:13,  2.44it/s]\u001b[A\n",
      "batch 48, dev loss: 3.4041: : 48it [00:14,  2.44it/s]\u001b[A\n",
      "batch 48, dev loss: 3.4041: : 49it [00:14,  2.49it/s]\u001b[A\n",
      "batch 49, dev loss: 3.5415: : 49it [00:14,  2.49it/s]\u001b[A\n",
      "batch 49, dev loss: 3.5415: : 50it [00:14,  2.82it/s]\u001b[A\n",
      "batch 50, dev loss: 3.4928: : 50it [00:14,  2.82it/s]\u001b[A\n",
      "batch 50, dev loss: 3.4928: : 51it [00:14,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 3.5308: : 51it [00:15,  2.58it/s]\u001b[A\n",
      "batch 51, dev loss: 3.5308: : 52it [00:15,  2.45it/s]\u001b[A\n",
      "batch 52, dev loss: 3.3131: : 52it [00:15,  2.45it/s]\u001b[A\n",
      "batch 52, dev loss: 3.3131: : 53it [00:15,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.5231: : 53it [00:16,  2.56it/s]\u001b[A\n",
      "batch 53, dev loss: 3.5231: : 54it [00:16,  2.43it/s]\u001b[A\n",
      "batch 54, dev loss: 3.2591: : 54it [00:16,  2.43it/s]\u001b[A\n",
      "batch 54, dev loss: 3.2591: : 55it [00:16,  2.36it/s]\u001b[A\n",
      "batch 55, dev loss: 3.463: : 55it [00:17,  2.36it/s] \u001b[A\n",
      "batch 55, dev loss: 3.463: : 56it [00:17,  2.16it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3247: : 56it [00:17,  2.16it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3247: : 57it [00:17,  2.27it/s]\u001b[A\n",
      "batch 57, dev loss: 3.2724: : 57it [00:17,  2.27it/s]\u001b[A\n",
      "batch 57, dev loss: 3.2724: : 58it [00:17,  2.23it/s]\u001b[A\n",
      "batch 58, dev loss: 3.659: : 58it [00:18,  2.23it/s] \u001b[A\n",
      "batch 58, dev loss: 3.659: : 59it [00:18,  2.19it/s]\u001b[A\n",
      "batch 59, dev loss: 3.4868: : 59it [00:18,  2.19it/s]\u001b[A\n",
      "batch 59, dev loss: 3.4868: : 60it [00:18,  2.24it/s]\u001b[A\n",
      "batch 60, dev loss: 3.2862: : 60it [00:19,  2.24it/s]\u001b[A\n",
      "batch 60, dev loss: 3.2862: : 61it [00:19,  2.30it/s]\u001b[A\n",
      "batch 61, dev loss: 3.2284: : 61it [00:19,  2.30it/s]\u001b[A\n",
      "batch 61, dev loss: 3.2284: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 2.9498: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 2.9498: : 63it [00:19,  2.57it/s]\u001b[A\n",
      "batch 63, dev loss: 3.5818: : 63it [00:20,  2.57it/s]\u001b[A\n",
      "batch 63, dev loss: 3.5818: : 64it [00:20,  2.74it/s]\u001b[A\n",
      "batch 64, dev loss: 3.3571: : 64it [00:20,  2.74it/s]\u001b[A\n",
      "batch 64, dev loss: 3.3571: : 65it [00:20,  2.71it/s]\u001b[A\n",
      "batch 65, dev loss: 3.2649: : 65it [00:20,  2.71it/s]\u001b[A\n",
      "batch 65, dev loss: 3.2649: : 66it [00:20,  2.86it/s]\u001b[A\n",
      "batch 66, dev loss: 3.4816: : 66it [00:21,  2.86it/s]\u001b[A\n",
      "batch 66, dev loss: 3.4816: : 67it [00:21,  2.77it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5947: : 67it [00:21,  2.77it/s]\u001b[A\n",
      "batch 67, dev loss: 2.5947: : 68it [00:21,  2.92it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5158: : 68it [00:21,  2.92it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5158: : 69it [00:21,  3.06it/s]\u001b[A\n",
      "batch 69, dev loss: 2.8724: : 69it [00:22,  3.06it/s]\u001b[A\n",
      "batch 69, dev loss: 2.8724: : 70it [00:22,  2.98it/s]\u001b[A\n",
      "batch 70, dev loss: 3.8373: : 70it [00:22,  2.98it/s]\u001b[A\n",
      "batch 70, dev loss: 3.8373: : 71it [00:22,  2.92it/s]\u001b[A\n",
      "batch 71, dev loss: 3.0846: : 71it [00:22,  2.92it/s]\u001b[A\n",
      "batch 71, dev loss: 3.0846: : 72it [00:22,  2.94it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9146: : 72it [00:23,  2.94it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9146: : 73it [00:23,  3.03it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9146: : 76it [00:23,  3.23it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.09s/it]\u001b[A\n",
      "2it [00:02,  1.25s/it]\u001b[A\n",
      "3it [00:03,  1.32s/it]\u001b[A\n",
      "4it [00:04,  1.04s/it]\u001b[A\n",
      "5it [00:04,  1.17it/s]\u001b[A\n",
      "6it [00:05,  1.25it/s]\u001b[A\n",
      "7it [00:07,  1.03it/s]\u001b[A\n",
      "8it [00:07,  1.04it/s]\u001b[A\n",
      "9it [00:09,  1.26s/it]\u001b[A\n",
      "10it [00:11,  1.33s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11it [00:12,  1.34s/it]\u001b[A\n",
      "12it [00:14,  1.42s/it]\u001b[A\n",
      "13it [00:15,  1.31s/it]\u001b[A\n",
      "14it [00:17,  1.48s/it]\u001b[A\n",
      "15it [00:19,  1.58s/it]\u001b[A\n",
      "16it [00:19,  1.31s/it]\u001b[A\n",
      "17it [00:22,  1.60s/it]\u001b[A\n",
      "18it [00:24,  1.75s/it]\u001b[A\n",
      "19it [00:25,  1.78s/it]\u001b[A\n",
      "20it [00:27,  1.73s/it]\u001b[A\n",
      "21it [00:29,  1.90s/it]\u001b[A\n",
      "22it [00:31,  1.81s/it]\u001b[A\n",
      "23it [00:33,  1.87s/it]\u001b[A\n",
      "24it [00:34,  1.46s/it]\u001b[A\n",
      "25it [00:36,  1.73s/it]\u001b[A\n",
      "26it [00:38,  1.85s/it]\u001b[A\n",
      "27it [00:40,  1.98s/it]\u001b[A\n",
      "28it [00:43,  2.15s/it]\u001b[A\n",
      "29it [00:45,  2.13s/it]\u001b[A\n",
      "30it [00:47,  2.16s/it]\u001b[A\n",
      "31it [00:49,  2.19s/it]\u001b[A\n",
      "32it [00:52,  2.37s/it]\u001b[A\n",
      "33it [00:55,  2.40s/it]\u001b[A\n",
      "34it [00:57,  2.39s/it]\u001b[A\n",
      "35it [01:00,  2.43s/it]\u001b[A\n",
      "36it [01:00,  1.86s/it]\u001b[A\n",
      "37it [01:03,  2.19s/it]\u001b[A\n",
      "38it [01:06,  2.50s/it]\u001b[A\n",
      "39it [01:09,  2.53s/it]\u001b[A\n",
      "40it [01:12,  2.63s/it]\u001b[A\n",
      "41it [01:12,  2.07s/it]\u001b[A\n",
      "42it [01:16,  2.51s/it]\u001b[A\n",
      "43it [01:18,  2.43s/it]\u001b[A\n",
      "44it [01:20,  2.26s/it]\u001b[A\n",
      "45it [01:23,  2.33s/it]\u001b[A\n",
      "46it [01:26,  2.75s/it]\u001b[A\n",
      "47it [01:30,  3.07s/it]\u001b[A\n",
      "48it [01:35,  3.45s/it]\u001b[A\n",
      "49it [01:35,  2.48s/it]\u001b[A\n",
      "50it [01:39,  2.93s/it]\u001b[A\n",
      "51it [01:43,  3.21s/it]\u001b[A\n",
      "52it [01:44,  2.81s/it]\u001b[A\n",
      "53it [01:48,  3.17s/it]\u001b[A\n",
      "54it [01:52,  3.36s/it]\u001b[A\n",
      "55it [01:56,  3.59s/it]\u001b[A\n",
      "56it [01:58,  3.13s/it]\u001b[A\n",
      "57it [02:03,  3.42s/it]\u001b[A\n",
      "58it [02:06,  3.42s/it]\u001b[A\n",
      "59it [02:09,  3.16s/it]\u001b[A\n",
      "60it [02:11,  2.94s/it]\u001b[A\n",
      "61it [02:13,  2.61s/it]\u001b[A\n",
      "62it [02:14,  2.16s/it]\u001b[A\n",
      "63it [02:15,  1.85s/it]\u001b[A\n",
      "64it [02:16,  1.51s/it]\u001b[A\n",
      "65it [02:16,  1.22s/it]\u001b[A\n",
      "66it [02:17,  1.00s/it]\u001b[A\n",
      "67it [02:17,  1.23it/s]\u001b[A\n",
      "68it [02:18,  1.40it/s]\u001b[A\n",
      "69it [02:18,  1.50it/s]\u001b[A\n",
      "70it [02:19,  1.99s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|███▌                                  | 638/6740 [00:00<00:00, 6377.41it/s]\u001b[A\n",
      " 19%|███████▏                             | 1299/6740 [00:00<00:00, 6511.13it/s]\u001b[A\n",
      " 30%|██████████▉                          | 2001/6740 [00:00<00:00, 6738.67it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 2675/6740 [00:00<00:00, 6631.62it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 3339/6740 [00:00<00:00, 6603.32it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 4000/6740 [00:00<00:00, 6546.82it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 4662/6740 [00:00<00:00, 6565.55it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 5319/6740 [00:00<00:00, 6425.34it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 5963/6740 [00:00<00:00, 6414.70it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 6497.15it/s]\u001b[A\n",
      "Epoch: 14, tfr: 1.0, loss(train/dev): 3.2992/3.5137, ppl(dev/test): 33.5723/39.0\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.349: : 0it [00:02, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.349: : 1it [00:02,  2.02s/it]\u001b[A\n",
      "batch 2, training loss: 3.2273: : 1it [00:02,  2.02s/it]\u001b[A\n",
      "batch 2, training loss: 3.2273: : 2it [00:02,  1.22s/it]\u001b[A\n",
      "batch 3, training loss: 3.2689: : 2it [00:03,  1.22s/it]\u001b[A\n",
      "batch 3, training loss: 3.2689: : 3it [00:03,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 3.2989: : 3it [00:04,  1.03it/s]\u001b[A\n",
      "batch 4, training loss: 3.2989: : 4it [00:04,  1.17it/s]\u001b[A\n",
      "batch 5, training loss: 3.0966: : 4it [00:04,  1.17it/s]\u001b[A\n",
      "batch 5, training loss: 3.0966: : 5it [00:04,  1.25it/s]\u001b[A\n",
      "batch 6, training loss: 3.3922: : 5it [00:05,  1.25it/s]\u001b[A\n",
      "batch 6, training loss: 3.3922: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 3.3017: : 6it [00:05,  1.37it/s]\u001b[A\n",
      "batch 7, training loss: 3.3017: : 7it [00:05,  1.45it/s]\u001b[A\n",
      "batch 8, training loss: 3.3635: : 7it [00:06,  1.45it/s]\u001b[A\n",
      "batch 8, training loss: 3.3635: : 8it [00:06,  1.51it/s]\u001b[A\n",
      "batch 9, training loss: 3.2398: : 8it [00:07,  1.51it/s]\u001b[A\n",
      "batch 9, training loss: 3.2398: : 9it [00:07,  1.50it/s]\u001b[A\n",
      "batch 10, training loss: 3.2678: : 9it [00:07,  1.50it/s]\u001b[A\n",
      "batch 10, training loss: 3.2678: : 10it [00:07,  1.47it/s]\u001b[A\n",
      "batch 11, training loss: 3.2583: : 10it [00:08,  1.47it/s]\u001b[A\n",
      "batch 11, training loss: 3.2583: : 11it [00:08,  1.46it/s]\u001b[A\n",
      "batch 12, training loss: 3.2875: : 11it [00:09,  1.46it/s]\u001b[A\n",
      "batch 12, training loss: 3.2875: : 12it [00:09,  1.59it/s]\u001b[A\n",
      "batch 13, training loss: 3.1283: : 12it [00:09,  1.59it/s]\u001b[A\n",
      "batch 13, training loss: 3.1283: : 13it [00:09,  1.62it/s]\u001b[A\n",
      "batch 14, training loss: 3.4547: : 13it [00:10,  1.62it/s]\u001b[A\n",
      "batch 14, training loss: 3.4547: : 14it [00:10,  1.61it/s]\u001b[A\n",
      "batch 15, training loss: 3.3138: : 14it [00:11,  1.61it/s]\u001b[A\n",
      "batch 15, training loss: 3.3138: : 15it [00:11,  1.58it/s]\u001b[A\n",
      "batch 16, training loss: 3.3152: : 15it [00:11,  1.58it/s]\u001b[A\n",
      "batch 16, training loss: 3.3152: : 16it [00:11,  1.54it/s]\u001b[A\n",
      "batch 17, training loss: 3.4252: : 16it [00:12,  1.54it/s]\u001b[A\n",
      "batch 17, training loss: 3.4252: : 17it [00:12,  1.50it/s]\u001b[A\n",
      "batch 18, training loss: 3.2829: : 17it [00:12,  1.50it/s]\u001b[A\n",
      "batch 18, training loss: 3.2829: : 18it [00:12,  1.55it/s]\u001b[A\n",
      "batch 19, training loss: 3.0817: : 18it [00:13,  1.55it/s]\u001b[A\n",
      "batch 19, training loss: 3.0817: : 19it [00:13,  1.57it/s]\u001b[A\n",
      "batch 20, training loss: 3.2022: : 19it [00:14,  1.57it/s]\u001b[A\n",
      "batch 20, training loss: 3.2022: : 20it [00:14,  1.76it/s]\u001b[A\n",
      "batch 21, training loss: 3.3392: : 20it [00:14,  1.76it/s]\u001b[A\n",
      "batch 21, training loss: 3.3392: : 21it [00:14,  1.89it/s]\u001b[A\n",
      "batch 22, training loss: 3.1001: : 21it [00:15,  1.89it/s]\u001b[A\n",
      "batch 22, training loss: 3.1001: : 22it [00:15,  1.82it/s]\u001b[A\n",
      "batch 23, training loss: 3.2629: : 22it [00:15,  1.82it/s]\u001b[A\n",
      "batch 23, training loss: 3.2629: : 23it [00:15,  1.71it/s]\u001b[A\n",
      "batch 24, training loss: 3.1938: : 23it [00:16,  1.71it/s]\u001b[A\n",
      "batch 24, training loss: 3.1938: : 24it [00:16,  1.60it/s]\u001b[A\n",
      "batch 25, training loss: 3.2899: : 24it [00:17,  1.60it/s]\u001b[A\n",
      "batch 25, training loss: 3.2899: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.1078: : 25it [00:17,  1.57it/s]\u001b[A\n",
      "batch 26, training loss: 3.1078: : 26it [00:17,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.2626: : 26it [00:18,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.2626: : 27it [00:18,  1.70it/s]\u001b[A\n",
      "batch 28, training loss: 3.0776: : 27it [00:18,  1.70it/s]\u001b[A\n",
      "batch 28, training loss: 3.0776: : 28it [00:18,  1.65it/s]\u001b[A\n",
      "batch 29, training loss: 3.2494: : 28it [00:19,  1.65it/s]\u001b[A\n",
      "batch 29, training loss: 3.2494: : 29it [00:19,  1.59it/s]\u001b[A\n",
      "batch 30, training loss: 3.3591: : 29it [00:20,  1.59it/s]\u001b[A\n",
      "batch 30, training loss: 3.3591: : 30it [00:20,  1.57it/s]\u001b[A\n",
      "batch 31, training loss: 3.1199: : 30it [00:20,  1.57it/s]\u001b[A\n",
      "batch 31, training loss: 3.1199: : 31it [00:20,  1.59it/s]\u001b[A\n",
      "batch 32, training loss: 3.3028: : 31it [00:21,  1.59it/s]\u001b[A\n",
      "batch 32, training loss: 3.3028: : 32it [00:21,  1.59it/s]\u001b[A\n",
      "batch 33, training loss: 3.19: : 32it [00:22,  1.59it/s]  \u001b[A\n",
      "batch 33, training loss: 3.19: : 33it [00:22,  1.59it/s]\u001b[A\n",
      "batch 34, training loss: 3.2048: : 33it [00:22,  1.59it/s]\u001b[A\n",
      "batch 34, training loss: 3.2048: : 34it [00:22,  1.56it/s]\u001b[A\n",
      "batch 35, training loss: 3.2642: : 34it [00:23,  1.56it/s]\u001b[A\n",
      "batch 35, training loss: 3.2642: : 35it [00:23,  1.50it/s]\u001b[A\n",
      "batch 36, training loss: 3.3364: : 35it [00:24,  1.50it/s]\u001b[A\n",
      "batch 36, training loss: 3.3364: : 36it [00:24,  1.49it/s]\u001b[A\n",
      "batch 37, training loss: 3.1893: : 36it [00:24,  1.49it/s]\u001b[A\n",
      "batch 37, training loss: 3.1893: : 37it [00:24,  1.61it/s]\u001b[A\n",
      "batch 38, training loss: 2.9969: : 37it [00:25,  1.61it/s]\u001b[A\n",
      "batch 38, training loss: 2.9969: : 38it [00:25,  1.65it/s]\u001b[A\n",
      "batch 39, training loss: 3.1797: : 38it [00:25,  1.65it/s]\u001b[A\n",
      "batch 39, training loss: 3.1797: : 39it [00:25,  1.61it/s]\u001b[A\n",
      "batch 40, training loss: 3.1993: : 39it [00:26,  1.61it/s]\u001b[A\n",
      "batch 40, training loss: 3.1993: : 40it [00:26,  1.57it/s]\u001b[A\n",
      "batch 41, training loss: 3.382: : 40it [00:27,  1.57it/s] \u001b[A\n",
      "batch 41, training loss: 3.382: : 41it [00:27,  1.55it/s]\u001b[A\n",
      "batch 42, training loss: 3.2189: : 41it [00:27,  1.55it/s]\u001b[A\n",
      "batch 42, training loss: 3.2189: : 42it [00:27,  1.56it/s]\u001b[A\n",
      "batch 43, training loss: 3.0474: : 42it [00:28,  1.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 3.0474: : 43it [00:28,  1.56it/s]\u001b[A\n",
      "batch 44, training loss: 2.999: : 43it [00:29,  1.56it/s] \u001b[A\n",
      "batch 44, training loss: 2.999: : 44it [00:29,  1.59it/s]\u001b[A\n",
      "batch 45, training loss: 3.0528: : 44it [00:29,  1.59it/s]\u001b[A\n",
      "batch 45, training loss: 3.0528: : 45it [00:29,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 3.1704: : 45it [00:30,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 3.1704: : 46it [00:30,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.1845: : 46it [00:31,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.1845: : 47it [00:31,  1.49it/s]\u001b[A\n",
      "batch 48, training loss: 3.0559: : 47it [00:31,  1.49it/s]\u001b[A\n",
      "batch 48, training loss: 3.0559: : 48it [00:31,  1.51it/s]\u001b[A\n",
      "batch 49, training loss: 3.3608: : 48it [00:32,  1.51it/s]\u001b[A\n",
      "batch 49, training loss: 3.3608: : 49it [00:32,  1.58it/s]\u001b[A\n",
      "batch 50, training loss: 3.1156: : 49it [00:32,  1.58it/s]\u001b[A\n",
      "batch 50, training loss: 3.1156: : 50it [00:32,  1.65it/s]\u001b[A\n",
      "batch 51, training loss: 3.2077: : 50it [00:33,  1.65it/s]\u001b[A\n",
      "batch 51, training loss: 3.2077: : 51it [00:33,  1.61it/s]\u001b[A\n",
      "batch 52, training loss: 3.0408: : 51it [00:34,  1.61it/s]\u001b[A\n",
      "batch 52, training loss: 3.0408: : 52it [00:34,  1.58it/s]\u001b[A\n",
      "batch 53, training loss: 3.0781: : 52it [00:34,  1.58it/s]\u001b[A\n",
      "batch 53, training loss: 3.0781: : 53it [00:34,  1.70it/s]\u001b[A\n",
      "batch 54, training loss: 3.0252: : 53it [00:35,  1.70it/s]\u001b[A\n",
      "batch 54, training loss: 3.0252: : 54it [00:35,  1.71it/s]\u001b[A\n",
      "batch 55, training loss: 3.1463: : 54it [00:35,  1.71it/s]\u001b[A\n",
      "batch 55, training loss: 3.1463: : 55it [00:35,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.1596: : 55it [00:36,  1.65it/s]\u001b[A\n",
      "batch 56, training loss: 3.1596: : 56it [00:36,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 3.1196: : 56it [00:37,  1.58it/s]\u001b[A\n",
      "batch 57, training loss: 3.1196: : 57it [00:37,  1.54it/s]\u001b[A\n",
      "batch 58, training loss: 3.1984: : 57it [00:37,  1.54it/s]\u001b[A\n",
      "batch 58, training loss: 3.1984: : 58it [00:37,  1.55it/s]\u001b[A\n",
      "batch 59, training loss: 3.0821: : 58it [00:38,  1.55it/s]\u001b[A\n",
      "batch 59, training loss: 3.0821: : 59it [00:38,  1.57it/s]\u001b[A\n",
      "batch 60, training loss: 3.0331: : 59it [00:39,  1.57it/s]\u001b[A\n",
      "batch 60, training loss: 3.0331: : 60it [00:39,  1.63it/s]\u001b[A\n",
      "batch 61, training loss: 3.2639: : 60it [00:39,  1.63it/s]\u001b[A\n",
      "batch 61, training loss: 3.2639: : 61it [00:39,  1.76it/s]\u001b[A\n",
      "batch 62, training loss: 3.1177: : 61it [00:39,  1.76it/s]\u001b[A\n",
      "batch 62, training loss: 3.1177: : 62it [00:39,  1.91it/s]\u001b[A\n",
      "batch 63, training loss: 3.1786: : 62it [00:40,  1.91it/s]\u001b[A\n",
      "batch 63, training loss: 3.1786: : 63it [00:40,  1.96it/s]\u001b[A\n",
      "batch 64, training loss: 3.1694: : 63it [00:40,  1.96it/s]\u001b[A\n",
      "batch 64, training loss: 3.1694: : 64it [00:40,  2.10it/s]\u001b[A\n",
      "batch 65, training loss: 3.2021: : 64it [00:41,  2.10it/s]\u001b[A\n",
      "batch 65, training loss: 3.2021: : 65it [00:41,  1.96it/s]\u001b[A\n",
      "batch 66, training loss: 3.2481: : 65it [00:42,  1.96it/s]\u001b[A\n",
      "batch 66, training loss: 3.2481: : 66it [00:42,  1.82it/s]\u001b[A\n",
      "batch 67, training loss: 3.1137: : 66it [00:42,  1.82it/s]\u001b[A\n",
      "batch 67, training loss: 3.1137: : 67it [00:42,  1.69it/s]\u001b[A\n",
      "batch 68, training loss: 3.175: : 67it [00:43,  1.69it/s] \u001b[A\n",
      "batch 68, training loss: 3.175: : 68it [00:43,  1.64it/s]\u001b[A\n",
      "batch 69, training loss: 3.0859: : 68it [00:44,  1.64it/s]\u001b[A\n",
      "batch 69, training loss: 3.0859: : 69it [00:44,  1.63it/s]\u001b[A\n",
      "batch 70, training loss: 3.2653: : 69it [00:44,  1.63it/s]\u001b[A\n",
      "batch 70, training loss: 3.2653: : 70it [00:44,  1.62it/s]\u001b[A\n",
      "batch 71, training loss: 3.1273: : 70it [00:45,  1.62it/s]\u001b[A\n",
      "batch 71, training loss: 3.1273: : 71it [00:45,  1.61it/s]\u001b[A\n",
      "batch 72, training loss: 3.081: : 71it [00:46,  1.61it/s] \u001b[A\n",
      "batch 72, training loss: 3.081: : 72it [00:46,  1.54it/s]\u001b[A\n",
      "batch 73, training loss: 3.155: : 72it [00:46,  1.54it/s]\u001b[A\n",
      "batch 73, training loss: 3.155: : 73it [00:46,  1.50it/s]\u001b[A\n",
      "batch 74, training loss: 3.0535: : 73it [00:47,  1.50it/s]\u001b[A\n",
      "batch 74, training loss: 3.0535: : 74it [00:47,  1.50it/s]\u001b[A\n",
      "batch 75, training loss: 3.2291: : 74it [00:47,  1.50it/s]\u001b[A\n",
      "batch 75, training loss: 3.2291: : 75it [00:47,  1.63it/s]\u001b[A\n",
      "batch 76, training loss: 3.0457: : 75it [00:48,  1.63it/s]\u001b[A\n",
      "batch 76, training loss: 3.0457: : 76it [00:48,  1.65it/s]\u001b[A\n",
      "batch 77, training loss: 3.1995: : 76it [00:49,  1.65it/s]\u001b[A\n",
      "batch 77, training loss: 3.1995: : 77it [00:49,  1.63it/s]\u001b[A\n",
      "batch 78, training loss: 3.1393: : 77it [00:49,  1.63it/s]\u001b[A\n",
      "batch 78, training loss: 3.1393: : 78it [00:49,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 3.2281: : 78it [00:50,  1.58it/s]\u001b[A\n",
      "batch 79, training loss: 3.2281: : 79it [00:50,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 3.1546: : 79it [00:51,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 3.1546: : 80it [00:51,  1.55it/s]\u001b[A\n",
      "batch 81, training loss: 3.1101: : 80it [00:51,  1.55it/s]\u001b[A\n",
      "batch 81, training loss: 3.1101: : 81it [00:51,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 3.2835: : 81it [00:52,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 3.2835: : 82it [00:52,  1.59it/s]\u001b[A\n",
      "batch 83, training loss: 3.124: : 82it [00:53,  1.59it/s] \u001b[A\n",
      "batch 83, training loss: 3.124: : 83it [00:53,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 3.266: : 83it [00:53,  1.56it/s]\u001b[A\n",
      "batch 84, training loss: 3.266: : 84it [00:53,  1.51it/s]\u001b[A\n",
      "batch 85, training loss: 3.2754: : 84it [00:54,  1.51it/s]\u001b[A\n",
      "batch 85, training loss: 3.2754: : 85it [00:54,  1.49it/s]\u001b[A\n",
      "batch 86, training loss: 3.2632: : 85it [00:55,  1.49it/s]\u001b[A\n",
      "batch 86, training loss: 3.2632: : 86it [00:55,  1.49it/s]\u001b[A\n",
      "batch 87, training loss: 3.227: : 86it [00:55,  1.49it/s] \u001b[A\n",
      "batch 87, training loss: 3.227: : 87it [00:55,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 3.2798: : 87it [00:56,  1.59it/s]\u001b[A\n",
      "batch 88, training loss: 3.2798: : 88it [00:56,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 3.3839: : 88it [00:57,  1.54it/s]\u001b[A\n",
      "batch 89, training loss: 3.3839: : 89it [00:57,  1.49it/s]\u001b[A\n",
      "batch 90, training loss: 3.2953: : 89it [00:57,  1.49it/s]\u001b[A\n",
      "batch 90, training loss: 3.2953: : 90it [00:57,  1.44it/s]\u001b[A\n",
      "batch 91, training loss: 3.4333: : 90it [00:58,  1.44it/s]\u001b[A\n",
      "batch 91, training loss: 3.4333: : 91it [00:58,  1.43it/s]\u001b[A\n",
      "batch 92, training loss: 3.2744: : 91it [00:59,  1.43it/s]\u001b[A\n",
      "batch 92, training loss: 3.2744: : 92it [00:59,  1.39it/s]\u001b[A\n",
      "batch 93, training loss: 3.2452: : 92it [01:00,  1.39it/s]\u001b[A\n",
      "batch 93, training loss: 3.2452: : 93it [01:00,  1.36it/s]\u001b[A\n",
      "batch 94, training loss: 3.3276: : 93it [01:00,  1.36it/s]\u001b[A\n",
      "batch 94, training loss: 3.3276: : 94it [01:00,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 3.2742: : 94it [01:01,  1.34it/s]\u001b[A\n",
      "batch 95, training loss: 3.2742: : 95it [01:01,  1.35it/s]\u001b[A\n",
      "batch 96, training loss: 3.2977: : 95it [01:02,  1.35it/s]\u001b[A\n",
      "batch 96, training loss: 3.2977: : 96it [01:02,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 3.3788: : 96it [01:03,  1.32it/s]\u001b[A\n",
      "batch 97, training loss: 3.3788: : 97it [01:03,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 3.3907: : 97it [01:03,  1.31it/s]\u001b[A\n",
      "batch 98, training loss: 3.3907: : 98it [01:03,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.1732: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.1732: : 99it [01:04,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 3.0703: : 99it [01:05,  1.31it/s]\u001b[A\n",
      "batch 100, training loss: 3.0703: : 100it [01:05,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.2043: : 100it [01:06,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.2043: : 101it [01:06,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.2179: : 101it [01:06,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.2179: : 102it [01:06,  1.32it/s]\u001b[A\n",
      "batch 103, training loss: 3.1889: : 102it [01:07,  1.32it/s]\u001b[A\n",
      "batch 103, training loss: 3.1889: : 103it [01:07,  1.32it/s]\u001b[A\n",
      "batch 104, training loss: 3.0259: : 103it [01:08,  1.32it/s]\u001b[A\n",
      "batch 104, training loss: 3.0259: : 104it [01:08,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.1714: : 104it [01:09,  1.31it/s]\u001b[A\n",
      "batch 105, training loss: 3.1714: : 105it [01:09,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.3364: : 105it [01:09,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.3364: : 106it [01:09,  1.33it/s]\u001b[A\n",
      "batch 107, training loss: 3.0448: : 106it [01:10,  1.33it/s]\u001b[A\n",
      "batch 107, training loss: 3.0448: : 107it [01:10,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.2887: : 107it [01:11,  1.31it/s]\u001b[A\n",
      "batch 108, training loss: 3.2887: : 108it [01:11,  1.31it/s]\u001b[A\n",
      "batch 109, training loss: 3.3007: : 108it [01:12,  1.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 109, training loss: 3.3007: : 109it [01:12,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 3.4705: : 109it [01:13,  1.31it/s]\u001b[A\n",
      "batch 110, training loss: 3.4705: : 110it [01:13,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.2084: : 110it [01:13,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.2084: : 111it [01:13,  1.29it/s]\u001b[A\n",
      "batch 112, training loss: 3.2741: : 111it [01:14,  1.29it/s]\u001b[A\n",
      "batch 112, training loss: 3.2741: : 112it [01:14,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.2947: : 112it [01:15,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.2947: : 113it [01:15,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 3.3067: : 113it [01:16,  1.32it/s]\u001b[A\n",
      "batch 114, training loss: 3.3067: : 114it [01:16,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 3.2194: : 114it [01:16,  1.30it/s]\u001b[A\n",
      "batch 115, training loss: 3.2194: : 115it [01:16,  1.30it/s]\u001b[A\n",
      "batch 116, training loss: 3.2515: : 115it [01:17,  1.30it/s]\u001b[A\n",
      "batch 116, training loss: 3.2515: : 116it [01:17,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.2785: : 116it [01:18,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.2785: : 117it [01:18,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 3.3617: : 117it [01:19,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 3.3617: : 118it [01:19,  1.29it/s]\u001b[A\n",
      "batch 119, training loss: 3.2909: : 118it [01:19,  1.29it/s]\u001b[A\n",
      "batch 119, training loss: 3.2909: : 119it [01:19,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 3.2247: : 119it [01:20,  1.29it/s]\u001b[A\n",
      "batch 120, training loss: 3.2247: : 120it [01:20,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 3.4487: : 120it [01:21,  1.30it/s]\u001b[A\n",
      "batch 121, training loss: 3.4487: : 121it [01:21,  1.29it/s]\u001b[A\n",
      "batch 122, training loss: 3.13: : 121it [01:22,  1.29it/s]  \u001b[A\n",
      "batch 122, training loss: 3.13: : 122it [01:22,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.2976: : 122it [01:23,  1.29it/s]\u001b[A\n",
      "batch 123, training loss: 3.2976: : 123it [01:23,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.3029: : 123it [01:23,  1.30it/s]\u001b[A\n",
      "batch 124, training loss: 3.3029: : 124it [01:23,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.3163: : 124it [01:24,  1.29it/s]\u001b[A\n",
      "batch 125, training loss: 3.3163: : 125it [01:24,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 3.3939: : 125it [01:25,  1.29it/s]\u001b[A\n",
      "batch 126, training loss: 3.3939: : 126it [01:25,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 3.0718: : 126it [01:26,  1.29it/s]\u001b[A\n",
      "batch 127, training loss: 3.0718: : 127it [01:26,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 3.306: : 127it [01:26,  1.30it/s] \u001b[A\n",
      "batch 128, training loss: 3.306: : 128it [01:26,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.2441: : 128it [01:27,  1.30it/s]\u001b[A\n",
      "batch 129, training loss: 3.2441: : 129it [01:27,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 3.2322: : 129it [01:28,  1.29it/s]\u001b[A\n",
      "batch 130, training loss: 3.2322: : 130it [01:28,  1.31it/s]\u001b[A\n",
      "batch 131, training loss: 3.4269: : 130it [01:29,  1.31it/s]\u001b[A\n",
      "batch 131, training loss: 3.4269: : 131it [01:29,  1.30it/s]\u001b[A\n",
      "batch 132, training loss: 3.2236: : 131it [01:29,  1.30it/s]\u001b[A\n",
      "batch 132, training loss: 3.2236: : 132it [01:29,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.1087: : 132it [01:30,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.1087: : 133it [01:30,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.3212: : 133it [01:31,  1.30it/s]\u001b[A\n",
      "batch 134, training loss: 3.3212: : 134it [01:31,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.1984: : 134it [01:32,  1.31it/s]\u001b[A\n",
      "batch 135, training loss: 3.1984: : 135it [01:32,  1.30it/s]\u001b[A\n",
      "batch 136, training loss: 3.137: : 135it [01:33,  1.30it/s] \u001b[A\n",
      "batch 136, training loss: 3.137: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.3119: : 136it [01:33,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.3119: : 137it [01:33,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 3.3151: : 137it [01:34,  1.30it/s]\u001b[A\n",
      "batch 138, training loss: 3.3151: : 138it [01:34,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.1001: : 138it [01:35,  1.29it/s]\u001b[A\n",
      "batch 139, training loss: 3.1001: : 139it [01:35,  1.29it/s]\u001b[A\n",
      "batch 140, training loss: 3.2086: : 139it [01:36,  1.29it/s]\u001b[A\n",
      "batch 140, training loss: 3.2086: : 140it [01:36,  1.29it/s]\u001b[A\n",
      "batch 141, training loss: 3.1949: : 140it [01:36,  1.29it/s]\u001b[A\n",
      "batch 141, training loss: 3.1949: : 141it [01:36,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.2355: : 141it [01:37,  1.29it/s]\u001b[A\n",
      "batch 142, training loss: 3.2355: : 142it [01:37,  1.29it/s]\u001b[A\n",
      "batch 143, training loss: 3.0716: : 142it [01:38,  1.29it/s]\u001b[A\n",
      "batch 143, training loss: 3.0716: : 143it [01:38,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 3.1549: : 143it [01:39,  1.29it/s]\u001b[A\n",
      "batch 144, training loss: 3.1549: : 144it [01:39,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.2425: : 144it [01:40,  1.30it/s]\u001b[A\n",
      "batch 145, training loss: 3.2425: : 145it [01:40,  1.29it/s]\u001b[A\n",
      "batch 146, training loss: 3.2228: : 145it [01:40,  1.29it/s]\u001b[A\n",
      "batch 146, training loss: 3.2228: : 146it [01:40,  1.30it/s]\u001b[A\n",
      "batch 147, training loss: 3.139: : 146it [01:41,  1.30it/s] \u001b[A\n",
      "batch 147, training loss: 3.139: : 147it [01:41,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 3.2175: : 147it [01:42,  1.31it/s]\u001b[A\n",
      "batch 148, training loss: 3.2175: : 148it [01:42,  1.38it/s]\u001b[A\n",
      "batch 149, training loss: 3.2947: : 148it [01:42,  1.38it/s]\u001b[A\n",
      "batch 149, training loss: 3.2947: : 149it [01:42,  1.55it/s]\u001b[A\n",
      "batch 150, training loss: 3.274: : 149it [01:43,  1.55it/s] \u001b[A\n",
      "batch 150, training loss: 3.274: : 150it [01:43,  1.66it/s]\u001b[A\n",
      "batch 151, training loss: 3.2181: : 150it [01:43,  1.66it/s]\u001b[A\n",
      "batch 151, training loss: 3.2181: : 151it [01:43,  1.53it/s]\u001b[A\n",
      "batch 152, training loss: 3.1427: : 151it [01:44,  1.53it/s]\u001b[A\n",
      "batch 152, training loss: 3.1427: : 152it [01:44,  1.54it/s]\u001b[A\n",
      "batch 153, training loss: 3.1486: : 152it [01:45,  1.54it/s]\u001b[A\n",
      "batch 153, training loss: 3.1486: : 153it [01:45,  1.47it/s]\u001b[A\n",
      "batch 154, training loss: 3.2796: : 153it [01:46,  1.47it/s]\u001b[A\n",
      "batch 154, training loss: 3.2796: : 154it [01:46,  1.41it/s]\u001b[A\n",
      "batch 155, training loss: 3.3981: : 154it [01:46,  1.41it/s]\u001b[A\n",
      "batch 155, training loss: 3.3981: : 155it [01:46,  1.38it/s]\u001b[A\n",
      "batch 156, training loss: 3.0031: : 155it [01:47,  1.38it/s]\u001b[A\n",
      "batch 156, training loss: 3.0031: : 156it [01:47,  1.36it/s]\u001b[A\n",
      "batch 157, training loss: 3.2001: : 156it [01:48,  1.36it/s]\u001b[A\n",
      "batch 157, training loss: 3.2001: : 157it [01:48,  1.34it/s]\u001b[A\n",
      "batch 158, training loss: 3.2091: : 157it [01:49,  1.34it/s]\u001b[A\n",
      "batch 158, training loss: 3.2091: : 158it [01:49,  1.33it/s]\u001b[A\n",
      "batch 159, training loss: 3.1229: : 158it [01:49,  1.33it/s]\u001b[A\n",
      "batch 159, training loss: 3.1229: : 159it [01:49,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 3.3063: : 159it [01:50,  1.32it/s]\u001b[A\n",
      "batch 160, training loss: 3.3063: : 160it [01:50,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.1862: : 160it [01:51,  1.33it/s]\u001b[A\n",
      "batch 161, training loss: 3.1862: : 161it [01:51,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 3.1538: : 161it [01:52,  1.31it/s]\u001b[A\n",
      "batch 162, training loss: 3.1538: : 162it [01:52,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.4062: : 162it [01:52,  1.31it/s]\u001b[A\n",
      "batch 163, training loss: 3.4062: : 163it [01:52,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.2257: : 163it [01:53,  1.31it/s]\u001b[A\n",
      "batch 164, training loss: 3.2257: : 164it [01:53,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 3.1835: : 164it [01:54,  1.30it/s]\u001b[A\n",
      "batch 165, training loss: 3.1835: : 165it [01:54,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 3.1639: : 165it [01:55,  1.30it/s]\u001b[A\n",
      "batch 166, training loss: 3.1639: : 166it [01:55,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.2878: : 166it [01:56,  1.30it/s]\u001b[A\n",
      "batch 167, training loss: 3.2878: : 167it [01:56,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.2705: : 167it [01:56,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.2705: : 168it [01:56,  1.31it/s]\u001b[A\n",
      "batch 169, training loss: 3.297: : 168it [01:57,  1.31it/s] \u001b[A\n",
      "batch 169, training loss: 3.297: : 169it [01:57,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 3.2054: : 169it [01:58,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 3.2054: : 170it [01:58,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 2.0131: : 170it [01:58,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 2.0131: : 171it [01:58,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.4283: : 171it [01:59,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.4283: : 172it [01:59,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 3.2559: : 172it [02:00,  1.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 173, training loss: 3.2559: : 173it [02:00,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 3.4388: : 173it [02:01,  1.37it/s]\u001b[A\n",
      "batch 174, training loss: 3.4388: : 174it [02:01,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 3.3389: : 174it [02:01,  1.34it/s]\u001b[A\n",
      "batch 175, training loss: 3.3389: : 175it [02:01,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 3.3204: : 175it [02:02,  1.31it/s]\u001b[A\n",
      "batch 176, training loss: 3.3204: : 176it [02:02,  1.28it/s]\u001b[A\n",
      "batch 177, training loss: 3.285: : 176it [02:03,  1.28it/s] \u001b[A\n",
      "batch 177, training loss: 3.285: : 177it [02:03,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 3.3034: : 177it [02:04,  1.28it/s]\u001b[A\n",
      "batch 178, training loss: 3.3034: : 178it [02:04,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 3.3872: : 178it [02:05,  1.27it/s]\u001b[A\n",
      "batch 179, training loss: 3.3872: : 179it [02:05,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 3.3614: : 179it [02:05,  1.25it/s]\u001b[A\n",
      "batch 180, training loss: 3.3614: : 180it [02:05,  1.24it/s]\u001b[A\n",
      "batch 181, training loss: 3.4446: : 180it [02:06,  1.24it/s]\u001b[A\n",
      "batch 181, training loss: 3.4446: : 181it [02:06,  1.23it/s]\u001b[A\n",
      "batch 182, training loss: 3.356: : 181it [02:07,  1.23it/s] \u001b[A\n",
      "batch 182, training loss: 3.356: : 182it [02:07,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 3.4975: : 182it [02:08,  1.23it/s]\u001b[A\n",
      "batch 183, training loss: 3.4975: : 183it [02:08,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.2009: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.2009: : 184it [02:09,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 3.2547: : 184it [02:10,  1.25it/s]\u001b[A\n",
      "batch 185, training loss: 3.2547: : 185it [02:10,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 3.4047: : 185it [02:10,  1.24it/s]\u001b[A\n",
      "batch 186, training loss: 3.4047: : 186it [02:10,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 3.4507: : 186it [02:11,  1.23it/s]\u001b[A\n",
      "batch 187, training loss: 3.4507: : 187it [02:11,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 3.4827: : 187it [02:12,  1.23it/s]\u001b[A\n",
      "batch 188, training loss: 3.4827: : 188it [02:12,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.5276: : 188it [02:13,  1.23it/s]\u001b[A\n",
      "batch 189, training loss: 3.5276: : 189it [02:13,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.2471: : 189it [02:14,  1.23it/s]\u001b[A\n",
      "batch 190, training loss: 3.2471: : 190it [02:14,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 3.2387: : 190it [02:14,  1.23it/s]\u001b[A\n",
      "batch 191, training loss: 3.2387: : 191it [02:14,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.2946: : 191it [02:15,  1.22it/s]\u001b[A\n",
      "batch 192, training loss: 3.2946: : 192it [02:15,  1.19it/s]\u001b[A\n",
      "batch 193, training loss: 3.2396: : 192it [02:16,  1.19it/s]\u001b[A\n",
      "batch 193, training loss: 3.2396: : 193it [02:16,  1.20it/s]\u001b[A\n",
      "batch 194, training loss: 3.3496: : 193it [02:17,  1.20it/s]\u001b[A\n",
      "batch 194, training loss: 3.3496: : 194it [02:17,  1.21it/s]\u001b[A\n",
      "batch 195, training loss: 3.2726: : 194it [02:18,  1.21it/s]\u001b[A\n",
      "batch 195, training loss: 3.2726: : 195it [02:18,  1.22it/s]\u001b[A\n",
      "batch 196, training loss: 3.1849: : 195it [02:19,  1.22it/s]\u001b[A\n",
      "batch 196, training loss: 3.1849: : 196it [02:19,  1.23it/s]\u001b[A\n",
      "batch 197, training loss: 3.4021: : 196it [02:19,  1.23it/s]\u001b[A\n",
      "batch 197, training loss: 3.4021: : 197it [02:19,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.2794: : 197it [02:20,  1.23it/s]\u001b[A\n",
      "batch 198, training loss: 3.2794: : 198it [02:20,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.2695: : 198it [02:21,  1.24it/s]\u001b[A\n",
      "batch 199, training loss: 3.2695: : 199it [02:21,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 3.4035: : 199it [02:22,  1.22it/s]\u001b[A\n",
      "batch 200, training loss: 3.4035: : 200it [02:22,  1.19it/s]\u001b[A\n",
      "batch 201, training loss: 3.2384: : 200it [02:23,  1.19it/s]\u001b[A\n",
      "batch 201, training loss: 3.2384: : 201it [02:23,  1.21it/s]\u001b[A\n",
      "batch 202, training loss: 3.1295: : 201it [02:23,  1.21it/s]\u001b[A\n",
      "batch 202, training loss: 3.1295: : 202it [02:23,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.3141: : 202it [02:24,  1.23it/s]\u001b[A\n",
      "batch 203, training loss: 3.3141: : 203it [02:24,  1.23it/s]\u001b[A\n",
      "batch 204, training loss: 3.3401: : 203it [02:25,  1.23it/s]\u001b[A\n",
      "batch 204, training loss: 3.3401: : 204it [02:25,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 3.3172: : 204it [02:26,  1.23it/s]\u001b[A\n",
      "batch 205, training loss: 3.3172: : 205it [02:26,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 3.4104: : 205it [02:27,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 3.4104: : 206it [02:27,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.2301: : 206it [02:27,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.2301: : 207it [02:27,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.2591: : 207it [02:28,  1.24it/s]\u001b[A\n",
      "batch 208, training loss: 3.2591: : 208it [02:28,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 3.2608: : 208it [02:29,  1.24it/s]\u001b[A\n",
      "batch 209, training loss: 3.2608: : 209it [02:29,  1.23it/s]\u001b[A\n",
      "batch 210, training loss: 3.3393: : 209it [02:30,  1.23it/s]\u001b[A\n",
      "batch 210, training loss: 3.3393: : 210it [02:30,  1.22it/s]\u001b[A\n",
      "batch 211, training loss: 3.2525: : 210it [02:31,  1.22it/s]\u001b[A\n",
      "batch 211, training loss: 3.2525: : 211it [02:31,  1.22it/s]\u001b[A\n",
      "batch 212, training loss: 3.2667: : 211it [02:32,  1.22it/s]\u001b[A\n",
      "batch 212, training loss: 3.2667: : 212it [02:32,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.4679: : 212it [02:32,  1.23it/s]\u001b[A\n",
      "batch 213, training loss: 3.4679: : 213it [02:32,  1.23it/s]\u001b[A\n",
      "batch 214, training loss: 3.3417: : 213it [02:33,  1.23it/s]\u001b[A\n",
      "batch 214, training loss: 3.3417: : 214it [02:33,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.1922: : 214it [02:34,  1.23it/s]\u001b[A\n",
      "batch 215, training loss: 3.1922: : 215it [02:34,  1.24it/s]\u001b[A\n",
      "batch 216, training loss: 3.311: : 215it [02:35,  1.24it/s] \u001b[A\n",
      "batch 216, training loss: 3.311: : 216it [02:35,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.433: : 216it [02:36,  1.23it/s]\u001b[A\n",
      "batch 217, training loss: 3.433: : 217it [02:36,  1.23it/s]\u001b[A\n",
      "batch 218, training loss: 3.3719: : 217it [02:36,  1.23it/s]\u001b[A\n",
      "batch 218, training loss: 3.3719: : 218it [02:36,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.5202: : 218it [02:37,  1.23it/s]\u001b[A\n",
      "batch 219, training loss: 3.5202: : 219it [02:37,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 3.4716: : 219it [02:38,  1.23it/s]\u001b[A\n",
      "batch 220, training loss: 3.4716: : 220it [02:38,  1.23it/s]\u001b[A\n",
      "batch 221, training loss: 3.307: : 220it [02:39,  1.23it/s] \u001b[A\n",
      "batch 221, training loss: 3.307: : 221it [02:39,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 3.3844: : 221it [02:40,  1.22it/s]\u001b[A\n",
      "batch 222, training loss: 3.3844: : 222it [02:40,  1.23it/s]\u001b[A\n",
      "batch 223, training loss: 3.3775: : 222it [02:41,  1.23it/s]\u001b[A\n",
      "batch 223, training loss: 3.3775: : 223it [02:41,  1.23it/s]\u001b[A\n",
      "batch 224, training loss: 3.2531: : 223it [02:41,  1.23it/s]\u001b[A\n",
      "batch 224, training loss: 3.2531: : 224it [02:41,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.4242: : 224it [02:42,  1.23it/s]\u001b[A\n",
      "batch 225, training loss: 3.4242: : 225it [02:42,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.3632: : 225it [02:43,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.3632: : 226it [02:43,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 3.2568: : 226it [02:44,  1.24it/s]\u001b[A\n",
      "batch 227, training loss: 3.2568: : 227it [02:44,  1.23it/s]\u001b[A\n",
      "batch 228, training loss: 3.3274: : 227it [02:45,  1.23it/s]\u001b[A\n",
      "batch 228, training loss: 3.3274: : 228it [02:45,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 3.3071: : 228it [02:45,  1.24it/s]\u001b[A\n",
      "batch 229, training loss: 3.3071: : 229it [02:45,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 3.4378: : 229it [02:46,  1.24it/s]\u001b[A\n",
      "batch 230, training loss: 3.4378: : 230it [02:46,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.4691: : 230it [02:47,  1.23it/s]\u001b[A\n",
      "batch 231, training loss: 3.4691: : 231it [02:47,  1.24it/s]\u001b[A\n",
      "batch 232, training loss: 3.295: : 231it [02:48,  1.24it/s] \u001b[A\n",
      "batch 232, training loss: 3.295: : 232it [02:48,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.3247: : 232it [02:49,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.3247: : 233it [02:49,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 3.5009: : 233it [02:49,  1.23it/s]\u001b[A\n",
      "batch 234, training loss: 3.5009: : 234it [02:49,  1.23it/s]\u001b[A\n",
      "batch 235, training loss: 3.3315: : 234it [02:50,  1.23it/s]\u001b[A\n",
      "batch 235, training loss: 3.3315: : 235it [02:50,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 3.3389: : 235it [02:51,  1.23it/s]\u001b[A\n",
      "batch 236, training loss: 3.3389: : 236it [02:51,  1.22it/s]\u001b[A\n",
      "batch 237, training loss: 3.2522: : 236it [02:52,  1.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 237, training loss: 3.2522: : 237it [02:52,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 3.4087: : 237it [02:53,  1.23it/s]\u001b[A\n",
      "batch 238, training loss: 3.4087: : 238it [02:53,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 3.3055: : 238it [02:54,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 3.3055: : 239it [02:54,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 3.3192: : 239it [02:54,  1.22it/s]\u001b[A\n",
      "batch 240, training loss: 3.3192: : 240it [02:54,  1.22it/s]\u001b[A\n",
      "batch 241, training loss: 3.3421: : 240it [02:55,  1.22it/s]\u001b[A\n",
      "batch 241, training loss: 3.3421: : 241it [02:55,  1.21it/s]\u001b[A\n",
      "batch 242, training loss: 3.2089: : 241it [02:56,  1.21it/s]\u001b[A\n",
      "batch 242, training loss: 3.2089: : 242it [02:56,  1.22it/s]\u001b[A\n",
      "batch 243, training loss: 3.3654: : 242it [02:57,  1.22it/s]\u001b[A\n",
      "batch 243, training loss: 3.3654: : 243it [02:57,  1.22it/s]\u001b[A\n",
      "batch 244, training loss: 3.2363: : 243it [02:58,  1.22it/s]\u001b[A\n",
      "batch 244, training loss: 3.2363: : 244it [02:58,  1.22it/s]\u001b[A\n",
      "batch 245, training loss: 3.4104: : 244it [02:58,  1.22it/s]\u001b[A\n",
      "batch 245, training loss: 3.4104: : 245it [02:58,  1.20it/s]\u001b[A\n",
      "batch 246, training loss: 3.3303: : 245it [02:59,  1.20it/s]\u001b[A\n",
      "batch 246, training loss: 3.3303: : 246it [02:59,  1.20it/s]\u001b[A\n",
      "batch 247, training loss: 3.334: : 246it [03:00,  1.20it/s] \u001b[A\n",
      "batch 247, training loss: 3.334: : 247it [03:00,  1.21it/s]\u001b[A\n",
      "batch 248, training loss: 3.2728: : 247it [03:01,  1.21it/s]\u001b[A\n",
      "batch 248, training loss: 3.2728: : 248it [03:01,  1.22it/s]\u001b[A\n",
      "batch 249, training loss: 3.2242: : 248it [03:02,  1.22it/s]\u001b[A\n",
      "batch 249, training loss: 3.2242: : 249it [03:02,  1.23it/s]\u001b[A\n",
      "batch 250, training loss: 3.4081: : 249it [03:03,  1.23it/s]\u001b[A\n",
      "batch 250, training loss: 3.4081: : 250it [03:03,  1.23it/s]\u001b[A\n",
      "batch 251, training loss: 3.4055: : 250it [03:03,  1.23it/s]\u001b[A\n",
      "batch 251, training loss: 3.4055: : 251it [03:03,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 2.2333: : 251it [03:04,  1.25it/s]\u001b[A\n",
      "batch 252, training loss: 2.2333: : 252it [03:04,  1.44it/s]\u001b[A\n",
      "batch 253, training loss: 3.2929: : 252it [03:05,  1.44it/s]\u001b[A\n",
      "batch 253, training loss: 3.2929: : 253it [03:05,  1.34it/s]\u001b[A\n",
      "batch 254, training loss: 3.3762: : 253it [03:06,  1.34it/s]\u001b[A\n",
      "batch 254, training loss: 3.3762: : 254it [03:06,  1.24it/s]\u001b[A\n",
      "batch 255, training loss: 3.2055: : 254it [03:06,  1.24it/s]\u001b[A\n",
      "batch 255, training loss: 3.2055: : 255it [03:06,  1.21it/s]\u001b[A\n",
      "batch 256, training loss: 3.2876: : 255it [03:07,  1.21it/s]\u001b[A\n",
      "batch 256, training loss: 3.2876: : 256it [03:07,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 3.3586: : 256it [03:08,  1.16it/s]\u001b[A\n",
      "batch 257, training loss: 3.3586: : 257it [03:08,  1.17it/s]\u001b[A\n",
      "batch 258, training loss: 3.416: : 257it [03:09,  1.17it/s] \u001b[A\n",
      "batch 258, training loss: 3.416: : 258it [03:09,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.3168: : 258it [03:10,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.3168: : 259it [03:10,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.2697: : 259it [03:11,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.2697: : 260it [03:11,  1.11it/s]\u001b[A\n",
      "batch 261, training loss: 3.3603: : 260it [03:12,  1.11it/s]\u001b[A\n",
      "batch 261, training loss: 3.3603: : 261it [03:12,  1.10it/s]\u001b[A\n",
      "batch 262, training loss: 3.3205: : 261it [03:13,  1.10it/s]\u001b[A\n",
      "batch 262, training loss: 3.3205: : 262it [03:13,  1.17it/s]\u001b[A\n",
      "batch 263, training loss: 3.2799: : 262it [03:13,  1.17it/s]\u001b[A\n",
      "batch 263, training loss: 3.2799: : 263it [03:13,  1.28it/s]\u001b[A\n",
      "batch 264, training loss: 3.4323: : 263it [03:14,  1.28it/s]\u001b[A\n",
      "batch 264, training loss: 3.4323: : 264it [03:14,  1.24it/s]\u001b[A\n",
      "batch 265, training loss: 3.1921: : 264it [03:15,  1.24it/s]\u001b[A\n",
      "batch 265, training loss: 3.1921: : 265it [03:15,  1.18it/s]\u001b[A\n",
      "batch 266, training loss: 3.3697: : 265it [03:16,  1.18it/s]\u001b[A\n",
      "batch 266, training loss: 3.3697: : 266it [03:16,  1.14it/s]\u001b[A\n",
      "batch 267, training loss: 3.3653: : 266it [03:17,  1.14it/s]\u001b[A\n",
      "batch 267, training loss: 3.3653: : 267it [03:17,  1.14it/s]\u001b[A\n",
      "batch 268, training loss: 3.2148: : 267it [03:18,  1.14it/s]\u001b[A\n",
      "batch 268, training loss: 3.2148: : 268it [03:18,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.2898: : 268it [03:19,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.2898: : 269it [03:19,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.3067: : 269it [03:20,  1.09it/s]\u001b[A\n",
      "batch 270, training loss: 3.3067: : 270it [03:20,  1.09it/s]\u001b[A\n",
      "batch 271, training loss: 3.279: : 270it [03:21,  1.09it/s] \u001b[A\n",
      "batch 271, training loss: 3.279: : 271it [03:21,  1.12it/s]\u001b[A\n",
      "batch 272, training loss: 3.3001: : 271it [03:21,  1.12it/s]\u001b[A\n",
      "batch 272, training loss: 3.3001: : 272it [03:21,  1.16it/s]\u001b[A\n",
      "batch 273, training loss: 3.3915: : 272it [03:22,  1.16it/s]\u001b[A\n",
      "batch 273, training loss: 3.3915: : 273it [03:22,  1.13it/s]\u001b[A\n",
      "batch 274, training loss: 3.3856: : 273it [03:23,  1.13it/s]\u001b[A\n",
      "batch 274, training loss: 3.3856: : 274it [03:23,  1.12it/s]\u001b[A\n",
      "batch 275, training loss: 3.2632: : 274it [03:24,  1.12it/s]\u001b[A\n",
      "batch 275, training loss: 3.2632: : 275it [03:24,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.1498: : 275it [03:25,  1.11it/s]\u001b[A\n",
      "batch 276, training loss: 3.1498: : 276it [03:25,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 3.2839: : 276it [03:26,  1.11it/s]\u001b[A\n",
      "batch 277, training loss: 3.2839: : 277it [03:26,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.2317: : 277it [03:27,  1.10it/s]\u001b[A\n",
      "batch 278, training loss: 3.2317: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.2254: : 278it [03:28,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.2254: : 279it [03:28,  1.08it/s]\u001b[A\n",
      "batch 280, training loss: 3.1555: : 279it [03:29,  1.08it/s]\u001b[A\n",
      "batch 280, training loss: 3.1555: : 280it [03:29,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.2809: : 280it [03:30,  1.09it/s]\u001b[A\n",
      "batch 281, training loss: 3.2809: : 281it [03:30,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.1405: : 281it [03:31,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.1405: : 282it [03:31,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.238: : 282it [03:31,  1.10it/s] \u001b[A\n",
      "batch 283, training loss: 3.238: : 283it [03:31,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.2588: : 283it [03:32,  1.09it/s]\u001b[A\n",
      "batch 284, training loss: 3.2588: : 284it [03:32,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.2888: : 284it [03:33,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.2888: : 285it [03:33,  1.09it/s]\u001b[A\n",
      "batch 286, training loss: 3.4531: : 285it [03:34,  1.09it/s]\u001b[A\n",
      "batch 286, training loss: 3.4531: : 286it [03:34,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.1584: : 286it [03:35,  1.11it/s]\u001b[A\n",
      "batch 287, training loss: 3.1584: : 287it [03:35,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.1846: : 287it [03:36,  1.10it/s]\u001b[A\n",
      "batch 288, training loss: 3.1846: : 288it [03:36,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.3529: : 288it [03:37,  1.10it/s]\u001b[A\n",
      "batch 289, training loss: 3.3529: : 289it [03:37,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.1383: : 289it [03:38,  1.09it/s]\u001b[A\n",
      "batch 290, training loss: 3.1383: : 290it [03:38,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.3853: : 290it [03:39,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.3853: : 291it [03:39,  1.08it/s]\u001b[A\n",
      "batch 292, training loss: 3.2074: : 291it [03:40,  1.08it/s]\u001b[A\n",
      "batch 292, training loss: 3.2074: : 292it [03:40,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 3.3027: : 292it [03:41,  1.09it/s]\u001b[A\n",
      "batch 293, training loss: 3.3027: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.3652: : 293it [03:41,  1.09it/s]\u001b[A\n",
      "batch 294, training loss: 3.3652: : 294it [03:41,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.3396: : 294it [03:42,  1.11it/s]\u001b[A\n",
      "batch 295, training loss: 3.3396: : 295it [03:42,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.1789: : 295it [03:43,  1.09it/s]\u001b[A\n",
      "batch 296, training loss: 3.1789: : 296it [03:43,  1.09it/s]\u001b[A\n",
      "batch 297, training loss: 3.3317: : 296it [03:44,  1.09it/s]\u001b[A\n",
      "batch 297, training loss: 3.3317: : 297it [03:44,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.2489: : 297it [03:45,  1.10it/s]\u001b[A\n",
      "batch 298, training loss: 3.2489: : 298it [03:45,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.3212: : 298it [03:46,  1.09it/s]\u001b[A\n",
      "batch 299, training loss: 3.3212: : 299it [03:46,  1.08it/s]\u001b[A\n",
      "batch 300, training loss: 3.383: : 299it [03:47,  1.08it/s] \u001b[A\n",
      "batch 300, training loss: 3.383: : 300it [03:47,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.3208: : 300it [03:48,  1.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 301, training loss: 3.3208: : 301it [03:48,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.3433: : 301it [03:49,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.3433: : 302it [03:49,  1.12it/s]\u001b[A\n",
      "batch 303, training loss: 3.2424: : 302it [03:50,  1.12it/s]\u001b[A\n",
      "batch 303, training loss: 3.2424: : 303it [03:50,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 3.3288: : 303it [03:51,  1.11it/s]\u001b[A\n",
      "batch 304, training loss: 3.3288: : 304it [03:51,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 3.2608: : 304it [03:52,  1.10it/s]\u001b[A\n",
      "batch 305, training loss: 3.2608: : 305it [03:52,  1.10it/s]\u001b[A\n",
      "batch 306, training loss: 3.3167: : 305it [03:52,  1.10it/s]\u001b[A\n",
      "batch 306, training loss: 3.3167: : 306it [03:52,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 3.4676: : 306it [03:53,  1.10it/s]\u001b[A\n",
      "batch 307, training loss: 3.4676: : 307it [03:53,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.0815: : 307it [03:54,  1.09it/s]\u001b[A\n",
      "batch 308, training loss: 3.0815: : 308it [03:54,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.4828: : 308it [03:55,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.4828: : 309it [03:55,  1.08it/s]\u001b[A\n",
      "batch 310, training loss: 3.2208: : 309it [03:56,  1.08it/s]\u001b[A\n",
      "batch 310, training loss: 3.2208: : 310it [03:56,  1.11it/s]\u001b[A\n",
      "batch 311, training loss: 3.3326: : 310it [03:57,  1.11it/s]\u001b[A\n",
      "batch 311, training loss: 3.3326: : 311it [03:57,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 3.1745: : 311it [03:58,  1.10it/s]\u001b[A\n",
      "batch 312, training loss: 3.1745: : 312it [03:58,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.2614: : 312it [03:59,  1.09it/s]\u001b[A\n",
      "batch 313, training loss: 3.2614: : 313it [03:59,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 3.2894: : 313it [04:00,  1.09it/s]\u001b[A\n",
      "batch 314, training loss: 3.2894: : 314it [04:00,  1.09it/s]\u001b[A\n",
      "batch 315, training loss: 3.3425: : 314it [04:01,  1.09it/s]\u001b[A\n",
      "batch 315, training loss: 3.3425: : 315it [04:01,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 3.4784: : 315it [04:02,  1.08it/s]\u001b[A\n",
      "batch 316, training loss: 3.4784: : 316it [04:02,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.1447: : 316it [04:02,  1.08it/s]\u001b[A\n",
      "batch 317, training loss: 3.1447: : 317it [04:02,  1.14it/s]\u001b[A\n",
      "batch 318, training loss: 3.3554: : 317it [04:03,  1.14it/s]\u001b[A\n",
      "batch 318, training loss: 3.3554: : 318it [04:03,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 3.4221: : 318it [04:04,  1.10it/s]\u001b[A\n",
      "batch 319, training loss: 3.4221: : 319it [04:04,  1.07it/s]\u001b[A\n",
      "batch 320, training loss: 3.403: : 319it [04:05,  1.07it/s] \u001b[A\n",
      "batch 320, training loss: 3.403: : 320it [04:05,  1.02it/s]\u001b[A\n",
      "batch 321, training loss: 3.4881: : 320it [04:06,  1.02it/s]\u001b[A\n",
      "batch 321, training loss: 3.4881: : 321it [04:06,  1.01it/s]\u001b[A\n",
      "batch 322, training loss: 3.5066: : 321it [04:07,  1.01it/s]\u001b[A\n",
      "batch 322, training loss: 3.5066: : 322it [04:07,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 3.2608: : 322it [04:08,  1.02it/s]\u001b[A\n",
      "batch 323, training loss: 3.2608: : 323it [04:08,  1.03it/s]\u001b[A\n",
      "batch 324, training loss: 3.3576: : 323it [04:09,  1.03it/s]\u001b[A\n",
      "batch 324, training loss: 3.3576: : 324it [04:09,  1.01it/s]\u001b[A\n",
      "batch 325, training loss: 3.4495: : 324it [04:10,  1.01it/s]\u001b[A\n",
      "batch 325, training loss: 3.4495: : 325it [04:10,  1.01s/it]\u001b[A\n",
      "batch 326, training loss: 3.3412: : 325it [04:11,  1.01s/it]\u001b[A\n",
      "batch 326, training loss: 3.3412: : 326it [04:11,  1.00it/s]\u001b[A\n",
      "batch 327, training loss: 3.4998: : 326it [04:12,  1.00it/s]\u001b[A\n",
      "batch 327, training loss: 3.4998: : 327it [04:12,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.3838: : 327it [04:13,  1.00it/s]\u001b[A\n",
      "batch 328, training loss: 3.3838: : 328it [04:13,  1.01s/it]\u001b[A\n",
      "batch 329, training loss: 3.2788: : 328it [04:15,  1.01s/it]\u001b[A\n",
      "batch 329, training loss: 3.2788: : 329it [04:15,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 3.3612: : 329it [04:15,  1.01s/it]\u001b[A\n",
      "batch 330, training loss: 3.3612: : 330it [04:15,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 3.3871: : 330it [04:17,  1.01s/it]\u001b[A\n",
      "batch 331, training loss: 3.3871: : 331it [04:17,  1.03s/it]\u001b[A\n",
      "batch 332, training loss: 3.2933: : 331it [04:18,  1.03s/it]\u001b[A\n",
      "batch 332, training loss: 3.2933: : 332it [04:18,  1.01s/it]\u001b[A\n",
      "batch 333, training loss: 3.2842: : 332it [04:19,  1.01s/it]\u001b[A\n",
      "batch 333, training loss: 3.2842: : 333it [04:19,  1.01s/it]\u001b[A\n",
      "batch 334, training loss: 3.3304: : 333it [04:20,  1.01s/it]\u001b[A\n",
      "batch 334, training loss: 3.3304: : 334it [04:20,  1.02s/it]\u001b[A\n",
      "batch 335, training loss: 3.5048: : 334it [04:21,  1.02s/it]\u001b[A\n",
      "batch 335, training loss: 3.5048: : 335it [04:21,  1.02s/it]\u001b[A\n",
      "batch 336, training loss: 3.367: : 335it [04:22,  1.02s/it] \u001b[A\n",
      "batch 336, training loss: 3.367: : 336it [04:22,  1.02s/it]\u001b[A\n",
      "batch 337, training loss: 3.4491: : 336it [04:23,  1.02s/it]\u001b[A\n",
      "batch 337, training loss: 3.4491: : 337it [04:23,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 3.2908: : 337it [04:24,  1.02it/s]\u001b[A\n",
      "batch 338, training loss: 3.2908: : 338it [04:24,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 3.4033: : 338it [04:25,  1.01it/s]\u001b[A\n",
      "batch 339, training loss: 3.4033: : 339it [04:25,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 3.6082: : 339it [04:26,  1.00it/s]\u001b[A\n",
      "batch 340, training loss: 3.6082: : 340it [04:26,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.3131: : 340it [04:27,  1.00s/it]\u001b[A\n",
      "batch 341, training loss: 3.3131: : 341it [04:27,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.3934: : 341it [04:27,  1.01s/it]\u001b[A\n",
      "batch 342, training loss: 3.3934: : 342it [04:27,  1.03it/s]\u001b[A\n",
      "batch 343, training loss: 3.3666: : 342it [04:28,  1.03it/s]\u001b[A\n",
      "batch 343, training loss: 3.3666: : 343it [04:28,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 3.4595: : 343it [04:30,  1.01it/s]\u001b[A\n",
      "batch 344, training loss: 3.4595: : 344it [04:30,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.4733: : 344it [04:31,  1.01s/it]\u001b[A\n",
      "batch 345, training loss: 3.4733: : 345it [04:31,  1.00s/it]\u001b[A\n",
      "batch 346, training loss: 3.401: : 345it [04:32,  1.00s/it] \u001b[A\n",
      "batch 346, training loss: 3.401: : 346it [04:32,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 3.3123: : 346it [04:33,  1.01s/it]\u001b[A\n",
      "batch 347, training loss: 3.3123: : 347it [04:33,  1.02s/it]\u001b[A\n",
      "batch 348, training loss: 3.3906: : 347it [04:34,  1.02s/it]\u001b[A\n",
      "batch 348, training loss: 3.3906: : 348it [04:34,  1.02s/it]\u001b[A\n",
      "batch 349, training loss: 3.5554: : 348it [04:35,  1.02s/it]\u001b[A\n",
      "batch 349, training loss: 3.5554: : 349it [04:35,  1.00s/it]\u001b[A\n",
      "batch 350, training loss: 3.3848: : 349it [04:36,  1.00s/it]\u001b[A\n",
      "batch 350, training loss: 3.3848: : 350it [04:36,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 3.3215: : 350it [04:37,  1.01s/it]\u001b[A\n",
      "batch 351, training loss: 3.3215: : 351it [04:37,  1.02it/s]\u001b[A\n",
      "batch 352, training loss: 3.3614: : 351it [04:38,  1.02it/s]\u001b[A\n",
      "batch 352, training loss: 3.3614: : 352it [04:38,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.3757: : 352it [04:39,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.3757: : 353it [04:39,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 3.4493: : 353it [04:40,  1.00it/s]\u001b[A\n",
      "batch 354, training loss: 3.4493: : 354it [04:40,  1.02it/s]\u001b[A\n",
      "batch 355, training loss: 3.3973: : 354it [04:41,  1.02it/s]\u001b[A\n",
      "batch 355, training loss: 3.3973: : 355it [04:41,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.2562: : 355it [04:42,  1.00s/it]\u001b[A\n",
      "batch 356, training loss: 3.2562: : 356it [04:42,  1.01s/it]\u001b[A\n",
      "batch 357, training loss: 3.2774: : 356it [04:43,  1.01s/it]\u001b[A\n",
      "batch 357, training loss: 3.2774: : 357it [04:43,  1.02s/it]\u001b[A\n",
      "batch 358, training loss: 3.4879: : 357it [04:44,  1.02s/it]\u001b[A\n",
      "batch 358, training loss: 3.4879: : 358it [04:44,  1.03s/it]\u001b[A\n",
      "batch 359, training loss: 3.2775: : 358it [04:45,  1.03s/it]\u001b[A\n",
      "batch 359, training loss: 3.2775: : 359it [04:45,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 3.383: : 359it [04:45,  1.00s/it] \u001b[A\n",
      "batch 360, training loss: 3.383: : 360it [04:45,  1.10it/s]\u001b[A\n",
      "batch 361, training loss: 3.3702: : 360it [04:46,  1.10it/s]\u001b[A\n",
      "batch 361, training loss: 3.3702: : 361it [04:46,  1.09it/s]\u001b[A\n",
      "batch 362, training loss: 3.4357: : 361it [04:47,  1.09it/s]\u001b[A\n",
      "batch 362, training loss: 3.4357: : 362it [04:47,  1.08it/s]\u001b[A\n",
      "batch 363, training loss: 3.3879: : 362it [04:48,  1.08it/s]\u001b[A\n",
      "batch 363, training loss: 3.3879: : 363it [04:48,  1.03it/s]\u001b[A\n",
      "batch 364, training loss: 3.3071: : 363it [04:49,  1.03it/s]\u001b[A\n",
      "batch 364, training loss: 3.3071: : 364it [04:49,  1.02it/s]\u001b[A\n",
      "batch 365, training loss: 3.2499: : 364it [04:50,  1.02it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365, training loss: 3.2499: : 365it [04:50,  1.01it/s]\u001b[A\n",
      "batch 366, training loss: 3.3854: : 365it [04:51,  1.01it/s]\u001b[A\n",
      "batch 366, training loss: 3.3854: : 366it [04:51,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.3374: : 366it [04:52,  1.01s/it]\u001b[A\n",
      "batch 367, training loss: 3.3374: : 367it [04:52,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.4175: : 367it [04:53,  1.01s/it]\u001b[A\n",
      "batch 368, training loss: 3.4175: : 368it [04:53,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.4285: : 368it [04:54,  1.01s/it]\u001b[A\n",
      "batch 369, training loss: 3.4285: : 369it [04:54,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.3525: : 369it [04:55,  1.01s/it]\u001b[A\n",
      "batch 370, training loss: 3.3525: : 370it [04:55,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.3651: : 370it [04:56,  1.01it/s]\u001b[A\n",
      "batch 371, training loss: 3.3651: : 371it [04:56,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.3153: : 371it [04:57,  1.00s/it]\u001b[A\n",
      "batch 372, training loss: 3.3153: : 372it [04:57,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 3.3401: : 372it [04:58,  1.01s/it]\u001b[A\n",
      "batch 373, training loss: 3.3401: : 373it [04:58,  1.01s/it]\u001b[A\n",
      "batch 374, training loss: 3.3507: : 373it [04:59,  1.01s/it]\u001b[A\n",
      "batch 374, training loss: 3.3507: : 374it [04:59,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 2.9345: : 374it [05:00,  1.01s/it]\u001b[A\n",
      "batch 375, training loss: 2.9345: : 375it [05:00,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.3436: : 375it [05:01,  1.13it/s]\u001b[A\n",
      "batch 376, training loss: 3.3436: : 376it [05:01,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 3.4146: : 376it [05:02,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 3.4146: : 377it [05:02,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.3572: : 377it [05:03,  1.02s/it]\u001b[A\n",
      "batch 378, training loss: 3.3572: : 378it [05:03,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.3374: : 378it [05:04,  1.03s/it]\u001b[A\n",
      "batch 379, training loss: 3.3374: : 379it [05:04,  1.04s/it]\u001b[A\n",
      "batch 380, training loss: 3.3043: : 379it [05:06,  1.04s/it]\u001b[A\n",
      "batch 380, training loss: 3.3043: : 380it [05:06,  1.06s/it]\u001b[A\n",
      "batch 381, training loss: 3.3916: : 380it [05:07,  1.06s/it]\u001b[A\n",
      "batch 381, training loss: 3.3916: : 381it [05:07,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 3.2767: : 381it [05:08,  1.05s/it]\u001b[A\n",
      "batch 382, training loss: 3.2767: : 382it [05:08,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 3.3259: : 382it [05:09,  1.08s/it]\u001b[A\n",
      "batch 383, training loss: 3.3259: : 383it [05:09,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 3.3615: : 383it [05:10,  1.08s/it]\u001b[A\n",
      "batch 384, training loss: 3.3615: : 384it [05:10,  1.09s/it]\u001b[A\n",
      "batch 385, training loss: 3.3025: : 384it [05:11,  1.09s/it]\u001b[A\n",
      "batch 385, training loss: 3.3025: : 385it [05:11,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.28: : 385it [05:12,  1.09s/it]  \u001b[A\n",
      "batch 386, training loss: 3.28: : 386it [05:12,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.3387: : 386it [05:13,  1.09s/it]\u001b[A\n",
      "batch 387, training loss: 3.3387: : 387it [05:13,  1.09s/it]\u001b[A\n",
      "batch 388, training loss: 3.155: : 387it [05:14,  1.09s/it] \u001b[A\n",
      "batch 388, training loss: 3.155: : 388it [05:14,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.3177: : 388it [05:15,  1.10s/it]\u001b[A\n",
      "batch 389, training loss: 3.3177: : 389it [05:15,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.3998: : 389it [05:16,  1.11s/it]\u001b[A\n",
      "batch 390, training loss: 3.3998: : 390it [05:16,  1.09s/it]\u001b[A\n",
      "batch 391, training loss: 3.4272: : 390it [05:18,  1.09s/it]\u001b[A\n",
      "batch 391, training loss: 3.4272: : 391it [05:18,  1.10s/it]\u001b[A\n",
      "batch 392, training loss: 3.3733: : 391it [05:19,  1.10s/it]\u001b[A\n",
      "batch 392, training loss: 3.3733: : 392it [05:19,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.1805: : 392it [05:20,  1.11s/it]\u001b[A\n",
      "batch 393, training loss: 3.1805: : 393it [05:20,  1.12s/it]\u001b[A\n",
      "batch 394, training loss: 3.157: : 393it [05:21,  1.12s/it] \u001b[A\n",
      "batch 394, training loss: 3.157: : 394it [05:21,  1.12s/it]\u001b[A\n",
      "batch 395, training loss: 3.2458: : 394it [05:22,  1.12s/it]\u001b[A\n",
      "batch 395, training loss: 3.2458: : 395it [05:22,  1.12s/it]\u001b[A\n",
      "batch 396, training loss: 3.4122: : 395it [05:23,  1.12s/it]\u001b[A\n",
      "batch 396, training loss: 3.4122: : 396it [05:23,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.2475: : 396it [05:24,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.2475: : 397it [05:24,  1.13s/it]\u001b[A\n",
      "batch 398, training loss: 3.3457: : 397it [05:25,  1.13s/it]\u001b[A\n",
      "batch 398, training loss: 3.3457: : 398it [05:25,  1.11s/it]\u001b[A\n",
      "batch 399, training loss: 3.457: : 398it [05:27,  1.11s/it] \u001b[A\n",
      "batch 399, training loss: 3.457: : 399it [05:27,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.2466: : 399it [05:28,  1.11s/it]\u001b[A\n",
      "batch 400, training loss: 3.2466: : 400it [05:28,  1.13s/it]\u001b[A\n",
      "batch 401, training loss: 3.1952: : 400it [05:29,  1.13s/it]\u001b[A\n",
      "batch 401, training loss: 3.1952: : 401it [05:29,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 3.2804: : 401it [05:30,  1.11s/it]\u001b[A\n",
      "batch 402, training loss: 3.2804: : 402it [05:30,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 3.4325: : 402it [05:31,  1.11s/it]\u001b[A\n",
      "batch 403, training loss: 3.4325: : 403it [05:31,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.0571: : 403it [05:32,  1.12s/it]\u001b[A\n",
      "batch 404, training loss: 3.0571: : 404it [05:32,  1.13s/it]\u001b[A\n",
      "batch 405, training loss: 3.3583: : 404it [05:33,  1.13s/it]\u001b[A\n",
      "batch 405, training loss: 3.3583: : 405it [05:33,  1.14s/it]\u001b[A\n",
      "batch 406, training loss: 3.2584: : 405it [05:34,  1.14s/it]\u001b[A\n",
      "batch 406, training loss: 3.2584: : 406it [05:34,  1.11s/it]\u001b[A\n",
      "batch 407, training loss: 3.33: : 406it [05:36,  1.11s/it]  \u001b[A\n",
      "batch 407, training loss: 3.33: : 407it [05:36,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.1825: : 407it [05:37,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.1825: : 408it [05:37,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.3431: : 408it [05:38,  1.11s/it]\u001b[A\n",
      "batch 409, training loss: 3.3431: : 409it [05:38,  1.12s/it]\u001b[A\n",
      "batch 410, training loss: 3.2261: : 409it [05:39,  1.12s/it]\u001b[A\n",
      "batch 410, training loss: 3.2261: : 410it [05:39,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.3296: : 410it [05:40,  1.11s/it]\u001b[A\n",
      "batch 411, training loss: 3.3296: : 411it [05:40,  1.12s/it]\u001b[A\n",
      "batch 412, training loss: 3.3532: : 411it [05:41,  1.12s/it]\u001b[A\n",
      "batch 412, training loss: 3.3532: : 412it [05:41,  1.11s/it]\u001b[A\n",
      "batch 413, training loss: 3.2558: : 412it [05:42,  1.11s/it]\u001b[A\n",
      "batch 413, training loss: 3.2558: : 413it [05:42,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.1934: : 413it [05:43,  1.09s/it]\u001b[A\n",
      "batch 414, training loss: 3.1934: : 414it [05:43,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.1104: : 414it [05:44,  1.10s/it]\u001b[A\n",
      "batch 415, training loss: 3.1104: : 415it [05:44,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 3.1342: : 415it [05:45,  1.09s/it]\u001b[A\n",
      "batch 416, training loss: 3.1342: : 416it [05:45,  1.08s/it]\u001b[A\n",
      "batch 417, training loss: 3.3913: : 416it [05:47,  1.08s/it]\u001b[A\n",
      "batch 417, training loss: 3.3913: : 417it [05:47,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.2861: : 417it [05:48,  1.10s/it]\u001b[A\n",
      "batch 418, training loss: 3.2861: : 418it [05:48,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.1004: : 418it [05:49,  1.10s/it]\u001b[A\n",
      "batch 419, training loss: 3.1004: : 419it [05:49,  1.08s/it]\u001b[A\n",
      "batch 420, training loss: 3.217: : 419it [05:50,  1.08s/it] \u001b[A\n",
      "batch 420, training loss: 3.217: : 420it [05:50,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.3309: : 420it [05:51,  1.09s/it]\u001b[A\n",
      "batch 421, training loss: 3.3309: : 421it [05:51,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.3562: : 421it [05:52,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.3562: : 422it [05:52,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.1425: : 422it [05:53,  1.10s/it]\u001b[A\n",
      "batch 423, training loss: 3.1425: : 423it [05:53,  1.11s/it]\u001b[A\n",
      "batch 424, training loss: 3.3238: : 423it [05:54,  1.11s/it]\u001b[A\n",
      "batch 424, training loss: 3.3238: : 424it [05:54,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.3706: : 424it [05:55,  1.11s/it]\u001b[A\n",
      "batch 425, training loss: 3.3706: : 425it [05:55,  1.12s/it]\u001b[A\n",
      "batch 426, training loss: 3.3012: : 425it [05:57,  1.12s/it]\u001b[A\n",
      "batch 426, training loss: 3.3012: : 426it [05:57,  1.14s/it]\u001b[A\n",
      "batch 427, training loss: 3.3545: : 426it [05:58,  1.14s/it]\u001b[A\n",
      "batch 427, training loss: 3.3545: : 427it [05:58,  1.16s/it]\u001b[A\n",
      "batch 428, training loss: 3.4854: : 427it [05:59,  1.16s/it]\u001b[A\n",
      "batch 428, training loss: 3.4854: : 428it [05:59,  1.17s/it]\u001b[A\n",
      "batch 429, training loss: 3.3294: : 428it [06:00,  1.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 429, training loss: 3.3294: : 429it [06:00,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.3818: : 429it [06:01,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.3818: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.4323: : 430it [06:03,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.4323: : 431it [06:03,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 3.2588: : 431it [06:04,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 3.2588: : 432it [06:04,  1.22s/it]\u001b[A\n",
      "batch 433, training loss: 3.3045: : 432it [06:05,  1.22s/it]\u001b[A\n",
      "batch 433, training loss: 3.3045: : 433it [06:05,  1.22s/it]\u001b[A\n",
      "batch 434, training loss: 3.3296: : 433it [06:06,  1.22s/it]\u001b[A\n",
      "batch 434, training loss: 3.3296: : 434it [06:06,  1.23s/it]\u001b[A\n",
      "batch 435, training loss: 3.305: : 434it [06:08,  1.23s/it] \u001b[A\n",
      "batch 435, training loss: 3.305: : 435it [06:08,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3391: : 435it [06:09,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3391: : 436it [06:09,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.3025: : 436it [06:10,  1.22s/it]\u001b[A\n",
      "batch 437, training loss: 3.3025: : 437it [06:10,  1.22s/it]\u001b[A\n",
      "batch 438, training loss: 3.3184: : 437it [06:11,  1.22s/it]\u001b[A\n",
      "batch 438, training loss: 3.3184: : 438it [06:11,  1.23s/it]\u001b[A\n",
      "batch 439, training loss: 3.4418: : 438it [06:12,  1.23s/it]\u001b[A\n",
      "batch 439, training loss: 3.4418: : 439it [06:12,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 3.2727: : 439it [06:14,  1.23s/it]\u001b[A\n",
      "batch 440, training loss: 3.2727: : 440it [06:14,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 3.5701: : 440it [06:15,  1.23s/it]\u001b[A\n",
      "batch 441, training loss: 3.5701: : 441it [06:15,  1.23s/it]\u001b[A\n",
      "batch 442, training loss: 3.209: : 441it [06:16,  1.23s/it] \u001b[A\n",
      "batch 442, training loss: 3.209: : 442it [06:16,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 3.3215: : 442it [06:17,  1.22s/it]\u001b[A\n",
      "batch 443, training loss: 3.3215: : 443it [06:17,  1.22s/it]\u001b[A\n",
      "batch 444, training loss: 3.3042: : 443it [06:18,  1.22s/it]\u001b[A\n",
      "batch 444, training loss: 3.3042: : 444it [06:18,  1.12s/it]\u001b[A\n",
      "batch 445, training loss: 3.3621: : 444it [06:19,  1.12s/it]\u001b[A\n",
      "batch 445, training loss: 3.3621: : 445it [06:19,  1.14s/it]\u001b[A\n",
      "batch 446, training loss: 3.3273: : 445it [06:21,  1.14s/it]\u001b[A\n",
      "batch 446, training loss: 3.3273: : 446it [06:21,  1.15s/it]\u001b[A\n",
      "batch 447, training loss: 3.3093: : 446it [06:22,  1.15s/it]\u001b[A\n",
      "batch 447, training loss: 3.3093: : 447it [06:22,  1.18s/it]\u001b[A\n",
      "batch 448, training loss: 3.2215: : 447it [06:23,  1.18s/it]\u001b[A\n",
      "batch 448, training loss: 3.2215: : 448it [06:23,  1.18s/it]\u001b[A\n",
      "batch 449, training loss: 3.2962: : 448it [06:24,  1.18s/it]\u001b[A\n",
      "batch 449, training loss: 3.2962: : 449it [06:24,  1.18s/it]\u001b[A\n",
      "batch 450, training loss: 3.3894: : 449it [06:25,  1.18s/it]\u001b[A\n",
      "batch 450, training loss: 3.3894: : 450it [06:25,  1.20s/it]\u001b[A\n",
      "batch 451, training loss: 3.4006: : 450it [06:27,  1.20s/it]\u001b[A\n",
      "batch 451, training loss: 3.4006: : 451it [06:27,  1.21s/it]\u001b[A\n",
      "batch 452, training loss: 3.3713: : 451it [06:28,  1.21s/it]\u001b[A\n",
      "batch 452, training loss: 3.3713: : 452it [06:28,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.4558: : 452it [06:29,  1.22s/it]\u001b[A\n",
      "batch 453, training loss: 3.4558: : 453it [06:29,  1.22s/it]\u001b[A\n",
      "batch 454, training loss: 3.343: : 453it [06:30,  1.22s/it] \u001b[A\n",
      "batch 454, training loss: 3.343: : 454it [06:30,  1.22s/it]\u001b[A\n",
      "batch 455, training loss: 3.2475: : 454it [06:32,  1.22s/it]\u001b[A\n",
      "batch 455, training loss: 3.2475: : 455it [06:32,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.2569: : 455it [06:33,  1.22s/it]\u001b[A\n",
      "batch 456, training loss: 3.2569: : 456it [06:33,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 3.3755: : 456it [06:34,  1.22s/it]\u001b[A\n",
      "batch 457, training loss: 3.3755: : 457it [06:34,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 3.2737: : 457it [06:35,  1.22s/it]\u001b[A\n",
      "batch 458, training loss: 3.2737: : 458it [06:35,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.3257: : 458it [06:37,  1.23s/it]\u001b[A\n",
      "batch 459, training loss: 3.3257: : 459it [06:37,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.3116: : 459it [06:38,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.3116: : 460it [06:38,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.4122: : 460it [06:39,  1.24s/it]\u001b[A\n",
      "batch 461, training loss: 3.4122: : 461it [06:39,  1.22s/it]\u001b[A\n",
      "batch 462, training loss: 3.3737: : 461it [06:40,  1.22s/it]\u001b[A\n",
      "batch 462, training loss: 3.3737: : 462it [06:40,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.2125: : 462it [06:41,  1.23s/it]\u001b[A\n",
      "batch 463, training loss: 3.2125: : 463it [06:41,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 3.3346: : 463it [06:43,  1.22s/it]\u001b[A\n",
      "batch 464, training loss: 3.3346: : 464it [06:43,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.2679: : 464it [06:44,  1.23s/it]\u001b[A\n",
      "batch 465, training loss: 3.2679: : 465it [06:44,  1.17s/it]\u001b[A\n",
      "batch 466, training loss: 3.157: : 465it [06:45,  1.17s/it] \u001b[A\n",
      "batch 466, training loss: 3.157: : 466it [06:45,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.1079: : 466it [06:46,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.1079: : 467it [06:46,  1.24s/it]\u001b[A\n",
      "batch 468, training loss: 3.3258: : 467it [06:48,  1.24s/it]\u001b[A\n",
      "batch 468, training loss: 3.3258: : 468it [06:48,  1.26s/it]\u001b[A\n",
      "batch 469, training loss: 3.2019: : 468it [06:49,  1.26s/it]\u001b[A\n",
      "batch 469, training loss: 3.2019: : 469it [06:49,  1.28s/it]\u001b[A\n",
      "batch 470, training loss: 3.3579: : 469it [06:50,  1.28s/it]\u001b[A\n",
      "batch 470, training loss: 3.3579: : 470it [06:50,  1.28s/it]\u001b[A\n",
      "batch 471, training loss: 3.3017: : 470it [06:51,  1.28s/it]\u001b[A\n",
      "batch 471, training loss: 3.3017: : 471it [06:51,  1.28s/it]\u001b[A\n",
      "batch 472, training loss: 3.26: : 471it [06:53,  1.28s/it]  \u001b[A\n",
      "batch 472, training loss: 3.26: : 472it [06:53,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.2655: : 472it [06:54,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.2655: : 473it [06:54,  1.29s/it]\u001b[A\n",
      "batch 474, training loss: 3.3025: : 473it [06:55,  1.29s/it]\u001b[A\n",
      "batch 474, training loss: 3.3025: : 474it [06:55,  1.31s/it]\u001b[A\n",
      "batch 475, training loss: 3.2073: : 474it [06:57,  1.31s/it]\u001b[A\n",
      "batch 475, training loss: 3.2073: : 475it [06:57,  1.31s/it]\u001b[A\n",
      "batch 476, training loss: 3.3154: : 475it [06:58,  1.31s/it]\u001b[A\n",
      "batch 476, training loss: 3.3154: : 476it [06:58,  1.31s/it]\u001b[A\n",
      "batch 477, training loss: 3.1402: : 476it [06:59,  1.31s/it]\u001b[A\n",
      "batch 477, training loss: 3.1402: : 477it [06:59,  1.28s/it]\u001b[A\n",
      "batch 478, training loss: 3.1936: : 477it [07:01,  1.28s/it]\u001b[A\n",
      "batch 478, training loss: 3.1936: : 478it [07:01,  1.29s/it]\u001b[A\n",
      "batch 479, training loss: 3.1932: : 478it [07:02,  1.29s/it]\u001b[A\n",
      "batch 479, training loss: 3.1932: : 479it [07:02,  1.27s/it]\u001b[A\n",
      "batch 480, training loss: 3.2204: : 479it [07:03,  1.27s/it]\u001b[A\n",
      "batch 480, training loss: 3.2204: : 480it [07:03,  1.29s/it]\u001b[A\n",
      "batch 481, training loss: 3.2409: : 480it [07:04,  1.29s/it]\u001b[A\n",
      "batch 481, training loss: 3.2409: : 481it [07:04,  1.27s/it]\u001b[A\n",
      "batch 482, training loss: 3.2164: : 481it [07:06,  1.27s/it]\u001b[A\n",
      "batch 482, training loss: 3.2164: : 482it [07:06,  1.29s/it]\u001b[A\n",
      "batch 483, training loss: 3.1756: : 482it [07:07,  1.29s/it]\u001b[A\n",
      "batch 483, training loss: 3.1756: : 483it [07:07,  1.30s/it]\u001b[A\n",
      "batch 484, training loss: 3.2559: : 483it [07:08,  1.30s/it]\u001b[A\n",
      "batch 484, training loss: 3.2559: : 484it [07:08,  1.31s/it]\u001b[A\n",
      "batch 485, training loss: 3.1785: : 484it [07:10,  1.31s/it]\u001b[A\n",
      "batch 485, training loss: 3.1785: : 485it [07:10,  1.30s/it]\u001b[A\n",
      "batch 486, training loss: 3.2834: : 485it [07:11,  1.30s/it]\u001b[A\n",
      "batch 486, training loss: 3.2834: : 486it [07:11,  1.29s/it]\u001b[A\n",
      "batch 487, training loss: 3.3755: : 486it [07:12,  1.29s/it]\u001b[A\n",
      "batch 487, training loss: 3.3755: : 487it [07:12,  1.29s/it]\u001b[A\n",
      "batch 488, training loss: 3.1843: : 487it [07:13,  1.29s/it]\u001b[A\n",
      "batch 488, training loss: 3.1843: : 488it [07:13,  1.27s/it]\u001b[A\n",
      "batch 489, training loss: 3.1227: : 488it [07:15,  1.27s/it]\u001b[A\n",
      "batch 489, training loss: 3.1227: : 489it [07:15,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 3.1585: : 489it [07:16,  1.27s/it]\u001b[A\n",
      "batch 490, training loss: 3.1585: : 490it [07:16,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.2366: : 490it [07:17,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.2366: : 491it [07:17,  1.28s/it]\u001b[A\n",
      "batch 492, training loss: 3.1318: : 491it [07:18,  1.28s/it]\u001b[A\n",
      "batch 492, training loss: 3.1318: : 492it [07:18,  1.26s/it]\u001b[A\n",
      "batch 493, training loss: 3.3116: : 492it [07:20,  1.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 493, training loss: 3.3116: : 493it [07:20,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 3.3352: : 493it [07:21,  1.27s/it]\u001b[A\n",
      "batch 494, training loss: 3.3352: : 494it [07:21,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.1416: : 494it [07:22,  1.26s/it]\u001b[A\n",
      "batch 495, training loss: 3.1416: : 495it [07:22,  1.28s/it]\u001b[A\n",
      "batch 496, training loss: 3.1707: : 495it [07:24,  1.28s/it]\u001b[A\n",
      "batch 496, training loss: 3.1707: : 496it [07:24,  1.30s/it]\u001b[A\n",
      "batch 497, training loss: 3.0788: : 496it [07:25,  1.30s/it]\u001b[A\n",
      "batch 497, training loss: 3.0788: : 497it [07:25,  1.30s/it]\u001b[A\n",
      "batch 498, training loss: 3.1982: : 497it [07:26,  1.30s/it]\u001b[A\n",
      "batch 498, training loss: 3.1982: : 498it [07:26,  1.30s/it]\u001b[A\n",
      "batch 499, training loss: 3.0249: : 498it [07:27,  1.30s/it]\u001b[A\n",
      "batch 499, training loss: 3.0249: : 499it [07:27,  1.16s/it]\u001b[A\n",
      "batch 500, training loss: 3.3918: : 499it [07:28,  1.16s/it]\u001b[A\n",
      "batch 500, training loss: 3.3918: : 500it [07:28,  1.22s/it]\u001b[A\n",
      "batch 501, training loss: 3.2535: : 500it [07:30,  1.22s/it]\u001b[A\n",
      "batch 501, training loss: 3.2535: : 501it [07:30,  1.28s/it]\u001b[A\n",
      "batch 502, training loss: 3.1934: : 501it [07:31,  1.28s/it]\u001b[A\n",
      "batch 502, training loss: 3.1934: : 502it [07:31,  1.32s/it]\u001b[A\n",
      "batch 503, training loss: 3.3037: : 502it [07:33,  1.32s/it]\u001b[A\n",
      "batch 503, training loss: 3.3037: : 503it [07:33,  1.34s/it]\u001b[A\n",
      "batch 504, training loss: 3.2926: : 503it [07:34,  1.34s/it]\u001b[A\n",
      "batch 504, training loss: 3.2926: : 504it [07:34,  1.36s/it]\u001b[A\n",
      "batch 505, training loss: 3.3749: : 504it [07:35,  1.36s/it]\u001b[A\n",
      "batch 505, training loss: 3.3749: : 505it [07:35,  1.37s/it]\u001b[A\n",
      "batch 506, training loss: 3.2587: : 505it [07:37,  1.37s/it]\u001b[A\n",
      "batch 506, training loss: 3.2587: : 506it [07:37,  1.37s/it]\u001b[A\n",
      "batch 507, training loss: 3.1764: : 506it [07:38,  1.37s/it]\u001b[A\n",
      "batch 507, training loss: 3.1764: : 507it [07:38,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 3.3181: : 507it [07:40,  1.36s/it]\u001b[A\n",
      "batch 508, training loss: 3.3181: : 508it [07:40,  1.38s/it]\u001b[A\n",
      "batch 509, training loss: 3.1962: : 508it [07:41,  1.38s/it]\u001b[A\n",
      "batch 509, training loss: 3.1962: : 509it [07:41,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.2824: : 509it [07:42,  1.37s/it]\u001b[A\n",
      "batch 510, training loss: 3.2824: : 510it [07:42,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.2693: : 510it [07:44,  1.38s/it]\u001b[A\n",
      "batch 511, training loss: 3.2693: : 511it [07:44,  1.38s/it]\u001b[A\n",
      "batch 512, training loss: 3.2551: : 511it [07:45,  1.38s/it]\u001b[A\n",
      "batch 512, training loss: 3.2551: : 512it [07:45,  1.37s/it]\u001b[A\n",
      "batch 513, training loss: 3.3181: : 512it [07:46,  1.37s/it]\u001b[A\n",
      "batch 513, training loss: 3.3181: : 513it [07:46,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 3.1944: : 513it [07:48,  1.36s/it]\u001b[A\n",
      "batch 514, training loss: 3.1944: : 514it [07:48,  1.37s/it]\u001b[A\n",
      "batch 515, training loss: 3.3699: : 514it [07:49,  1.37s/it]\u001b[A\n",
      "batch 515, training loss: 3.3699: : 515it [07:49,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 3.2174: : 515it [07:51,  1.37s/it]\u001b[A\n",
      "batch 516, training loss: 3.2174: : 516it [07:51,  1.38s/it]\u001b[A\n",
      "batch 517, training loss: 3.23: : 516it [07:52,  1.38s/it]  \u001b[A\n",
      "batch 517, training loss: 3.23: : 517it [07:52,  1.29s/it]\u001b[A\n",
      "batch 518, training loss: 3.3317: : 517it [07:53,  1.29s/it]\u001b[A\n",
      "batch 518, training loss: 3.3317: : 518it [07:53,  1.16s/it]\u001b[A\n",
      "batch 519, training loss: 3.112: : 518it [07:54,  1.16s/it] \u001b[A\n",
      "batch 519, training loss: 3.112: : 519it [07:54,  1.25s/it]\u001b[A\n",
      "batch 520, training loss: 3.2864: : 519it [07:55,  1.25s/it]\u001b[A\n",
      "batch 520, training loss: 3.2864: : 520it [07:55,  1.30s/it]\u001b[A\n",
      "batch 521, training loss: 3.3065: : 520it [07:57,  1.30s/it]\u001b[A\n",
      "batch 521, training loss: 3.3065: : 521it [07:57,  1.31s/it]\u001b[A\n",
      "batch 522, training loss: 3.3139: : 521it [07:58,  1.31s/it]\u001b[A\n",
      "batch 522, training loss: 3.3139: : 522it [07:58,  1.34s/it]\u001b[A\n",
      "batch 523, training loss: 3.2067: : 522it [07:59,  1.34s/it]\u001b[A\n",
      "batch 523, training loss: 3.2067: : 523it [07:59,  1.32s/it]\u001b[A\n",
      "batch 524, training loss: 3.3839: : 523it [08:01,  1.32s/it]\u001b[A\n",
      "batch 524, training loss: 3.3839: : 524it [08:01,  1.32s/it]\u001b[A\n",
      "batch 525, training loss: 3.4301: : 524it [08:02,  1.32s/it]\u001b[A\n",
      "batch 525, training loss: 3.4301: : 525it [08:02,  1.30s/it]\u001b[A\n",
      "batch 526, training loss: 3.2234: : 525it [08:03,  1.30s/it]\u001b[A\n",
      "batch 526, training loss: 3.2234: : 526it [08:03,  1.31s/it]\u001b[A\n",
      "batch 527, training loss: 3.2257: : 526it [08:04,  1.31s/it]\u001b[A\n",
      "batch 527, training loss: 3.2257: : 527it [08:04,  1.27s/it]\u001b[A\n",
      "batch 528, training loss: 3.2484: : 527it [08:06,  1.27s/it]\u001b[A\n",
      "batch 528, training loss: 3.2484: : 528it [08:06,  1.30s/it]\u001b[A\n",
      "batch 529, training loss: 3.3018: : 528it [08:07,  1.30s/it]\u001b[A\n",
      "batch 529, training loss: 3.3018: : 529it [08:07,  1.36s/it]\u001b[A\n",
      "batch 530, training loss: 3.2853: : 529it [08:09,  1.36s/it]\u001b[A\n",
      "batch 530, training loss: 3.2853: : 530it [08:09,  1.43s/it]\u001b[A\n",
      "batch 531, training loss: 3.2094: : 530it [08:10,  1.43s/it]\u001b[A\n",
      "batch 531, training loss: 3.2094: : 531it [08:10,  1.43s/it]\u001b[A\n",
      "batch 532, training loss: 3.0338: : 531it [08:12,  1.43s/it]\u001b[A\n",
      "batch 532, training loss: 3.0338: : 532it [08:12,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.1658: : 532it [08:13,  1.43s/it]\u001b[A\n",
      "batch 533, training loss: 3.1658: : 533it [08:13,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.3253: : 533it [08:15,  1.46s/it]\u001b[A\n",
      "batch 534, training loss: 3.3253: : 534it [08:15,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.1938: : 534it [08:16,  1.48s/it]\u001b[A\n",
      "batch 535, training loss: 3.1938: : 535it [08:16,  1.48s/it]\u001b[A\n",
      "batch 536, training loss: 3.0542: : 535it [08:18,  1.48s/it]\u001b[A\n",
      "batch 536, training loss: 3.0542: : 536it [08:18,  1.47s/it]\u001b[A\n",
      "batch 537, training loss: 3.1357: : 536it [08:19,  1.47s/it]\u001b[A\n",
      "batch 537, training loss: 3.1357: : 537it [08:19,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.2246: : 537it [08:21,  1.46s/it]\u001b[A\n",
      "batch 538, training loss: 3.2246: : 538it [08:21,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.1949: : 538it [08:22,  1.48s/it]\u001b[A\n",
      "batch 539, training loss: 3.1949: : 539it [08:22,  1.50s/it]\u001b[A\n",
      "batch 540, training loss: 3.1552: : 539it [08:24,  1.50s/it]\u001b[A\n",
      "batch 540, training loss: 3.1552: : 540it [08:24,  1.51s/it]\u001b[A\n",
      "batch 541, training loss: 3.2662: : 540it [08:25,  1.51s/it]\u001b[A\n",
      "batch 541, training loss: 3.2662: : 541it [08:25,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.2976: : 541it [08:27,  1.48s/it]\u001b[A\n",
      "batch 542, training loss: 3.2976: : 542it [08:27,  1.46s/it]\u001b[A\n",
      "batch 543, training loss: 3.1389: : 542it [08:28,  1.46s/it]\u001b[A\n",
      "batch 543, training loss: 3.1389: : 543it [08:28,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 3.2037: : 543it [08:30,  1.47s/it]\u001b[A\n",
      "batch 544, training loss: 3.2037: : 544it [08:30,  1.50s/it]\u001b[A\n",
      "batch 545, training loss: 3.2154: : 544it [08:31,  1.50s/it]\u001b[A\n",
      "batch 545, training loss: 3.2154: : 545it [08:31,  1.46s/it]\u001b[A\n",
      "batch 546, training loss: 3.136: : 545it [08:33,  1.46s/it] \u001b[A\n",
      "batch 546, training loss: 3.136: : 546it [08:33,  1.46s/it]\u001b[A\n",
      "batch 547, training loss: 3.0546: : 546it [08:34,  1.46s/it]\u001b[A\n",
      "batch 547, training loss: 3.0546: : 547it [08:34,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 2.8019: : 547it [08:35,  1.47s/it]\u001b[A\n",
      "batch 548, training loss: 2.8019: : 548it [08:35,  1.28s/it]\u001b[A\n",
      "batch 549, training loss: 3.2021: : 548it [08:36,  1.28s/it]\u001b[A\n",
      "batch 549, training loss: 3.2021: : 549it [08:36,  1.37s/it]\u001b[A\n",
      "batch 550, training loss: 3.1605: : 549it [08:38,  1.37s/it]\u001b[A\n",
      "batch 550, training loss: 3.1605: : 550it [08:38,  1.44s/it]\u001b[A\n",
      "batch 551, training loss: 3.1533: : 550it [08:40,  1.44s/it]\u001b[A\n",
      "batch 551, training loss: 3.1533: : 551it [08:40,  1.50s/it]\u001b[A\n",
      "batch 552, training loss: 3.2074: : 551it [08:41,  1.50s/it]\u001b[A\n",
      "batch 552, training loss: 3.2074: : 552it [08:41,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.2018: : 552it [08:43,  1.54s/it]\u001b[A\n",
      "batch 553, training loss: 3.2018: : 553it [08:43,  1.57s/it]\u001b[A\n",
      "batch 554, training loss: 2.9365: : 553it [08:45,  1.57s/it]\u001b[A\n",
      "batch 554, training loss: 2.9365: : 554it [08:45,  1.57s/it]\u001b[A\n",
      "batch 555, training loss: 3.0704: : 554it [08:46,  1.57s/it]\u001b[A\n",
      "batch 555, training loss: 3.0704: : 555it [08:46,  1.57s/it]\u001b[A\n",
      "batch 556, training loss: 3.2695: : 555it [08:48,  1.57s/it]\u001b[A\n",
      "batch 556, training loss: 3.2695: : 556it [08:48,  1.55s/it]\u001b[A\n",
      "batch 557, training loss: 3.0711: : 556it [08:49,  1.55s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 557, training loss: 3.0711: : 557it [08:49,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.0376: : 557it [08:51,  1.52s/it]\u001b[A\n",
      "batch 558, training loss: 3.0376: : 558it [08:51,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.1492: : 558it [08:52,  1.54s/it]\u001b[A\n",
      "batch 559, training loss: 3.1492: : 559it [08:52,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.1363: : 559it [08:54,  1.55s/it]\u001b[A\n",
      "batch 560, training loss: 3.1363: : 560it [08:54,  1.57s/it]\u001b[A\n",
      "batch 561, training loss: 3.1117: : 560it [08:55,  1.57s/it]\u001b[A\n",
      "batch 561, training loss: 3.1117: : 561it [08:55,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 3.0068: : 561it [08:57,  1.58s/it]\u001b[A\n",
      "batch 562, training loss: 3.0068: : 562it [08:57,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.1455: : 562it [08:59,  1.60s/it]\u001b[A\n",
      "batch 563, training loss: 3.1455: : 563it [08:59,  1.60s/it]\u001b[A\n",
      "batch 564, training loss: 2.9984: : 563it [09:00,  1.60s/it]\u001b[A\n",
      "batch 564, training loss: 2.9984: : 564it [09:00,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 3.1199: : 564it [09:02,  1.61s/it]\u001b[A\n",
      "batch 565, training loss: 3.1199: : 565it [09:02,  1.57s/it]\u001b[A\n",
      "batch 566, training loss: 3.2452: : 565it [09:03,  1.57s/it]\u001b[A\n",
      "batch 566, training loss: 3.2452: : 566it [09:03,  1.61s/it]\u001b[A\n",
      "batch 567, training loss: 3.207: : 566it [09:05,  1.61s/it] \u001b[A\n",
      "batch 567, training loss: 3.207: : 567it [09:05,  1.62s/it]\u001b[A\n",
      "batch 568, training loss: 3.2762: : 567it [09:07,  1.62s/it]\u001b[A\n",
      "batch 568, training loss: 3.2762: : 568it [09:07,  1.66s/it]\u001b[A\n",
      "batch 569, training loss: 3.2654: : 568it [09:09,  1.66s/it]\u001b[A\n",
      "batch 569, training loss: 3.2654: : 569it [09:09,  1.68s/it]\u001b[A\n",
      "batch 570, training loss: 3.4255: : 569it [09:10,  1.68s/it]\u001b[A\n",
      "batch 570, training loss: 3.4255: : 570it [09:10,  1.69s/it]\u001b[A\n",
      "batch 571, training loss: 3.2113: : 570it [09:12,  1.69s/it]\u001b[A\n",
      "batch 571, training loss: 3.2113: : 571it [09:12,  1.69s/it]\u001b[A\n",
      "batch 572, training loss: 3.342: : 571it [09:14,  1.69s/it] \u001b[A\n",
      "batch 572, training loss: 3.342: : 572it [09:14,  1.69s/it]\u001b[A\n",
      "batch 573, training loss: 3.2285: : 572it [09:15,  1.69s/it]\u001b[A\n",
      "batch 573, training loss: 3.2285: : 573it [09:15,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 3.3053: : 573it [09:17,  1.67s/it]\u001b[A\n",
      "batch 574, training loss: 3.3053: : 574it [09:17,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 2.6205: : 574it [09:18,  1.67s/it]\u001b[A\n",
      "batch 575, training loss: 2.6205: : 575it [09:18,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.3113: : 575it [09:20,  1.42s/it]\u001b[A\n",
      "batch 576, training loss: 3.3113: : 576it [09:20,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.1589: : 576it [09:21,  1.51s/it]\u001b[A\n",
      "batch 577, training loss: 3.1589: : 577it [09:21,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.1367: : 577it [09:23,  1.59s/it]\u001b[A\n",
      "batch 578, training loss: 3.1367: : 578it [09:23,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 3.2183: : 578it [09:25,  1.62s/it]\u001b[A\n",
      "batch 579, training loss: 3.2183: : 579it [09:25,  1.64s/it]\u001b[A\n",
      "batch 580, training loss: 3.1199: : 579it [09:26,  1.64s/it]\u001b[A\n",
      "batch 580, training loss: 3.1199: : 580it [09:26,  1.68s/it]\u001b[A\n",
      "batch 581, training loss: 3.1269: : 580it [09:28,  1.68s/it]\u001b[A\n",
      "batch 581, training loss: 3.1269: : 581it [09:28,  1.52s/it]\u001b[A\n",
      "batch 582, training loss: 3.1759: : 581it [09:29,  1.52s/it]\u001b[A\n",
      "batch 582, training loss: 3.1759: : 582it [09:29,  1.57s/it]\u001b[A\n",
      "batch 583, training loss: 2.2199: : 582it [09:30,  1.57s/it]\u001b[A\n",
      "batch 583, training loss: 2.2199: : 583it [09:30,  1.30s/it]\u001b[A\n",
      "batch 584, training loss: 3.3272: : 583it [09:32,  1.30s/it]\u001b[A\n",
      "batch 584, training loss: 3.3272: : 584it [09:32,  1.48s/it]\u001b[A\n",
      "batch 585, training loss: 3.2695: : 584it [09:34,  1.48s/it]\u001b[A\n",
      "batch 585, training loss: 3.2695: : 585it [09:34,  1.60s/it]\u001b[A\n",
      "batch 586, training loss: 3.3576: : 585it [09:35,  1.60s/it]\u001b[A\n",
      "batch 586, training loss: 3.3576: : 586it [09:35,  1.57s/it]\u001b[A\n",
      "batch 587, training loss: 3.2409: : 586it [09:37,  1.57s/it]\u001b[A\n",
      "batch 587, training loss: 3.2409: : 587it [09:37,  1.60s/it]\u001b[A\n",
      "batch 588, training loss: 3.1511: : 587it [09:39,  1.60s/it]\u001b[A\n",
      "batch 588, training loss: 3.1511: : 588it [09:39,  1.69s/it]\u001b[A\n",
      "batch 589, training loss: 3.1996: : 588it [09:41,  1.69s/it]\u001b[A\n",
      "batch 589, training loss: 3.1996: : 589it [09:41,  1.77s/it]\u001b[A\n",
      "batch 590, training loss: 3.235: : 589it [09:43,  1.77s/it] \u001b[A\n",
      "batch 590, training loss: 3.235: : 590it [09:43,  1.84s/it]\u001b[A\n",
      "batch 591, training loss: 3.1351: : 590it [09:45,  1.84s/it]\u001b[A\n",
      "batch 591, training loss: 3.1351: : 591it [09:45,  1.88s/it]\u001b[A\n",
      "batch 592, training loss: 3.1694: : 591it [09:46,  1.88s/it]\u001b[A\n",
      "batch 592, training loss: 3.1694: : 592it [09:46,  1.83s/it]\u001b[A\n",
      "batch 593, training loss: 3.0729: : 592it [09:48,  1.83s/it]\u001b[A\n",
      "batch 593, training loss: 3.0729: : 593it [09:48,  1.88s/it]\u001b[A\n",
      "batch 594, training loss: 3.2152: : 593it [09:51,  1.88s/it]\u001b[A\n",
      "batch 594, training loss: 3.2152: : 594it [09:51,  1.94s/it]\u001b[A\n",
      "batch 595, training loss: 3.2512: : 594it [09:52,  1.94s/it]\u001b[A\n",
      "batch 595, training loss: 3.2512: : 595it [09:52,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.2774: : 595it [09:54,  1.87s/it]\u001b[A\n",
      "batch 596, training loss: 3.2774: : 596it [09:54,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.0874: : 596it [09:56,  1.94s/it]\u001b[A\n",
      "batch 597, training loss: 3.0874: : 597it [09:56,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.2845: : 597it [09:59,  1.96s/it]\u001b[A\n",
      "batch 598, training loss: 3.2845: : 598it [09:59,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.0676: : 598it [10:00,  2.05s/it]\u001b[A\n",
      "batch 599, training loss: 3.0676: : 599it [10:00,  1.85s/it]\u001b[A\n",
      "batch 600, training loss: 3.124: : 599it [10:02,  1.85s/it] \u001b[A\n",
      "batch 600, training loss: 3.124: : 600it [10:02,  2.00s/it]\u001b[A\n",
      "batch 601, training loss: 1.9725: : 600it [10:03,  2.00s/it]\u001b[A\n",
      "batch 601, training loss: 1.9725: : 601it [10:03,  1.67s/it]\u001b[A\n",
      "batch 602, training loss: 3.1284: : 601it [10:05,  1.67s/it]\u001b[A\n",
      "batch 602, training loss: 3.1284: : 602it [10:05,  1.78s/it]\u001b[A\n",
      "batch 603, training loss: 3.1264: : 602it [10:07,  1.78s/it]\u001b[A\n",
      "batch 603, training loss: 3.1264: : 603it [10:07,  1.82s/it]\u001b[A\n",
      "batch 604, training loss: 3.0648: : 603it [10:09,  1.82s/it]\u001b[A\n",
      "batch 604, training loss: 3.0648: : 604it [10:09,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 3.1902: : 604it [10:11,  1.84s/it]\u001b[A\n",
      "batch 605, training loss: 3.1902: : 605it [10:11,  1.80s/it]\u001b[A\n",
      "batch 606, training loss: 2.9256: : 605it [10:12,  1.80s/it]\u001b[A\n",
      "batch 606, training loss: 2.9256: : 606it [10:12,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 2.7647: : 606it [10:14,  1.74s/it]\u001b[A\n",
      "batch 607, training loss: 2.7647: : 607it [10:14,  1.68s/it]\u001b[A\n",
      "batch 608, training loss: 2.8064: : 607it [10:15,  1.68s/it]\u001b[A\n",
      "batch 608, training loss: 2.8064: : 608it [10:15,  1.54s/it]\u001b[A\n",
      "batch 609, training loss: 2.8545: : 608it [10:16,  1.54s/it]\u001b[A\n",
      "batch 609, training loss: 2.8545: : 609it [10:16,  1.48s/it]\u001b[A\n",
      "batch 610, training loss: 2.217: : 609it [10:18,  1.48s/it] \u001b[A\n",
      "batch 610, training loss: 2.217: : 610it [10:18,  1.40s/it]\u001b[A\n",
      "batch 611, training loss: 2.2743: : 610it [10:19,  1.40s/it]\u001b[A\n",
      "batch 611, training loss: 2.2743: : 611it [10:19,  1.32s/it]\u001b[A\n",
      "batch 612, training loss: 1.5095: : 611it [10:20,  1.32s/it]\u001b[A\n",
      "batch 612, training loss: 1.5095: : 612it [10:20,  1.27s/it]\u001b[A\n",
      "batch 613, training loss: 1.9798: : 612it [10:21,  1.27s/it]\u001b[A\n",
      "batch 613, training loss: 1.9798: : 613it [10:21,  1.21s/it]\u001b[A\n",
      "batch 613, training loss: 1.9798: : 614it [10:21,  1.13it/s]\u001b[A\n",
      "batch 613, training loss: 1.9798: : 616it [10:22,  1.01s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-dev-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-dev-hier.pkl exist, load directly\n",
      "\n",
      "batch 0, dev loss: 3.637: : 0it [00:00, ?it/s]\u001b[A\n",
      "batch 0, dev loss: 3.637: : 1it [00:00,  2.71it/s]\u001b[A\n",
      "batch 1, dev loss: 3.7977: : 1it [00:00,  2.71it/s]\u001b[A\n",
      "batch 1, dev loss: 3.7977: : 2it [00:00,  3.62it/s]\u001b[A\n",
      "batch 2, dev loss: 3.3859: : 2it [00:00,  3.62it/s]\u001b[A\n",
      "batch 2, dev loss: 3.3859: : 3it [00:00,  3.99it/s]\u001b[A\n",
      "batch 3, dev loss: 3.5787: : 3it [00:01,  3.99it/s]\u001b[A\n",
      "batch 3, dev loss: 3.5787: : 4it [00:01,  4.17it/s]\u001b[A\n",
      "batch 4, dev loss: 3.5432: : 4it [00:01,  4.17it/s]\u001b[A\n",
      "batch 4, dev loss: 3.5432: : 5it [00:01,  4.24it/s]\u001b[A\n",
      "batch 5, dev loss: 3.5413: : 5it [00:01,  4.24it/s]\u001b[A\n",
      "batch 5, dev loss: 3.5413: : 6it [00:01,  4.34it/s]\u001b[A\n",
      "batch 6, dev loss: 3.6224: : 6it [00:01,  4.34it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6, dev loss: 3.6224: : 7it [00:01,  4.67it/s]\u001b[A\n",
      "batch 7, dev loss: 3.434: : 7it [00:01,  4.67it/s] \u001b[A\n",
      "batch 7, dev loss: 3.434: : 8it [00:01,  4.84it/s]\u001b[A\n",
      "batch 8, dev loss: 3.6222: : 8it [00:02,  4.84it/s]\u001b[A\n",
      "batch 8, dev loss: 3.6222: : 9it [00:02,  4.77it/s]\u001b[A\n",
      "batch 9, dev loss: 3.5488: : 9it [00:02,  4.77it/s]\u001b[A\n",
      "batch 9, dev loss: 3.5488: : 10it [00:02,  4.86it/s]\u001b[A\n",
      "batch 10, dev loss: 3.6058: : 10it [00:02,  4.86it/s]\u001b[A\n",
      "batch 10, dev loss: 3.6058: : 11it [00:02,  4.66it/s]\u001b[A\n",
      "batch 11, dev loss: 3.6245: : 11it [00:02,  4.66it/s]\u001b[A\n",
      "batch 11, dev loss: 3.6245: : 12it [00:02,  4.42it/s]\u001b[A\n",
      "batch 12, dev loss: 3.4677: : 12it [00:02,  4.42it/s]\u001b[A\n",
      "batch 12, dev loss: 3.4677: : 13it [00:02,  4.29it/s]\u001b[A\n",
      "batch 13, dev loss: 3.6198: : 13it [00:03,  4.29it/s]\u001b[A\n",
      "batch 13, dev loss: 3.6198: : 14it [00:03,  4.17it/s]\u001b[A\n",
      "batch 14, dev loss: 3.7786: : 14it [00:03,  4.17it/s]\u001b[A\n",
      "batch 14, dev loss: 3.7786: : 15it [00:03,  4.02it/s]\u001b[A\n",
      "batch 15, dev loss: 3.5258: : 15it [00:03,  4.02it/s]\u001b[A\n",
      "batch 15, dev loss: 3.5258: : 16it [00:03,  4.58it/s]\u001b[A\n",
      "batch 16, dev loss: 3.8806: : 16it [00:03,  4.58it/s]\u001b[A\n",
      "batch 16, dev loss: 3.8806: : 17it [00:03,  4.17it/s]\u001b[A\n",
      "batch 17, dev loss: 3.6617: : 17it [00:04,  4.17it/s]\u001b[A\n",
      "batch 17, dev loss: 3.6617: : 18it [00:04,  3.97it/s]\u001b[A\n",
      "batch 18, dev loss: 3.5379: : 18it [00:04,  3.97it/s]\u001b[A\n",
      "batch 18, dev loss: 3.5379: : 19it [00:04,  4.10it/s]\u001b[A\n",
      "batch 19, dev loss: 3.7253: : 19it [00:04,  4.10it/s]\u001b[A\n",
      "batch 19, dev loss: 3.7253: : 20it [00:04,  4.10it/s]\u001b[A\n",
      "batch 20, dev loss: 3.587: : 20it [00:04,  4.10it/s] \u001b[A\n",
      "batch 20, dev loss: 3.587: : 21it [00:04,  3.96it/s]\u001b[A\n",
      "batch 21, dev loss: 3.4405: : 21it [00:05,  3.96it/s]\u001b[A\n",
      "batch 21, dev loss: 3.4405: : 22it [00:05,  4.02it/s]\u001b[A\n",
      "batch 22, dev loss: 3.6426: : 22it [00:05,  4.02it/s]\u001b[A\n",
      "batch 22, dev loss: 3.6426: : 23it [00:05,  4.03it/s]\u001b[A\n",
      "batch 23, dev loss: 3.7324: : 23it [00:05,  4.03it/s]\u001b[A\n",
      "batch 23, dev loss: 3.7324: : 24it [00:05,  4.49it/s]\u001b[A\n",
      "batch 24, dev loss: 3.6356: : 24it [00:05,  4.49it/s]\u001b[A\n",
      "batch 24, dev loss: 3.6356: : 25it [00:05,  4.24it/s]\u001b[A\n",
      "batch 25, dev loss: 3.566: : 25it [00:06,  4.24it/s] \u001b[A\n",
      "batch 25, dev loss: 3.566: : 26it [00:06,  3.97it/s]\u001b[A\n",
      "batch 26, dev loss: 3.574: : 26it [00:06,  3.97it/s]\u001b[A\n",
      "batch 26, dev loss: 3.574: : 27it [00:06,  3.78it/s]\u001b[A\n",
      "batch 27, dev loss: 3.4823: : 27it [00:06,  3.78it/s]\u001b[A\n",
      "batch 27, dev loss: 3.4823: : 28it [00:06,  3.56it/s]\u001b[A\n",
      "batch 28, dev loss: 3.7595: : 28it [00:07,  3.56it/s]\u001b[A\n",
      "batch 28, dev loss: 3.7595: : 29it [00:07,  3.31it/s]\u001b[A\n",
      "batch 29, dev loss: 3.6425: : 29it [00:07,  3.31it/s]\u001b[A\n",
      "batch 29, dev loss: 3.6425: : 30it [00:07,  3.31it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0723: : 30it [00:07,  3.31it/s]\u001b[A\n",
      "batch 30, dev loss: 4.0723: : 31it [00:07,  4.06it/s]\u001b[A\n",
      "batch 31, dev loss: 3.6377: : 31it [00:07,  4.06it/s]\u001b[A\n",
      "batch 31, dev loss: 3.6377: : 32it [00:07,  3.58it/s]\u001b[A\n",
      "batch 32, dev loss: 3.7427: : 32it [00:08,  3.58it/s]\u001b[A\n",
      "batch 32, dev loss: 3.7427: : 33it [00:08,  3.36it/s]\u001b[A\n",
      "batch 33, dev loss: 3.4537: : 33it [00:08,  3.36it/s]\u001b[A\n",
      "batch 33, dev loss: 3.4537: : 34it [00:08,  3.16it/s]\u001b[A\n",
      "batch 34, dev loss: 3.8806: : 34it [00:08,  3.16it/s]\u001b[A\n",
      "batch 34, dev loss: 3.8806: : 35it [00:08,  3.05it/s]\u001b[A\n",
      "batch 35, dev loss: 3.722: : 35it [00:09,  3.05it/s] \u001b[A\n",
      "batch 35, dev loss: 3.722: : 36it [00:09,  3.02it/s]\u001b[A\n",
      "batch 36, dev loss: 3.6495: : 36it [00:09,  3.02it/s]\u001b[A\n",
      "batch 36, dev loss: 3.6495: : 37it [00:09,  3.34it/s]\u001b[A\n",
      "batch 37, dev loss: 3.4125: : 37it [00:09,  3.34it/s]\u001b[A\n",
      "batch 37, dev loss: 3.4125: : 38it [00:09,  3.07it/s]\u001b[A\n",
      "batch 38, dev loss: 3.6936: : 38it [00:10,  3.07it/s]\u001b[A\n",
      "batch 38, dev loss: 3.6936: : 39it [00:10,  3.07it/s]\u001b[A\n",
      "batch 39, dev loss: 3.6852: : 39it [00:10,  3.07it/s]\u001b[A\n",
      "batch 39, dev loss: 3.6852: : 40it [00:10,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 3.7347: : 40it [00:10,  2.97it/s]\u001b[A\n",
      "batch 40, dev loss: 3.7347: : 41it [00:10,  3.01it/s]\u001b[A\n",
      "batch 41, dev loss: 3.5198: : 41it [00:11,  3.01it/s]\u001b[A\n",
      "batch 41, dev loss: 3.5198: : 42it [00:11,  3.12it/s]\u001b[A\n",
      "batch 42, dev loss: 3.6285: : 42it [00:11,  3.12it/s]\u001b[A\n",
      "batch 42, dev loss: 3.6285: : 43it [00:11,  2.94it/s]\u001b[A\n",
      "batch 43, dev loss: 3.6365: : 43it [00:12,  2.94it/s]\u001b[A\n",
      "batch 43, dev loss: 3.6365: : 44it [00:12,  2.79it/s]\u001b[A\n",
      "batch 44, dev loss: 3.5622: : 44it [00:12,  2.79it/s]\u001b[A\n",
      "batch 44, dev loss: 3.5622: : 45it [00:12,  2.73it/s]\u001b[A\n",
      "batch 45, dev loss: 3.8503: : 45it [00:12,  2.73it/s]\u001b[A\n",
      "batch 45, dev loss: 3.8503: : 46it [00:12,  2.68it/s]\u001b[A\n",
      "batch 46, dev loss: 3.3888: : 46it [00:13,  2.68it/s]\u001b[A\n",
      "batch 46, dev loss: 3.3888: : 47it [00:13,  2.65it/s]\u001b[A\n",
      "batch 47, dev loss: 3.5782: : 47it [00:13,  2.65it/s]\u001b[A\n",
      "batch 47, dev loss: 3.5782: : 48it [00:13,  2.53it/s]\u001b[A\n",
      "batch 48, dev loss: 3.4471: : 48it [00:13,  2.53it/s]\u001b[A\n",
      "batch 48, dev loss: 3.4471: : 49it [00:13,  2.60it/s]\u001b[A\n",
      "batch 49, dev loss: 3.5558: : 49it [00:14,  2.60it/s]\u001b[A\n",
      "batch 49, dev loss: 3.5558: : 50it [00:14,  2.83it/s]\u001b[A\n",
      "batch 50, dev loss: 3.5113: : 50it [00:14,  2.83it/s]\u001b[A\n",
      "batch 50, dev loss: 3.5113: : 51it [00:14,  2.65it/s]\u001b[A\n",
      "batch 51, dev loss: 3.5599: : 51it [00:15,  2.65it/s]\u001b[A\n",
      "batch 51, dev loss: 3.5599: : 52it [00:15,  2.49it/s]\u001b[A\n",
      "batch 52, dev loss: 3.3232: : 52it [00:15,  2.49it/s]\u001b[A\n",
      "batch 52, dev loss: 3.3232: : 53it [00:15,  2.61it/s]\u001b[A\n",
      "batch 53, dev loss: 3.5696: : 53it [00:16,  2.61it/s]\u001b[A\n",
      "batch 53, dev loss: 3.5696: : 54it [00:16,  2.34it/s]\u001b[A\n",
      "batch 54, dev loss: 3.2833: : 54it [00:16,  2.34it/s]\u001b[A\n",
      "batch 54, dev loss: 3.2833: : 55it [00:16,  2.24it/s]\u001b[A\n",
      "batch 55, dev loss: 3.4861: : 55it [00:16,  2.24it/s]\u001b[A\n",
      "batch 55, dev loss: 3.4861: : 56it [00:16,  2.20it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3416: : 56it [00:17,  2.20it/s]\u001b[A\n",
      "batch 56, dev loss: 3.3416: : 57it [00:17,  2.26it/s]\u001b[A\n",
      "batch 57, dev loss: 3.2964: : 57it [00:17,  2.26it/s]\u001b[A\n",
      "batch 57, dev loss: 3.2964: : 58it [00:17,  2.15it/s]\u001b[A\n",
      "batch 58, dev loss: 3.6886: : 58it [00:18,  2.15it/s]\u001b[A\n",
      "batch 58, dev loss: 3.6886: : 59it [00:18,  2.21it/s]\u001b[A\n",
      "batch 59, dev loss: 3.5056: : 59it [00:18,  2.21it/s]\u001b[A\n",
      "batch 59, dev loss: 3.5056: : 60it [00:18,  2.23it/s]\u001b[A\n",
      "batch 60, dev loss: 3.3067: : 60it [00:19,  2.23it/s]\u001b[A\n",
      "batch 60, dev loss: 3.3067: : 61it [00:19,  2.32it/s]\u001b[A\n",
      "batch 61, dev loss: 3.2466: : 61it [00:19,  2.32it/s]\u001b[A\n",
      "batch 61, dev loss: 3.2466: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 2.9702: : 62it [00:19,  2.54it/s]\u001b[A\n",
      "batch 62, dev loss: 2.9702: : 63it [00:19,  2.60it/s]\u001b[A\n",
      "batch 63, dev loss: 3.5841: : 63it [00:20,  2.60it/s]\u001b[A\n",
      "batch 63, dev loss: 3.5841: : 64it [00:20,  2.75it/s]\u001b[A\n",
      "batch 64, dev loss: 3.377: : 64it [00:20,  2.75it/s] \u001b[A\n",
      "batch 64, dev loss: 3.377: : 65it [00:20,  2.72it/s]\u001b[A\n",
      "batch 65, dev loss: 3.2687: : 65it [00:20,  2.72it/s]\u001b[A\n",
      "batch 65, dev loss: 3.2687: : 66it [00:20,  2.82it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5227: : 66it [00:21,  2.82it/s]\u001b[A\n",
      "batch 66, dev loss: 3.5227: : 67it [00:21,  2.76it/s]\u001b[A\n",
      "batch 67, dev loss: 2.626: : 67it [00:21,  2.76it/s] \u001b[A\n",
      "batch 67, dev loss: 2.626: : 68it [00:21,  3.01it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5017: : 68it [00:21,  3.01it/s]\u001b[A\n",
      "batch 68, dev loss: 2.5017: : 69it [00:21,  3.03it/s]\u001b[A\n",
      "batch 69, dev loss: 2.8986: : 69it [00:22,  3.03it/s]\u001b[A\n",
      "batch 69, dev loss: 2.8986: : 70it [00:22,  3.05it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9074: : 70it [00:22,  3.05it/s]\u001b[A\n",
      "batch 70, dev loss: 3.9074: : 71it [00:22,  2.89it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1333: : 71it [00:22,  2.89it/s]\u001b[A\n",
      "batch 71, dev loss: 3.1333: : 72it [00:22,  2.97it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9129: : 72it [00:23,  2.97it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9129: : 73it [00:23,  2.84it/s]\u001b[A\n",
      "batch 72, dev loss: 3.9129: : 76it [00:23,  3.24it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-test-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-test-hier.pkl exist, load directly\n",
      "\n",
      "1it [00:01,  1.25s/it]\u001b[A\n",
      "2it [00:02,  1.34s/it]\u001b[A\n",
      "3it [00:03,  1.18s/it]\u001b[A\n",
      "4it [00:05,  1.26s/it]\u001b[A\n",
      "5it [00:06,  1.29s/it]\u001b[A\n",
      "6it [00:07,  1.31s/it]\u001b[A\n",
      "7it [00:09,  1.33s/it]\u001b[A\n",
      "8it [00:09,  1.12s/it]\u001b[A\n",
      "9it [00:10,  1.03it/s]\u001b[A\n",
      "10it [00:12,  1.22s/it]\u001b[A\n",
      "11it [00:14,  1.43s/it]\u001b[A\n",
      "12it [00:15,  1.43s/it]\u001b[A\n",
      "13it [00:17,  1.50s/it]\u001b[A\n",
      "14it [00:19,  1.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15it [00:20,  1.53s/it]\u001b[A\n",
      "16it [00:21,  1.35s/it]\u001b[A\n",
      "17it [00:23,  1.61s/it]\u001b[A\n",
      "18it [00:25,  1.68s/it]\u001b[A\n",
      "19it [00:26,  1.63s/it]\u001b[A\n",
      "20it [00:28,  1.72s/it]\u001b[A\n",
      "21it [00:31,  1.87s/it]\u001b[A\n",
      "22it [00:33,  1.93s/it]\u001b[A\n",
      "23it [00:34,  1.85s/it]\u001b[A\n",
      "24it [00:35,  1.47s/it]\u001b[A\n",
      "25it [00:37,  1.82s/it]\u001b[A\n",
      "26it [00:40,  1.88s/it]\u001b[A\n",
      "27it [00:42,  2.06s/it]\u001b[A\n",
      "28it [00:44,  2.17s/it]\u001b[A\n",
      "29it [00:47,  2.18s/it]\u001b[A\n",
      "30it [00:49,  2.09s/it]\u001b[A\n",
      "31it [00:51,  2.11s/it]\u001b[A\n",
      "32it [00:54,  2.33s/it]\u001b[A\n",
      "33it [00:56,  2.39s/it]\u001b[A\n",
      "34it [00:59,  2.53s/it]\u001b[A\n",
      "35it [01:02,  2.63s/it]\u001b[A\n",
      "36it [01:02,  2.00s/it]\u001b[A\n",
      "37it [01:05,  2.19s/it]\u001b[A\n",
      "38it [01:08,  2.50s/it]\u001b[A\n",
      "39it [01:12,  2.78s/it]\u001b[A\n",
      "40it [01:15,  2.84s/it]\u001b[A\n",
      "41it [01:15,  2.22s/it]\u001b[A\n",
      "42it [01:19,  2.55s/it]\u001b[A\n",
      "43it [01:22,  2.83s/it]\u001b[A\n",
      "44it [01:24,  2.53s/it]\u001b[A\n",
      "45it [01:26,  2.28s/it]\u001b[A\n",
      "46it [01:29,  2.70s/it]\u001b[A\n",
      "47it [01:33,  3.01s/it]\u001b[A\n",
      "48it [01:36,  3.13s/it]\u001b[A\n",
      "49it [01:37,  2.25s/it]\u001b[A\n",
      "50it [01:40,  2.66s/it]\u001b[A\n",
      "51it [01:44,  3.01s/it]\u001b[A\n",
      "52it [01:46,  2.81s/it]\u001b[A\n",
      "53it [01:50,  3.11s/it]\u001b[A\n",
      "54it [01:54,  3.17s/it]\u001b[A\n",
      "55it [01:58,  3.42s/it]\u001b[A\n",
      "56it [02:00,  3.01s/it]\u001b[A\n",
      "57it [02:04,  3.48s/it]\u001b[A\n",
      "58it [02:08,  3.50s/it]\u001b[A\n",
      "59it [02:10,  3.11s/it]\u001b[A\n",
      "60it [02:12,  2.75s/it]\u001b[A\n",
      "61it [02:13,  2.39s/it]\u001b[A\n",
      "62it [02:15,  2.00s/it]\u001b[A\n",
      "63it [02:16,  1.78s/it]\u001b[A\n",
      "64it [02:17,  1.50s/it]\u001b[A\n",
      "65it [02:17,  1.25s/it]\u001b[A\n",
      "66it [02:18,  1.01it/s]\u001b[A\n",
      "67it [02:18,  1.20it/s]\u001b[A\n",
      "68it [02:19,  1.39it/s]\u001b[A\n",
      "69it [02:19,  1.49it/s]\u001b[A\n",
      "70it [02:20,  2.00s/it]\u001b[A\n",
      "[!] write the translate result into ./processed/dailydialog/HRED/pure-pred.txt\n",
      "[!] measure the performance and write into tensorboard\n",
      "\n",
      "  0%|                                                  | 0/6740 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██▉                                   | 524/6740 [00:00<00:01, 5237.52it/s]\u001b[A\n",
      " 16%|█████▊                               | 1048/6740 [00:00<00:01, 3132.84it/s]\u001b[A\n",
      " 24%|████████▊                            | 1612/6740 [00:00<00:01, 3970.49it/s]\u001b[A\n",
      " 32%|███████████▉                         | 2181/6740 [00:00<00:01, 4526.05it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 2728/6740 [00:00<00:00, 4821.91it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 3271/6740 [00:00<00:00, 5010.23it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 3815/6740 [00:00<00:00, 5142.12it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 4352/6740 [00:00<00:00, 5209.90it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 4932/6740 [00:01<00:00, 5387.85it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 5485/6740 [00:01<00:00, 5427.51it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 6064/6740 [00:01<00:00, 5534.65it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 5059.81it/s]\u001b[A\n",
      "Epoch: 15, tfr: 1.0, loss(train/dev): 3.2495/3.5325, ppl(dev/test): 34.2094/39.7\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 3.3008: : 0it [00:02, ?it/s]\u001b[A\n",
      "batch 1, training loss: 3.3008: : 1it [00:02,  2.02s/it]\u001b[A\n",
      "batch 2, training loss: 3.2017: : 1it [00:02,  2.02s/it]\u001b[A\n",
      "batch 2, training loss: 3.2017: : 2it [00:02,  1.17s/it]\u001b[A\n",
      "batch 3, training loss: 3.2: : 2it [00:03,  1.17s/it]   \u001b[A\n",
      "batch 3, training loss: 3.2: : 3it [00:03,  1.07it/s]\u001b[A\n",
      "batch 4, training loss: 3.1852: : 3it [00:03,  1.07it/s]\u001b[A\n",
      "batch 4, training loss: 3.1852: : 4it [00:03,  1.19it/s]\u001b[A\n",
      "batch 5, training loss: 3.0135: : 4it [00:04,  1.19it/s]\u001b[A\n",
      "batch 5, training loss: 3.0135: : 5it [00:04,  1.26it/s]\u001b[A\n",
      "batch 6, training loss: 3.3109: : 5it [00:05,  1.26it/s]\u001b[A\n",
      "batch 6, training loss: 3.3109: : 6it [00:05,  1.38it/s]\u001b[A\n",
      "batch 7, training loss: 3.2774: : 6it [00:05,  1.38it/s]\u001b[A\n",
      "batch 7, training loss: 3.2774: : 7it [00:05,  1.46it/s]\u001b[A\n",
      "batch 8, training loss: 3.3059: : 7it [00:06,  1.46it/s]\u001b[A\n",
      "batch 8, training loss: 3.3059: : 8it [00:06,  1.52it/s]\u001b[A\n",
      "batch 9, training loss: 3.1535: : 8it [00:07,  1.52it/s]\u001b[A\n",
      "batch 9, training loss: 3.1535: : 9it [00:07,  1.51it/s]\u001b[A\n",
      "batch 10, training loss: 3.1674: : 9it [00:07,  1.51it/s]\u001b[A\n",
      "batch 10, training loss: 3.1674: : 10it [00:07,  1.47it/s]\u001b[A\n",
      "batch 11, training loss: 3.164: : 10it [00:08,  1.47it/s] \u001b[A\n",
      "batch 11, training loss: 3.164: : 11it [00:08,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 3.2416: : 11it [00:09,  1.47it/s]\u001b[A\n",
      "batch 12, training loss: 3.2416: : 12it [00:09,  1.60it/s]\u001b[A\n",
      "batch 13, training loss: 3.0875: : 12it [00:09,  1.60it/s]\u001b[A\n",
      "batch 13, training loss: 3.0875: : 13it [00:09,  1.64it/s]\u001b[A\n",
      "batch 14, training loss: 3.3938: : 13it [00:10,  1.64it/s]\u001b[A\n",
      "batch 14, training loss: 3.3938: : 14it [00:10,  1.60it/s]\u001b[A\n",
      "batch 15, training loss: 3.2431: : 14it [00:10,  1.60it/s]\u001b[A\n",
      "batch 15, training loss: 3.2431: : 15it [00:10,  1.56it/s]\u001b[A\n",
      "batch 16, training loss: 3.2559: : 15it [00:11,  1.56it/s]\u001b[A\n",
      "batch 16, training loss: 3.2559: : 16it [00:11,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.3587: : 16it [00:12,  1.55it/s]\u001b[A\n",
      "batch 17, training loss: 3.3587: : 17it [00:12,  1.55it/s]\u001b[A\n",
      "batch 18, training loss: 3.2279: : 17it [00:12,  1.55it/s]\u001b[A\n",
      "batch 18, training loss: 3.2279: : 18it [00:12,  1.56it/s]\u001b[A\n",
      "batch 19, training loss: 3.019: : 18it [00:13,  1.56it/s] \u001b[A\n",
      "batch 19, training loss: 3.019: : 19it [00:13,  1.58it/s]\u001b[A\n",
      "batch 20, training loss: 3.1538: : 19it [00:14,  1.58it/s]\u001b[A\n",
      "batch 20, training loss: 3.1538: : 20it [00:14,  1.56it/s]\u001b[A\n",
      "batch 21, training loss: 3.2925: : 20it [00:14,  1.56it/s]\u001b[A\n",
      "batch 21, training loss: 3.2925: : 21it [00:14,  1.51it/s]\u001b[A\n",
      "batch 22, training loss: 3.0475: : 21it [00:15,  1.51it/s]\u001b[A\n",
      "batch 22, training loss: 3.0475: : 22it [00:15,  1.49it/s]\u001b[A\n",
      "batch 23, training loss: 3.1987: : 22it [00:16,  1.49it/s]\u001b[A\n",
      "batch 23, training loss: 3.1987: : 23it [00:16,  1.49it/s]\u001b[A\n",
      "batch 24, training loss: 3.1373: : 23it [00:16,  1.49it/s]\u001b[A\n",
      "batch 24, training loss: 3.1373: : 24it [00:16,  1.62it/s]\u001b[A\n",
      "batch 25, training loss: 3.2206: : 24it [00:17,  1.62it/s]\u001b[A\n",
      "batch 25, training loss: 3.2206: : 25it [00:17,  1.65it/s]\u001b[A\n",
      "batch 26, training loss: 3.0518: : 25it [00:17,  1.65it/s]\u001b[A\n",
      "batch 26, training loss: 3.0518: : 26it [00:17,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.1903: : 26it [00:18,  1.61it/s]\u001b[A\n",
      "batch 27, training loss: 3.1903: : 27it [00:18,  1.57it/s]\u001b[A\n",
      "batch 28, training loss: 3.0296: : 27it [00:19,  1.57it/s]\u001b[A\n",
      "batch 28, training loss: 3.0296: : 28it [00:19,  1.54it/s]\u001b[A\n",
      "batch 29, training loss: 3.1906: : 28it [00:19,  1.54it/s]\u001b[A\n",
      "batch 29, training loss: 3.1906: : 29it [00:19,  1.73it/s]\u001b[A\n",
      "batch 30, training loss: 3.3003: : 29it [00:20,  1.73it/s]\u001b[A\n",
      "batch 30, training loss: 3.3003: : 30it [00:20,  1.93it/s]\u001b[A\n",
      "batch 31, training loss: 3.0747: : 30it [00:20,  1.93it/s]\u001b[A\n",
      "batch 31, training loss: 3.0747: : 31it [00:20,  1.85it/s]\u001b[A\n",
      "batch 32, training loss: 3.2517: : 31it [00:21,  1.85it/s]\u001b[A\n",
      "batch 32, training loss: 3.2517: : 32it [00:21,  1.78it/s]\u001b[A\n",
      "batch 33, training loss: 3.1324: : 32it [00:21,  1.78it/s]\u001b[A\n",
      "batch 33, training loss: 3.1324: : 33it [00:21,  1.67it/s]\u001b[A\n",
      "batch 34, training loss: 3.1593: : 33it [00:22,  1.67it/s]\u001b[A\n",
      "batch 34, training loss: 3.1593: : 34it [00:22,  1.58it/s]\u001b[A\n",
      "batch 35, training loss: 3.2134: : 34it [00:23,  1.58it/s]\u001b[A\n",
      "batch 35, training loss: 3.2134: : 35it [00:23,  1.58it/s]\u001b[A\n",
      "batch 36, training loss: 3.3008: : 35it [00:23,  1.58it/s]\u001b[A\n",
      "batch 36, training loss: 3.3008: : 36it [00:23,  1.62it/s]\u001b[A\n",
      "batch 37, training loss: 3.1526: : 36it [00:24,  1.62it/s]\u001b[A\n",
      "batch 37, training loss: 3.1526: : 37it [00:24,  1.68it/s]\u001b[A\n",
      "batch 38, training loss: 2.9517: : 37it [00:25,  1.68it/s]\u001b[A\n",
      "batch 38, training loss: 2.9517: : 38it [00:25,  1.63it/s]\u001b[A\n",
      "batch 39, training loss: 3.1116: : 38it [00:25,  1.63it/s]\u001b[A\n",
      "batch 39, training loss: 3.1116: : 39it [00:25,  1.58it/s]\u001b[A\n",
      "batch 40, training loss: 3.1621: : 39it [00:26,  1.58it/s]\u001b[A\n",
      "batch 40, training loss: 3.1621: : 40it [00:26,  1.54it/s]\u001b[A\n",
      "batch 41, training loss: 3.3219: : 40it [00:27,  1.54it/s]\u001b[A\n",
      "batch 41, training loss: 3.3219: : 41it [00:27,  1.50it/s]\u001b[A\n",
      "batch 42, training loss: 3.1524: : 41it [00:27,  1.50it/s]\u001b[A\n",
      "batch 42, training loss: 3.1524: : 42it [00:27,  1.55it/s]\u001b[A\n",
      "batch 43, training loss: 2.9928: : 42it [00:28,  1.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 43, training loss: 2.9928: : 43it [00:28,  1.58it/s]\u001b[A\n",
      "batch 44, training loss: 2.9579: : 43it [00:28,  1.58it/s]\u001b[A\n",
      "batch 44, training loss: 2.9579: : 44it [00:28,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 2.9982: : 44it [00:29,  1.61it/s]\u001b[A\n",
      "batch 45, training loss: 2.9982: : 45it [00:29,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 3.1161: : 45it [00:30,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 3.1161: : 46it [00:30,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.1461: : 46it [00:30,  1.52it/s]\u001b[A\n",
      "batch 47, training loss: 3.1461: : 47it [00:30,  1.53it/s]\u001b[A\n",
      "batch 48, training loss: 3.0056: : 47it [00:31,  1.53it/s]\u001b[A\n",
      "batch 48, training loss: 3.0056: : 48it [00:31,  1.61it/s]\u001b[A\n",
      "batch 49, training loss: 3.2898: : 48it [00:32,  1.61it/s]\u001b[A\n",
      "batch 49, training loss: 3.2898: : 49it [00:32,  1.65it/s]\u001b[A\n",
      "batch 50, training loss: 3.0728: : 49it [00:32,  1.65it/s]\u001b[A\n",
      "batch 50, training loss: 3.0728: : 50it [00:32,  1.61it/s]\u001b[A\n",
      "batch 51, training loss: 3.1659: : 50it [00:33,  1.61it/s]\u001b[A\n",
      "batch 51, training loss: 3.1659: : 51it [00:33,  1.56it/s]\u001b[A\n",
      "batch 52, training loss: 3.0044: : 51it [00:34,  1.56it/s]\u001b[A\n",
      "batch 52, training loss: 3.0044: : 52it [00:34,  1.54it/s]\u001b[A\n",
      "batch 53, training loss: 3.0345: : 52it [00:34,  1.54it/s]\u001b[A\n",
      "batch 53, training loss: 3.0345: : 53it [00:34,  1.52it/s]\u001b[A\n",
      "batch 54, training loss: 2.9752: : 53it [00:35,  1.52it/s]\u001b[A\n",
      "batch 54, training loss: 2.9752: : 54it [00:35,  1.56it/s]\u001b[A\n",
      "batch 55, training loss: 3.0904: : 54it [00:36,  1.56it/s]\u001b[A\n",
      "batch 55, training loss: 3.0904: : 55it [00:36,  1.54it/s]\u001b[A\n",
      "batch 56, training loss: 3.1059: : 55it [00:36,  1.54it/s]\u001b[A\n",
      "batch 56, training loss: 3.1059: : 56it [00:36,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 3.0861: : 56it [00:37,  1.60it/s]\u001b[A\n",
      "batch 57, training loss: 3.0861: : 57it [00:37,  1.57it/s]\u001b[A\n",
      "batch 58, training loss: 3.1644: : 57it [00:37,  1.57it/s]\u001b[A\n",
      "batch 58, training loss: 3.1644: : 58it [00:37,  1.52it/s]\u001b[A\n",
      "batch 59, training loss: 3.0214: : 58it [00:38,  1.52it/s]\u001b[A\n",
      "batch 59, training loss: 3.0214: : 59it [00:38,  1.49it/s]\u001b[A\n",
      "batch 60, training loss: 2.9812: : 59it [00:39,  1.49it/s]\u001b[A\n",
      "batch 60, training loss: 2.9812: : 60it [00:39,  1.49it/s]\u001b[A\n",
      "batch 61, training loss: 3.2235: : 60it [00:39,  1.49it/s]\u001b[A\n",
      "batch 61, training loss: 3.2235: : 61it [00:39,  1.61it/s]\u001b[A\n",
      "batch 62, training loss: 3.0638: : 61it [00:40,  1.61it/s]\u001b[A\n",
      "batch 62, training loss: 3.0638: : 62it [00:40,  1.65it/s]\u001b[A\n",
      "batch 63, training loss: 3.1257: : 62it [00:41,  1.65it/s]\u001b[A\n",
      "batch 63, training loss: 3.1257: : 63it [00:41,  1.62it/s]\u001b[A\n",
      "batch 64, training loss: 3.1098: : 63it [00:41,  1.62it/s]\u001b[A\n",
      "batch 64, training loss: 3.1098: : 64it [00:41,  1.58it/s]\u001b[A\n",
      "batch 65, training loss: 3.1685: : 64it [00:42,  1.58it/s]\u001b[A\n",
      "batch 65, training loss: 3.1685: : 65it [00:42,  1.54it/s]\u001b[A\n",
      "batch 66, training loss: 3.202: : 65it [00:43,  1.54it/s] \u001b[A\n",
      "batch 66, training loss: 3.202: : 66it [00:43,  1.53it/s]\u001b[A\n",
      "batch 67, training loss: 3.0683: : 66it [00:43,  1.53it/s]\u001b[A\n",
      "batch 67, training loss: 3.0683: : 67it [00:43,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 3.1174: : 67it [00:44,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 3.1174: : 68it [00:44,  1.59it/s]\u001b[A\n",
      "batch 69, training loss: 3.0437: : 68it [00:44,  1.59it/s]\u001b[A\n",
      "batch 69, training loss: 3.0437: : 69it [00:44,  1.57it/s]\u001b[A\n",
      "batch 70, training loss: 3.2307: : 69it [00:45,  1.57it/s]\u001b[A\n",
      "batch 70, training loss: 3.2307: : 70it [00:45,  1.54it/s]\u001b[A\n",
      "batch 71, training loss: 3.0787: : 70it [00:46,  1.54it/s]\u001b[A\n",
      "batch 71, training loss: 3.0787: : 71it [00:46,  1.50it/s]\u001b[A\n",
      "batch 72, training loss: 3.0323: : 71it [00:47,  1.50it/s]\u001b[A\n",
      "batch 72, training loss: 3.0323: : 72it [00:47,  1.51it/s]\u001b[A\n",
      "batch 73, training loss: 3.1091: : 72it [00:47,  1.51it/s]\u001b[A\n",
      "batch 73, training loss: 3.1091: : 73it [00:47,  1.56it/s]\u001b[A\n",
      "batch 74, training loss: 3.0158: : 73it [00:48,  1.56it/s]\u001b[A\n",
      "batch 74, training loss: 3.0158: : 74it [00:48,  1.65it/s]\u001b[A\n",
      "batch 75, training loss: 3.2117: : 74it [00:48,  1.65it/s]\u001b[A\n",
      "batch 75, training loss: 3.2117: : 75it [00:48,  1.60it/s]\u001b[A\n",
      "batch 76, training loss: 2.9796: : 75it [00:49,  1.60it/s]\u001b[A\n",
      "batch 76, training loss: 2.9796: : 76it [00:49,  1.57it/s]\u001b[A\n",
      "batch 77, training loss: 3.1479: : 76it [00:50,  1.57it/s]\u001b[A\n",
      "batch 77, training loss: 3.1479: : 77it [00:50,  1.55it/s]\u001b[A\n",
      "batch 78, training loss: 3.0775: : 77it [00:50,  1.55it/s]\u001b[A\n",
      "batch 78, training loss: 3.0775: : 78it [00:50,  1.56it/s]\u001b[A\n",
      "batch 79, training loss: 3.1679: : 78it [00:51,  1.56it/s]\u001b[A\n",
      "batch 79, training loss: 3.1679: : 79it [00:51,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 3.1193: : 79it [00:52,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 3.1193: : 80it [00:52,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 3.0665: : 80it [00:52,  1.59it/s]\u001b[A\n",
      "batch 81, training loss: 3.0665: : 81it [00:52,  1.57it/s]\u001b[A\n",
      "batch 82, training loss: 3.2258: : 81it [00:53,  1.57it/s]\u001b[A\n",
      "batch 82, training loss: 3.2258: : 82it [00:53,  1.52it/s]\u001b[A\n",
      "batch 83, training loss: 3.0784: : 82it [00:54,  1.52it/s]\u001b[A\n",
      "batch 83, training loss: 3.0784: : 83it [00:54,  1.49it/s]\u001b[A\n",
      "batch 84, training loss: 3.2142: : 83it [00:54,  1.49it/s]\u001b[A\n",
      "batch 84, training loss: 3.2142: : 84it [00:54,  1.49it/s]\u001b[A\n",
      "batch 85, training loss: 3.2148: : 84it [00:55,  1.49it/s]\u001b[A\n",
      "batch 85, training loss: 3.2148: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.1997: : 85it [00:55,  1.56it/s]\u001b[A\n",
      "batch 86, training loss: 3.1997: : 86it [00:55,  1.67it/s]\u001b[A\n",
      "batch 87, training loss: 3.1723: : 86it [00:56,  1.67it/s]\u001b[A\n",
      "batch 87, training loss: 3.1723: : 87it [00:56,  1.64it/s]\u001b[A\n",
      "batch 88, training loss: 3.2409: : 87it [00:57,  1.64it/s]\u001b[A\n",
      "batch 88, training loss: 3.2409: : 88it [00:57,  1.53it/s]\u001b[A\n",
      "batch 89, training loss: 3.3415: : 88it [00:58,  1.53it/s]\u001b[A\n",
      "batch 89, training loss: 3.3415: : 89it [00:58,  1.43it/s]\u001b[A\n",
      "batch 90, training loss: 3.2571: : 89it [00:58,  1.43it/s]\u001b[A\n",
      "batch 90, training loss: 3.2571: : 90it [00:58,  1.38it/s]\u001b[A\n",
      "batch 91, training loss: 3.3661: : 90it [00:59,  1.38it/s]\u001b[A\n",
      "batch 91, training loss: 3.3661: : 91it [00:59,  1.35it/s]\u001b[A\n",
      "batch 92, training loss: 3.2214: : 91it [01:00,  1.35it/s]\u001b[A\n",
      "batch 92, training loss: 3.2214: : 92it [01:00,  1.33it/s]\u001b[A\n",
      "batch 93, training loss: 3.1928: : 92it [01:01,  1.33it/s]\u001b[A\n",
      "batch 93, training loss: 3.1928: : 93it [01:01,  1.32it/s]\u001b[A\n",
      "batch 94, training loss: 3.2865: : 93it [01:01,  1.32it/s]\u001b[A\n",
      "batch 94, training loss: 3.2865: : 94it [01:01,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.2198: : 94it [01:02,  1.32it/s]\u001b[A\n",
      "batch 95, training loss: 3.2198: : 95it [01:02,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 3.2558: : 95it [01:03,  1.30it/s]\u001b[A\n",
      "batch 96, training loss: 3.2558: : 96it [01:03,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 3.3281: : 96it [01:04,  1.30it/s]\u001b[A\n",
      "batch 97, training loss: 3.3281: : 97it [01:04,  1.32it/s]\u001b[A\n",
      "batch 98, training loss: 3.3245: : 97it [01:04,  1.32it/s]\u001b[A\n",
      "batch 98, training loss: 3.3245: : 98it [01:04,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.1408: : 98it [01:05,  1.31it/s]\u001b[A\n",
      "batch 99, training loss: 3.1408: : 99it [01:05,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.0438: : 99it [01:06,  1.30it/s]\u001b[A\n",
      "batch 100, training loss: 3.0438: : 100it [01:06,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.1599: : 100it [01:07,  1.30it/s]\u001b[A\n",
      "batch 101, training loss: 3.1599: : 101it [01:07,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.1867: : 101it [01:08,  1.30it/s]\u001b[A\n",
      "batch 102, training loss: 3.1867: : 102it [01:08,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.1168: : 102it [01:08,  1.30it/s]\u001b[A\n",
      "batch 103, training loss: 3.1168: : 103it [01:08,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 2.9751: : 103it [01:09,  1.30it/s]\u001b[A\n",
      "batch 104, training loss: 2.9751: : 104it [01:09,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 3.1103: : 104it [01:10,  1.32it/s]\u001b[A\n",
      "batch 105, training loss: 3.1103: : 105it [01:10,  1.31it/s]\u001b[A\n",
      "batch 106, training loss: 3.308: : 105it [01:11,  1.31it/s] \u001b[A\n",
      "batch 106, training loss: 3.308: : 106it [01:11,  1.30it/s]\u001b[A\n",
      "batch 107, training loss: 3.0026: : 106it [01:11,  1.30it/s]\u001b[A\n",
      "batch 107, training loss: 3.0026: : 107it [01:11,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.2328: : 107it [01:12,  1.30it/s]\u001b[A\n",
      "batch 108, training loss: 3.2328: : 108it [01:12,  1.30it/s]\u001b[A\n",
      "batch 109, training loss: 3.2603: : 108it [01:13,  1.30it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 109, training loss: 3.2603: : 109it [01:13,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 3.4202: : 109it [01:14,  1.29it/s]\u001b[A\n",
      "batch 110, training loss: 3.4202: : 110it [01:14,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.1564: : 110it [01:14,  1.30it/s]\u001b[A\n",
      "batch 111, training loss: 3.1564: : 111it [01:14,  1.32it/s]\u001b[A\n",
      "batch 112, training loss: 3.2166: : 111it [01:15,  1.32it/s]\u001b[A\n",
      "batch 112, training loss: 3.2166: : 112it [01:15,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.2643: : 112it [01:16,  1.30it/s]\u001b[A\n",
      "batch 113, training loss: 3.2643: : 113it [01:16,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.2502: : 113it [01:17,  1.30it/s]\u001b[A\n",
      "batch 114, training loss: 3.2502: : 114it [01:17,  1.29it/s]\u001b[A\n",
      "batch 115, training loss: 3.1793: : 114it [01:18,  1.29it/s]\u001b[A\n",
      "batch 115, training loss: 3.1793: : 115it [01:18,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.2012: : 115it [01:18,  1.29it/s]\u001b[A\n",
      "batch 116, training loss: 3.2012: : 116it [01:18,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.2261: : 116it [01:19,  1.29it/s]\u001b[A\n",
      "batch 117, training loss: 3.2261: : 117it [01:19,  1.31it/s]\u001b[A\n",
      "batch 118, training loss: 3.313: : 117it [01:20,  1.31it/s] \u001b[A\n",
      "batch 118, training loss: 3.313: : 118it [01:20,  1.32it/s]\u001b[A\n",
      "batch 119, training loss: 3.2429: : 118it [01:21,  1.32it/s]\u001b[A\n",
      "batch 119, training loss: 3.2429: : 119it [01:21,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 3.1858: : 119it [01:21,  1.32it/s]\u001b[A\n",
      "batch 120, training loss: 3.1858: : 120it [01:21,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 3.4021: : 120it [01:22,  1.31it/s]\u001b[A\n",
      "batch 121, training loss: 3.4021: : 121it [01:22,  1.31it/s]\u001b[A\n",
      "batch 122, training loss: 3.0897: : 121it [01:23,  1.31it/s]\u001b[A\n",
      "batch 122, training loss: 3.0897: : 122it [01:23,  1.31it/s]\u001b[A\n",
      "batch 123, training loss: 3.2287: : 122it [01:24,  1.31it/s]\u001b[A\n",
      "batch 123, training loss: 3.2287: : 123it [01:24,  1.31it/s]\u001b[A\n",
      "batch 124, training loss: 3.2652: : 123it [01:24,  1.31it/s]\u001b[A\n",
      "batch 124, training loss: 3.2652: : 124it [01:24,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 3.2739: : 124it [01:25,  1.30it/s]\u001b[A\n",
      "batch 125, training loss: 3.2739: : 125it [01:25,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.3489: : 125it [01:26,  1.30it/s]\u001b[A\n",
      "batch 126, training loss: 3.3489: : 126it [01:26,  1.32it/s]\u001b[A\n",
      "batch 127, training loss: 3.0355: : 126it [01:27,  1.32it/s]\u001b[A\n",
      "batch 127, training loss: 3.0355: : 127it [01:27,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 3.2485: : 127it [01:27,  1.30it/s]\u001b[A\n",
      "batch 128, training loss: 3.2485: : 128it [01:27,  1.31it/s]\u001b[A\n",
      "batch 129, training loss: 3.1927: : 128it [01:28,  1.31it/s]\u001b[A\n",
      "batch 129, training loss: 3.1927: : 129it [01:28,  1.30it/s]\u001b[A\n",
      "batch 130, training loss: 3.176: : 129it [01:29,  1.30it/s] \u001b[A\n",
      "batch 130, training loss: 3.176: : 130it [01:29,  1.30it/s]\u001b[A\n",
      "batch 131, training loss: 3.3779: : 130it [01:30,  1.30it/s]\u001b[A\n",
      "batch 131, training loss: 3.3779: : 131it [01:30,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.1882: : 131it [01:31,  1.29it/s]\u001b[A\n",
      "batch 132, training loss: 3.1882: : 132it [01:31,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.0776: : 132it [01:31,  1.30it/s]\u001b[A\n",
      "batch 133, training loss: 3.0776: : 133it [01:31,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 3.2735: : 133it [01:32,  1.32it/s]\u001b[A\n",
      "batch 134, training loss: 3.2735: : 134it [01:32,  1.32it/s]\u001b[A\n",
      "batch 135, training loss: 3.1783: : 134it [01:33,  1.32it/s]\u001b[A\n",
      "batch 135, training loss: 3.1783: : 135it [01:33,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.1023: : 135it [01:34,  1.31it/s]\u001b[A\n",
      "batch 136, training loss: 3.1023: : 136it [01:34,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.2576: : 136it [01:34,  1.31it/s]\u001b[A\n",
      "batch 137, training loss: 3.2576: : 137it [01:34,  1.32it/s]\u001b[A\n",
      "batch 138, training loss: 3.2519: : 137it [01:35,  1.32it/s]\u001b[A\n",
      "batch 138, training loss: 3.2519: : 138it [01:35,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.0477: : 138it [01:36,  1.30it/s]\u001b[A\n",
      "batch 139, training loss: 3.0477: : 139it [01:36,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 3.1712: : 139it [01:37,  1.30it/s]\u001b[A\n",
      "batch 140, training loss: 3.1712: : 140it [01:37,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.1728: : 140it [01:37,  1.30it/s]\u001b[A\n",
      "batch 141, training loss: 3.1728: : 141it [01:37,  1.30it/s]\u001b[A\n",
      "batch 142, training loss: 3.1802: : 141it [01:38,  1.30it/s]\u001b[A\n",
      "batch 142, training loss: 3.1802: : 142it [01:38,  1.29it/s]\u001b[A\n",
      "batch 143, training loss: 3.034: : 142it [01:39,  1.29it/s] \u001b[A\n",
      "batch 143, training loss: 3.034: : 143it [01:39,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.1026: : 143it [01:40,  1.31it/s]\u001b[A\n",
      "batch 144, training loss: 3.1026: : 144it [01:40,  1.32it/s]\u001b[A\n",
      "batch 145, training loss: 3.1998: : 144it [01:40,  1.32it/s]\u001b[A\n",
      "batch 145, training loss: 3.1998: : 145it [01:40,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.1811: : 145it [01:41,  1.32it/s]\u001b[A\n",
      "batch 146, training loss: 3.1811: : 146it [01:41,  1.30it/s]\u001b[A\n",
      "batch 147, training loss: 3.0942: : 146it [01:42,  1.30it/s]\u001b[A\n",
      "batch 147, training loss: 3.0942: : 147it [01:42,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.1777: : 147it [01:43,  1.32it/s]\u001b[A\n",
      "batch 148, training loss: 3.1777: : 148it [01:43,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.2378: : 148it [01:43,  1.32it/s]\u001b[A\n",
      "batch 149, training loss: 3.2378: : 149it [01:43,  1.32it/s]\u001b[A\n",
      "batch 150, training loss: 3.2223: : 149it [01:44,  1.32it/s]\u001b[A\n",
      "batch 150, training loss: 3.2223: : 150it [01:44,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 3.1567: : 150it [01:45,  1.30it/s]\u001b[A\n",
      "batch 151, training loss: 3.1567: : 151it [01:45,  1.32it/s]\u001b[A\n",
      "batch 152, training loss: 3.1059: : 151it [01:46,  1.32it/s]\u001b[A\n",
      "batch 152, training loss: 3.1059: : 152it [01:46,  1.33it/s]\u001b[A\n",
      "batch 153, training loss: 3.0899: : 152it [01:47,  1.33it/s]\u001b[A\n",
      "batch 153, training loss: 3.0899: : 153it [01:47,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.2363: : 153it [01:47,  1.31it/s]\u001b[A\n",
      "batch 154, training loss: 3.2363: : 154it [01:47,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 3.3383: : 154it [01:48,  1.31it/s]\u001b[A\n",
      "batch 155, training loss: 3.3383: : 155it [01:48,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 2.9494: : 155it [01:49,  1.30it/s]\u001b[A\n",
      "batch 156, training loss: 2.9494: : 156it [01:49,  1.36it/s]\u001b[A\n",
      "batch 157, training loss: 3.1639: : 156it [01:49,  1.36it/s]\u001b[A\n",
      "batch 157, training loss: 3.1639: : 157it [01:49,  1.51it/s]\u001b[A\n",
      "batch 158, training loss: 3.1536: : 157it [01:50,  1.51it/s]\u001b[A\n",
      "batch 158, training loss: 3.1536: : 158it [01:50,  1.66it/s]\u001b[A\n",
      "batch 159, training loss: 3.081: : 158it [01:50,  1.66it/s] \u001b[A\n",
      "batch 159, training loss: 3.081: : 159it [01:50,  1.59it/s]\u001b[A\n",
      "batch 160, training loss: 3.2338: : 159it [01:51,  1.59it/s]\u001b[A\n",
      "batch 160, training loss: 3.2338: : 160it [01:51,  1.54it/s]\u001b[A\n",
      "batch 161, training loss: 3.1383: : 160it [01:52,  1.54it/s]\u001b[A\n",
      "batch 161, training loss: 3.1383: : 161it [01:52,  1.47it/s]\u001b[A\n",
      "batch 162, training loss: 3.0948: : 161it [01:53,  1.47it/s]\u001b[A\n",
      "batch 162, training loss: 3.0948: : 162it [01:53,  1.41it/s]\u001b[A\n",
      "batch 163, training loss: 3.3655: : 162it [01:53,  1.41it/s]\u001b[A\n",
      "batch 163, training loss: 3.3655: : 163it [01:53,  1.37it/s]\u001b[A\n",
      "batch 164, training loss: 3.1837: : 163it [01:54,  1.37it/s]\u001b[A\n",
      "batch 164, training loss: 3.1837: : 164it [01:54,  1.35it/s]\u001b[A\n",
      "batch 165, training loss: 3.1327: : 164it [01:55,  1.35it/s]\u001b[A\n",
      "batch 165, training loss: 3.1327: : 165it [01:55,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.1184: : 165it [01:56,  1.32it/s]\u001b[A\n",
      "batch 166, training loss: 3.1184: : 166it [01:56,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 3.2232: : 166it [01:56,  1.32it/s]\u001b[A\n",
      "batch 167, training loss: 3.2232: : 167it [01:56,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.2129: : 167it [01:57,  1.32it/s]\u001b[A\n",
      "batch 168, training loss: 3.2129: : 168it [01:57,  1.33it/s]\u001b[A\n",
      "batch 169, training loss: 3.2522: : 168it [01:58,  1.33it/s]\u001b[A\n",
      "batch 169, training loss: 3.2522: : 169it [01:58,  1.31it/s]\u001b[A\n",
      "batch 170, training loss: 3.175: : 169it [01:59,  1.31it/s] \u001b[A\n",
      "batch 170, training loss: 3.175: : 170it [01:59,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 1.9376: : 170it [01:59,  1.30it/s]\u001b[A\n",
      "batch 171, training loss: 1.9376: : 171it [01:59,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.3797: : 171it [02:00,  1.57it/s]\u001b[A\n",
      "batch 172, training loss: 3.3797: : 172it [02:00,  1.46it/s]\u001b[A\n",
      "batch 173, training loss: 3.2239: : 172it [02:01,  1.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 173, training loss: 3.2239: : 173it [02:01,  1.38it/s]\u001b[A\n",
      "batch 174, training loss: 3.3793: : 173it [02:02,  1.38it/s]\u001b[A\n",
      "batch 174, training loss: 3.3793: : 174it [02:02,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 3.2965: : 174it [02:02,  1.33it/s]\u001b[A\n",
      "batch 175, training loss: 3.2965: : 175it [02:02,  1.34it/s]\u001b[A\n",
      "batch 176, training loss: 3.2783: : 175it [02:03,  1.34it/s]\u001b[A\n",
      "batch 176, training loss: 3.2783: : 176it [02:03,  1.29it/s]\u001b[A\n",
      "batch 177, training loss: 3.2447: : 176it [02:04,  1.29it/s]\u001b[A\n",
      "batch 177, training loss: 3.2447: : 177it [02:04,  1.27it/s]\u001b[A\n",
      "batch 178, training loss: 3.2669: : 177it [02:05,  1.27it/s]\u001b[A\n",
      "batch 178, training loss: 3.2669: : 178it [02:05,  1.25it/s]\u001b[A\n",
      "batch 179, training loss: 3.3272: : 178it [02:06,  1.25it/s]\u001b[A\n",
      "batch 179, training loss: 3.3272: : 179it [02:06,  1.23it/s]\u001b[A\n",
      "batch 180, training loss: 3.3258: : 179it [02:06,  1.23it/s]\u001b[A\n",
      "batch 180, training loss: 3.3258: : 180it [02:06,  1.22it/s]\u001b[A\n",
      "batch 181, training loss: 3.3999: : 180it [02:07,  1.22it/s]\u001b[A\n",
      "batch 181, training loss: 3.3999: : 181it [02:07,  1.21it/s]\u001b[A\n",
      "batch 182, training loss: 3.3112: : 181it [02:08,  1.21it/s]\u001b[A\n",
      "batch 182, training loss: 3.3112: : 182it [02:08,  1.22it/s]\u001b[A\n",
      "batch 183, training loss: 3.4499: : 182it [02:09,  1.22it/s]\u001b[A\n",
      "batch 183, training loss: 3.4499: : 183it [02:09,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.1558: : 183it [02:10,  1.23it/s]\u001b[A\n",
      "batch 184, training loss: 3.1558: : 184it [02:10,  1.22it/s]\u001b[A\n",
      "batch 185, training loss: 3.1901: : 184it [02:10,  1.22it/s]\u001b[A\n",
      "batch 185, training loss: 3.1901: : 185it [02:10,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 3.3806: : 185it [02:11,  1.25it/s]\u001b[A\n",
      "batch 186, training loss: 3.3806: : 186it [02:11,  1.25it/s]\u001b[A\n",
      "batch 187, training loss: 3.3978: : 186it [02:12,  1.25it/s]\u001b[A\n",
      "batch 187, training loss: 3.3978: : 187it [02:12,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 3.4132: : 187it [02:13,  1.24it/s]\u001b[A\n",
      "batch 188, training loss: 3.4132: : 188it [02:13,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 3.4715: : 188it [02:14,  1.25it/s]\u001b[A\n",
      "batch 189, training loss: 3.4715: : 189it [02:14,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 3.2053: : 189it [02:14,  1.24it/s]\u001b[A\n",
      "batch 190, training loss: 3.2053: : 190it [02:14,  1.27it/s]\u001b[A\n",
      "batch 191, training loss: 3.1879: : 190it [02:15,  1.27it/s]\u001b[A\n",
      "batch 191, training loss: 3.1879: : 191it [02:15,  1.25it/s]\u001b[A\n",
      "batch 192, training loss: 3.247: : 191it [02:16,  1.25it/s] \u001b[A\n",
      "batch 192, training loss: 3.247: : 192it [02:16,  1.26it/s]\u001b[A\n",
      "batch 193, training loss: 3.2104: : 192it [02:17,  1.26it/s]\u001b[A\n",
      "batch 193, training loss: 3.2104: : 193it [02:17,  1.25it/s]\u001b[A\n",
      "batch 194, training loss: 3.3021: : 193it [02:18,  1.25it/s]\u001b[A\n",
      "batch 194, training loss: 3.3021: : 194it [02:18,  1.25it/s]\u001b[A\n",
      "batch 195, training loss: 3.2237: : 194it [02:18,  1.25it/s]\u001b[A\n",
      "batch 195, training loss: 3.2237: : 195it [02:18,  1.25it/s]\u001b[A\n",
      "batch 196, training loss: 3.1384: : 195it [02:19,  1.25it/s]\u001b[A\n",
      "batch 196, training loss: 3.1384: : 196it [02:19,  1.25it/s]\u001b[A\n",
      "batch 197, training loss: 3.3519: : 196it [02:20,  1.25it/s]\u001b[A\n",
      "batch 197, training loss: 3.3519: : 197it [02:20,  1.27it/s]\u001b[A\n",
      "batch 198, training loss: 3.2318: : 197it [02:21,  1.27it/s]\u001b[A\n",
      "batch 198, training loss: 3.2318: : 198it [02:21,  1.25it/s]\u001b[A\n",
      "batch 199, training loss: 3.2142: : 198it [02:22,  1.25it/s]\u001b[A\n",
      "batch 199, training loss: 3.2142: : 199it [02:22,  1.26it/s]\u001b[A\n",
      "batch 200, training loss: 3.3343: : 199it [02:22,  1.26it/s]\u001b[A\n",
      "batch 200, training loss: 3.3343: : 200it [02:22,  1.25it/s]\u001b[A\n",
      "batch 201, training loss: 3.1877: : 200it [02:23,  1.25it/s]\u001b[A\n",
      "batch 201, training loss: 3.1877: : 201it [02:23,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.0793: : 201it [02:24,  1.24it/s]\u001b[A\n",
      "batch 202, training loss: 3.0793: : 202it [02:24,  1.25it/s]\u001b[A\n",
      "batch 203, training loss: 3.278: : 202it [02:25,  1.25it/s] \u001b[A\n",
      "batch 203, training loss: 3.278: : 203it [02:25,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.298: : 203it [02:26,  1.24it/s]\u001b[A\n",
      "batch 204, training loss: 3.298: : 204it [02:26,  1.25it/s]\u001b[A\n",
      "batch 205, training loss: 3.2706: : 204it [02:26,  1.25it/s]\u001b[A\n",
      "batch 205, training loss: 3.2706: : 205it [02:26,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 3.3614: : 205it [02:27,  1.24it/s]\u001b[A\n",
      "batch 206, training loss: 3.3614: : 206it [02:27,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.1733: : 206it [02:28,  1.23it/s]\u001b[A\n",
      "batch 207, training loss: 3.1733: : 207it [02:28,  1.27it/s]\u001b[A\n",
      "batch 208, training loss: 3.2243: : 207it [02:29,  1.27it/s]\u001b[A\n",
      "batch 208, training loss: 3.2243: : 208it [02:29,  1.25it/s]\u001b[A\n",
      "batch 209, training loss: 3.2223: : 208it [02:30,  1.25it/s]\u001b[A\n",
      "batch 209, training loss: 3.2223: : 209it [02:30,  1.26it/s]\u001b[A\n",
      "batch 210, training loss: 3.2941: : 209it [02:30,  1.26it/s]\u001b[A\n",
      "batch 210, training loss: 3.2941: : 210it [02:30,  1.25it/s]\u001b[A\n",
      "batch 211, training loss: 3.1996: : 210it [02:31,  1.25it/s]\u001b[A\n",
      "batch 211, training loss: 3.1996: : 211it [02:31,  1.27it/s]\u001b[A\n",
      "batch 212, training loss: 3.2205: : 211it [02:32,  1.27it/s]\u001b[A\n",
      "batch 212, training loss: 3.2205: : 212it [02:32,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 3.4087: : 212it [02:33,  1.24it/s]\u001b[A\n",
      "batch 213, training loss: 3.4087: : 213it [02:33,  1.27it/s]\u001b[A\n",
      "batch 214, training loss: 3.2763: : 213it [02:34,  1.27it/s]\u001b[A\n",
      "batch 214, training loss: 3.2763: : 214it [02:34,  1.26it/s]\u001b[A\n",
      "batch 215, training loss: 3.165: : 214it [02:34,  1.26it/s] \u001b[A\n",
      "batch 215, training loss: 3.165: : 215it [02:34,  1.25it/s]\u001b[A\n",
      "batch 216, training loss: 3.2638: : 215it [02:35,  1.25it/s]\u001b[A\n",
      "batch 216, training loss: 3.2638: : 216it [02:35,  1.25it/s]\u001b[A\n",
      "batch 217, training loss: 3.385: : 216it [02:36,  1.25it/s] \u001b[A\n",
      "batch 217, training loss: 3.385: : 217it [02:36,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.3431: : 217it [02:37,  1.24it/s]\u001b[A\n",
      "batch 218, training loss: 3.3431: : 218it [02:37,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 3.4832: : 218it [02:38,  1.24it/s]\u001b[A\n",
      "batch 219, training loss: 3.4832: : 219it [02:38,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.4167: : 219it [02:38,  1.24it/s]\u001b[A\n",
      "batch 220, training loss: 3.4167: : 220it [02:38,  1.25it/s]\u001b[A\n",
      "batch 221, training loss: 3.264: : 220it [02:39,  1.25it/s] \u001b[A\n",
      "batch 221, training loss: 3.264: : 221it [02:39,  1.27it/s]\u001b[A\n",
      "batch 222, training loss: 3.3344: : 221it [02:40,  1.27it/s]\u001b[A\n",
      "batch 222, training loss: 3.3344: : 222it [02:40,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.3331: : 222it [02:41,  1.25it/s]\u001b[A\n",
      "batch 223, training loss: 3.3331: : 223it [02:41,  1.26it/s]\u001b[A\n",
      "batch 224, training loss: 3.1962: : 223it [02:42,  1.26it/s]\u001b[A\n",
      "batch 224, training loss: 3.1962: : 224it [02:42,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 3.3796: : 224it [02:42,  1.25it/s]\u001b[A\n",
      "batch 225, training loss: 3.3796: : 225it [02:42,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.3289: : 225it [02:43,  1.24it/s]\u001b[A\n",
      "batch 226, training loss: 3.3289: : 226it [02:43,  1.25it/s]\u001b[A\n",
      "batch 227, training loss: 3.1868: : 226it [02:44,  1.25it/s]\u001b[A\n",
      "batch 227, training loss: 3.1868: : 227it [02:44,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.2764: : 227it [02:45,  1.24it/s]\u001b[A\n",
      "batch 228, training loss: 3.2764: : 228it [02:45,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.2484: : 228it [02:46,  1.27it/s]\u001b[A\n",
      "batch 229, training loss: 3.2484: : 229it [02:46,  1.25it/s]\u001b[A\n",
      "batch 230, training loss: 3.4054: : 229it [02:46,  1.25it/s]\u001b[A\n",
      "batch 230, training loss: 3.4054: : 230it [02:46,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 3.4293: : 230it [02:47,  1.22it/s]\u001b[A\n",
      "batch 231, training loss: 3.4293: : 231it [02:47,  1.25it/s]\u001b[A\n",
      "batch 232, training loss: 3.244: : 231it [02:48,  1.25it/s] \u001b[A\n",
      "batch 232, training loss: 3.244: : 232it [02:48,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.2624: : 232it [02:49,  1.23it/s]\u001b[A\n",
      "batch 233, training loss: 3.2624: : 233it [02:49,  1.26it/s]\u001b[A\n",
      "batch 234, training loss: 3.4542: : 233it [02:50,  1.26it/s]\u001b[A\n",
      "batch 234, training loss: 3.4542: : 234it [02:50,  1.25it/s]\u001b[A\n",
      "batch 235, training loss: 3.2812: : 234it [02:50,  1.25it/s]\u001b[A\n",
      "batch 235, training loss: 3.2812: : 235it [02:50,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 3.2921: : 235it [02:51,  1.24it/s]\u001b[A\n",
      "batch 236, training loss: 3.2921: : 236it [02:51,  1.25it/s]\u001b[A\n",
      "batch 237, training loss: 3.2055: : 236it [02:52,  1.25it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 237, training loss: 3.2055: : 237it [02:52,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.3638: : 237it [02:53,  1.24it/s]\u001b[A\n",
      "batch 238, training loss: 3.3638: : 238it [02:53,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 3.2574: : 238it [02:54,  1.24it/s]\u001b[A\n",
      "batch 239, training loss: 3.2574: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.2529: : 239it [02:54,  1.24it/s]\u001b[A\n",
      "batch 240, training loss: 3.2529: : 240it [02:54,  1.25it/s]\u001b[A\n",
      "batch 241, training loss: 3.3006: : 240it [02:55,  1.25it/s]\u001b[A\n",
      "batch 241, training loss: 3.3006: : 241it [02:55,  1.27it/s]\u001b[A\n",
      "batch 242, training loss: 3.1491: : 241it [02:56,  1.27it/s]\u001b[A\n",
      "batch 242, training loss: 3.1491: : 242it [02:56,  1.25it/s]\u001b[A\n",
      "batch 243, training loss: 3.3287: : 242it [02:57,  1.25it/s]\u001b[A\n",
      "batch 243, training loss: 3.3287: : 243it [02:57,  1.26it/s]\u001b[A\n",
      "batch 244, training loss: 3.1815: : 243it [02:58,  1.26it/s]\u001b[A\n",
      "batch 244, training loss: 3.1815: : 244it [02:58,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 3.3705: : 244it [02:58,  1.25it/s]\u001b[A\n",
      "batch 245, training loss: 3.3705: : 245it [02:58,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 3.2853: : 245it [02:59,  1.24it/s]\u001b[A\n",
      "batch 246, training loss: 3.2853: : 246it [02:59,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 3.2771: : 246it [03:00,  1.24it/s]\u001b[A\n",
      "batch 247, training loss: 3.2771: : 247it [03:00,  1.25it/s]\u001b[A\n",
      "batch 248, training loss: 3.2207: : 247it [03:01,  1.25it/s]\u001b[A\n",
      "batch 248, training loss: 3.2207: : 248it [03:01,  1.27it/s]\u001b[A\n",
      "batch 249, training loss: 3.1663: : 248it [03:02,  1.27it/s]\u001b[A\n",
      "batch 249, training loss: 3.1663: : 249it [03:02,  1.26it/s]\u001b[A\n",
      "batch 250, training loss: 3.3542: : 249it [03:02,  1.26it/s]\u001b[A\n",
      "batch 250, training loss: 3.3542: : 250it [03:02,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 3.3437: : 250it [03:03,  1.26it/s]\u001b[A\n",
      "batch 251, training loss: 3.3437: : 251it [03:03,  1.26it/s]\u001b[A\n",
      "batch 252, training loss: 2.0885: : 251it [03:04,  1.26it/s]\u001b[A\n",
      "batch 252, training loss: 2.0885: : 252it [03:04,  1.48it/s]\u001b[A\n",
      "batch 253, training loss: 3.2372: : 252it [03:05,  1.48it/s]\u001b[A\n",
      "batch 253, training loss: 3.2372: : 253it [03:05,  1.36it/s]\u001b[A\n",
      "batch 254, training loss: 3.3392: : 253it [03:05,  1.36it/s]\u001b[A\n",
      "batch 254, training loss: 3.3392: : 254it [03:05,  1.26it/s]\u001b[A\n",
      "batch 255, training loss: 3.1508: : 254it [03:06,  1.26it/s]\u001b[A\n",
      "batch 255, training loss: 3.1508: : 255it [03:06,  1.21it/s]\u001b[A\n",
      "batch 256, training loss: 3.2426: : 255it [03:07,  1.21it/s]\u001b[A\n",
      "batch 256, training loss: 3.2426: : 256it [03:07,  1.17it/s]\u001b[A\n",
      "batch 257, training loss: 3.3144: : 256it [03:08,  1.17it/s]\u001b[A\n",
      "batch 257, training loss: 3.3144: : 257it [03:08,  1.17it/s]\u001b[A\n",
      "batch 258, training loss: 3.3529: : 257it [03:09,  1.17it/s]\u001b[A\n",
      "batch 258, training loss: 3.3529: : 258it [03:09,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.2709: : 258it [03:10,  1.14it/s]\u001b[A\n",
      "batch 259, training loss: 3.2709: : 259it [03:10,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.2356: : 259it [03:11,  1.13it/s]\u001b[A\n",
      "batch 260, training loss: 3.2356: : 260it [03:11,  1.12it/s]\u001b[A\n",
      "batch 261, training loss: 3.305: : 260it [03:12,  1.12it/s] \u001b[A\n",
      "batch 261, training loss: 3.305: : 261it [03:12,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 3.2708: : 261it [03:13,  1.11it/s]\u001b[A\n",
      "batch 262, training loss: 3.2708: : 262it [03:13,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 3.2331: : 262it [03:14,  1.10it/s]\u001b[A\n",
      "batch 263, training loss: 3.2331: : 263it [03:14,  1.09it/s]\u001b[A\n",
      "batch 264, training loss: 3.3641: : 263it [03:15,  1.09it/s]\u001b[A\n",
      "batch 264, training loss: 3.3641: : 264it [03:15,  1.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.1615: : 264it [03:15,  1.09it/s]\u001b[A\n",
      "batch 265, training loss: 3.1615: : 265it [03:15,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 3.3185: : 265it [03:16,  1.11it/s]\u001b[A\n",
      "batch 266, training loss: 3.3185: : 266it [03:16,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 3.3204: : 266it [03:17,  1.10it/s]\u001b[A\n",
      "batch 267, training loss: 3.3204: : 267it [03:17,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.1719: : 267it [03:18,  1.09it/s]\u001b[A\n",
      "batch 268, training loss: 3.1719: : 268it [03:18,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.2528: : 268it [03:19,  1.10it/s]\u001b[A\n",
      "batch 269, training loss: 3.2528: : 269it [03:19,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 3.2568: : 269it [03:20,  1.11it/s]\u001b[A\n",
      "batch 270, training loss: 3.2568: : 270it [03:20,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.2178: : 270it [03:21,  1.10it/s]\u001b[A\n",
      "batch 271, training loss: 3.2178: : 271it [03:21,  1.12it/s]\u001b[A\n",
      "batch 272, training loss: 3.2385: : 271it [03:21,  1.12it/s]\u001b[A\n",
      "batch 272, training loss: 3.2385: : 272it [03:21,  1.25it/s]\u001b[A\n",
      "batch 273, training loss: 3.3303: : 272it [03:22,  1.25it/s]\u001b[A\n",
      "batch 273, training loss: 3.3303: : 273it [03:22,  1.20it/s]\u001b[A\n",
      "batch 274, training loss: 3.3333: : 273it [03:23,  1.20it/s]\u001b[A\n",
      "batch 274, training loss: 3.3333: : 274it [03:23,  1.17it/s]\u001b[A\n",
      "batch 275, training loss: 3.2255: : 274it [03:24,  1.17it/s]\u001b[A\n",
      "batch 275, training loss: 3.2255: : 275it [03:24,  1.15it/s]\u001b[A\n",
      "batch 276, training loss: 3.096: : 275it [03:25,  1.15it/s] \u001b[A\n",
      "batch 276, training loss: 3.096: : 276it [03:25,  1.14it/s]\u001b[A\n",
      "batch 277, training loss: 3.2487: : 276it [03:26,  1.14it/s]\u001b[A\n",
      "batch 277, training loss: 3.2487: : 277it [03:26,  1.12it/s]\u001b[A\n",
      "batch 278, training loss: 3.1718: : 277it [03:27,  1.12it/s]\u001b[A\n",
      "batch 278, training loss: 3.1718: : 278it [03:27,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.1717: : 278it [03:28,  1.11it/s]\u001b[A\n",
      "batch 279, training loss: 3.1717: : 279it [03:28,  1.13it/s]\u001b[A\n",
      "batch 280, training loss: 3.0996: : 279it [03:29,  1.13it/s]\u001b[A\n",
      "batch 280, training loss: 3.0996: : 280it [03:29,  1.12it/s]\u001b[A\n",
      "batch 281, training loss: 3.227: : 280it [03:30,  1.12it/s] \u001b[A\n",
      "batch 281, training loss: 3.227: : 281it [03:30,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.1049: : 281it [03:31,  1.10it/s]\u001b[A\n",
      "batch 282, training loss: 3.1049: : 282it [03:31,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.1926: : 282it [03:31,  1.10it/s]\u001b[A\n",
      "batch 283, training loss: 3.1926: : 283it [03:31,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.2133: : 283it [03:32,  1.10it/s]\u001b[A\n",
      "batch 284, training loss: 3.2133: : 284it [03:32,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.2332: : 284it [03:33,  1.09it/s]\u001b[A\n",
      "batch 285, training loss: 3.2332: : 285it [03:33,  1.09it/s]\u001b[A\n",
      "batch 286, training loss: 3.4051: : 285it [03:34,  1.09it/s]\u001b[A\n",
      "batch 286, training loss: 3.4051: : 286it [03:34,  1.08it/s]\u001b[A\n",
      "batch 287, training loss: 3.116: : 286it [03:35,  1.08it/s] \u001b[A\n",
      "batch 287, training loss: 3.116: : 287it [03:35,  1.11it/s]\u001b[A\n",
      "batch 288, training loss: 3.1474: : 287it [03:36,  1.11it/s]\u001b[A\n",
      "batch 288, training loss: 3.1474: : 288it [03:36,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 3.3015: : 288it [03:37,  1.11it/s]\u001b[A\n",
      "batch 289, training loss: 3.3015: : 289it [03:37,  1.10it/s]\u001b[A\n",
      "batch 290, training loss: 3.0969: : 289it [03:38,  1.10it/s]\u001b[A\n",
      "batch 290, training loss: 3.0969: : 290it [03:38,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.3388: : 290it [03:39,  1.09it/s]\u001b[A\n",
      "batch 291, training loss: 3.3388: : 291it [03:39,  1.12it/s]\u001b[A\n",
      "batch 292, training loss: 3.1763: : 291it [03:40,  1.12it/s]\u001b[A\n",
      "batch 292, training loss: 3.1763: : 292it [03:40,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.2687: : 292it [03:41,  1.10it/s]\u001b[A\n",
      "batch 293, training loss: 3.2687: : 293it [03:41,  1.10it/s]\u001b[A\n",
      "batch 294, training loss: 3.3079: : 293it [03:41,  1.10it/s]\u001b[A\n",
      "batch 294, training loss: 3.3079: : 294it [03:41,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 3.2806: : 294it [03:42,  1.10it/s]\u001b[A\n",
      "batch 295, training loss: 3.2806: : 295it [03:42,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.1363: : 295it [03:43,  1.10it/s]\u001b[A\n",
      "batch 296, training loss: 3.1363: : 296it [03:43,  1.09it/s]\u001b[A\n",
      "batch 297, training loss: 3.2726: : 296it [03:44,  1.09it/s]\u001b[A\n",
      "batch 297, training loss: 3.2726: : 297it [03:44,  1.12it/s]\u001b[A\n",
      "batch 298, training loss: 3.1974: : 297it [03:45,  1.12it/s]\u001b[A\n",
      "batch 298, training loss: 3.1974: : 298it [03:45,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 3.2622: : 298it [03:46,  1.10it/s]\u001b[A\n",
      "batch 299, training loss: 3.2622: : 299it [03:46,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 3.3605: : 299it [03:47,  1.09it/s]\u001b[A\n",
      "batch 300, training loss: 3.3605: : 300it [03:47,  1.09it/s]\u001b[A\n",
      "batch 301, training loss: 3.268: : 300it [03:48,  1.09it/s] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 301, training loss: 3.268: : 301it [03:48,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.2874: : 301it [03:49,  1.10it/s]\u001b[A\n",
      "batch 302, training loss: 3.2874: : 302it [03:49,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.1864: : 302it [03:50,  1.09it/s]\u001b[A\n",
      "batch 303, training loss: 3.1864: : 303it [03:50,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.2731: : 303it [03:51,  1.09it/s]\u001b[A\n",
      "batch 304, training loss: 3.2731: : 304it [03:51,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 3.2258: : 304it [03:51,  1.09it/s]\u001b[A\n",
      "batch 305, training loss: 3.2258: : 305it [03:51,  1.11it/s]\u001b[A\n",
      "batch 306, training loss: 3.2734: : 305it [03:52,  1.11it/s]\u001b[A\n",
      "batch 306, training loss: 3.2734: : 306it [03:52,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 3.4302: : 306it [03:53,  1.11it/s]\u001b[A\n",
      "batch 307, training loss: 3.4302: : 307it [03:53,  1.10it/s]\u001b[A\n",
      "batch 308, training loss: 3.0343: : 307it [03:54,  1.10it/s]\u001b[A\n",
      "batch 308, training loss: 3.0343: : 308it [03:54,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.4307: : 308it [03:55,  1.09it/s]\u001b[A\n",
      "batch 309, training loss: 3.4307: : 309it [03:55,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 3.1803: : 309it [03:56,  1.10it/s]\u001b[A\n",
      "batch 310, training loss: 3.1803: : 310it [03:56,  1.08it/s]\u001b[A\n",
      "batch 311, training loss: 3.2773: : 310it [03:57,  1.08it/s]\u001b[A\n",
      "batch 311, training loss: 3.2773: : 311it [03:57,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.1293: : 311it [03:58,  1.09it/s]\u001b[A\n",
      "batch 312, training loss: 3.1293: : 312it [03:58,  1.08it/s]\u001b[A\n",
      "batch 313, training loss: 3.2206: : 312it [03:59,  1.08it/s]\u001b[A\n",
      "batch 313, training loss: 3.2206: : 313it [03:59,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.2369: : 313it [04:00,  1.11it/s]\u001b[A\n",
      "batch 314, training loss: 3.2369: : 314it [04:00,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.2807: : 314it [04:01,  1.10it/s]\u001b[A\n",
      "batch 315, training loss: 3.2807: : 315it [04:01,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 3.4225: : 315it [04:02,  1.09it/s]\u001b[A\n",
      "batch 316, training loss: 3.4225: : 316it [04:02,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.0888: : 316it [04:02,  1.09it/s]\u001b[A\n",
      "batch 317, training loss: 3.0888: : 317it [04:02,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 3.3164: : 317it [04:03,  1.16it/s]\u001b[A\n",
      "batch 318, training loss: 3.3164: : 318it [04:03,  1.09it/s]\u001b[A\n",
      "batch 319, training loss: 3.3727: : 318it [04:04,  1.09it/s]\u001b[A\n",
      "batch 319, training loss: 3.3727: : 319it [04:04,  1.06it/s]\u001b[A\n",
      "batch 320, training loss: 3.3496: : 319it [04:05,  1.06it/s]\u001b[A\n",
      "batch 320, training loss: 3.3496: : 320it [04:05,  1.03it/s]\u001b[A\n",
      "batch 321, training loss: 3.4436: : 320it [04:06,  1.03it/s]\u001b[A\n",
      "batch 321, training loss: 3.4436: : 321it [04:06,  1.01it/s]\u001b[A\n",
      "batch 322, training loss: 3.475: : 321it [04:07,  1.01it/s] \u001b[A\n",
      "batch 322, training loss: 3.475: : 322it [04:07,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.2303: : 322it [04:08,  1.01it/s]\u001b[A\n",
      "batch 323, training loss: 3.2303: : 323it [04:08,  1.00it/s]\u001b[A\n",
      "batch 324, training loss: 3.3116: : 323it [04:09,  1.00it/s]\u001b[A\n",
      "batch 324, training loss: 3.3116: : 324it [04:09,  1.01s/it]\u001b[A\n",
      "batch 325, training loss: 3.4064: : 324it [04:10,  1.01s/it]\u001b[A\n",
      "batch 325, training loss: 3.4064: : 325it [04:10,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 3.2966: : 325it [04:11,  1.02it/s]\u001b[A\n",
      "batch 326, training loss: 3.2966: : 326it [04:11,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 3.4577: : 326it [04:12,  1.01it/s]\u001b[A\n",
      "batch 327, training loss: 3.4577: : 327it [04:12,  1.04it/s]\u001b[A\n",
      "batch 328, training loss: 3.3398: : 327it [04:13,  1.04it/s]\u001b[A\n",
      "batch 328, training loss: 3.3398: : 328it [04:13,  1.03it/s]\u001b[A\n",
      "batch 329, training loss: 3.2272: : 328it [04:14,  1.03it/s]\u001b[A\n",
      "batch 329, training loss: 3.2272: : 329it [04:14,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.3124: : 329it [04:15,  1.01it/s]\u001b[A\n",
      "batch 330, training loss: 3.3124: : 330it [04:15,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 3.3609: : 330it [04:16,  1.00it/s]\u001b[A\n",
      "batch 331, training loss: 3.3609: : 331it [04:16,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.2412: : 331it [04:17,  1.01s/it]\u001b[A\n",
      "batch 332, training loss: 3.2412: : 332it [04:17,  1.00it/s]\u001b[A\n",
      "batch 333, training loss: 3.2593: : 332it [04:18,  1.00it/s]\u001b[A\n",
      "batch 333, training loss: 3.2593: : 333it [04:18,  1.01s/it]\u001b[A\n",
      "batch 334, training loss: 3.292: : 333it [04:19,  1.01s/it] \u001b[A\n",
      "batch 334, training loss: 3.292: : 334it [04:19,  1.03s/it]\u001b[A\n",
      "batch 335, training loss: 3.4585: : 334it [04:20,  1.03s/it]\u001b[A\n",
      "batch 335, training loss: 3.4585: : 335it [04:20,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 3.3251: : 335it [04:21,  1.01s/it]\u001b[A\n",
      "batch 336, training loss: 3.3251: : 336it [04:21,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 3.4185: : 336it [04:22,  1.01s/it]\u001b[A\n",
      "batch 337, training loss: 3.4185: : 337it [04:22,  1.02s/it]\u001b[A\n",
      "batch 338, training loss: 3.2481: : 337it [04:23,  1.02s/it]\u001b[A\n",
      "batch 338, training loss: 3.2481: : 338it [04:23,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 3.3636: : 338it [04:24,  1.01s/it]\u001b[A\n",
      "batch 339, training loss: 3.3636: : 339it [04:24,  1.01s/it]\u001b[A\n",
      "batch 340, training loss: 3.5581: : 339it [04:25,  1.01s/it]\u001b[A\n",
      "batch 340, training loss: 3.5581: : 340it [04:25,  1.02s/it]\u001b[A\n",
      "batch 341, training loss: 3.2752: : 340it [04:26,  1.02s/it]\u001b[A\n",
      "batch 341, training loss: 3.2752: : 341it [04:26,  1.02s/it]\u001b[A\n",
      "batch 342, training loss: 3.3369: : 341it [04:27,  1.02s/it]\u001b[A\n",
      "batch 342, training loss: 3.3369: : 342it [04:27,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.3121: : 342it [04:28,  1.01s/it]\u001b[A\n",
      "batch 343, training loss: 3.3121: : 343it [04:28,  1.10it/s]\u001b[A\n",
      "batch 344, training loss: 3.401: : 343it [04:29,  1.10it/s] \u001b[A\n",
      "batch 344, training loss: 3.401: : 344it [04:29,  1.12it/s]\u001b[A\n",
      "batch 345, training loss: 3.413: : 344it [04:30,  1.12it/s]\u001b[A\n",
      "batch 345, training loss: 3.413: : 345it [04:30,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 3.3663: : 345it [04:31,  1.08it/s]\u001b[A\n",
      "batch 346, training loss: 3.3663: : 346it [04:31,  1.04it/s]\u001b[A\n",
      "batch 347, training loss: 3.2756: : 346it [04:32,  1.04it/s]\u001b[A\n",
      "batch 347, training loss: 3.2756: : 347it [04:32,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 3.3492: : 347it [04:33,  1.03it/s]\u001b[A\n",
      "batch 348, training loss: 3.3492: : 348it [04:33,  1.02it/s]\u001b[A\n",
      "batch 349, training loss: 3.5264: : 348it [04:34,  1.02it/s]\u001b[A\n",
      "batch 349, training loss: 3.5264: : 349it [04:34,  1.01it/s]\u001b[A\n",
      "batch 350, training loss: 3.3178: : 349it [04:35,  1.01it/s]\u001b[A\n",
      "batch 350, training loss: 3.3178: : 350it [04:35,  1.00s/it]\u001b[A\n",
      "batch 351, training loss: 3.2723: : 350it [04:36,  1.00s/it]\u001b[A\n",
      "batch 351, training loss: 3.2723: : 351it [04:36,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 3.3188: : 351it [04:37,  1.01s/it]\u001b[A\n",
      "batch 352, training loss: 3.3188: : 352it [04:37,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.3348: : 352it [04:38,  1.00s/it]\u001b[A\n",
      "batch 353, training loss: 3.3348: : 353it [04:38,  1.01s/it]\u001b[A\n",
      "batch 354, training loss: 3.3976: : 353it [04:39,  1.01s/it]\u001b[A\n",
      "batch 354, training loss: 3.3976: : 354it [04:39,  1.01s/it]\u001b[A\n",
      "batch 355, training loss: 3.352: : 354it [04:40,  1.01s/it] \u001b[A\n",
      "batch 355, training loss: 3.352: : 355it [04:40,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 3.2011: : 355it [04:41,  1.02it/s]\u001b[A\n",
      "batch 356, training loss: 3.2011: : 356it [04:41,  1.00it/s]\u001b[A\n",
      "batch 357, training loss: 3.2376: : 356it [04:42,  1.00it/s]\u001b[A\n",
      "batch 357, training loss: 3.2376: : 357it [04:42,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.4506: : 357it [04:43,  1.01s/it]\u001b[A\n",
      "batch 358, training loss: 3.4506: : 358it [04:43,  1.02s/it]\u001b[A\n",
      "batch 359, training loss: 3.255: : 358it [04:44,  1.02s/it] \u001b[A\n",
      "batch 359, training loss: 3.255: : 359it [04:44,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 3.3196: : 359it [04:45,  1.00s/it]\u001b[A\n",
      "batch 360, training loss: 3.3196: : 360it [04:45,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 3.3331: : 360it [04:46,  1.00s/it]\u001b[A\n",
      "batch 361, training loss: 3.3331: : 361it [04:46,  1.00s/it]\u001b[A\n",
      "batch 362, training loss: 3.3913: : 361it [04:47,  1.00s/it]\u001b[A\n",
      "batch 362, training loss: 3.3913: : 362it [04:47,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.3463: : 362it [04:48,  1.02s/it]\u001b[A\n",
      "batch 363, training loss: 3.3463: : 363it [04:48,  1.01s/it]\u001b[A\n",
      "batch 364, training loss: 3.273: : 363it [04:49,  1.01s/it] \u001b[A\n",
      "batch 364, training loss: 3.273: : 364it [04:49,  1.00it/s]\u001b[A\n",
      "batch 365, training loss: 3.2075: : 364it [04:50,  1.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 365, training loss: 3.2075: : 365it [04:50,  1.02s/it]\u001b[A\n",
      "batch 366, training loss: 3.3353: : 365it [04:51,  1.02s/it]\u001b[A\n",
      "batch 366, training loss: 3.3353: : 366it [04:51,  1.02s/it]\u001b[A\n",
      "batch 367, training loss: 3.2917: : 366it [04:52,  1.02s/it]\u001b[A\n",
      "batch 367, training loss: 3.2917: : 367it [04:52,  1.02s/it]\u001b[A\n",
      "batch 368, training loss: 3.3685: : 367it [04:53,  1.02s/it]\u001b[A\n",
      "batch 368, training loss: 3.3685: : 368it [04:53,  1.10it/s]\u001b[A\n",
      "batch 369, training loss: 3.3892: : 368it [04:54,  1.10it/s]\u001b[A\n",
      "batch 369, training loss: 3.3892: : 369it [04:54,  1.11it/s]\u001b[A\n",
      "batch 370, training loss: 3.3158: : 369it [04:55,  1.11it/s]\u001b[A\n",
      "batch 370, training loss: 3.3158: : 370it [04:55,  1.06it/s]\u001b[A\n",
      "batch 371, training loss: 3.3103: : 370it [04:56,  1.06it/s]\u001b[A\n",
      "batch 371, training loss: 3.3103: : 371it [04:56,  1.08it/s]\u001b[A\n",
      "batch 372, training loss: 3.2728: : 371it [04:57,  1.08it/s]\u001b[A\n",
      "batch 372, training loss: 3.2728: : 372it [04:57,  1.04it/s]\u001b[A\n",
      "batch 373, training loss: 3.2831: : 372it [04:58,  1.04it/s]\u001b[A\n",
      "batch 373, training loss: 3.2831: : 373it [04:58,  1.06it/s]\u001b[A\n",
      "batch 374, training loss: 3.3209: : 373it [04:59,  1.06it/s]\u001b[A\n",
      "batch 374, training loss: 3.3209: : 374it [04:59,  1.04it/s]\u001b[A\n",
      "batch 375, training loss: 2.825: : 374it [04:59,  1.04it/s] \u001b[A\n",
      "batch 375, training loss: 2.825: : 375it [04:59,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 3.3065: : 375it [05:00,  1.17it/s]\u001b[A\n",
      "batch 376, training loss: 3.3065: : 376it [05:00,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 3.3748: : 376it [05:02,  1.05it/s]\u001b[A\n",
      "batch 377, training loss: 3.3748: : 377it [05:02,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 3.3232: : 377it [05:03,  1.01s/it]\u001b[A\n",
      "batch 378, training loss: 3.3232: : 378it [05:03,  1.06s/it]\u001b[A\n",
      "batch 379, training loss: 3.2721: : 378it [05:04,  1.06s/it]\u001b[A\n",
      "batch 379, training loss: 3.2721: : 379it [05:04,  1.07s/it]\u001b[A\n",
      "batch 380, training loss: 3.248: : 379it [05:05,  1.07s/it] \u001b[A\n",
      "batch 380, training loss: 3.248: : 380it [05:05,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.3299: : 380it [05:06,  1.08s/it]\u001b[A\n",
      "batch 381, training loss: 3.3299: : 381it [05:06,  1.06s/it]\u001b[A\n",
      "batch 382, training loss: 3.2244: : 381it [05:07,  1.06s/it]\u001b[A\n",
      "batch 382, training loss: 3.2244: : 382it [05:07,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.2693: : 382it [05:08,  1.09s/it]\u001b[A\n",
      "batch 383, training loss: 3.2693: : 383it [05:08,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 3.3226: : 383it [05:09,  1.10s/it]\u001b[A\n",
      "batch 384, training loss: 3.3226: : 384it [05:09,  1.07s/it]\u001b[A\n",
      "batch 385, training loss: 3.2491: : 384it [05:10,  1.07s/it]\u001b[A\n",
      "batch 385, training loss: 3.2491: : 385it [05:10,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.2288: : 385it [05:12,  1.09s/it]\u001b[A\n",
      "batch 386, training loss: 3.2288: : 386it [05:12,  1.11s/it]\u001b[A\n",
      "batch 387, training loss: 3.2867: : 386it [05:13,  1.11s/it]\u001b[A\n",
      "batch 387, training loss: 3.2867: : 387it [05:13,  1.13s/it]\u001b[A\n",
      "batch 388, training loss: 3.1124: : 387it [05:14,  1.13s/it]\u001b[A\n",
      "batch 388, training loss: 3.1124: : 388it [05:14,  1.12s/it]\u001b[A\n",
      "batch 389, training loss: 3.2762: : 388it [05:15,  1.12s/it]\u001b[A\n",
      "batch 389, training loss: 3.2762: : 389it [05:15,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 3.3298: : 389it [05:16,  1.12s/it]\u001b[A\n",
      "batch 390, training loss: 3.3298: : 390it [05:16,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 3.3666: : 390it [05:17,  1.12s/it]\u001b[A\n",
      "batch 391, training loss: 3.3666: : 391it [05:17,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 3.3254: : 391it [05:18,  1.09s/it]\u001b[A\n",
      "batch 392, training loss: 3.3254: : 392it [05:18,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 3.1316: : 392it [05:19,  1.10s/it]\u001b[A\n",
      "batch 393, training loss: 3.1316: : 393it [05:19,  1.10s/it]\u001b[A\n",
      "batch 394, training loss: 3.1079: : 393it [05:20,  1.10s/it]\u001b[A\n",
      "batch 394, training loss: 3.1079: : 394it [05:20,  1.09s/it]\u001b[A\n",
      "batch 395, training loss: 3.2038: : 394it [05:21,  1.09s/it]\u001b[A\n",
      "batch 395, training loss: 3.2038: : 395it [05:21,  1.09s/it]\u001b[A\n",
      "batch 396, training loss: 3.349: : 395it [05:23,  1.09s/it] \u001b[A\n",
      "batch 396, training loss: 3.349: : 396it [05:23,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.195: : 396it [05:24,  1.12s/it]\u001b[A\n",
      "batch 397, training loss: 3.195: : 397it [05:24,  1.12s/it]\u001b[A\n",
      "batch 398, training loss: 3.3083: : 397it [05:25,  1.12s/it]\u001b[A\n",
      "batch 398, training loss: 3.3083: : 398it [05:25,  1.09s/it]\u001b[A\n",
      "batch 399, training loss: 3.4086: : 398it [05:26,  1.09s/it]\u001b[A\n",
      "batch 399, training loss: 3.4086: : 399it [05:26,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 3.2107: : 399it [05:27,  1.10s/it]\u001b[A\n",
      "batch 400, training loss: 3.2107: : 400it [05:27,  1.11s/it]\u001b[A\n",
      "batch 401, training loss: 3.1729: : 400it [05:28,  1.11s/it]\u001b[A\n",
      "batch 401, training loss: 3.1729: : 401it [05:28,  1.13s/it]\u001b[A\n",
      "batch 402, training loss: 3.2319: : 401it [05:29,  1.13s/it]\u001b[A\n",
      "batch 402, training loss: 3.2319: : 402it [05:29,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.3783: : 402it [05:30,  1.13s/it]\u001b[A\n",
      "batch 403, training loss: 3.3783: : 403it [05:30,  1.11s/it]\u001b[A\n",
      "batch 404, training loss: 3.0343: : 403it [05:32,  1.11s/it]\u001b[A\n",
      "batch 404, training loss: 3.0343: : 404it [05:32,  1.12s/it]\u001b[A\n",
      "batch 405, training loss: 3.326: : 404it [05:33,  1.12s/it] \u001b[A\n",
      "batch 405, training loss: 3.326: : 405it [05:33,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.2184: : 405it [05:34,  1.11s/it]\u001b[A\n",
      "batch 406, training loss: 3.2184: : 406it [05:34,  1.12s/it]\u001b[A\n",
      "batch 407, training loss: 3.2965: : 406it [05:35,  1.12s/it]\u001b[A\n",
      "batch 407, training loss: 3.2965: : 407it [05:35,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.1544: : 407it [05:36,  1.12s/it]\u001b[A\n",
      "batch 408, training loss: 3.1544: : 408it [05:36,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 3.2885: : 408it [05:37,  1.10s/it]\u001b[A\n",
      "batch 409, training loss: 3.2885: : 409it [05:37,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.1823: : 409it [05:38,  1.11s/it]\u001b[A\n",
      "batch 410, training loss: 3.1823: : 410it [05:38,  1.13s/it]\u001b[A\n",
      "batch 411, training loss: 3.2769: : 410it [05:39,  1.13s/it]\u001b[A\n",
      "batch 411, training loss: 3.2769: : 411it [05:39,  1.13s/it]\u001b[A\n",
      "batch 412, training loss: 3.3083: : 411it [05:41,  1.13s/it]\u001b[A\n",
      "batch 412, training loss: 3.3083: : 412it [05:41,  1.13s/it]\u001b[A\n",
      "batch 413, training loss: 3.2095: : 412it [05:42,  1.13s/it]\u001b[A\n",
      "batch 413, training loss: 3.2095: : 413it [05:42,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.1508: : 413it [05:43,  1.10s/it]\u001b[A\n",
      "batch 414, training loss: 3.1508: : 414it [05:43,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.0504: : 414it [05:44,  1.11s/it]\u001b[A\n",
      "batch 415, training loss: 3.0504: : 415it [05:44,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.0964: : 415it [05:45,  1.11s/it]\u001b[A\n",
      "batch 416, training loss: 3.0964: : 416it [05:45,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.3439: : 416it [05:46,  1.12s/it]\u001b[A\n",
      "batch 417, training loss: 3.3439: : 417it [05:46,  1.12s/it]\u001b[A\n",
      "batch 418, training loss: 3.2283: : 417it [05:47,  1.12s/it]\u001b[A\n",
      "batch 418, training loss: 3.2283: : 418it [05:47,  1.12s/it]\u001b[A\n",
      "batch 419, training loss: 3.0687: : 418it [05:48,  1.12s/it]\u001b[A\n",
      "batch 419, training loss: 3.0687: : 419it [05:48,  1.12s/it]\u001b[A\n",
      "batch 420, training loss: 3.1537: : 419it [05:49,  1.12s/it]\u001b[A\n",
      "batch 420, training loss: 3.1537: : 420it [05:49,  1.08s/it]\u001b[A\n",
      "batch 421, training loss: 3.2812: : 420it [05:50,  1.08s/it]\u001b[A\n",
      "batch 421, training loss: 3.2812: : 421it [05:50,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.2898: : 421it [05:52,  1.10s/it]\u001b[A\n",
      "batch 422, training loss: 3.2898: : 422it [05:52,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.0823: : 422it [05:53,  1.11s/it]\u001b[A\n",
      "batch 423, training loss: 3.0823: : 423it [05:53,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.2681: : 423it [05:54,  1.08s/it]\u001b[A\n",
      "batch 424, training loss: 3.2681: : 424it [05:54,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 3.3313: : 424it [05:55,  1.13s/it]\u001b[A\n",
      "batch 425, training loss: 3.3313: : 425it [05:55,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 3.2474: : 425it [05:56,  1.16s/it]\u001b[A\n",
      "batch 426, training loss: 3.2474: : 426it [05:56,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.3088: : 426it [05:57,  1.16s/it]\u001b[A\n",
      "batch 427, training loss: 3.3088: : 427it [05:57,  1.19s/it]\u001b[A\n",
      "batch 428, training loss: 3.4409: : 427it [05:59,  1.19s/it]\u001b[A\n",
      "batch 428, training loss: 3.4409: : 428it [05:59,  1.19s/it]\u001b[A\n",
      "batch 429, training loss: 3.287: : 428it [06:00,  1.19s/it] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 429, training loss: 3.287: : 429it [06:00,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.3329: : 429it [06:01,  1.19s/it]\u001b[A\n",
      "batch 430, training loss: 3.3329: : 430it [06:01,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.3756: : 430it [06:02,  1.20s/it]\u001b[A\n",
      "batch 431, training loss: 3.3756: : 431it [06:02,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 3.2107: : 431it [06:04,  1.21s/it]\u001b[A\n",
      "batch 432, training loss: 3.2107: : 432it [06:04,  1.22s/it]\u001b[A\n",
      "batch 433, training loss: 3.2625: : 432it [06:05,  1.22s/it]\u001b[A\n",
      "batch 433, training loss: 3.2625: : 433it [06:05,  1.20s/it]\u001b[A\n",
      "batch 434, training loss: 3.2792: : 433it [06:06,  1.20s/it]\u001b[A\n",
      "batch 434, training loss: 3.2792: : 434it [06:06,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 3.2864: : 434it [06:07,  1.22s/it]\u001b[A\n",
      "batch 435, training loss: 3.2864: : 435it [06:07,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3038: : 435it [06:08,  1.22s/it]\u001b[A\n",
      "batch 436, training loss: 3.3038: : 436it [06:08,  1.23s/it]\u001b[A\n",
      "batch 437, training loss: 3.2557: : 436it [06:10,  1.23s/it]\u001b[A\n",
      "batch 437, training loss: 3.2557: : 437it [06:10,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.2763: : 437it [06:11,  1.21s/it]\u001b[A\n",
      "batch 438, training loss: 3.2763: : 438it [06:11,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.4108: : 438it [06:12,  1.22s/it]\u001b[A\n",
      "batch 439, training loss: 3.4108: : 439it [06:12,  1.21s/it]\u001b[A\n",
      "batch 440, training loss: 3.2307: : 439it [06:13,  1.21s/it]\u001b[A\n",
      "batch 440, training loss: 3.2307: : 440it [06:13,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 3.5079: : 440it [06:14,  1.20s/it]\u001b[A\n",
      "batch 441, training loss: 3.5079: : 441it [06:14,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.1689: : 441it [06:16,  1.21s/it]\u001b[A\n",
      "batch 442, training loss: 3.1689: : 442it [06:16,  1.21s/it]\u001b[A\n",
      "batch 443, training loss: 3.296: : 442it [06:17,  1.21s/it] \u001b[A\n",
      "batch 443, training loss: 3.296: : 443it [06:17,  1.22s/it]\u001b[A\n",
      "batch 444, training loss: 3.2714: : 443it [06:18,  1.22s/it]\u001b[A\n",
      "batch 444, training loss: 3.2714: : 444it [06:18,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.3214: : 444it [06:19,  1.22s/it]\u001b[A\n",
      "batch 445, training loss: 3.3214: : 445it [06:19,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.2809: : 445it [06:21,  1.22s/it]\u001b[A\n",
      "batch 446, training loss: 3.2809: : 446it [06:21,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.2499: : 446it [06:22,  1.22s/it]\u001b[A\n",
      "batch 447, training loss: 3.2499: : 447it [06:22,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.1761: : 447it [06:23,  1.23s/it]\u001b[A\n",
      "batch 448, training loss: 3.1761: : 448it [06:23,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 3.2424: : 448it [06:24,  1.22s/it]\u001b[A\n",
      "batch 449, training loss: 3.2424: : 449it [06:24,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.3383: : 449it [06:25,  1.23s/it]\u001b[A\n",
      "batch 450, training loss: 3.3383: : 450it [06:25,  1.18s/it]\u001b[A\n",
      "batch 451, training loss: 3.359: : 450it [06:26,  1.18s/it] \u001b[A\n",
      "batch 451, training loss: 3.359: : 451it [06:26,  1.14s/it]\u001b[A\n",
      "batch 452, training loss: 3.3276: : 451it [06:28,  1.14s/it]\u001b[A\n",
      "batch 452, training loss: 3.3276: : 452it [06:28,  1.15s/it]\u001b[A\n",
      "batch 453, training loss: 3.4125: : 452it [06:29,  1.15s/it]\u001b[A\n",
      "batch 453, training loss: 3.4125: : 453it [06:29,  1.17s/it]\u001b[A\n",
      "batch 454, training loss: 3.278: : 453it [06:30,  1.17s/it] \u001b[A\n",
      "batch 454, training loss: 3.278: : 454it [06:30,  1.17s/it]\u001b[A\n",
      "batch 455, training loss: 3.1946: : 454it [06:31,  1.17s/it]\u001b[A\n",
      "batch 455, training loss: 3.1946: : 455it [06:31,  1.19s/it]\u001b[A\n",
      "batch 456, training loss: 3.2128: : 455it [06:32,  1.19s/it]\u001b[A\n",
      "batch 456, training loss: 3.2128: : 456it [06:32,  1.20s/it]\u001b[A\n",
      "batch 457, training loss: 3.3174: : 456it [06:34,  1.20s/it]\u001b[A\n",
      "batch 457, training loss: 3.3174: : 457it [06:34,  1.21s/it]\u001b[A\n",
      "batch 458, training loss: 3.2294: : 457it [06:35,  1.21s/it]\u001b[A\n",
      "batch 458, training loss: 3.2294: : 458it [06:35,  1.21s/it]\u001b[A\n",
      "batch 459, training loss: 3.2849: : 458it [06:36,  1.21s/it]\u001b[A\n",
      "batch 459, training loss: 3.2849: : 459it [06:36,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.2734: : 459it [06:37,  1.23s/it]\u001b[A\n",
      "batch 460, training loss: 3.2734: : 460it [06:37,  1.22s/it]\u001b[A\n",
      "batch 461, training loss: 3.3557: : 460it [06:39,  1.22s/it]\u001b[A\n",
      "batch 461, training loss: 3.3557: : 461it [06:39,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.3227: : 461it [06:40,  1.23s/it]\u001b[A\n",
      "batch 462, training loss: 3.3227: : 462it [06:40,  1.22s/it]\u001b[A\n",
      "batch 463, training loss: 3.1864: : 462it [06:41,  1.22s/it]\u001b[A\n",
      "batch 463, training loss: 3.1864: : 463it [06:41,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.3109: : 463it [06:42,  1.23s/it]\u001b[A\n",
      "batch 464, training loss: 3.3109: : 464it [06:42,  1.22s/it]\u001b[A\n",
      "batch 465, training loss: 3.2109: : 464it [06:43,  1.22s/it]\u001b[A\n",
      "batch 465, training loss: 3.2109: : 465it [06:43,  1.18s/it]\u001b[A\n",
      "batch 466, training loss: 3.1205: : 465it [06:45,  1.18s/it]\u001b[A\n",
      "batch 466, training loss: 3.1205: : 466it [06:45,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.0665: : 466it [06:46,  1.21s/it]\u001b[A\n",
      "batch 467, training loss: 3.0665: : 467it [06:46,  1.24s/it]\u001b[A\n",
      "batch 468, training loss: 3.2668: : 467it [06:47,  1.24s/it]\u001b[A\n",
      "batch 468, training loss: 3.2668: : 468it [06:47,  1.26s/it]\u001b[A\n",
      "batch 469, training loss: 3.157: : 468it [06:49,  1.26s/it] \u001b[A\n",
      "batch 469, training loss: 3.157: : 469it [06:49,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.3127: : 469it [06:50,  1.27s/it]\u001b[A\n",
      "batch 470, training loss: 3.3127: : 470it [06:50,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 3.2408: : 470it [06:51,  1.27s/it]\u001b[A\n",
      "batch 471, training loss: 3.2408: : 471it [06:51,  1.27s/it]\u001b[A\n",
      "batch 472, training loss: 3.228: : 471it [06:52,  1.27s/it] \u001b[A\n",
      "batch 472, training loss: 3.228: : 472it [06:52,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.2032: : 472it [06:54,  1.27s/it]\u001b[A\n",
      "batch 473, training loss: 3.2032: : 473it [06:54,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 3.2641: : 473it [06:55,  1.26s/it]\u001b[A\n",
      "batch 474, training loss: 3.2641: : 474it [06:55,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.1492: : 474it [06:56,  1.27s/it]\u001b[A\n",
      "batch 475, training loss: 3.1492: : 475it [06:56,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 3.2591: : 475it [06:57,  1.26s/it]\u001b[A\n",
      "batch 476, training loss: 3.2591: : 476it [06:57,  1.27s/it]\u001b[A\n",
      "batch 477, training loss: 3.103: : 476it [06:59,  1.27s/it] \u001b[A\n",
      "batch 477, training loss: 3.103: : 477it [06:59,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.158: : 477it [07:00,  1.26s/it]\u001b[A\n",
      "batch 478, training loss: 3.158: : 478it [07:00,  1.25s/it]\u001b[A\n",
      "batch 479, training loss: 3.1411: : 478it [07:01,  1.25s/it]\u001b[A\n",
      "batch 479, training loss: 3.1411: : 479it [07:01,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.1906: : 479it [07:02,  1.26s/it]\u001b[A\n",
      "batch 480, training loss: 3.1906: : 480it [07:02,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.1964: : 480it [07:04,  1.26s/it]\u001b[A\n",
      "batch 481, training loss: 3.1964: : 481it [07:04,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.1797: : 481it [07:05,  1.26s/it]\u001b[A\n",
      "batch 482, training loss: 3.1797: : 482it [07:05,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.1269: : 482it [07:06,  1.26s/it]\u001b[A\n",
      "batch 483, training loss: 3.1269: : 483it [07:06,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.1969: : 483it [07:07,  1.26s/it]\u001b[A\n",
      "batch 484, training loss: 3.1969: : 484it [07:07,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.1371: : 484it [07:09,  1.26s/it]\u001b[A\n",
      "batch 485, training loss: 3.1371: : 485it [07:09,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.2421: : 485it [07:10,  1.26s/it]\u001b[A\n",
      "batch 486, training loss: 3.2421: : 486it [07:10,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.3266: : 486it [07:11,  1.26s/it]\u001b[A\n",
      "batch 487, training loss: 3.3266: : 487it [07:11,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.1363: : 487it [07:12,  1.26s/it]\u001b[A\n",
      "batch 488, training loss: 3.1363: : 488it [07:12,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.0864: : 488it [07:14,  1.26s/it]\u001b[A\n",
      "batch 489, training loss: 3.0864: : 489it [07:14,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.1177: : 489it [07:15,  1.26s/it]\u001b[A\n",
      "batch 490, training loss: 3.1177: : 490it [07:15,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.1957: : 490it [07:16,  1.26s/it]\u001b[A\n",
      "batch 491, training loss: 3.1957: : 491it [07:16,  1.26s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "!./run.sh train dailydialog HRED 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106047d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
