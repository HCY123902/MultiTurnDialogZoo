{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c848b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== vocab begin ==========\n",
      "[!] Begin to generate the vocab\n",
      "[!] ./processed/dailydialog: already exists\n",
      "100%|████████████████████████████████████| 76052/76052 [01:37<00:00, 783.84it/s]\n",
      "[!] whole vocab size: 17292\n",
      "[!] Save the vocab into ./processed/dailydialog/iptvocab.pkl, vocab_size: 17296\n",
      "100%|███████████████████████████████████| 76052/76052 [00:21<00:00, 3564.69it/s]\n",
      "[!] whole vocab size: 17317\n",
      "[!] Save the vocab into ./processed/dailydialog/optvocab.pkl, vocab_size: 17321\n",
      "100%|███████████████████████████████████| 76052/76052 [00:19<00:00, 3828.61it/s]\n",
      "100%|████████████████████████████████████| 76052/76052 [01:30<00:00, 842.57it/s]\n",
      "[!] whole vocab size: 18087\n",
      "[!] Save the vocab into ./processed/dailydialog/vocab.pkl, vocab_size: 18091\n",
      "========== vocab done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh vocab dailydialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30aa670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75f13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 21 20:26:40 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN RTX           On   | 00000000:D8:00.0 Off |                  N/A |\r\n",
      "| 58%   77C    P2   203W / 280W |  13821MiB / 24190MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      4549      C   python                                      6731MiB |\r\n",
      "|    0      5505      C   python                                      7079MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2f3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== stat begin ==========\n",
      "[!] analyze the graph coverage information\n",
      "[!] train information:\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-train\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.4574/262/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 77.7447/860/2\n",
      "[!] turn length(max/min/avg): 34/1/5.0943\n",
      "[!] the metadata of dailydialog-tgt-train\n",
      "[!] length of the responses(avg, max, min): 14.8881/279/2\n",
      "[!] total words: 23868\n",
      "==========================================\n",
      "[!] test information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-test\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.5436/119/3\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.1785/446/3\n",
      "[!] turn length(max/min/avg): 25/1/4.9653\n",
      "[!] the metadata of dailydialog-tgt-test\n",
      "[!] length of the responses(avg, max, min): 15.0675/205/3\n",
      "[!] total words: 7305\n",
      "==========================================\n",
      "[!] dev information\n",
      "==========================================\n",
      "[!] the metadata of dailydialog-src-dev\n",
      "[!] length of the sentenes(avg, max, min) for hierarchical: 14.2159/109/2\n",
      "[!] length of the sentenes(avg, max, min) for no-hierarchical: 76.6121/674/3\n",
      "[!] turn length(max/min/avg): 30/1/5.1007\n",
      "[!] the metadata of dailydialog-tgt-dev\n",
      "[!] length of the responses(avg, max, min): 14.6976/167/2\n",
      "[!] total words: 7137\n",
      "==========================================\n",
      "========== stat done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh stat dailydialog 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce5147c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== train begin ==========\n",
      "[!] ./processed/dailydialog/HRED: already exists\n",
      "[!] ./processed/dailydialog/HRED/trainlog.txt doesn't exist\n",
      "[!] ./processed/dailydialog/HRED/metadata.txt doesn't exist\n",
      "rm: cannot remove 'tblogs/dailydialog/HRED/*': No such file or directory\n",
      "[!] back up finished\n",
      "[!] Begin to train the model\n",
      "/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] cannot find the module \"torch_geometric\", ignore it\n",
      "[!] Parameters:\n",
      "Namespace(batch_size=128, bleu='nltk', context_hidden=512, context_threshold=2, contextrnn=True, d_model=512, dataset='dailydialog', debug=False, decoder_hidden=512, dev_graph='./processed/dailydialog/dev-graph.pkl', dim_feedforward=2048, dropout=0.3, dynamic_tfr=15, dynamic_tfr_counter=10, dynamic_tfr_threshold=1.0, dynamic_tfr_weight=0.0, embed_size=256, epochs=45, gat_heads=8, grad_clip=3.0, graph=0, hierarchical=1, kl_annealing_iter=20000, lr=0.0003, lr_gamma=0.5, lr_mini=1e-06, max_threshold=100, maxlen=50, min_threshold=0, model='HRED', nhead=4, num_decoder_layers=8, num_encoder_layers=8, patience=5, position_embed_size=30, pred='./processed/dailydialog/HRED/pure-pred.txt', resume_training_epoch=0, seed=30, src_dev='./data/dailydialog/src-dev.txt', src_test='./data/dailydialog/src-test.txt', src_train='./data/dailydialog/src-train.txt', src_vocab='./processed/dailydialog/iptvocab.pkl', teach_force=1.0, test_graph='./processed/dailydialog/test-graph.pkl', tgt_dev='./data/dailydialog/tgt-dev.txt', tgt_maxlen=30, tgt_test='./data/dailydialog/tgt-test.txt', tgt_train='./data/dailydialog/tgt-train.txt', tgt_vocab='./processed/dailydialog/optvocab.pkl', train_graph='./processed/dailydialog/train-graph.pkl', transformer_decode=0, utter_hidden=512, utter_n_layer=2, warmup_step=4000, z_hidden=100)\n",
      "[!] load vocab over, src/tgt vocab size: 17296, 17321\n",
      "[!] Net:\n",
      "HRED(\n",
      "  (utter_encoder): Utterance_encoder(\n",
      "    (embed): Embedding(17296, 256)\n",
      "    (gru): GRU(256, 512, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (context_encoder): Context_encoder(\n",
      "    (gru): GRU(512, 512, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(17321, 256)\n",
      "    (gru): GRU(768, 512, num_layers=2, dropout=0.3)\n",
      "    (out): Linear(in_features=512, out_features=17321, bias=True)\n",
      "    (attn): Attention(\n",
      "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[!] Parameters size: 32060073\n",
      "[!] Optimizer Adam\n",
      "  0%|                                                    | 0/45 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A[!] preprocessed file ./data/dailydialog/src-train-hier.pkl exist, load directly\n",
      "[!] preprocessed file ./data/dailydialog/tgt-train-hier.pkl exist, load directly\n",
      "\n",
      "batch 1, training loss: 9.7814: : 0it [00:02, ?it/s]\u001b[A\n",
      "batch 1, training loss: 9.7814: : 1it [00:02,  2.06s/it]\u001b[A\n",
      "batch 2, training loss: 9.7077: : 1it [00:02,  2.06s/it]\u001b[A\n",
      "batch 2, training loss: 9.7077: : 2it [00:02,  1.15s/it]\u001b[A\n",
      "batch 3, training loss: 9.6151: : 2it [00:03,  1.15s/it]\u001b[A\n",
      "batch 3, training loss: 9.6151: : 3it [00:03,  1.07it/s]\u001b[A\n",
      "batch 4, training loss: 9.4958: : 3it [00:03,  1.07it/s]\u001b[A\n",
      "batch 4, training loss: 9.4958: : 4it [00:03,  1.28it/s]\u001b[A\n",
      "batch 5, training loss: 9.2853: : 4it [00:04,  1.28it/s]\u001b[A\n",
      "batch 5, training loss: 9.2853: : 5it [00:04,  1.32it/s]\u001b[A\n",
      "batch 6, training loss: 9.0697: : 5it [00:05,  1.32it/s]\u001b[A\n",
      "batch 6, training loss: 9.0697: : 6it [00:05,  1.46it/s]\u001b[A\n",
      "batch 7, training loss: 8.762: : 6it [00:05,  1.46it/s] \u001b[A\n",
      "batch 7, training loss: 8.762: : 7it [00:05,  1.45it/s]\u001b[A\n",
      "batch 8, training loss: 8.4716: : 7it [00:06,  1.45it/s]\u001b[A\n",
      "batch 8, training loss: 8.4716: : 8it [00:06,  1.56it/s]\u001b[A\n",
      "batch 9, training loss: 8.0666: : 8it [00:06,  1.56it/s]\u001b[A\n",
      "batch 9, training loss: 8.0666: : 9it [00:06,  1.51it/s]\u001b[A\n",
      "batch 10, training loss: 7.7377: : 9it [00:07,  1.51it/s]\u001b[A\n",
      "batch 10, training loss: 7.7377: : 10it [00:07,  1.60it/s]\u001b[A\n",
      "batch 11, training loss: 7.4952: : 10it [00:08,  1.60it/s]\u001b[A\n",
      "batch 11, training loss: 7.4952: : 11it [00:08,  1.54it/s]\u001b[A\n",
      "batch 12, training loss: 7.3158: : 11it [00:08,  1.54it/s]\u001b[A\n",
      "batch 12, training loss: 7.3158: : 12it [00:08,  1.62it/s]\u001b[A\n",
      "batch 13, training loss: 7.0413: : 12it [00:09,  1.62it/s]\u001b[A\n",
      "batch 13, training loss: 7.0413: : 13it [00:09,  1.55it/s]\u001b[A\n",
      "batch 14, training loss: 6.896: : 13it [00:10,  1.55it/s] \u001b[A\n",
      "batch 14, training loss: 6.896: : 14it [00:10,  1.64it/s]\u001b[A\n",
      "batch 15, training loss: 6.632: : 14it [00:10,  1.64it/s]\u001b[A\n",
      "batch 15, training loss: 6.632: : 15it [00:10,  1.56it/s]\u001b[A\n",
      "batch 16, training loss: 6.4768: : 15it [00:11,  1.56it/s]\u001b[A\n",
      "batch 16, training loss: 6.4768: : 16it [00:11,  1.64it/s]\u001b[A\n",
      "batch 17, training loss: 6.4264: : 16it [00:11,  1.64it/s]\u001b[A\n",
      "batch 17, training loss: 6.4264: : 17it [00:11,  1.56it/s]\u001b[A\n",
      "batch 18, training loss: 6.1727: : 17it [00:12,  1.56it/s]\u001b[A\n",
      "batch 18, training loss: 6.1727: : 18it [00:12,  1.64it/s]\u001b[A\n",
      "batch 19, training loss: 5.9228: : 18it [00:13,  1.64it/s]\u001b[A\n",
      "batch 19, training loss: 5.9228: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 5.8473: : 19it [00:13,  1.56it/s]\u001b[A\n",
      "batch 20, training loss: 5.8473: : 20it [00:13,  1.64it/s]\u001b[A\n",
      "batch 21, training loss: 5.7938: : 20it [00:14,  1.64it/s]\u001b[A\n",
      "batch 21, training loss: 5.7938: : 21it [00:14,  1.56it/s]\u001b[A\n",
      "batch 22, training loss: 5.5228: : 21it [00:15,  1.56it/s]\u001b[A\n",
      "batch 22, training loss: 5.5228: : 22it [00:15,  1.64it/s]\u001b[A\n",
      "batch 23, training loss: 5.5886: : 22it [00:15,  1.64it/s]\u001b[A\n",
      "batch 23, training loss: 5.5886: : 23it [00:15,  1.56it/s]\u001b[A\n",
      "batch 24, training loss: 5.4585: : 23it [00:16,  1.56it/s]\u001b[A\n",
      "batch 24, training loss: 5.4585: : 24it [00:16,  1.64it/s]\u001b[A\n",
      "batch 25, training loss: 5.536: : 24it [00:17,  1.64it/s] \u001b[A\n",
      "batch 25, training loss: 5.536: : 25it [00:17,  1.56it/s]\u001b[A\n",
      "batch 26, training loss: 5.3356: : 25it [00:17,  1.56it/s]\u001b[A\n",
      "batch 26, training loss: 5.3356: : 26it [00:17,  1.64it/s]\u001b[A\n",
      "batch 27, training loss: 5.4095: : 26it [00:18,  1.64it/s]\u001b[A\n",
      "batch 27, training loss: 5.4095: : 27it [00:18,  1.56it/s]\u001b[A\n",
      "batch 28, training loss: 5.3324: : 27it [00:18,  1.56it/s]\u001b[A\n",
      "batch 28, training loss: 5.3324: : 28it [00:18,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 5.3805: : 28it [00:19,  1.64it/s]\u001b[A\n",
      "batch 29, training loss: 5.3805: : 29it [00:19,  1.57it/s]\u001b[A\n",
      "batch 30, training loss: 5.4009: : 29it [00:20,  1.57it/s]\u001b[A\n",
      "batch 30, training loss: 5.4009: : 30it [00:20,  1.65it/s]\u001b[A\n",
      "batch 31, training loss: 5.2411: : 30it [00:20,  1.65it/s]\u001b[A\n",
      "batch 31, training loss: 5.2411: : 31it [00:20,  1.57it/s]\u001b[A\n",
      "batch 32, training loss: 5.4206: : 31it [00:21,  1.57it/s]\u001b[A\n",
      "batch 32, training loss: 5.4206: : 32it [00:21,  1.64it/s]\u001b[A\n",
      "batch 33, training loss: 5.3331: : 32it [00:21,  1.64it/s]\u001b[A\n",
      "batch 33, training loss: 5.3331: : 33it [00:21,  1.56it/s]\u001b[A\n",
      "batch 34, training loss: 5.3135: : 33it [00:22,  1.56it/s]\u001b[A\n",
      "batch 34, training loss: 5.3135: : 34it [00:22,  1.64it/s]\u001b[A\n",
      "batch 35, training loss: 5.365: : 34it [00:23,  1.64it/s] \u001b[A\n",
      "batch 35, training loss: 5.365: : 35it [00:23,  1.56it/s]\u001b[A\n",
      "batch 36, training loss: 5.38: : 35it [00:23,  1.56it/s] \u001b[A\n",
      "batch 36, training loss: 5.38: : 36it [00:23,  1.64it/s]\u001b[A\n",
      "batch 37, training loss: 5.2593: : 36it [00:24,  1.64it/s]\u001b[A\n",
      "batch 37, training loss: 5.2593: : 37it [00:24,  1.57it/s]\u001b[A\n",
      "batch 38, training loss: 5.1932: : 37it [00:25,  1.57it/s]\u001b[A\n",
      "batch 38, training loss: 5.1932: : 38it [00:25,  1.66it/s]\u001b[A\n",
      "batch 39, training loss: 5.3364: : 38it [00:25,  1.66it/s]\u001b[A\n",
      "batch 39, training loss: 5.3364: : 39it [00:25,  1.57it/s]\u001b[A\n",
      "batch 40, training loss: 5.4295: : 39it [00:26,  1.57it/s]\u001b[A\n",
      "batch 40, training loss: 5.4295: : 40it [00:26,  1.64it/s]\u001b[A\n",
      "batch 41, training loss: 5.5945: : 40it [00:26,  1.64it/s]\u001b[A\n",
      "batch 41, training loss: 5.5945: : 41it [00:26,  1.56it/s]\u001b[A\n",
      "batch 42, training loss: 5.4384: : 41it [00:27,  1.56it/s]\u001b[A\n",
      "batch 42, training loss: 5.4384: : 42it [00:27,  1.64it/s]\u001b[A\n",
      "batch 43, training loss: 5.2739: : 42it [00:28,  1.64it/s]\u001b[A\n",
      "batch 43, training loss: 5.2739: : 43it [00:28,  1.56it/s]\u001b[A\n",
      "batch 44, training loss: 5.2605: : 43it [00:28,  1.56it/s]\u001b[A\n",
      "batch 44, training loss: 5.2605: : 44it [00:28,  1.64it/s]\u001b[A\n",
      "batch 45, training loss: 5.2512: : 44it [00:29,  1.64it/s]\u001b[A\n",
      "batch 45, training loss: 5.2512: : 45it [00:29,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 5.4362: : 45it [00:30,  1.58it/s]\u001b[A\n",
      "batch 46, training loss: 5.4362: : 46it [00:30,  1.64it/s]\u001b[A\n",
      "batch 47, training loss: 5.3386: : 46it [00:30,  1.64it/s]\u001b[A\n",
      "batch 47, training loss: 5.3386: : 47it [00:30,  1.57it/s]\u001b[A\n",
      "batch 48, training loss: 5.3119: : 47it [00:31,  1.57it/s]\u001b[A\n",
      "batch 48, training loss: 5.3119: : 48it [00:31,  1.65it/s]\u001b[A\n",
      "batch 49, training loss: 5.4518: : 48it [00:31,  1.65it/s]\u001b[A\n",
      "batch 49, training loss: 5.4518: : 49it [00:31,  1.57it/s]\u001b[A\n",
      "batch 50, training loss: 5.279: : 49it [00:32,  1.57it/s] \u001b[A\n",
      "batch 50, training loss: 5.279: : 50it [00:32,  1.65it/s]\u001b[A\n",
      "batch 51, training loss: 5.3624: : 50it [00:33,  1.65it/s]\u001b[A\n",
      "batch 51, training loss: 5.3624: : 51it [00:33,  1.56it/s]\u001b[A\n",
      "batch 52, training loss: 5.2568: : 51it [00:33,  1.56it/s]\u001b[A\n",
      "batch 52, training loss: 5.2568: : 52it [00:33,  1.65it/s]\u001b[A\n",
      "batch 53, training loss: 5.2879: : 52it [00:34,  1.65it/s]\u001b[A\n",
      "batch 53, training loss: 5.2879: : 53it [00:34,  1.56it/s]\u001b[A\n",
      "batch 54, training loss: 5.1635: : 53it [00:34,  1.56it/s]\u001b[A\n",
      "batch 54, training loss: 5.1635: : 54it [00:34,  1.65it/s]\u001b[A\n",
      "batch 55, training loss: 5.3357: : 54it [00:35,  1.65it/s]\u001b[A\n",
      "batch 55, training loss: 5.3357: : 55it [00:35,  1.56it/s]\u001b[A\n",
      "batch 56, training loss: 5.3149: : 55it [00:36,  1.56it/s]\u001b[A\n",
      "batch 56, training loss: 5.3149: : 56it [00:36,  1.64it/s]\u001b[A\n",
      "batch 57, training loss: 5.2812: : 56it [00:36,  1.64it/s]\u001b[A\n",
      "batch 57, training loss: 5.2812: : 57it [00:36,  1.56it/s]\u001b[A\n",
      "batch 58, training loss: 5.3535: : 57it [00:37,  1.56it/s]\u001b[A\n",
      "batch 58, training loss: 5.3535: : 58it [00:37,  1.64it/s]\u001b[A\n",
      "batch 59, training loss: 5.2147: : 58it [00:38,  1.64it/s]\u001b[A\n",
      "batch 59, training loss: 5.2147: : 59it [00:38,  1.55it/s]\u001b[A\n",
      "batch 60, training loss: 5.2474: : 59it [00:38,  1.55it/s]\u001b[A\n",
      "batch 60, training loss: 5.2474: : 60it [00:38,  1.64it/s]\u001b[A\n",
      "batch 61, training loss: 5.3468: : 60it [00:39,  1.64it/s]\u001b[A\n",
      "batch 61, training loss: 5.3468: : 61it [00:39,  1.56it/s]\u001b[A\n",
      "batch 62, training loss: 5.2067: : 61it [00:39,  1.56it/s]\u001b[A\n",
      "batch 62, training loss: 5.2067: : 62it [00:39,  1.65it/s]\u001b[A\n",
      "batch 63, training loss: 5.3322: : 62it [00:40,  1.65it/s]\u001b[A\n",
      "batch 63, training loss: 5.3322: : 63it [00:40,  1.57it/s]\u001b[A\n",
      "batch 64, training loss: 5.3117: : 63it [00:41,  1.57it/s]\u001b[A\n",
      "batch 64, training loss: 5.3117: : 64it [00:41,  1.64it/s]\u001b[A\n",
      "batch 65, training loss: 5.3059: : 64it [00:41,  1.64it/s]\u001b[A\n",
      "batch 65, training loss: 5.3059: : 65it [00:41,  1.55it/s]\u001b[A\n",
      "batch 66, training loss: 5.3398: : 65it [00:42,  1.55it/s]\u001b[A\n",
      "batch 66, training loss: 5.3398: : 66it [00:42,  1.63it/s]\u001b[A\n",
      "batch 67, training loss: 5.2407: : 66it [00:43,  1.63it/s]\u001b[A\n",
      "batch 67, training loss: 5.2407: : 67it [00:43,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 5.2983: : 67it [00:43,  1.55it/s]\u001b[A\n",
      "batch 68, training loss: 5.2983: : 68it [00:43,  1.63it/s]\u001b[A\n",
      "batch 69, training loss: 5.2103: : 68it [00:44,  1.63it/s]\u001b[A\n",
      "batch 69, training loss: 5.2103: : 69it [00:44,  1.55it/s]\u001b[A\n",
      "batch 70, training loss: 5.3268: : 69it [00:45,  1.55it/s]\u001b[A\n",
      "batch 70, training loss: 5.3268: : 70it [00:45,  1.64it/s]\u001b[A\n",
      "batch 71, training loss: 5.2037: : 70it [00:45,  1.64it/s]\u001b[A\n",
      "batch 71, training loss: 5.2037: : 71it [00:45,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 5.2381: : 71it [00:46,  1.56it/s]\u001b[A\n",
      "batch 72, training loss: 5.2381: : 72it [00:46,  1.63it/s]\u001b[A\n",
      "batch 73, training loss: 5.2901: : 72it [00:47,  1.63it/s]\u001b[A\n",
      "batch 73, training loss: 5.2901: : 73it [00:47,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 5.1964: : 73it [00:47,  1.55it/s]\u001b[A\n",
      "batch 74, training loss: 5.1964: : 74it [00:47,  1.62it/s]\u001b[A\n",
      "batch 75, training loss: 5.2834: : 74it [00:48,  1.62it/s]\u001b[A\n",
      "batch 75, training loss: 5.2834: : 75it [00:48,  1.55it/s]\u001b[A\n",
      "batch 76, training loss: 5.157: : 75it [00:48,  1.55it/s] \u001b[A\n",
      "batch 76, training loss: 5.157: : 76it [00:48,  1.63it/s]\u001b[A\n",
      "batch 77, training loss: 5.3078: : 76it [00:49,  1.63it/s]\u001b[A\n",
      "batch 77, training loss: 5.3078: : 77it [00:49,  1.56it/s]\u001b[A\n",
      "batch 78, training loss: 5.2835: : 77it [00:50,  1.56it/s]\u001b[A\n",
      "batch 78, training loss: 5.2835: : 78it [00:50,  1.64it/s]\u001b[A\n",
      "batch 79, training loss: 5.2492: : 78it [00:50,  1.64it/s]\u001b[A\n",
      "batch 79, training loss: 5.2492: : 79it [00:50,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 5.2114: : 79it [00:51,  1.56it/s]\u001b[A\n",
      "batch 80, training loss: 5.2114: : 80it [00:51,  1.64it/s]\u001b[A\n",
      "batch 81, training loss: 5.2185: : 80it [00:52,  1.64it/s]\u001b[A\n",
      "batch 81, training loss: 5.2185: : 81it [00:52,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 5.2855: : 81it [00:52,  1.56it/s]\u001b[A\n",
      "batch 82, training loss: 5.2855: : 82it [00:52,  1.62it/s]\u001b[A\n",
      "batch 83, training loss: 5.1823: : 82it [00:53,  1.62it/s]\u001b[A\n",
      "batch 83, training loss: 5.1823: : 83it [00:53,  1.54it/s]\u001b[A\n",
      "batch 84, training loss: 5.266: : 83it [00:53,  1.54it/s] \u001b[A\n",
      "batch 84, training loss: 5.266: : 84it [00:53,  1.62it/s]\u001b[A\n",
      "batch 85, training loss: 5.3287: : 84it [00:54,  1.62it/s]\u001b[A\n",
      "batch 85, training loss: 5.3287: : 85it [00:54,  1.55it/s]\u001b[A\n",
      "batch 86, training loss: 5.1917: : 85it [00:55,  1.55it/s]\u001b[A\n",
      "batch 86, training loss: 5.1917: : 86it [00:55,  1.63it/s]\u001b[A\n",
      "batch 87, training loss: 5.2776: : 86it [00:55,  1.63it/s]\u001b[A\n",
      "batch 87, training loss: 5.2776: : 87it [00:55,  1.60it/s]\u001b[A\n",
      "batch 88, training loss: 6.3434: : 87it [00:56,  1.60it/s]\u001b[A\n",
      "batch 88, training loss: 6.3434: : 88it [00:56,  1.62it/s]\u001b[A\n",
      "batch 89, training loss: 6.3529: : 88it [00:57,  1.62it/s]\u001b[A\n",
      "batch 89, training loss: 6.3529: : 89it [00:57,  1.51it/s]\u001b[A\n",
      "batch 90, training loss: 6.1982: : 89it [00:57,  1.51it/s]\u001b[A\n",
      "batch 90, training loss: 6.1982: : 90it [00:57,  1.59it/s]\u001b[A\n",
      "batch 91, training loss: 6.2743: : 90it [00:58,  1.59it/s]\u001b[A\n",
      "batch 91, training loss: 6.2743: : 91it [00:58,  1.50it/s]\u001b[A\n",
      "batch 92, training loss: 6.2313: : 91it [00:58,  1.50it/s]\u001b[A\n",
      "batch 92, training loss: 6.2313: : 92it [00:58,  1.58it/s]\u001b[A\n",
      "batch 93, training loss: 6.112: : 92it [00:59,  1.58it/s] \u001b[A\n",
      "batch 93, training loss: 6.112: : 93it [00:59,  1.50it/s]\u001b[A\n",
      "batch 94, training loss: 6.1163: : 93it [01:00,  1.50it/s]\u001b[A\n",
      "batch 94, training loss: 6.1163: : 94it [01:00,  1.58it/s]\u001b[A\n",
      "batch 95, training loss: 6.09: : 94it [01:01,  1.58it/s]  \u001b[A\n",
      "batch 95, training loss: 6.09: : 95it [01:01,  1.49it/s]\u001b[A\n",
      "batch 96, training loss: 5.9796: : 95it [01:01,  1.49it/s]\u001b[A\n",
      "batch 96, training loss: 5.9796: : 96it [01:01,  1.58it/s]\u001b[A\n",
      "batch 97, training loss: 5.945: : 96it [01:02,  1.58it/s] \u001b[A\n",
      "batch 97, training loss: 5.945: : 97it [01:02,  1.49it/s]\u001b[A\n",
      "batch 98, training loss: 5.8678: : 97it [01:02,  1.49it/s]\u001b[A\n",
      "batch 98, training loss: 5.8678: : 98it [01:02,  1.58it/s]\u001b[A\n",
      "batch 99, training loss: 5.722: : 98it [01:03,  1.58it/s] \u001b[A\n",
      "batch 99, training loss: 5.722: : 99it [01:03,  1.49it/s]\u001b[A\n",
      "batch 100, training loss: 5.5577: : 99it [01:04,  1.49it/s]\u001b[A\n",
      "batch 100, training loss: 5.5577: : 100it [01:04,  1.58it/s]\u001b[A\n",
      "batch 101, training loss: 5.6656: : 100it [01:04,  1.58it/s]\u001b[A\n",
      "batch 101, training loss: 5.6656: : 101it [01:04,  1.51it/s]\u001b[A\n",
      "batch 102, training loss: 5.5773: : 101it [01:05,  1.51it/s]\u001b[A\n",
      "batch 102, training loss: 5.5773: : 102it [01:05,  1.58it/s]\u001b[A\n",
      "batch 103, training loss: 5.533: : 102it [01:06,  1.58it/s] \u001b[A\n",
      "batch 103, training loss: 5.533: : 103it [01:06,  1.49it/s]\u001b[A\n",
      "batch 104, training loss: 5.4454: : 103it [01:06,  1.49it/s]\u001b[A\n",
      "batch 104, training loss: 5.4454: : 104it [01:06,  1.58it/s]\u001b[A\n",
      "batch 105, training loss: 5.5042: : 104it [01:07,  1.58it/s]\u001b[A\n",
      "batch 105, training loss: 5.5042: : 105it [01:07,  1.50it/s]\u001b[A\n",
      "batch 106, training loss: 5.5633: : 105it [01:08,  1.50it/s]\u001b[A\n",
      "batch 106, training loss: 5.5633: : 106it [01:08,  1.58it/s]\u001b[A\n",
      "batch 107, training loss: 5.3111: : 106it [01:08,  1.58it/s]\u001b[A\n",
      "batch 107, training loss: 5.3111: : 107it [01:08,  1.49it/s]\u001b[A\n",
      "batch 108, training loss: 5.4468: : 107it [01:09,  1.49it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 108, training loss: 5.4468: : 108it [01:09,  1.58it/s]\u001b[A\n",
      "batch 109, training loss: 5.472: : 108it [01:10,  1.58it/s] \u001b[A\n",
      "batch 109, training loss: 5.472: : 109it [01:10,  1.50it/s]\u001b[A\n",
      "batch 110, training loss: 5.6326: : 109it [01:10,  1.50it/s]\u001b[A\n",
      "batch 110, training loss: 5.6326: : 110it [01:10,  1.58it/s]\u001b[A\n",
      "batch 111, training loss: 5.3356: : 110it [01:11,  1.58it/s]\u001b[A\n",
      "batch 111, training loss: 5.3356: : 111it [01:11,  1.49it/s]\u001b[A\n",
      "batch 112, training loss: 5.3633: : 111it [01:11,  1.49it/s]\u001b[A\n",
      "batch 112, training loss: 5.3633: : 112it [01:11,  1.58it/s]\u001b[A\n",
      "batch 113, training loss: 5.3354: : 112it [01:12,  1.58it/s]\u001b[A\n",
      "batch 113, training loss: 5.3354: : 113it [01:12,  1.49it/s]\u001b[A\n",
      "batch 114, training loss: 5.405: : 113it [01:13,  1.49it/s] \u001b[A\n",
      "batch 114, training loss: 5.405: : 114it [01:13,  1.58it/s]\u001b[A\n",
      "batch 115, training loss: 5.2962: : 114it [01:14,  1.58it/s]\u001b[A\n",
      "batch 115, training loss: 5.2962: : 115it [01:14,  1.49it/s]\u001b[A\n",
      "batch 116, training loss: 5.2652: : 115it [01:14,  1.49it/s]\u001b[A\n",
      "batch 116, training loss: 5.2652: : 116it [01:14,  1.57it/s]\u001b[A\n",
      "batch 117, training loss: 5.26: : 116it [01:15,  1.57it/s]  \u001b[A\n",
      "batch 117, training loss: 5.26: : 117it [01:15,  1.49it/s]\u001b[A\n",
      "batch 118, training loss: 5.3528: : 117it [01:15,  1.49it/s]\u001b[A\n",
      "batch 118, training loss: 5.3528: : 118it [01:15,  1.58it/s]\u001b[A\n",
      "batch 119, training loss: 5.2341: : 118it [01:16,  1.58it/s]\u001b[A\n",
      "batch 119, training loss: 5.2341: : 119it [01:16,  1.49it/s]\u001b[A\n",
      "batch 120, training loss: 5.1408: : 119it [01:17,  1.49it/s]\u001b[A\n",
      "batch 120, training loss: 5.1408: : 120it [01:17,  1.58it/s]\u001b[A\n",
      "batch 121, training loss: 5.3116: : 120it [01:17,  1.58it/s]\u001b[A\n",
      "batch 121, training loss: 5.3116: : 121it [01:17,  1.54it/s]\u001b[A\n",
      "batch 122, training loss: 5.126: : 121it [01:18,  1.54it/s] \u001b[A\n",
      "batch 122, training loss: 5.126: : 122it [01:18,  1.64it/s]\u001b[A\n",
      "batch 123, training loss: 5.2249: : 122it [01:18,  1.64it/s]\u001b[A\n",
      "batch 123, training loss: 5.2249: : 123it [01:18,  1.77it/s]\u001b[A\n",
      "batch 124, training loss: 5.2101: : 123it [01:19,  1.77it/s]\u001b[A\n",
      "batch 124, training loss: 5.2101: : 124it [01:19,  1.60it/s]\u001b[A\n",
      "batch 125, training loss: 5.2461: : 124it [01:20,  1.60it/s]\u001b[A\n",
      "batch 125, training loss: 5.2461: : 125it [01:20,  1.66it/s]\u001b[A\n",
      "batch 126, training loss: 5.3299: : 125it [01:20,  1.66it/s]\u001b[A\n",
      "batch 126, training loss: 5.3299: : 126it [01:20,  1.53it/s]\u001b[A\n",
      "batch 127, training loss: 5.0744: : 126it [01:21,  1.53it/s]\u001b[A\n",
      "batch 127, training loss: 5.0744: : 127it [01:21,  1.61it/s]\u001b[A\n",
      "batch 128, training loss: 5.2011: : 127it [01:22,  1.61it/s]\u001b[A\n",
      "batch 128, training loss: 5.2011: : 128it [01:22,  1.51it/s]\u001b[A\n",
      "batch 129, training loss: 5.2207: : 128it [01:22,  1.51it/s]\u001b[A\n",
      "batch 129, training loss: 5.2207: : 129it [01:22,  1.58it/s]\u001b[A\n",
      "batch 130, training loss: 5.2612: : 129it [01:23,  1.58it/s]\u001b[A\n",
      "batch 130, training loss: 5.2612: : 130it [01:23,  1.49it/s]\u001b[A\n",
      "batch 131, training loss: 5.3226: : 130it [01:24,  1.49it/s]\u001b[A\n",
      "batch 131, training loss: 5.3226: : 131it [01:24,  1.58it/s]\u001b[A\n",
      "batch 132, training loss: 5.1739: : 131it [01:24,  1.58it/s]\u001b[A\n",
      "batch 132, training loss: 5.1739: : 132it [01:24,  1.49it/s]\u001b[A\n",
      "batch 133, training loss: 5.0457: : 132it [01:25,  1.49it/s]\u001b[A\n",
      "batch 133, training loss: 5.0457: : 133it [01:25,  1.57it/s]\u001b[A\n",
      "batch 134, training loss: 5.1766: : 133it [01:26,  1.57it/s]\u001b[A\n",
      "batch 134, training loss: 5.1766: : 134it [01:26,  1.48it/s]\u001b[A\n",
      "batch 135, training loss: 5.1769: : 134it [01:26,  1.48it/s]\u001b[A\n",
      "batch 135, training loss: 5.1769: : 135it [01:26,  1.57it/s]\u001b[A\n",
      "batch 136, training loss: 5.086: : 135it [01:27,  1.57it/s] \u001b[A\n",
      "batch 136, training loss: 5.086: : 136it [01:27,  1.48it/s]\u001b[A\n",
      "batch 137, training loss: 5.2431: : 136it [01:28,  1.48it/s]\u001b[A\n",
      "batch 137, training loss: 5.2431: : 137it [01:28,  1.57it/s]\u001b[A\n",
      "batch 138, training loss: 5.2099: : 137it [01:28,  1.57it/s]\u001b[A\n",
      "batch 138, training loss: 5.2099: : 138it [01:28,  1.48it/s]\u001b[A^C\n",
      "batch 138, training loss: 5.2099: : 138it [01:28,  1.55it/s]\n",
      "  0%|                                                    | 0/45 [01:28<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 797, in <module>\n",
      "    main(**args_dict)\n",
      "  File \"train.py\", line 633, in main\n",
      "    kl_annealing_iter=kwargs['kl_annealing_iter'])\n",
      "  File \"train.py\", line 76, in train\n",
      "    output = net(sbatch, tbatch, turn_lengths)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/c/chengyu/VisDial/MultiTurnDialogZoo/model/HRED.py\", line 211, in forward\n",
      "    context_output, hidden = self.context_encoder(turns)\n",
      "  File \"/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/c/chengyu/VisDial/MultiTurnDialogZoo/model/HRED.py\", line 104, in forward\n",
      "    hidden = hidden.cuda()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!./run.sh train dailydialog HRED 0 45 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to 'nohup.out'\r\n"
     ]
    }
   ],
   "source": [
    "!nohup ./run.sh train dailydialog HRED 0 45 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6ceb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== eval begin ==========\n",
      "/home/c/chengyu/anaconda3/envs/dialogzoo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "100%|█████████████████████████████████████| 6740/6740 [00:01<00:00, 4394.77it/s]\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Model HRED Result\n",
      "BLEU-1: 0.4382\n",
      "BLEU-2: 0.2738\n",
      "BLEU-3: 0.176\n",
      "BLEU-4: 0.1093\n",
      "ROUGE: 0.0627\n",
      "Distinct-1: 0.033; Distinct-2: 0.157\n",
      "Ref distinct-1: 0.0588; Ref distinct-2: 0.3619\n",
      "BERTScore: 0.1552\n",
      "========== eval done ==========\n"
     ]
    }
   ],
   "source": [
    "!./run.sh eval dailydialog HRED 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ec3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
